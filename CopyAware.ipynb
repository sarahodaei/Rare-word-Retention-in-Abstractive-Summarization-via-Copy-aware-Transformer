{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soTROQsf4lft"
      },
      "source": [
        "# Loading The CNN/DailyMail Dataset\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860,
          "referenced_widgets": [
            "8079007e915746fdb2c10612d6bad911",
            "f89fa66a2070439394d4ee85f2d17417",
            "fa7ba1e4bcb94b1f876979a342a1566d",
            "dad7e891bb864cea8f0a17ef2568904b",
            "19cc75710ba34031a183fde036c358d2",
            "268cb6686c5d4765a77a5d951f1059a2",
            "3a3b5b6e3d2f497fa549e9cc2e22b2fa",
            "667f7a8571fa4224bd7fbaa12f6456c2",
            "3c02cf3a82a148e483acfc4b0ca0bd73"
          ]
        },
        "id": "hFYb1fWQzK4r",
        "outputId": "867909ae-39ad-49b8-91d4-16ce81c5a5bc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8079007e915746fdb2c10612d6bad911",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f89fa66a2070439394d4ee85f2d17417",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa7ba1e4bcb94b1f876979a342a1566d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00001-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dad7e891bb864cea8f0a17ef2568904b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00002-of-00003.parquet:   0%|          | 0.00/259M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19cc75710ba34031a183fde036c358d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation-00000-of-00001.parquet:   0%|          | 0.00/34.7M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "268cb6686c5d4765a77a5d951f1059a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "test-00000-of-00001.parquet:   0%|          | 0.00/30.0M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a3b5b6e3d2f497fa549e9cc2e22b2fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "667f7a8571fa4224bd7fbaa12f6456c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c02cf3a82a148e483acfc4b0ca0bd73",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['article', 'highlights', 'id'],\n",
            "        num_rows: 287113\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['article', 'highlights', 'id'],\n",
            "        num_rows: 13368\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['article', 'highlights', 'id'],\n",
            "        num_rows: 11490\n",
            "    })\n",
            "})\n",
            "\n",
            "--- Sample Article ---\n",
            "\n",
            "LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don't think I'll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he'll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I'll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe's earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say 'kid star goes off the rails,'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter's latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer's \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he's legally an adult: \"I just think I'm going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.\n",
            "\n",
            "--- Sample Summary ---\n",
            "\n",
            "Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\n",
            "Young actor says he has no plans to fritter his cash away .\n",
            "Radcliffe's earnings from first five Potter films have been held in trust fund .\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets --quiet\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load CNN/DailyMail non-anonymized (v3.0.0)\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "\n",
        "\n",
        "print(dataset)\n",
        "\n",
        "print(\"\\n--- Sample Article ---\\n\")\n",
        "print(dataset[\"train\"][0][\"article\"])\n",
        "\n",
        "print(\"\\n--- Sample Summary ---\\n\")\n",
        "print(dataset[\"train\"][0][\"highlights\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0Vobt2rAUdX",
        "outputId": "a614779c-70ad-4435-db78-5036e96d8a95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 287113\n",
            "Validation size: 13368\n",
            "Test size: 11490\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "\n",
        "\n",
        "import random\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "# Convert splits to DataFrame for easier handling\n",
        "df_train = pd.DataFrame(dataset[\"train\"])\n",
        "df_val = pd.DataFrame(dataset[\"validation\"])\n",
        "df_test = pd.DataFrame(dataset[\"test\"])\n",
        "\n",
        "# === 1. Dataset sizes ===\n",
        "print(f\"Train size: {len(df_train)}\")\n",
        "print(f\"Validation size: {len(df_val)}\")\n",
        "print(f\"Test size: {len(df_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLQRpRsc44KM"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LtGt6yaxSCS",
        "outputId": "158745d0-2cd1-48fd-80dd-b8ddc3d895c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Train Set ===\n",
            "Total rows: 287113\n",
            "NaN values in 'article': 0\n",
            "Empty/placeholder values in 'article': 0\n",
            "Rows with < 5 tokens in 'article': 0\n",
            "--------------------------------------------------\n",
            "=== Validation Set ===\n",
            "Total rows: 13368\n",
            "NaN values in 'article': 0\n",
            "Empty/placeholder values in 'article': 0\n",
            "Rows with < 5 tokens in 'article': 0\n",
            "--------------------------------------------------\n",
            "=== Test Set ===\n",
            "Total rows: 11490\n",
            "NaN values in 'article': 0\n",
            "Empty/placeholder values in 'article': 0\n",
            "Rows with < 5 tokens in 'article': 0\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# === Check for empty placeholders in 'article' ===\n",
        "def check_empty_and_short(df, text_col=\"article\", min_tokens=5):\n",
        "    # Count true NaN\n",
        "    nan_count = df[text_col].isna().sum()\n",
        "\n",
        "    # Count \"empty\" placeholders: empty string, whitespace-only, or literal placeholders\n",
        "    empty_placeholder_count = df[text_col].astype(str).str.strip().isin(\n",
        "        [\"\", \"nan\", \"NaN\", \"None\"]\n",
        "    ).sum()\n",
        "\n",
        "    # Count rows with less than min_tokens tokens\n",
        "    short_rows_count = df[text_col].astype(str).apply(lambda x: len(x.strip().split())).lt(min_tokens).sum()\n",
        "\n",
        "    print(f\"Total rows: {len(df)}\")\n",
        "    print(f\"NaN values in '{text_col}': {nan_count}\")\n",
        "    print(f\"Empty/placeholder values in '{text_col}': {empty_placeholder_count}\")\n",
        "    print(f\"Rows with < {min_tokens} tokens in '{text_col}': {short_rows_count}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\"=== Train Set ===\")\n",
        "check_empty_and_short(df_train)\n",
        "\n",
        "print(\"=== Validation Set ===\")\n",
        "check_empty_and_short(df_val)\n",
        "\n",
        "print(\"=== Test Set ===\")\n",
        "check_empty_and_short(df_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJUnQBz1dnMu",
        "outputId": "f8de1d51-3704-44d4-cba2-7cc6b955d517"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Empty strings in df_train['article']: 0\n",
            "Empty strings in df_train['highlights']: 0\n",
            "Empty strings in df_val['article']: 0\n",
            "Empty strings in df_val['highlights']: 0\n",
            "Empty strings in df_test['article']: 0\n",
            "Empty strings in df_test['highlights']: 0\n"
          ]
        }
      ],
      "source": [
        "# Function to count empty strings in a DataFrame column\n",
        "def count_empty_strings(df, column):\n",
        "    return (df[column] == '').sum()\n",
        "\n",
        "# Count empty strings in each dataframe and column\n",
        "empty_train_articles = count_empty_strings(df_train, 'article')\n",
        "empty_train_highlights = count_empty_strings(df_train, 'highlights')\n",
        "empty_val_articles = count_empty_strings(df_val, 'article')\n",
        "empty_val_highlights = count_empty_strings(df_val, 'highlights')\n",
        "empty_test_articles = count_empty_strings(df_test, 'article')\n",
        "empty_test_highlights = count_empty_strings(df_test, 'highlights')\n",
        "\n",
        "print(f\"Empty strings in df_train['article']: {empty_train_articles}\")\n",
        "print(f\"Empty strings in df_train['highlights']: {empty_train_highlights}\")\n",
        "print(f\"Empty strings in df_val['article']: {empty_val_articles}\")\n",
        "print(f\"Empty strings in df_val['highlights']: {empty_val_highlights}\")\n",
        "print(f\"Empty strings in df_test['article']: {empty_test_articles}\")\n",
        "print(f\"Empty strings in df_test['highlights']: {empty_test_highlights}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyBnbdWjSAol",
        "outputId": "2d755f57-22e4-424a-d478-b8afb502fa6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== [train] 'article' newline/paragraph audit ===\n",
            "Rows: 287,113\n",
            "- Rows with ANY \\n:                  90  ( 0.03%)\n",
            "- Rows with paragraph breaks (\\n\\n): 1  ( 0.00%)\n",
            "- Rows with <PAR> token:              0  ( 0.00%)\n",
            "\n",
            "Token totals across all rows:\n",
            "- Total \\n chars:           246\n",
            "- Single-newline matches:   244\n",
            "- Paragraph-break matches:  1\n",
            "- <PAR> token matches:      0\n",
            "\n",
            "Examples with paragraph breaks (\\n\\n):\n",
            "- [row 53362] (Health.com) -- New FDA rules are helping ensure that we get the protection we pay for, and with skin cancer on the rise (one in five Americans will develop skin cancer in the course of a lifetime), these changes can't come soon enough. Key fixes: Sunscreens will be labeled \"water resistant\" (as opposed to waterproof or sweatproof); they can no longer be called \"sunblocks\" (as it overstates their …\n",
            "\n",
            "Examples with <PAR> tokens:\n",
            "\n",
            "Examples with single newlines only (no \\n\\n, no <PAR>):\n",
            "- [row 45840] (CNN) -- Some people think the only cure for the doldrums of winter is sunshine. Far from it. Why not up the ante? Don't suffer snow and ice at home when you can really do winter vacations and ice holidays in style. From snow villages to pyrotechnic-laced skiing theater to ice hotels built from blocks of the cold stuff, we've created a roundup of the best frozen getaways around the world. With vac…\n",
            "- [row 26841] (CNN) -- Anyone can turn on a TV, scan their newsfeeds or fire up a laptop to get a sense of the important topics being discussed at the Democratic National Convention. This article is not about those important topics. This is for those other moments -- the ones that make us SMH or LOL or unfollow certain people on Twitter. These are the moments that prompt us to show our phones to the person next…\n",
            "- [row 35617] (CNN) -- Brian Steel was taught from birth that he was \"handicapped.\" Singled out in school by policies and his peers, he grew up feeling unfairly judged because of the way his body worked. Steel was diagnosed with congenital fiber-type disproportion when he was 4 months old. People with this rare condition, also called short fiber syndrome, typically experience muscle weakness, particularly in th…\n",
            "- [row 58910] (Parenting.com) -- Revising its policy on circumcision for the first time in 13 years, the American Academy of Pediatrics now says that the preventative health benefits of infant circumcision clearly outweigh the risks. The AAP is also emphasizing that the procedure should be covered by third party payers, including Medicaid, so more families have access to it. However, the organization stopped sh…\n",
            "- [row 57105] (T+L) -- Can just traveling to a city make you better looking? Liz Eckert would say so. On a recent visit to southern California, the Connecticut real estate agent was struck by the gorgeous scenery—as in, the people. She chalked it up to the locals' regular access to sand, surf, and sunshine. \"After one week there, I actually looked and felt healthier,\" she says. \"My skin was glowing, my hair was…\n",
            "\n",
            "=== [validation] 'article' newline/paragraph audit ===\n",
            "Rows: 13,368\n",
            "- Rows with ANY \\n:                  0  ( 0.00%)\n",
            "- Rows with paragraph breaks (\\n\\n): 0  ( 0.00%)\n",
            "- Rows with <PAR> token:              0  ( 0.00%)\n",
            "\n",
            "Token totals across all rows:\n",
            "- Total \\n chars:           0\n",
            "- Single-newline matches:   0\n",
            "- Paragraph-break matches:  0\n",
            "- <PAR> token matches:      0\n",
            "\n",
            "Examples with paragraph breaks (\\n\\n):\n",
            "\n",
            "Examples with <PAR> tokens:\n",
            "\n",
            "Examples with single newlines only (no \\n\\n, no <PAR>):\n",
            "\n",
            "=== [test] 'article' newline/paragraph audit ===\n",
            "Rows: 11,490\n",
            "- Rows with ANY \\n:                  0  ( 0.00%)\n",
            "- Rows with paragraph breaks (\\n\\n): 0  ( 0.00%)\n",
            "- Rows with <PAR> token:              0  ( 0.00%)\n",
            "\n",
            "Token totals across all rows:\n",
            "- Total \\n chars:           0\n",
            "- Single-newline matches:   0\n",
            "- Paragraph-break matches:  0\n",
            "- <PAR> token matches:      0\n",
            "\n",
            "Examples with paragraph breaks (\\n\\n):\n",
            "\n",
            "Examples with <PAR> tokens:\n",
            "\n",
            "Examples with single newlines only (no \\n\\n, no <PAR>):\n"
          ]
        }
      ],
      "source": [
        "# === Newline / Paragraph / <PAR> audit for CNN/DailyMail ===\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "# --- Config ---\n",
        "TEXT_COL = \"article\"\n",
        "SHOW_N_EXAMPLES = 5\n",
        "DO_PLOTS = False\n",
        "\n",
        "\n",
        "SINGLE_NL_RE   = re.compile(r'(?<!\\n)\\n(?!\\n)')  # a single \\n not adjacent to another\n",
        "PARA_BREAK_RE  = re.compile(r'\\n{2,}')           # 2+ consecutive \\n = paragraph break\n",
        "PAR_TOKEN_RE   = re.compile(r'<\\s*par\\s*>', re.IGNORECASE)  # <PAR> token, case-insensitive\n",
        "\n",
        "def count_patterns(text: str):\n",
        "    if text is None:\n",
        "        return 0, 0, 0, 0\n",
        "    t = str(text).replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
        "    total_nl = len(re.findall(r'\\n', t))\n",
        "    n_single  = len(SINGLE_NL_RE.findall(t))\n",
        "    n_par     = len(PARA_BREAK_RE.findall(t))\n",
        "    n_par_tok = len(PAR_TOKEN_RE.findall(t))\n",
        "    return total_nl, n_single, n_par, n_par_tok\n",
        "\n",
        "def audit_split(df: pd.DataFrame, name: str):\n",
        "    if TEXT_COL not in df.columns:\n",
        "        raise ValueError(f\"[{name}] Column '{TEXT_COL}' not found. Available: {list(df.columns)}\")\n",
        "    s = df[TEXT_COL].astype(str)\n",
        "\n",
        "    counts = s.map(count_patterns)\n",
        "    # unpack into columns\n",
        "    cols = [\"total_newlines\", \"single_newlines\", \"paragraph_breaks\", \"par_tokens\"]\n",
        "    cdf = pd.DataFrame(counts.tolist(), columns=cols)\n",
        "\n",
        "    # summary\n",
        "    total = len(cdf)\n",
        "    any_nl_rows   = int((cdf[\"total_newlines\"] > 0).sum())\n",
        "    any_para_rows = int((cdf[\"paragraph_breaks\"] > 0).sum())\n",
        "    any_par_tok   = int((cdf[\"par_tokens\"] > 0).sum())\n",
        "\n",
        "    print(f\"\\n=== [{name}] '{TEXT_COL}' newline/paragraph audit ===\")\n",
        "    print(f\"Rows: {total:,}\")\n",
        "    print(f\"- Rows with ANY \\\\n:                  {any_nl_rows:,}  ({any_nl_rows/total:6.2%})\")\n",
        "    print(f\"- Rows with paragraph breaks (\\\\n\\\\n): {any_para_rows:,}  ({any_para_rows/total:6.2%})\")\n",
        "    print(f\"- Rows with <PAR> token:              {any_par_tok:,}  ({any_par_tok/total:6.2%})\")\n",
        "\n",
        "    # totals across rows\n",
        "    print(\"\\nToken totals across all rows:\")\n",
        "    print(f\"- Total \\\\n chars:           {int(cdf['total_newlines'].sum()):,}\")\n",
        "    print(f\"- Single-newline matches:   {int(cdf['single_newlines'].sum()):,}\")\n",
        "    print(f\"- Paragraph-break matches:  {int(cdf['paragraph_breaks'].sum()):,}\")\n",
        "    print(f\"- <PAR> token matches:      {int(cdf['par_tokens'].sum()):,}\")\n",
        "\n",
        "    # examples: rows containing paragraph breaks OR <PAR>, else single \\n\n",
        "    def visualize(text):\n",
        "        t = str(text).replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
        "        # make breaks visible\n",
        "        t = t.replace(\"\\n\\n\", \" ¶¶ \").replace(\"\\n\", \" ⏎ \")\n",
        "        # compress spaces for display\n",
        "        t = re.sub(r\"\\s+\", \" \", t)\n",
        "        return t[:400] + (\"…\" if len(t) > 400 else \"\")\n",
        "\n",
        "    print(\"\\nExamples with paragraph breaks (\\\\n\\\\n):\")\n",
        "    idx_para = cdf.index[cdf[\"paragraph_breaks\"] > 0].tolist()\n",
        "    for i in random.sample(idx_para, min(SHOW_N_EXAMPLES, len(idx_para))):\n",
        "        print(f\"- [row {i}] {visualize(df.at[i, TEXT_COL])}\")\n",
        "\n",
        "    print(\"\\nExamples with <PAR> tokens:\")\n",
        "    idx_par_tok = cdf.index[cdf[\"par_tokens\"] > 0].tolist()\n",
        "    for i in random.sample(idx_par_tok, min(SHOW_N_EXAMPLES, len(idx_par_tok))):\n",
        "        print(f\"- [row {i}] {visualize(df.at[i, TEXT_COL])}\")\n",
        "\n",
        "    print(\"\\nExamples with single newlines only (no \\\\n\\\\n, no <PAR>):\")\n",
        "    mask_single_only = (cdf[\"single_newlines\"] > 0) & (cdf[\"paragraph_breaks\"] == 0) & (cdf[\"par_tokens\"] == 0)\n",
        "    idx_single = cdf.index[mask_single_only].tolist()\n",
        "    for i in random.sample(idx_single, min(SHOW_N_EXAMPLES, len(idx_single))):\n",
        "        print(f\"- [row {i}] {visualize(df.at[i, TEXT_COL])}\")\n",
        "\n",
        "\n",
        "    if DO_PLOTS:\n",
        "        fig1 = plt.figure()\n",
        "        plt.hist(cdf[\"paragraph_breaks\"], bins=30)\n",
        "        plt.title(f\"{name}: Paragraph breaks per row\")\n",
        "        plt.xlabel(\"# paragraph breaks (>=2 newlines)\")\n",
        "        plt.ylabel(\"Rows\")\n",
        "        plt.show()\n",
        "\n",
        "        fig2 = plt.figure()\n",
        "        plt.hist(cdf[\"single_newlines\"], bins=30)\n",
        "        plt.title(f\"{name}: Single newlines per row\")\n",
        "        plt.xlabel(\"# single newlines (isolated)\")\n",
        "        plt.ylabel(\"Rows\")\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    return cdf\n",
        "\n",
        "train_stats = audit_split(df_train, \"train\")\n",
        "val_stats   = audit_split(df_val,   \"validation\")\n",
        "test_stats  = audit_split(df_test,  \"test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1281
        },
        "id": "pPiLkuyk4CHk",
        "outputId": "2f8b8467-a961-42d1-f2f3-c1c28cdec92a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 287113\n",
            "Validation size: 13368\n",
            "Test size: 11490\n",
            "\n",
            "--- Article Length Stats ---\n",
            "{'mean': np.float64(691.8703263175126), 'median': np.float64(632.0), '95th': np.float64(1363.0), 'max': np.int64(2347)}\n",
            "\n",
            "--- Summary Length Stats ---\n",
            "{'mean': np.float64(51.574101486174435), 'median': np.float64(48.0), '95th': np.float64(90.0), 'max': np.int64(1296)}\n",
            "\n",
            "Exact duplicates within train: 3098\n",
            "\n",
            "Exact duplicates within validation: 0\n",
            "\n",
            "Exact duplicates within test: 2\n",
            "Train–Val overlap: 1\n",
            "Train–Test overlap: 0\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# === 2. Article & summary length stats (in tokens, whitespace split) ===\n",
        "def length_stats(texts):\n",
        "    token_counts = [len(t.split()) for t in texts]\n",
        "    return {\n",
        "        \"mean\": np.mean(token_counts),\n",
        "        \"median\": np.median(token_counts),\n",
        "        \"95th\": np.percentile(token_counts, 95),\n",
        "        \"max\": np.max(token_counts)\n",
        "    }\n",
        "\n",
        "print(\"\\n--- Article Length Stats ---\")\n",
        "print(length_stats(df_train[\"article\"]))\n",
        "\n",
        "print(\"\\n--- Summary Length Stats ---\")\n",
        "print(length_stats(df_train[\"highlights\"]))\n",
        "\n",
        "# === 3. Duplicate checks ===\n",
        "# Exact duplicates within train\n",
        "dup_within_train = df_train.duplicated(subset=[\"article\", \"highlights\"]).sum()\n",
        "print(f\"\\nExact duplicates within train: {dup_within_train}\")\n",
        "\n",
        "# Exact duplicates within val\n",
        "dup_within_val = df_val.duplicated(subset=[\"article\", \"highlights\"]).sum()\n",
        "print(f\"\\nExact duplicates within validation: {dup_within_val}\")\n",
        "\n",
        "# Exact duplicates within test\n",
        "dup_within_test = df_test.duplicated(subset=[\"article\", \"highlights\"]).sum()\n",
        "print(f\"\\nExact duplicates within test: {dup_within_test}\")\n",
        "\n",
        "# Across splits (hash-based to avoid slow string compare, No data leakage)\n",
        "train_hashes = set(df_train[\"article\"].apply(lambda x: hash(x)))\n",
        "val_overlap = sum(df_val[\"article\"].apply(lambda x: hash(x) in train_hashes))\n",
        "test_overlap = sum(df_test[\"article\"].apply(lambda x: hash(x) in train_hashes))\n",
        "print(f\"Train–Val overlap: {val_overlap}\")\n",
        "print(f\"Train–Test overlap: {test_overlap}\")\n",
        "\n",
        "# === 4. Length distribution plots ===\n",
        "plt.figure(figsize=(12,5))\n",
        "sns.histplot([len(t.split()) for t in df_train[\"article\"]], bins=50, kde=False)\n",
        "plt.title(\"Train Article Length Distribution (tokens)\")\n",
        "plt.xlabel(\"Tokens\"); plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "sns.histplot([len(t.split()) for t in df_train[\"highlights\"]], bins=50, kde=False, color=\"orange\")\n",
        "plt.title(\"Train Summary Length Distribution (tokens)\")\n",
        "plt.xlabel(\"Tokens\"); plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niDiOTYO7LJW",
        "outputId": "605102ce-5012-45e2-b6d9-cf9bbabbfb6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Sentence Counts (Articles) ---\n",
            "{'mean': 42.40437388763309, 'median': 36, '95th': 91, 'max': 419}\n",
            "\n",
            "--- Sentence Counts (Summaries) ---\n",
            "{'mean': 3.9208186323851586, 'median': 4, '95th': 7, 'max': 111}\n",
            "\n",
            "Vocabulary size (train articles): 2057411\n",
            "Top 20 tokens: [('the', 9791515), ('to', 5332131), ('of', 4554342), ('a', 4487198), ('and', 4480885), ('in', 3825318), ('.', 3071328), ('was', 1884632), ('for', 1779885), ('that', 1717024), ('on', 1625620), ('is', 1615285), ('with', 1355927), ('The', 1340704), ('his', 1152032), ('he', 1115250), ('at', 1102048), ('as', 1084391), ('have', 935511), ('from', 919740)]\n",
            "\n",
            "OOV rate (val articles vs train vocab): 0.0082\n",
            "OOV rate (test articles vs train vocab): 0.0089\n",
            "\n",
            "--- Most common characters in train articles ---\n",
            "[(' ', 198374876), ('e', 110213170), ('t', 76541405), ('a', 75935698), ('o', 65760642), ('i', 63934679), ('n', 62800248), ('s', 57745492), ('r', 56997274), ('h', 44878880), ('l', 36671732), ('d', 36100106), ('c', 25684605), ('u', 23342033), ('m', 20341464), ('f', 18712908), ('g', 18466318), ('p', 17070839), ('w', 16211782), ('y', 15509287)]\n",
            "\n",
            "Approx. number of unique non-ASCII tokens in train: 1733591\n",
            "Sample non-ASCII tokens: ['£20', '»', 'José', 'Brasileirão,but', 'Leão.', 'Atlético', 'Lanús,', 'Leão', '£1', '\"Für', '•', '2½', 'Renée', 'Lancôme', '©', 'Ingénue:', 'fiancé,', 'fiancé', 'SPAM®', 'Mütter']\n",
            "\n",
            "Punctuation-only token ratio in train: 0.0195\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import gc\n",
        "import math\n",
        "import numpy as np\n",
        "import string\n",
        "from collections import Counter, defaultdict\n",
        "from itertools import islice\n",
        "\n",
        "# ---------------------------\n",
        "# Helpers: iterate in small batches to reduce RAM\n",
        "# ---------------------------\n",
        "def batched_iter(texts, batch_size=2048):\n",
        "    it = iter(texts)\n",
        "    while True:\n",
        "        chunk = list(islice(it, batch_size))\n",
        "        if not chunk:\n",
        "            break\n",
        "        yield chunk\n",
        "\n",
        "# ---------------------------\n",
        "# 5) Sentence-level stats (streaming; no big lists)\n",
        "#    Uses a compact histogram of sentence-counts to get exact median/95th.\n",
        "# ---------------------------\n",
        "_SENT_SPLIT_RE = re.compile(r'[.!?]+')\n",
        "\n",
        "def sentence_stats_stream(texts, batch_size=2048):\n",
        "    count_hist = Counter()   # key: sentence_count, val: frequency\n",
        "    total_docs = 0\n",
        "    total_sent = 0\n",
        "    max_sent = 0\n",
        "\n",
        "    for chunk in batched_iter(texts, batch_size):\n",
        "        for t in chunk:\n",
        "            # Count sentences without materializing splits for entire corpus\n",
        "            # Note: split produces empty tail sometimes; guard for that.\n",
        "            parts = _SENT_SPLIT_RE.split(t)\n",
        "            s_count = sum(1 for p in parts if p.strip())\n",
        "            total_docs += 1\n",
        "            total_sent += s_count\n",
        "            max_sent = max(max_sent, s_count)\n",
        "            count_hist[s_count] += 1\n",
        "        del chunk\n",
        "        gc.collect()\n",
        "\n",
        "    if total_docs == 0:\n",
        "        return {\"mean\": 0, \"median\": 0, \"95th\": 0, \"max\": 0}\n",
        "\n",
        "    mean_val = total_sent / total_docs\n",
        "\n",
        "    # Compute quantiles from histogram\n",
        "    def quantile_from_hist(hist, q):\n",
        "        target = q * total_docs\n",
        "        cum = 0\n",
        "        for k in sorted(hist.keys()):\n",
        "            cum += hist[k]\n",
        "            if cum >= target:\n",
        "                return k\n",
        "        return max(hist) if hist else 0\n",
        "\n",
        "    median_val = quantile_from_hist(count_hist, 0.5)\n",
        "    p95_val = quantile_from_hist(count_hist, 0.95)\n",
        "\n",
        "    return {\n",
        "        \"mean\": float(mean_val),\n",
        "        \"median\": int(median_val),\n",
        "        \"95th\": int(p95_val),\n",
        "        \"max\": int(max_sent)\n",
        "    }\n",
        "\n",
        "print(\"\\n--- Sentence Counts (Articles) ---\")\n",
        "print(sentence_stats_stream(df_train[\"article\"]))\n",
        "print(\"\\n--- Sentence Counts (Summaries) ---\")\n",
        "print(sentence_stats_stream(df_train[\"highlights\"]))\n",
        "\n",
        "# ---------------------------\n",
        "# 6) Vocabulary size & most common tokens (streaming)\n",
        "#    No tokens list; directly update a Counter.\n",
        "# ---------------------------\n",
        "def vocab_info_stream(texts, top_n=20, batch_size=2048):\n",
        "    counter = Counter()\n",
        "    for chunk in batched_iter(texts, batch_size):\n",
        "        for t in chunk:\n",
        "            counter.update(t.split())\n",
        "        del chunk\n",
        "        gc.collect()\n",
        "    return len(counter), counter.most_common(top_n)\n",
        "\n",
        "vocab_size, top_tokens = vocab_info_stream(df_train[\"article\"], top_n=20)\n",
        "print(f\"\\nVocabulary size (train articles): {vocab_size}\")\n",
        "print(f\"Top 20 tokens: {top_tokens}\")\n",
        "\n",
        "# ---------------------------\n",
        "# 7) Rare/OOV token analysis (streaming)\n",
        "#    Build train vocab as a set in a single pass (usually fits in RAM).\n",
        "#    If RAM is still tight, enable `top_k_cap` to keep only frequent tokens.\n",
        "# ---------------------------\n",
        "def build_train_vocab_stream(texts, top_k_cap=None, batch_size=2048):\n",
        "    # two-pass optional: if top_k_cap is set, build frequency then keep top-k\n",
        "    if top_k_cap is None:\n",
        "        vocab = set()\n",
        "        for chunk in batched_iter(texts, batch_size):\n",
        "            for t in chunk:\n",
        "                vocab.update(t.split())\n",
        "            del chunk\n",
        "            gc.collect()\n",
        "        return vocab\n",
        "    else:\n",
        "        freq = Counter()\n",
        "        for chunk in batched_iter(texts, batch_size):\n",
        "            for t in chunk:\n",
        "                freq.update(t.split())\n",
        "            del chunk\n",
        "            gc.collect()\n",
        "        return set([w for w, _ in freq.most_common(top_k_cap)])\n",
        "\n",
        "def oov_rate_stream(texts, train_vocab, batch_size=2048):\n",
        "    total_tokens = 0\n",
        "    oov_tokens = 0\n",
        "    for chunk in batched_iter(texts, batch_size):\n",
        "        for t in chunk:\n",
        "            tokens = t.split()\n",
        "            total_tokens += len(tokens)\n",
        "            # iterate once; do not build temp lists\n",
        "            for tok in tokens:\n",
        "                if tok not in train_vocab:\n",
        "                    oov_tokens += 1\n",
        "        del chunk\n",
        "        gc.collect()\n",
        "    return 0.0 if total_tokens == 0 else (oov_tokens / total_tokens)\n",
        "\n",
        "# Build train vocab (exact). If memory is still an issue, set top_k_cap (e.g., 300_000).\n",
        "train_vocab = build_train_vocab_stream(df_train[\"article\"], top_k_cap=None)\n",
        "\n",
        "val_oov = oov_rate_stream(df_val[\"article\"], train_vocab)\n",
        "test_oov = oov_rate_stream(df_test[\"article\"], train_vocab)\n",
        "print(f\"\\nOOV rate (val articles vs train vocab): {val_oov:.4f}\")\n",
        "print(f\"OOV rate (test articles vs train vocab): {test_oov:.4f}\")\n",
        "\n",
        "# ---------------------------\n",
        "# 8) Character-level checks (streaming)\n",
        "#    Do not join all texts; update a character Counter incrementally.\n",
        "# ---------------------------\n",
        "def char_distribution_stream(texts, top_n=20, batch_size=4096):\n",
        "    chars = Counter()\n",
        "    for chunk in batched_iter(texts, batch_size):\n",
        "        for t in chunk:\n",
        "            chars.update(t)\n",
        "        del chunk\n",
        "        gc.collect()\n",
        "    return chars.most_common(top_n)\n",
        "\n",
        "print(\"\\n--- Most common characters in train articles ---\")\n",
        "print(char_distribution_stream(df_train[\"article\"], top_n=20))\n",
        "\n",
        "# ---------------------------\n",
        "# 9) Non-ASCII / unusual character scan (streaming)\n",
        "#    Keep a capped sample of unique non-ASCII tokens to avoid large sets.\n",
        "# ---------------------------\n",
        "def non_ascii_tokens_stream(texts, cap=5000, batch_size=2048):\n",
        "    # Uses a small reservoir-like sample; also tracks an approximate unique count.\n",
        "    sample = []\n",
        "    sample_set = set()\n",
        "    approx_seen = 0\n",
        "\n",
        "    def try_add(token):\n",
        "        nonlocal approx_seen\n",
        "        if token in sample_set:\n",
        "            return\n",
        "        approx_seen += 1\n",
        "        if len(sample) < cap:\n",
        "            sample.append(token)\n",
        "            sample_set.add(token)\n",
        "\n",
        "    for chunk in batched_iter(texts, batch_size):\n",
        "        for t in chunk:\n",
        "            for tok in t.split():\n",
        "                # any non-ASCII char?\n",
        "                # ord(c) > 127 is cheaper than unicodedata checks\n",
        "                for c in tok:\n",
        "                    if ord(c) > 127:\n",
        "                        try_add(tok)\n",
        "                        break\n",
        "        del chunk\n",
        "        gc.collect()\n",
        "    return approx_seen, sample[:20]\n",
        "\n",
        "na_count, na_sample = non_ascii_tokens_stream(df_train[\"article\"], cap=5000)\n",
        "print(f\"\\nApprox. number of unique non-ASCII tokens in train: {na_count}\")\n",
        "print(f\"Sample non-ASCII tokens: {na_sample}\")\n",
        "\n",
        "# ---------------------------\n",
        "# 10) Punctuation-only token density (streaming)\n",
        "# ---------------------------\n",
        "_PUNCT_SET = set(string.punctuation)\n",
        "\n",
        "def punctuation_density_stream(texts, batch_size=2048):\n",
        "    total_tokens = 0\n",
        "    punct_tokens = 0\n",
        "    for chunk in batched_iter(texts, batch_size):\n",
        "        for t in chunk:\n",
        "            for tok in t.split():\n",
        "                total_tokens += 1\n",
        "                # treat tokens made only of punctuation as punctuation-only\n",
        "                if tok and all(ch in _PUNCT_SET for ch in tok):\n",
        "                    punct_tokens += 1\n",
        "        del chunk\n",
        "        gc.collect()\n",
        "    return 0.0 if total_tokens == 0 else (punct_tokens / total_tokens)\n",
        "\n",
        "punct_density = punctuation_density_stream(df_train[\"article\"])\n",
        "print(f\"\\nPunctuation-only token ratio in train: {punct_density:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAw4bdbnDOLt",
        "outputId": "a1a32c18-3aaa-4fac-8af9-1ddb4d2c160a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Token Length Stats ---\n",
            "Train: {'mean': 691.8703263175126, 'median': 632.0, '95th': 1363.0, 'max': 2347}\n",
            "Val: {'mean': 676.0264063435069, 'median': 608.0, '95th': 1370.0, 'max': 1917}\n",
            "Test: {'mean': 683.5115752828547, 'median': 613.0, '95th': 1399.5499999999993, 'max': 1954}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from itertools import islice\n",
        "\n",
        "def batched_iter(texts, batch_size=2048):\n",
        "    it = iter(texts)\n",
        "    while True:\n",
        "        chunk = list(islice(it, batch_size))\n",
        "        if not chunk:\n",
        "            break\n",
        "        yield chunk\n",
        "\n",
        "def token_length_stats(texts, batch_size=2048):\n",
        "    lengths = []\n",
        "    for chunk in batched_iter(texts, batch_size):\n",
        "        lengths.extend(len(t.split()) for t in chunk)\n",
        "    arr = np.array(lengths)\n",
        "    return {\n",
        "        \"mean\": float(np.mean(arr)),\n",
        "        \"median\": float(np.median(arr)),\n",
        "        \"95th\": float(np.percentile(arr, 95)),\n",
        "        \"max\": int(np.max(arr))\n",
        "    }\n",
        "\n",
        "print(\"\\n--- Token Length Stats ---\")\n",
        "print(\"Train:\", token_length_stats(df_train[\"article\"]))\n",
        "print(\"Val:\", token_length_stats(df_val[\"article\"]))\n",
        "print(\"Test:\", token_length_stats(df_test[\"article\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "bby_0qePFZfX",
        "outputId": "3a2a5931-6658-4416-a782-b41935714ae4"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "def token_frequency_counter(texts, batch_size=2048):\n",
        "    counter = Counter()\n",
        "    for chunk in batched_iter(texts, batch_size):\n",
        "        for t in chunk:\n",
        "            counter.update(t.split())\n",
        "    return counter\n",
        "\n",
        "train_counter = token_frequency_counter(df_train[\"article\"])\n",
        "freqs = np.array([freq for _, freq in train_counter.most_common()])\n",
        "ranks = np.arange(1, len(freqs) + 1)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.loglog(ranks, freqs)\n",
        "plt.title(\"Zipf's Law - Train Articles\")\n",
        "plt.xlabel(\"Rank\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.grid(True, which=\"both\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeoKiMUcFcTY",
        "outputId": "ab141717-95b6-4aa9-e45d-ca4f90d3b690"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Vocabulary Overlap ---\n",
            "Train–Val overlap: 82.68%\n",
            "Train–Test overlap: 82.77%\n"
          ]
        }
      ],
      "source": [
        "def build_vocab(texts, batch_size=2048):\n",
        "    vocab = set()\n",
        "    for chunk in batched_iter(texts, batch_size):\n",
        "        for t in chunk:\n",
        "            vocab.update(t.split())\n",
        "    return vocab\n",
        "\n",
        "train_vocab = build_vocab(df_train[\"article\"])\n",
        "val_vocab = build_vocab(df_val[\"article\"])\n",
        "test_vocab = build_vocab(df_test[\"article\"])\n",
        "\n",
        "overlap_train_val = len(train_vocab & val_vocab) / len(val_vocab)\n",
        "overlap_train_test = len(train_vocab & test_vocab) / len(test_vocab)\n",
        "\n",
        "print(\"\\n--- Vocabulary Overlap ---\")\n",
        "print(f\"Train–Val overlap: {overlap_train_val:.2%}\")\n",
        "print(f\"Train–Test overlap: {overlap_train_test:.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pq3e1OvWFetE",
        "outputId": "e6eab438-93f7-44d4-d030-24fad3db9e68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Most Common Sentences (Train) ---\n",
            "132364x: S\n",
            "99545x: By\n",
            "60144x: UPDATED:\n",
            "58280x: |\n",
            "58240x: PUBLISHED:\n",
            "39727x: Scroll down for video\n",
            "29776x: '\n",
            "19943x: The\n",
            "17920x: m\n",
            "13322x: Daily Mail Reporter\n",
            "13139x: \"\n",
            "13096x: ’\n",
            "11123x: N\n",
            "10594x: com\n",
            "7701x: U\n",
            "6274x: The U\n",
            "5860x: He\n",
            "5624x: A\n",
            "5019x: 5\n",
            "4692x: 1\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def most_common_sentences(texts, top_n=20, batch_size=2048):\n",
        "    sent_counter = Counter()\n",
        "    for chunk in batched_iter(texts, batch_size):\n",
        "        for t in chunk:\n",
        "            for sent in t.split('.'):\n",
        "                s = sent.strip()\n",
        "                if s:\n",
        "                    sent_counter[s] += 1\n",
        "    return sent_counter.most_common(top_n)\n",
        "\n",
        "print(\"\\n--- Most Common Sentences (Train) ---\")\n",
        "for sent, count in most_common_sentences(df_train[\"article\"], top_n=20):\n",
        "    print(f\"{count}x: {sent}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Okjart22GX03",
        "outputId": "c44c3da9-c341-4916-a864-2201b639db62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Named Entity Counts (Sampled) ---\n",
            "PERSON: 34894\n",
            "ORG: 24111\n",
            "DATE: 23737\n",
            "GPE: 20552\n",
            "CARDINAL: 12654\n",
            "NORP: 7127\n",
            "TIME: 3488\n",
            "ORDINAL: 3043\n",
            "MONEY: 2685\n",
            "LOC: 1904\n",
            "FAC: 1511\n",
            "QUANTITY: 1107\n",
            "EVENT: 1027\n",
            "WORK_OF_ART: 1001\n",
            "PRODUCT: 728\n",
            "PERCENT: 353\n",
            "LAW: 195\n",
            "LANGUAGE: 171\n"
          ]
        }
      ],
      "source": [
        "!pip install -q spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"tagger\", \"textcat\"])\n",
        "entity_counter = Counter()\n",
        "\n",
        "# Sample to speed up in free tier\n",
        "sample_texts = df_train[\"article\"].sample(2000, random_state=0)\n",
        "\n",
        "for doc in nlp.pipe(sample_texts, batch_size=32):\n",
        "    entity_counter.update([ent.label_ for ent in doc.ents])\n",
        "\n",
        "print(\"\\n--- Named Entity Counts (Sampled) ---\")\n",
        "for ent, count in entity_counter.most_common():\n",
        "    print(f\"{ent}: {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V_dHRwdMncc"
      },
      "source": [
        "Interpretation of EDA Findings\n",
        "\n",
        "The exploratory data analysis (EDA) provides key insights into dataset structure, lexical diversity, and potential preprocessing requirements, with direct implications for the design of a pointer-generator summarization model.\n",
        "\n",
        "### 1. Sentence Length Distributions\n",
        "The dataset exhibits a strong structural imbalance between inputs and targets: articles are long and variable in sentence count, while summaries are short and concise. This is consistent with prior summarization corpora (See et al., 2017), where the model must distill lengthy narratives into compact abstracts. Such imbalance necessitates:\n",
        "- Input truncation or chunking to fit model limits (e.g., 512–1024 tokens)\n",
        "- Careful padding strategy to minimise wasted computation on short samples\n",
        "\n",
        "### 2. Vocabulary Size and Zipfian Distribution\n",
        "The corpus contains an unusually large vocabulary, with frequency counts following a steep Zipfian distribution (Zipf, 1935). This long-tailed lexical profile means:\n",
        "- A high proportion of tokens are rare, often corresponding to named entities, numerals, or technical terms\n",
        "- Subword segmentation (Sennrich et al., 2016) is essential to control embedding size\n",
        "- Rare-token prevalence strengthens the case for a copy mechanism to handle out-of-vocabulary or low-frequency terms effectively\n",
        "\n",
        "### 3. OOV Rates and Vocabulary Overlap\n",
        "Token-level OOV rates between validation/test and training sets are under 1%, indicating strong lexical overlap and low risk of catastrophic generalisation failure. However, ~17% of the **unique types** in validation/test are unseen in training, which aligns with findings from Luong et al. (2015) that these unseen types are disproportionately rare and often entity-like. This reinforces the value of a pointer network to reproduce such tokens from the source.\n",
        "\n",
        "### 4. Character Distribution and Non-ASCII Tokens\n",
        "The dominance of space and lower-case letters matches expectations for clean English text. However, the presence of over a million unique non-ASCII tokens — including accented characters and currency symbols — indicates multilingual or typographically rich segments. Unicode normalisation may be applied, but over-cleaning risks damaging factual fidelity for named entities, echoing concerns noted by Xue et al. (2021) for multilingual text.\n",
        "\n",
        "### 5. Punctuation Density\n",
        "The proportion of punctuation-only tokens (~2%) is within the normal range for professionally written corpora (Dodge et al., 2021). Aggressive punctuation filtering is not warranted; standard tokenisation will suffice.\n",
        "\n",
        "### 6. Boilerplate and Repetitive Content\n",
        "Repetition analysis revealed templated editorial lines and metadata markers (e.g., publication dates, author credits, promotional prompts). Prior audits of web-sourced corpora (Kreutzer et al., 2022) show that such boilerplate can bias sequence generation towards irrelevant content. Targeted removal or masking of these segments is recommended.\n",
        "\n",
        "### 7. Named Entity Distribution\n",
        "Named entities are dominated by PERSON, ORG, DATE, and GPE categories, a distribution characteristic of news-style datasets. These entities are often rare or absent from the fixed vocabulary, making them ideal candidates for pointer-based copying (See et al., 2017). This distribution confirms that the dataset can effectively test the model’s rare/factual token handling.\n",
        "\n",
        "### 8. Zipf’s Law Validation\n",
        "The log–log rank–frequency curve matches the power-law behaviour predicted by Zipf’s Law (Zipf, 1935), indicating a natural language distribution and reducing concerns of synthetic text contamination.\n",
        "\n",
        "---\n",
        "\n",
        "### Overall Implications\n",
        "The corpus is sufficiently clean for modelling but exhibits structural and lexical properties that should inform preprocessing and architecture choices:\n",
        "- **Long-tail rare tokens** justify integrating a copy mechanism\n",
        "- **Boilerplate repetition** should be removed to improve generalisation\n",
        "- **Length imbalance** between source and target requires truncation/chunking strategies\n",
        "\n",
        "These findings align with best practices in summarisation dataset preparation and support the suitability of this corpus for evaluating pointer-generator architectures.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7E_abhPiKYYh"
      },
      "outputs": [],
      "source": [
        "# Setup Helpers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import spacy\n",
        "from itertools import islice\n",
        "\n",
        "# === Batch iterator ===\n",
        "def batched_iter(texts, batch_size=2048):\n",
        "    it = iter(texts)\n",
        "    while True:\n",
        "        chunk = list(islice(it, batch_size))\n",
        "        if not chunk:\n",
        "            break\n",
        "        yield chunk\n",
        "\n",
        "# Load SpaCy model without heavy components\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"textcat\"])\n",
        "\n",
        "# Train vocab counter for rare/OOV calculations\n",
        "train_counter = Counter()\n",
        "for chunk in batched_iter(df_train[\"article\"]):\n",
        "    for t in chunk:\n",
        "        train_counter.update(t.split())\n",
        "\n",
        "train_vocab = set(train_counter.keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4A3kjTqeN34a",
        "outputId": "13158000-9349-49af-c432-e07f4c1141b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Rare/OOV Token Coverage (Summaries) ---\n",
            "Val: {'rare_token_ratio': 0.014286784595074289, 'oov_token_ratio': 0.007529626347620979}\n",
            "Test: {'rare_token_ratio': 0.014892644176079056, 'oov_token_ratio': 0.00813850649545848}\n"
          ]
        }
      ],
      "source": [
        "def rare_token_coverage(texts, train_counter, min_count=5):\n",
        "    total_tokens, rare_tokens, oov_tokens = 0, 0, 0\n",
        "    for chunk in batched_iter(texts):\n",
        "        for t in chunk:\n",
        "            tokens = t.split()\n",
        "            total_tokens += len(tokens)\n",
        "            rare_tokens += sum(1 for tok in tokens if train_counter[tok] < min_count)\n",
        "            oov_tokens += sum(1 for tok in tokens if tok not in train_vocab)\n",
        "    return {\n",
        "        \"rare_token_ratio\": rare_tokens / total_tokens,\n",
        "        \"oov_token_ratio\": oov_tokens / total_tokens\n",
        "    }\n",
        "\n",
        "print(\"\\n--- Rare/OOV Token Coverage (Summaries) ---\")\n",
        "print(\"Val:\", rare_token_coverage(df_val[\"highlights\"], train_counter))\n",
        "print(\"Test:\", rare_token_coverage(df_test[\"highlights\"], train_counter))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5DVxZ85OAMz",
        "outputId": "9c23c66f-d298-4c78-a970-2b4fff3e95cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- POS Distribution ---\n",
            "Articles: {'DET': 0.08683777174246986, 'NOUN': 0.17679574305973725, 'ADP': 0.10767515023410713, 'PROPN': 0.09849028547552628, 'AUX': 0.05403204994285574, 'ADJ': 0.0570926474383395, 'PUNCT': 0.13123210401484522, 'CCONJ': 0.025266365993634254, 'PART': 0.027016946653066743, 'VERB': 0.10737468201983459, 'PRON': 0.05765180096592236, 'SCONJ': 0.013362540400378504, 'ADV': 0.031726739827706976, 'SYM': 0.0018986641760780603, 'NUM': 0.02127978567829624, 'SPACE': 0.0011103191476288204, 'X': 0.000502623720398658, 'INTJ': 0.0006537795091738046}\n",
            "Summaries: {'PROPN': 0.12844190591215224, 'NOUN': 0.18936668092494277, 'VERB': 0.112598445009184, 'PUNCT': 0.1129842571857518, 'AUX': 0.048939435875500086, 'PRON': 0.03548633302300615, 'ADJ': 0.05377047530382709, 'SPACE': 0.048553623698932305, 'ADP': 0.1059138296890857, 'SYM': 0.003279403500826141, 'ADV': 0.020355785924565332, 'SCONJ': 0.007288495248639173, 'DET': 0.0568485854951396, 'CCONJ': 0.020011909854146224, 'NUM': 0.029439146516367663, 'PART': 0.026377810767514616, 'INTJ': 0.00018451886705415628, 'X': 0.00015935720336495316}\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def pos_distribution(texts, sample_size=2000):\n",
        "    pos_counts = Counter()\n",
        "    sample_texts = texts.sample(min(sample_size, len(texts)), random_state=0)\n",
        "    for doc in nlp.pipe(sample_texts, batch_size=32):\n",
        "        pos_counts.update([token.pos_ for token in doc])\n",
        "    total = sum(pos_counts.values())\n",
        "    return {pos: count / total for pos, count in pos_counts.items()}\n",
        "\n",
        "print(\"\\n--- POS Distribution ---\")\n",
        "print(\"Articles:\", pos_distribution(df_train[\"article\"]))\n",
        "print(\"Summaries:\", pos_distribution(df_train[\"highlights\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkJBCiLsOBxF",
        "outputId": "6d954551-810f-4f5f-a68a-4e027348ac5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Compression Ratios ---\n",
            "Train: {'mean_ratio': np.float64(0.0908753611831848), 'median_ratio': np.float64(0.07834101382488479), '95th': np.float64(0.18867924528301888)}\n",
            "Val: {'mean_ratio': np.float64(0.10494638143734762), 'median_ratio': np.float64(0.09223656341952266), '95th': np.float64(0.21419799498746858)}\n",
            "Test: {'mean_ratio': np.float64(0.10030218167695168), 'median_ratio': np.float64(0.08634817822823326), '95th': np.float64(0.20537367724867722)}\n"
          ]
        }
      ],
      "source": [
        "def compression_ratios(articles, summaries):\n",
        "    ratios = []\n",
        "    for art, summ in zip(articles, summaries):\n",
        "        art_len = len(art.split())\n",
        "        summ_len = len(summ.split())\n",
        "        if art_len > 0:\n",
        "            ratios.append(summ_len / art_len)\n",
        "    return {\n",
        "        \"mean_ratio\": np.mean(ratios),\n",
        "        \"median_ratio\": np.median(ratios),\n",
        "        \"95th\": np.percentile(ratios, 95)\n",
        "    }\n",
        "\n",
        "print(\"\\n--- Compression Ratios ---\")\n",
        "print(\"Train:\", compression_ratios(df_train[\"article\"], df_train[\"highlights\"]))\n",
        "print(\"Val:\", compression_ratios(df_val[\"article\"], df_val[\"highlights\"]))\n",
        "print(\"Test:\", compression_ratios(df_test[\"article\"], df_test[\"highlights\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVMR4YhYOD8p",
        "outputId": "ae452d5c-6edc-4895-a103-fcb085017055"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Entity Match Rate ---\n",
            "Val: 0.8012963205447166\n",
            "Test: 0.8012240923633329\n"
          ]
        }
      ],
      "source": [
        "def entity_match_rate(articles, summaries, sample_size=2000):\n",
        "    match_count, total_entities = 0, 0\n",
        "    sample_idx = np.random.default_rng(0).choice(len(articles), min(sample_size, len(articles)), replace=False)\n",
        "    for i in sample_idx:\n",
        "        art_doc = nlp(articles.iloc[i])\n",
        "        summ_doc = nlp(summaries.iloc[i])\n",
        "        art_ents = set(ent.text for ent in art_doc.ents)\n",
        "        summ_ents = [ent.text for ent in summ_doc.ents]\n",
        "        total_entities += len(summ_ents)\n",
        "        match_count += sum(1 for e in summ_ents if e in art_ents)\n",
        "    return match_count / total_entities if total_entities > 0 else 0\n",
        "\n",
        "print(\"\\n--- Entity Match Rate ---\")\n",
        "print(\"Val:\", entity_match_rate(df_val[\"article\"], df_val[\"highlights\"]))\n",
        "print(\"Test:\", entity_match_rate(df_test[\"article\"], df_test[\"highlights\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb4DtUYHOI_X",
        "outputId": "e42ca15e-4ccd-4a40-af7c-b79dcd1fec8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Token Overlap ---\n",
            "Val: {'mean_overlap': np.float64(0.8191435602159881), 'median_overlap': np.float64(0.8305084745762712), '95th': np.float64(0.9512195121951219)}\n",
            "Test: {'mean_overlap': np.float64(0.8196654652204275), 'median_overlap': np.float64(0.82952776336274), '95th': np.float64(0.9459700772200772)}\n"
          ]
        }
      ],
      "source": [
        "def token_overlap(articles, summaries, sample_size=2000):\n",
        "    overlaps = []\n",
        "    sample_idx = np.random.default_rng(0).choice(len(articles), min(sample_size, len(articles)), replace=False)\n",
        "    for i in sample_idx:\n",
        "        art_tokens = set(articles.iloc[i].split())\n",
        "        summ_tokens = summaries.iloc[i].split()\n",
        "        if len(summ_tokens) > 0:\n",
        "            match = sum(1 for tok in summ_tokens if tok in art_tokens)\n",
        "            overlaps.append(match / len(summ_tokens))\n",
        "    return {\n",
        "        \"mean_overlap\": np.mean(overlaps),\n",
        "        \"median_overlap\": np.median(overlaps),\n",
        "        \"95th\": np.percentile(overlaps, 95)\n",
        "    }\n",
        "\n",
        "print(\"\\n--- Token Overlap ---\")\n",
        "print(\"Val:\", token_overlap(df_val[\"article\"], df_val[\"highlights\"]))\n",
        "print(\"Test:\", token_overlap(df_test[\"article\"], df_test[\"highlights\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIrLmW3HOKkc",
        "outputId": "f0860770-0fe4-4288-a7d9-a700e87f3c12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Numerical/Date Token Ratios (Summaries) ---\n",
            "Val: {'numeric_ratio': 0.009916589434661723, 'date_like_ratio': 0.0014828544949026877}\n",
            "Test: {'numeric_ratio': 0.010030908775781385, 'date_like_ratio': 0.0014804436998093477}\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def numeric_date_analysis(texts, sample_size=5000):\n",
        "    num_count, date_like_count, total_tokens = 0, 0, 0\n",
        "    sample_texts = texts.sample(min(sample_size, len(texts)), random_state=0)\n",
        "    for t in sample_texts:\n",
        "        tokens = t.split()\n",
        "        total_tokens += len(tokens)\n",
        "        num_count += sum(1 for tok in tokens if tok.isdigit())\n",
        "        date_like_count += sum(1 for tok in tokens if re.match(r\"\\d{1,4}[-/]\\d{1,2}([-\\/]\\d{1,4})?\", tok))\n",
        "    return {\n",
        "        \"numeric_ratio\": num_count / total_tokens,\n",
        "        \"date_like_ratio\": date_like_count / total_tokens\n",
        "    }\n",
        "\n",
        "print(\"\\n--- Numerical/Date Token Ratios (Summaries) ---\")\n",
        "print(\"Val:\", numeric_date_analysis(df_val[\"highlights\"]))\n",
        "print(\"Test:\", numeric_date_analysis(df_test[\"highlights\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KZoDFLiOLac",
        "outputId": "0bc80d2c-519d-462b-d7f2-33a7b83bd581"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- N-gram Overlap ---\n",
            "Bigram overlap (Val vs Train): 0.2604617830899072\n",
            "Trigram overlap (Val vs Train): 0.11606236797625562\n"
          ]
        }
      ],
      "source": [
        "from nltk import ngrams\n",
        "\n",
        "def ngram_set(texts, n, sample_size=5000):\n",
        "    ngram_s = set()\n",
        "    sample_texts = texts.sample(min(sample_size, len(texts)), random_state=0)\n",
        "    for t in sample_texts:\n",
        "        toks = t.split()\n",
        "        ngram_s.update(ngrams(toks, n))\n",
        "    return ngram_s\n",
        "\n",
        "train_bigrams = ngram_set(df_train[\"article\"], 2)\n",
        "train_trigrams = ngram_set(df_train[\"article\"], 3)\n",
        "\n",
        "val_bigrams = ngram_set(df_val[\"article\"], 2)\n",
        "val_trigrams = ngram_set(df_val[\"article\"], 3)\n",
        "\n",
        "print(\"\\n--- N-gram Overlap ---\")\n",
        "print(\"Bigram overlap (Val vs Train):\", len(train_bigrams & val_bigrams) / len(val_bigrams))\n",
        "print(\"Trigram overlap (Val vs Train):\", len(train_trigrams & val_trigrams) / len(val_trigrams))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTuPxPYRQ_tV"
      },
      "source": [
        "Analysis of Extended EDA Metrics\n",
        "\n",
        "### 1. Rare and OOV Token Coverage\n",
        "- **Rare tokens (~1.4–1.5%)**: Around 1 in every 70 summary tokens appears fewer than five times in the training set.\n",
        "- **OOV tokens (~0.75–0.8%)**: Around 1 in every 125 summary tokens does not appear in the training vocabulary at all.\n",
        "- **Interpretation**: The dataset contains a stable but meaningful presence of rare and unseen words in summaries. This provides enough instances to evaluate a model’s ability to handle rare/factual terms, without creating instability during evaluation.\n",
        "\n",
        "### 2. Part-of-Speech (POS) Distribution\n",
        "- Summaries contain proportionally more **proper nouns (PROPN)** and **numbers (NUM)** compared to articles.\n",
        "- Articles have proportionally more **determiners (DET)** and **pronouns (PRON)**, reflecting narrative and structural content.\n",
        "- **Interpretation**: Summaries are more information-dense, focusing on named entities and factual elements. This pattern aligns with tasks that benefit from copy mechanisms.\n",
        "\n",
        "### 3. Compression Ratios\n",
        "- Summaries average around **10% of the length of articles**.\n",
        "- Even at the 95th percentile, summaries retain less than 22% of article tokens.\n",
        "- **Interpretation**: The task requires heavy content compression. Models must be selective in retaining key details, making accurate copying important for factual consistency.\n",
        "\n",
        "### 4. Entity Match Rate\n",
        "- Roughly **80%** of named entities in summaries also appear in the corresponding article.\n",
        "- **Interpretation**: The majority of summary entities are directly copyable from the source, confirming that the dataset supports learning effective pointer-generator behaviour.\n",
        "\n",
        "### 5. Token-Level Overlap (Extractiveness)\n",
        "- Average token overlap between summaries and articles is ~82%, with high median and 95th percentile values.\n",
        "- **Interpretation**: Despite being an abstractive task, the dataset is strongly extractive. The high overlap means many source tokens can be reused verbatim, which copy mechanisms can exploit.\n",
        "\n",
        "### 6. Numerical and Date Tokens\n",
        "- Numbers make up about **1%** of summary tokens; date-like expressions are about **0.15%**.\n",
        "- **Interpretation**: These elements are relatively rare but important for factual accuracy. Copying them from the source helps avoid errors or hallucinations.\n",
        "\n",
        "### 7. N-gram Overlap Between Splits\n",
        "- **Bigrams**: ~26% overlap between train and validation.\n",
        "- **Trigrams**: ~11.6% overlap between train and validation.\n",
        "- **Interpretation**: There is moderate novelty in word combinations across splits, reducing the risk of inflated evaluation scores from memorisation and ensuring a fair test of generalisation.\n",
        "\n",
        "---\n",
        "\n",
        "### Overall Interpretation\n",
        "The dataset is clean and suitable for abstractive summarisation with a pointer-generator approach.  \n",
        "Key properties supporting this conclusion:\n",
        "- A measurable presence of rare/OOV terms in summaries.\n",
        "- High proportion of named entities and factual elements in targets.\n",
        "- Strong extractive component with high token overlap.\n",
        "- High compression ratios requiring selective content retention.\n",
        "- Balanced n-gram overlap between splits for robust generalisation testing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU-LsfS_SLtS"
      },
      "source": [
        "# Exact Duplicates Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "collapsed": true,
        "id": "2gMhhemBRQRo",
        "outputId": "304d8a1a-abc6-457d-8006-74cd1a0d360e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df_train\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"article\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"By . Rebecca Camber . PUBLISHED: . 09:43 EST, 31 May 2012 . | . UPDATED: . 02:12 EST, 1 June 2012 . A boy of 12 who raped a nine-year-old girl after watching hard-core pornography online was spared jail yesterday as his lawyer warned of a generation of children growing up with a \\u2018skewed view\\u2019 on sex. The schoolboy, who is now 14, told police he had raped the little girl because he wanted to \\u2018feel grown up\\u2019 after watching porn online. In a disturbing case that has raised fresh concerns about the sexualisation of children, the teenager had unrestricted access to the web and was able to freely look at sexually explicit material. Attack: The boy, now aged 14, had unrestricted access to the internet and had watched the sex acts online (picture posed by model) He then \\u2018emulated\\u2019 the graphic footage and attacked a neighbour\\u2019s daughter, the High Court in Edinburgh heard. Yesterday his lawyer warned the case was just the \\u2018tip of the iceberg\\u2019 and many other cases were going unreported. The Daily Mail has campaigned for an automatic block on online porn, calling for a consultation on the introduction of content filtering systems for internet accounts. Sean Templeton, defending the boy, said: \\u2018There is a real risk that young people are growing up with a skewed view of what sex is and sexual activity. \\u2018This was an emulation of an adult act witnessed by him at this young age. He was afforded unfettered access to the internet and it has become apparent from a very young age, the age of 12, he was accessing hard-core pornography.\\u2019 Decision: Lady Smith opted not to jail the boy and sent him to a a . children's panel where he can be kept under continuing supervision . Mr Templeton said it was a great concern that children may be getting their sex education through internet pornography. He warned: \\u2018This is the tip of the iceberg. Many, many cases throughout the country may not be identified, not reported, not coming to anyone\\u2019s attention.\\u2019 The shocking case, on the Isle of Skye in Scotland, only came to light when the young victim felt ill and told her mother she feared there was \\u2018a baby in my tummy\\u2019. Asked if something had happened to her, she became hysterical before revealing what the boy had done. Police discovered the girl had been sexually assaulted a number of times in a shed in the garden of her home between December 1, 2010 and January 31 2011. The boy, who lived next door, also abused the girl in his bedroom in front of a 12-year-old friend. When he was interviewed by police the boy pointed out the sites he had viewed and when asked why he had copied them, he answered \\u2018temptation\\u2019 and \\u2018to feel grown up\\u2019. He was later placed in a secure unit hundreds of miles away. Earlier this month the teenager stood shaking in the dock as he pleaded guilty to one charge of rape and two of sexual assault. Sentencing: The boy, 12, was spared a jail term when he appeared before Edinburgh's High Court . The extraordinary case was heard in a closed courtroom with only relatives of the children and care workers present. Judge Lady Smith said he should be given advice on handling relationships and sexual development rather than face jail. She handed him a four year supervision order to be carried out under the Children\\u2019s Panel, which means he will be closely supervised by social workers until the age of 18. She told the boy he was being given an opportunity to \\u2018make something of yourself\\u2019. Judge Lady Smith went on: \\u2018You should not and must not regard pornography as any guide at all as to how to behave sexually. You should not have engaged in sexual activity of any sort with a nine-year-old girl or indeed any other young girl who, under our law, is not able to consent to it.\\u2019 The Daily Mail has led a campaign calling for greater curbs on access to online pornography. The Government is to consider introducing web filters to make adults \\u2018opt in\\u2019 if they wish to view pornography on their computers. David Cameron will consult on whether to introduce automatic filters \\u2013 or a less stringent system where users would have to make an \\u2018active choice\\u2019 on whether to view explicit sites when they sign up with an internet service provider. Last night Anne Houston, chief executive of the Children 1st charity, said the case illustrated the importance of the Mail\\u2019s campaign. She said: \\u2018It\\u2019s everyone\\u2019s responsibility to protect children and young people from the potential risks. \\u2018Internet providers play a huge role in supporting families to achieve this and we would support recent calls for them to block inappropriate websites unless an adult has legally opted in to view content. \\u2018Although the internet puts lots of possibilities at children and young people\\u2019s fingertips and provides access to many positive activities and opportunities, children, young people and their families must be aware that even if it feels like a\\u00a0 virtual world, the internet has real-life consequences.\\u2019\",\n          \"Ryder Cup winner Lee Westwood has criticised the United States team for airing 'dirty laundry in public' after their defeat at Gleneagles last month. In the immediate wake of Europe's 16 1/2 to 11 1/2 victory, American veteran Phil Mickelson openly criticised the captaincy of Tom Watson, sat a few feet away from him, at the post-tournament press conference. That has led to a fortnight of apologies, inquests and soul-searching about what can be done to end the European dominance of the event. The USA team were convincingly beaten by Europe in the 2014 Ryder Cup . 'I think it's a little bit disappointing to see the dirty laundry being out in public,' said Westwood, back on American soil playing in the PGA Tour's Frys.com Open in California this week. 'It's very difficult to pin-point in a team environment whose fault it specifically is. 'It's a combination of a lot of different things. Maybe Tom got a few things wrong. Maybe the US team just didn't quite play well enough in general. You know, if the other team plays well, you're going to lose. 'I'm just pleased that I don't have to sort it all out because I don't like to see people's great reputations being brought down by something that shouldn't really happen in public. USA captain Tom Watson (right) has taken full responsibility for any mistakes made in the Ryder Cup defeat . 'It should all be done behind closed doors and sorted out there, and the analysis should start there, really, and not be done in the press, in the media.' Westwood was part of the team which lost at Valhalla in 2008 and while there were a number of issues with Nick Faldo's captaincy then no-one in the European team broke ranks with their frustrations. 'I think there were a lot of people disappointed in '08 but we tried to come together and basically not say anything in public,' added Westwood at his pre-tournament press conference. 'Whenever you lose you're going to be disappointed and you're going to think things could have been done better. Phil Mickelson, speaking in a press conference after Europe has secured victory at Gleneagles, claimed American captain Tom Watson had 'strayed from a winning formula' and compared his unfavourably to 2008 leader Paul Azinger. He said: 'There were two things that allowed us to play our best that Paul Azinger did. 'First, he got everybody invested in the process. He got everybody invested in who they were going to play with, who the picks were going to be, who was going to be in their 'pod', when they would play, and they had a great leader for each pod. We hung out together. 'The other things that Paul did really well was he had a game plan for us - how we were going to go about doing this, how we were going to go about playing together, if so-and-so is playing well, if so-and-so is not playing well. 'Those two things helped us bring out our best golf. We use that same process in the Presidents Cup and we do really well. 'Unfortunately we have strayed from a winning formula for the last three Ryder Cups and we need to consider maybe getting back to that formula that helped us play our best.' Asked if he believed his comments were disloyal to Watson, he replied: 'I'm sorry you're taking it that way. I'm just talking about what Paul Azinger did to help us play our best.' Phil Mickelson was clear and specific in his criticism of American Ryder Cup captain Watson . Mickelson (centre) looks despondent after Europe collect the Ryder Cup trophy instead of USA . 'It's just a case of managing it and handling it and improving it for the next time professionally. 'Certainly for the future Ryder Cups the Europeans will remember how it's all been handled. 'The European team will remember the fall-out from this one. It can't do anything but build confidence for the European team going into the next one that it's been handled so publicly this time. 'I guess we'll see how easy it is to get the US team rattled by putting a bit of pressure on them. I don't think anything good can come of all this (for the Americans).' In response to all of the recent discussion about our Ryder Cup loss, I would like to make a few comments. First, I take complete and full responsibility for my communication, and I regret that my words may have made the players feel that I didn't appreciate their commitment and dedication to winning the Ryder Cup. My intentions throughout my term as captain were both to inspire and to be honest.Secondly, the guys gave everything.They played their hearts out. I was proud to get to know each and every one of them. I know they are all going to win tournaments, be on future Ryder Cup teams and have wonderful careers. Our team certainly showed guts when it took it to the other team early in Sunday's singles matches. We were indeed tied with them as the scoreboard turned wonderfully \\\"red\\\". Our players started fast as I had asked them to in my comments the night before. I asked them to really concentrate on holes 2-5, as the Europeans had won too many early battles on these particular holes. But in the end, the facts are that the other team played better. My hat's off and congratulations to them. As for Phil's comments, I completely understand his reaction in the moment. Earlier this week I had an open and candid conversation with him and it ended with a better understanding of each other's perspectives. Phil's heart and intentions for our team's success have always been in the right place. Phil is a great player, has great passion and I admire what he's done for golf. The bottom line is this. I was their captain. In hindsight whatever mistakes that were made were mine. And I take complete and full responsibility for them. I want to say again to the players, their families, the PGA and our country how proud and honored I was to captain this talented group of golfers, and how privileged I was to spend the past two years working this labor of my love for the Ryder Cup. Matt Kuchar, a member of the losing team at Gleneagles, said he had avoided everything to do with defeat since returning home. 'I played the Ryder Cup and then haven't turned on the TV since I was home. I've watched some football games and some baseball games, but I don't look online to see anything,' he said. 'Phil answered a question. It didn't seem like it was at all a shot at Tom Watson to me. 'Phil was asked the last time the US won they had a pod system under Paul Azinger, 'Why do you think they went away from the system, did you prefer the system?' and Phil said 'Yeah'. 'With that I guess it opened up a can of worms but you've got to remember Phil being asked a second question, and he said 'Listen, don't take it that way. This is not a shot at Tom', so I'm sure it's blown out of proportion.' Lee Westwood took to Twitter to show off his lift home after celebrating winning the 2014 Ryder Cup . Europe celebrate their Ryder Cup triumph over USA which came to an end after singles matches on Sunday .\",\n          \"(CNN) -- Attention cruise passengers: You now have more protection when things go awry. On Wednesday, Cruise Lines International Association announced the adoption of a new passenger bill of rights. \\\"The Cruise Industry Passenger Bill of Rights codifies many longstanding practices of CLIA members and goes beyond those to further inform cruise guests of the industry's commitment to their comfort and care,\\\" said Christine Duffy, president and CEO of CLIA, in a statement. The new policy comes on the heels of a series of high-profile setbacks for the industry. From mechanical failures to a deadly accident off the coast of Italy, incidents in the past year-and-a-half have battered the industry's reputation for safety. Among passengers' rights are an emergency power source in case of a main generator failure and a ship crew \\\"properly trained\\\" in emergency procedures. Generator failure played a key role in the days-long stranding of the Carnival Triumph in February after an engine fire knocked out power. In the case of the January 2012 Costa Concordia disaster off the coast of Italy, which killed 32, many passengers recounted a chaotic evacuation of the sinking ship. Another provision of the cruise industry rule mirrors the Department of Transportation's tarmac delay rule for airline passengers in giving cruise passengers the right to disembark a docked ship if adequate food, water and restroom facilities are not available onboard. The organization's new policy also provides the right to emergency medical attention, timely information related to itinerary changes and mechanical problems and full or partial refunds for voyages that are canceled or ended early due to mechanical failures. The bill also assumes responsibility for passenger transportation and lodging in cases when mechanical issues cut voyages short. The full bill of rights will be published on each cruise line's website, in addition to a toll-free number for passengers to call with questions about shipboard operations. The bill of rights is effective immediately for passengers who purchase cruises in North America for any itinerary on CLIA's more than two dozen North American member cruise lines, which include Carnival, Costa, Disney, Holland America, Royal Caribbean and others. The industry organization is also planning to request global recognition of the new policy from the International Maritime Organization.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"highlights\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Schoolboy was copying acts he had watched online, court hears .\\nHis lawyer warned that the case was just the 'tip of the iceberg', with many others going unreported .\\nDaily Mail has led a campaign calling for a default block on online porn which web users can opt out of .\",\n          \"Lee Westwood claims it was 'disappointing' to see USA Ryder Cup team publicly criticise their captain Tom Watson .\\nWatson was openly criticised by Phil Mickelson .\\nEurope defeated USA 16.5-11.5 at Gleneagles last month .\",\n          \"Cruise industry group announces passenger bill of rights .\\nIncludes right to an emergency power source and crew trained in emergency evacuation .\\nThe new policy applies to North American cruise lines that are members of the industry group .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-23e1a868-5b3b-4abe-8013-d300f25ca446\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article</th>\n",
              "      <th>highlights</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>80398</th>\n",
              "      <td>The accusations against the Revlon CEO are ugl...</td>\n",
              "      <td>Revlon chairman says CEO \"has my full support\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105327</th>\n",
              "      <td>By . Rebecca Camber . PUBLISHED: . 09:43 EST, ...</td>\n",
              "      <td>Schoolboy was copying acts he had watched onli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68412</th>\n",
              "      <td>(CNN) -- Attention cruise passengers: You now ...</td>\n",
              "      <td>Cruise industry group announces passenger bill...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>237706</th>\n",
              "      <td>Giancarlo Stanton has agreed to an eye-popping...</td>\n",
              "      <td>He will earn more than $50,000 every time he b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194920</th>\n",
              "      <td>Ryder Cup winner Lee Westwood has criticised t...</td>\n",
              "      <td>Lee Westwood claims it was 'disappointing' to ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-23e1a868-5b3b-4abe-8013-d300f25ca446')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-23e1a868-5b3b-4abe-8013-d300f25ca446 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-23e1a868-5b3b-4abe-8013-d300f25ca446');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a7631715-a757-498a-872c-a0022a671cfd\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a7631715-a757-498a-872c-a0022a671cfd')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a7631715-a757-498a-872c-a0022a671cfd button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                  article  \\\n",
              "80398   The accusations against the Revlon CEO are ugl...   \n",
              "105327  By . Rebecca Camber . PUBLISHED: . 09:43 EST, ...   \n",
              "68412   (CNN) -- Attention cruise passengers: You now ...   \n",
              "237706  Giancarlo Stanton has agreed to an eye-popping...   \n",
              "194920  Ryder Cup winner Lee Westwood has criticised t...   \n",
              "\n",
              "                                               highlights  \n",
              "80398   Revlon chairman says CEO \"has my full support\"...  \n",
              "105327  Schoolboy was copying acts he had watched onli...  \n",
              "68412   Cruise industry group announces passenger bill...  \n",
              "237706  He will earn more than $50,000 every time he b...  \n",
              "194920  Lee Westwood claims it was 'disappointing' to ...  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Random sample to inspect structure\n",
        "df_train.sample(5, random_state=0)[[\"article\", \"highlights\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWJ5qmsFnhqR"
      },
      "source": [
        "> **Exact Article and Highlight:**\n",
        "\n",
        "**training**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOKpJmdKkI_M",
        "outputId": "9936c699-0800-46a4-fb65-97db99bbeb74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exact article–highlight duplicates: 3098\n"
          ]
        }
      ],
      "source": [
        "# Count rows where both article and highlight are identical to another row\n",
        "exact_pair_dups = df_train.duplicated(subset=[\"article\", \"highlights\"])\n",
        "print(f\"Exact article–highlight duplicates: {exact_pair_dups.sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nz8LfKexnrUi",
        "outputId": "bd6728bc-7790-44bb-878a-63236ce223a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size after removing exact duplicates: 284015\n"
          ]
        }
      ],
      "source": [
        "# Drop exact duplicates and keep the first occurrence\n",
        "df_train = df_train.drop_duplicates(subset=[\"article\", \"highlights\"], keep=\"first\").reset_index(drop=True)\n",
        "\n",
        "print(f\"Train size after removing exact duplicates: {len(df_train)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3qIfNPHZjKo"
      },
      "source": [
        "**validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9nbkn-QZmip",
        "outputId": "d733b8e9-9e03-41f2-93a5-064c4efa0aca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exact article–highlight duplicates in val: 0\n",
            "Exact article–highlight duplicates in test: 2\n"
          ]
        }
      ],
      "source": [
        "# Count rows where both article and highlight are identical to another row\n",
        "exact_pair_dups_val = df_val.duplicated(subset=[\"article\", \"highlights\"])\n",
        "print(f\"Exact article–highlight duplicates in val: {exact_pair_dups_val.sum()}\")\n",
        "\n",
        "# Count rows where both article and highlight are identical to another row\n",
        "exact_pair_dups_test = df_test.duplicated(subset=[\"article\", \"highlights\"])\n",
        "print(f\"Exact article–highlight duplicates in test: {exact_pair_dups_test.sum()}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "075gMK-3aWIm",
        "outputId": "0ff4a21a-b3a7-49e8-acb0-afa18726c314"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test size after removing exact duplicates: 11488\n"
          ]
        }
      ],
      "source": [
        "df_test = df_test.drop_duplicates(subset=[\"article\", \"highlights\"], keep=\"first\").reset_index(drop=True)\n",
        "\n",
        "print(f\"Test size after removing exact duplicates: {len(df_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_E2ZGc7pYjM"
      },
      "source": [
        "\n",
        "\n",
        "> **Exact Article with Different Highlights (training):**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "e3s6HAQqtiJ3",
        "outputId": "54045949-95fd-4747-b906-296ea9de74a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of articles with different highlights: 10\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df_10\",\n  \"rows\": 20,\n  \"fields\": [\n    {\n      \"column\": \"article\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"The son of boxing champ Evander Holyfield seems to be following in his father's athletic footsteps. Elijah Holyfield, a junior at Woodward Academy in College Park, Georgia, has just fielded an offer to play college football for the University of Oregon Ducks. That's not all either, as Yahoo Sports\\u00a0reports the running back has also received recruitment letters from Arkansas, Boston College, Cal, Duke, Georgia, Louisville, Michigan, Michigan State, Ohio State, Ole Miss and Wisconsin, to name just a few. Scroll down for video . Athletic genes: Elijah Holyfield (above during a visit to Ohio State) has been recruited to play college football for over 20 major universities . Famous dad: Elijah (second from right) is the son of boxing champ Evander Holyfield (third from left, pictured with his children) In fact, the young Holyfield is so good, he has been ranked as the No. 137 recruit in the country by ESPN. Overall, 22 different colleges have extended offers to the 17-year-old. 'Oregon Just offered #GoDucks!' Elijah wrote on Twitter yesterday. Elijah, Evander's only child with ex-wife Dr. Janice Itson, is also a state champion in track. And not only is Evander proud of his son, he also seems to think he will be a better athlete than he ever was. '[My dad] was saying how different it was for me and him,' Elijah told The Oregonian. 'He was picking up, as far as people knowing him, at a later age. At a younger age, he didn't have a lot of money and was a small kid and wasn't that popular. He was saying how different it is for (me), because I'm so young and all the people on me, talking to me all the time \\u2013 because of him, because of me playing football. He just said to keep a cool head and just remember the main focus.' Multitalented: Elijah (far right) is also a track star, as his 4 x 100 relay team (above) placed sixth at the Georgia state championship last year . Painful memories: Evander (right) will long be remembered as the only 4-time World Heavyweight Champion, and for having fellow boxer Mike Tyson (left) bite his ear during a fight . Evander made millions over the course of his career, and is still the only 4-time World Heavyweight Champion in the history of boxing. Despite his huge success, he is perhaps best known to many for an incident in 1996 when fellow boxer Mike Tyson bit off part of his ear during his match. As for his son, Elijah says he is excited to visit Oregon later this year, but has made no choice yet on what college he might attend. 'They are always good and looked good against Michigan State this year,' Elijah said of the Ducks. 'I love the way they run the ball. That's the main thing I'm concerned with.'\",\n          \"(CNN) -- Freed Iranian-American journalist Roxana Saberi thanked friends and family Tuesday for their support during her ordeal in an Iranian prison, and said she plans to spend the next few days relaxing. Roxana Saberi smiles ouside her home in Tehran, Iran, on Tuesday. \\\"I am, of course, very happy to be free and to be with my parents again,\\\" a smiling Saberi, 32, told reporters. Saberi, who was dressed in a black tunic and a blue headscarf, said she was only now learning of a global support campaign on her behalf. \\\"I want to thank all the people all over the world, who, whether they knew me or not, helped me and my family during this period,\\\" she said. \\\"I don't have any specific plans for the moment. I just want to be with my parents and my friends and to relax.\\\" Reza Saberi, her father, said they plan to leave Iran soon. Saberi was convicted last month on espionage charges in a one-day trial that was closed to the public. She was sentenced to eight years in prison after being accused of spying for the United States. A judge changed Saberi's sentence during an appeal hearing Monday. The court agreed with her lawyers that, because Iran is not at war with the United States, Saberi cannot be punished for cooperating with agents of a hostile nation, according to Saberi's spokesman, Abdolsamad Khorramshahi. Her sentence was changed to a two-year jail term, suspended for five years, Iran's state-run news agency IRNA reported. Saberi was detained in January after initially being accused of buying a bottle of wine and working as a journalist without proper accreditation, according to the Committee to Protect Journalists, an advocacy group. She was soon charged with espionage. Saberi went on a hunger strike while imprisoned, but her father said she has since put on some weight.\",\n          \"Everett, Washington (CNN)  -- After more than two years of delays, Boeing's 787 Dreamliner made its maiden flight Tuesday in a three-hour trip that the maker described as a success. \\\"Today is truly a proud and historic day for the global team who has worked tirelessly to design and build the 787 Dreamliner -- the first all-new jet airplane of the 21st century,\\\" said Scott Fancher, vice president and general manager of the 787 program, in a news release. \\\"We look forward to the upcoming flight test program and soon bringing groundbreaking levels of efficiency, technology and passenger comfort to airlines and the flying public.\\\" More than 12,000 employees and guests watched as the plane took off at 10:27 a.m. from Paine Field in Everett, Washington. It landed more than three hours later and about 40 miles away at Seattle, Washington's Boeing Field after having flown at a speed of 207 mph at 15,000 feet -- typical for a maiden flight, the company said. During their time aloft, the chief pilot and captain tested some of the airplane's systems and structures while flight data were transmitted electronically to engineers at Boeing Field. \\\"The flight marks the beginning of a flight test program that will see six airplanes flying nearly around the clock and around the globe, with the airplane's first delivery scheduled for fourth quarter 2010,\\\" Boeing's news release said. Boeing promises passengers \\\"a better flying experience\\\" that includes bigger windows, more luggage space and better lighting. It promises airline operators greater efficiency by burning 20 percent less fuel than current models of comparable size and by providing as much as 45 percent more space for cargo. So far, 55 customers have ordered 840 of the planes. The official price of one is $150 million. \\\"We think this is going to be a very efficient airplane,\\\" Jim Albaugh, Boeing executive vice president and CEO, told CNN. \\\"It's going to change the way people travel.\\\" Despite the delays, Boeing's first new commercial airliner in more than a decade will still be relevant, Albaugh said Monday. \\\"It's more environmentally friendly, it's more efficient, uses less fuel, it's going to cost the operator less to fly, it's going to allow the passengers to pay less and feel better when they land,\\\" he said. Boeing's fuel claims are linked to its design. It is the first major airliner to be made mostly of composite materials and, as a result, is lighter. Depending on the configuration, the plane can seat 200 to 300 passengers and can travel more than 2,500 nautical miles. But production delays and technical problems have stolen some of the Dreamliner's luster. Many of the snags in the supply line have been blamed on the army of partners Boeing brought in to help with the construction. \\\"They did too much outsourcing, too soon, with too little oversight,\\\" said Scott Hamilton of the aviation consulting firm Leeham Co. \\\"The customers have been mightily [upset] over the creeping delays.\\\" Albaugh acknowledged that, \\\"in hindsight,\\\" the level of outsourcing may not have been the best strategy. \\\"There a few things we might have kept inside, yes,\\\" he said. With 10 months of flight tests ahead, the 787s won't start flying commercially until at least 2011, the company said. \\\"There's a lot of work to do,\\\" Albaugh said.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"highlights\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"NEW: Jorge Barahona is charged with attempted murder; home is being searched .\\nBarahona \\\"attempted to harm himself\\\" Thursday morning, police say .\\nHe \\\"refused to cooperate\\\" by not speaking,  which postponed his hearing .\\nCause of daughter's death has been determined but not made public .\",\n          \"Elijah Holyfield, the son of boxing champ Evander Holyfield, has received an offer to play for the University of Oregon Ducks .\\nThe high school junior has also fielded offers from over 20 other universities, including Boston College and Michigan .\\nThe running back ranked as the No.137 recruit in the country by ESPN .\",\n          \"St. Louis County Police Chief Jon Belmar said he does not believe the shooting was connected with the two protests occurring at the same time .\\nA police spokesperson said the officer was treated and released from the hospital and was expected to survive .\\nBelmar says the officer was wearing a body camera at the time but that it was off for unknown reasons .\\nThere were conflicting reports early on over the reason for the shooting, and it is not known that it was linked to the death of Michael Brown .\\nAnother officer was shot in an incident believed to be unrelated\\u00a0to Ferguson hours later .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"202f7a6543cc6b750304b9552d67e341287909ad\",\n          \"ee44f8ed179218193000cb33e38b3c1a2cc0a034\",\n          \"f12abf917117d93d51f10e59e87b67d6d1fe879d\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df_10"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-29cba5d9-4584-4e6c-bc52-a831f379c65d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article</th>\n",
              "      <th>highlights</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(CNN) -- A Florida exterminator whose dead daughter and injured son were found in his truck has been charged with attempted murder, and police were searching his Miami home Thursday night, police said. Chase Scott, spokesman for West Palm Beach Police, told CNN that officers were executing a search warrant for evidence in the home of Jorge and Carmen Barahona. Jorge Barahona, 53, was found unconscious beside his pest-control truck early Monday along a south Florida interstate by a road assistance ranger, along with his 10-year-old adopted son, who was inside the vehicle next to an open gas can, according to a probable-cause affidavit filed by detectives. Hours later, crews removing toxic chemicals from the truck discovered the boy's twin sister dead in a plastic bag. Earlier Thursday, Barahona was taken to a hospital Thursday after he \"attempted to harm himself,\" police said. Barahona, who was in custody in the Palm Beach County Jail, suffered a self-inflicted injury after deputies told him to get ready to go to a court hearing Thursday morning, West Palm Beach Police spokesman Scott Chase said. \"He immediately attempted to harm himself by thrusting himself backwards, causing an injury to his head,\" Chase said. \"He was immediately checked by emergency personnel and it was decided he was OK to appear in court.\" However, Barahona \"refused to cooperate\" by not speaking and the judge decided to delay the hearing until another date, Chase said. Authorities later decided to take Barahona to Wellington Regional Medical Center for observation, he said. Meanwhile, a medical examiner has determined a cause of death for the girl, but it will not be made public until investigators review the findings, Chase said Thursday. Authorities likely will decide whether and how to charge Barahona in her death based on the autopsy results, Police Capt. Mary Olsen said Wednesday. The children were among the four the Barahonas adopted from Florida's foster care system. When the boy was found Monday by the roadside assistance ranger, he \"appeared to be in respiratory distress and (was) trembling\" and his clothing \"was soaked with an unknown chemical,\" the probable-cause affidavit said. The ranger then found Barahona on the ground beside the truck and called for help. The boy was hospitalized in intensive care with severe burns to his abdomen, upper thighs and buttocks, the affidavit said. While examining the boy, doctors noted he had sustained previous injuries, including a broken collarbone, a broken arm, scarring to his buttocks and lower abdomen, and ligature marks on both wrists, police said. After Barahona and his son were taken to a hospital, a worker decontaminating the truck discovered the body of the girl, wrapped in a plastic bag, the document said. Barahona told police he was distraught over the death of his daughter, and had intended to commit suicide by dousing himself with gasoline and setting himself afire, the affidavit said. Barahona said he didn't go through with his suicide plan because his son was with him, the document added. \"Basically, to paraphrase, he was stating that he placed his daughter in a plastic bag being distraught over her death,\" Chase told reporters Wednesday. \"He drove here from South Florida accompanied by his son. \"He then pulled off to the side of the road saying that he poured gas on his self, intending to light himself on fire. His son's head was in his lap and he decided, after giving his son some sleeping pills, that he wasn't going to do that.\" Barahona told police that in dousing himself with gasoline, he inadvertently got some on the boy, Olsen said Wednesday. But his story doesn't add up, because there was no gasoline on the boy, she said. Instead, he was covered with another chemical whose composition had yet to be determined. \"That's why we're still treating this as a hazmat (hazardous materials case),\" Olsen said. The truck in which the children were found was taken to a secure location, Chase said, where an FBI evidence recovery team is going through the vehicle. Authorities are waiting for test results on the chemicals found in the truck. Chase said the substance on the boy's body and clothes was so potent that staff caring for the boy at the hospital became ill as well. The boy, who was transferred Wednesday morning to a specialized burn unit at Miami's Jackson Memorial Hospital, has not been able to talk to investigators because he is on a breathing tube, Olsen explained. Asked whether Barahona has expressed remorse, she said, \"He feels remorse, but we're not getting consistent statements with what we're seeing in our evidence.\" \"It's a complex case,\" she added. At a hearing Wednesday in Miami attended by Barahona's wife, Carmen, a judge ordered that the remaining two children in the home be placed in foster care. Florida's Department of Children and Families had opened a child protection investigation within the past few days to look into a complaint involving the Barahona family, and it wasn't the first such complaint, spokesman Mark Riordan said. Reporters in the courtroom Wednesday heard tales of abuse, mainly concerning the twins, from state officials and experts. The caller to the child protection hotline in the latest case reported that the twins were routinely locked in a bathroom for long periods of time and had been bound with tape, the court heard. The story was corroborated by interviews with the other two children in the home, officials said in court. An investigator told the court that she went to the family's home on February 11 but had not seen the children. Instead, she said, she left after speaking with Carmen Barahona, planning to return on Monday. Asked why she did not return sooner, she said, \"I'm not allowed to do investigations on a weekend.\" However, a spokesman for the department, John Harrell, said it is the job of investigators to follow through immediately or refer to someone else in the department to follow through when a matter is urgent. CNN's Kim Segal contributed to this report.</td>\n",
              "      <td>NEW: Jorge Barahona is charged with attempted murder; home is being searched .\\nBarahona \"attempted to harm himself\" Thursday morning, police say .\\nHe \"refused to cooperate\" by not speaking,  which postponed his hearing .\\nCause of daughter's death has been determined but not made public .</td>\n",
              "      <td>202f7a6543cc6b750304b9552d67e341287909ad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(CNN) -- A Florida exterminator whose dead daughter and injured son were found in his truck has been charged with attempted murder, and police were searching his Miami home Thursday night, police said. Chase Scott, spokesman for West Palm Beach Police, told CNN that officers were executing a search warrant for evidence in the home of Jorge and Carmen Barahona. Jorge Barahona, 53, was found unconscious beside his pest-control truck early Monday along a south Florida interstate by a road assistance ranger, along with his 10-year-old adopted son, who was inside the vehicle next to an open gas can, according to a probable-cause affidavit filed by detectives. Hours later, crews removing toxic chemicals from the truck discovered the boy's twin sister dead in a plastic bag. Earlier Thursday, Barahona was taken to a hospital Thursday after he \"attempted to harm himself,\" police said. Barahona, who was in custody in the Palm Beach County Jail, suffered a self-inflicted injury after deputies told him to get ready to go to a court hearing Thursday morning, West Palm Beach Police spokesman Scott Chase said. \"He immediately attempted to harm himself by thrusting himself backwards, causing an injury to his head,\" Chase said. \"He was immediately checked by emergency personnel and it was decided he was OK to appear in court.\" However, Barahona \"refused to cooperate\" by not speaking and the judge decided to delay the hearing until another date, Chase said. Authorities later decided to take Barahona to Wellington Regional Medical Center for observation, he said. Meanwhile, a medical examiner has determined a cause of death for the girl, but it will not be made public until investigators review the findings, Chase said Thursday. Authorities likely will decide whether and how to charge Barahona in her death based on the autopsy results, Police Capt. Mary Olsen said Wednesday. The children were among the four the Barahonas adopted from Florida's foster care system. When the boy was found Monday by the roadside assistance ranger, he \"appeared to be in respiratory distress and (was) trembling\" and his clothing \"was soaked with an unknown chemical,\" the probable-cause affidavit said. The ranger then found Barahona on the ground beside the truck and called for help. The boy was hospitalized in intensive care with severe burns to his abdomen, upper thighs and buttocks, the affidavit said. While examining the boy, doctors noted he had sustained previous injuries, including a broken collarbone, a broken arm, scarring to his buttocks and lower abdomen, and ligature marks on both wrists, police said. After Barahona and his son were taken to a hospital, a worker decontaminating the truck discovered the body of the girl, wrapped in a plastic bag, the document said. Barahona told police he was distraught over the death of his daughter, and had intended to commit suicide by dousing himself with gasoline and setting himself afire, the affidavit said. Barahona said he didn't go through with his suicide plan because his son was with him, the document added. \"Basically, to paraphrase, he was stating that he placed his daughter in a plastic bag being distraught over her death,\" Chase told reporters Wednesday. \"He drove here from South Florida accompanied by his son. \"He then pulled off to the side of the road saying that he poured gas on his self, intending to light himself on fire. His son's head was in his lap and he decided, after giving his son some sleeping pills, that he wasn't going to do that.\" Barahona told police that in dousing himself with gasoline, he inadvertently got some on the boy, Olsen said Wednesday. But his story doesn't add up, because there was no gasoline on the boy, she said. Instead, he was covered with another chemical whose composition had yet to be determined. \"That's why we're still treating this as a hazmat (hazardous materials case),\" Olsen said. The truck in which the children were found was taken to a secure location, Chase said, where an FBI evidence recovery team is going through the vehicle. Authorities are waiting for test results on the chemicals found in the truck. Chase said the substance on the boy's body and clothes was so potent that staff caring for the boy at the hospital became ill as well. The boy, who was transferred Wednesday morning to a specialized burn unit at Miami's Jackson Memorial Hospital, has not been able to talk to investigators because he is on a breathing tube, Olsen explained. Asked whether Barahona has expressed remorse, she said, \"He feels remorse, but we're not getting consistent statements with what we're seeing in our evidence.\" \"It's a complex case,\" she added. At a hearing Wednesday in Miami attended by Barahona's wife, Carmen, a judge ordered that the remaining two children in the home be placed in foster care. Florida's Department of Children and Families had opened a child protection investigation within the past few days to look into a complaint involving the Barahona family, and it wasn't the first such complaint, spokesman Mark Riordan said. Reporters in the courtroom Wednesday heard tales of abuse, mainly concerning the twins, from state officials and experts. The caller to the child protection hotline in the latest case reported that the twins were routinely locked in a bathroom for long periods of time and had been bound with tape, the court heard. The story was corroborated by interviews with the other two children in the home, officials said in court. An investigator told the court that she went to the family's home on February 11 but had not seen the children. Instead, she said, she left after speaking with Carmen Barahona, planning to return on Monday. Asked why she did not return sooner, she said, \"I'm not allowed to do investigations on a weekend.\" However, a spokesman for the department, John Harrell, said it is the job of investigators to follow through immediately or refer to someone else in the department to follow through when a matter is urgent. CNN's Kim Segal contributed to this report.</td>\n",
              "      <td>NEW: Jorge Barahona is charged with attempted murder; home is being searched .\\nBarahona \"attempted to harm himself\" Thursday morning, police say .\\nHe \"refused to cooperate\" by not speaking, which postponed his hearing .\\nCause of daughter's death has been determined but not made public .</td>\n",
              "      <td>c812abf550e08e3e91fa5961f30aa888ab92f249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(CNN) -- Freed Iranian-American journalist Roxana Saberi thanked friends and family Tuesday for their support during her ordeal in an Iranian prison, and said she plans to spend the next few days relaxing. Roxana Saberi smiles ouside her home in Tehran, Iran, on Tuesday. \"I am, of course, very happy to be free and to be with my parents again,\" a smiling Saberi, 32, told reporters. Saberi, who was dressed in a black tunic and a blue headscarf, said she was only now learning of a global support campaign on her behalf. \"I want to thank all the people all over the world, who, whether they knew me or not, helped me and my family during this period,\" she said. \"I don't have any specific plans for the moment. I just want to be with my parents and my friends and to relax.\" Reza Saberi, her father, said they plan to leave Iran soon. Saberi was convicted last month on espionage charges in a one-day trial that was closed to the public. She was sentenced to eight years in prison after being accused of spying for the United States. A judge changed Saberi's sentence during an appeal hearing Monday. The court agreed with her lawyers that, because Iran is not at war with the United States, Saberi cannot be punished for cooperating with agents of a hostile nation, according to Saberi's spokesman, Abdolsamad Khorramshahi. Her sentence was changed to a two-year jail term, suspended for five years, Iran's state-run news agency IRNA reported. Saberi was detained in January after initially being accused of buying a bottle of wine and working as a journalist without proper accreditation, according to the Committee to Protect Journalists, an advocacy group. She was soon charged with espionage. Saberi went on a hunger strike while imprisoned, but her father said she has since put on some weight.</td>\n",
              "      <td>Father of Roxana Saberi says they plan to leave Iran soon .\\nSaberi, 32, was convicted last month on espionage charges .\\nHer sentence was changed to a two-year jail term, suspended for five years .\\nShe has lived in Iran since 2003, reporting for international news organizations .</td>\n",
              "      <td>57aad99a31217dcf1ce2086c1cdc70981f7db28b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(CNN) -- Freed Iranian-American journalist Roxana Saberi thanked friends and family Tuesday for their support during her ordeal in an Iranian prison, and said she plans to spend the next few days relaxing. Roxana Saberi smiles ouside her home in Tehran, Iran, on Tuesday. \"I am, of course, very happy to be free and to be with my parents again,\" a smiling Saberi, 32, told reporters. Saberi, who was dressed in a black tunic and a blue headscarf, said she was only now learning of a global support campaign on her behalf. \"I want to thank all the people all over the world, who, whether they knew me or not, helped me and my family during this period,\" she said. \"I don't have any specific plans for the moment. I just want to be with my parents and my friends and to relax.\" Reza Saberi, her father, said they plan to leave Iran soon. Saberi was convicted last month on espionage charges in a one-day trial that was closed to the public. She was sentenced to eight years in prison after being accused of spying for the United States. A judge changed Saberi's sentence during an appeal hearing Monday. The court agreed with her lawyers that, because Iran is not at war with the United States, Saberi cannot be punished for cooperating with agents of a hostile nation, according to Saberi's spokesman, Abdolsamad Khorramshahi. Her sentence was changed to a two-year jail term, suspended for five years, Iran's state-run news agency IRNA reported. Saberi was detained in January after initially being accused of buying a bottle of wine and working as a journalist without proper accreditation, according to the Committee to Protect Journalists, an advocacy group. She was soon charged with espionage. Saberi went on a hunger strike while imprisoned, but her father said she has since put on some weight.</td>\n",
              "      <td>Father of Roxana Saberi says they plan to leave Iran soon .\\nSaberi, 33, was convicted last month on espionage charges .\\nHer sentence was changed to a two-year jail term, suspended for five years .\\nShe has lived in Iran since 2003, reporting for international news organizations .</td>\n",
              "      <td>edffbeec92ef3fb2994768405c0b78b23766e5e6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(CNN) -- It's called the \"Walmart of Weed.\" This newest big-box store is devoted to selling marijuana-growing equipment. Its biggest outlet yet is to open Wednesday in Phoenix, where Arizona voters approved medical cannabis last fall. The weGrow Store will be the first outside California, where medical weed has long been legalized. None of the chain's three stores sell the drug. Instead, they unabashedly promote medical marijuana by offering \"everything -- from supplies to services -- that cultivators need to grow it,\" the company says. What sets weGrow apart from the mom-and-pop stores selling the growing supplies is that weGrow openly states its hydroponic equipment is intended for pot, a spokeswoman said. The smaller stores often claim the hydroponic equipment is used for growing indoor tomatoes and often kick out a customer if they broach the subject of medical cannabis, said weGrow spokeswoman Melissa DiGianfilippo. While 16 states and the District of Columbia now allow for some sort of medical marijuana, there remains ambiguity about whether U.S. authorities would ever enforce federal laws against marijuana distribution, though 90% of hydroponics revenue is from cannabis growers, DiGianfilippo said. In fact, the state of Arizona filed a federal lawsuit last week asking the courts to determine whether the voter-approved initiative legalizing medical marijuana is, indeed, legal. Calling itself \"the nation's only hydroponics franchise that openly talks about medical marijuana,\" the Oakland, California-based weGrow sees a boom in Arizona and projects 100,000 Arizonans acquiring state medical cannabis cards, DiGianfilippo said. So far, about 4,000 Arizonans became state-authorized marijuana users after the state began issuing the cards on April 15, DiGianfilippo said. WeGrow chose Wednesday to open its superstore because that was supposed to be the day the state was scheduled to begin accepting applications to run a marijuana dispensary. But Arizona Gov. Jan Brewer and Attorney General Tom Horne filed a federal lawsuit Friday seeking a declaratory judgment regarding the legality of Proposition 203. Voters approved the Arizona Medical Marijuana Act last November and Brewer signed it into law in December. Nonetheless, the federal government considers marijuana a controlled substance. In a May 2, 2011, letter, U.S. Attorney Dennis Burke of Arizona warned state officials that \"growing, distributing and possessing marijuana, in any capacity, other than as a federally authorized research program, is a violation of federal law regardless of state laws that purport to permit such activities,\" according to Brewer. Burke declared that his office would \"vigorously prosecute individuals and organizations that participate in unlawful manufacturing, distribution and marketing activity involving marijuana, even if such activities are permitted under state law,\" according to Brewer's office. \"The State of Arizona has worked to follow the wishes of voters,\" Brewer said in a statement. \"But I won't stand aside while state employees and average Arizonans acting in good faith are unwittingly put at risk. In light of the explicit warnings on this issue offered by Arizona's U.S. Attorney, as well as many other federal prosecutors, clarity and judicial direction are in order.\" Brewer has directed that the Arizona Department of Health Services put on hold its process for licensing marijuana dispensaries, pending court action on this issue, Brewer's spokesman, Matthew Benson, told CNN on Tuesday. \"The agency will continue providing registration cards for individuals who receive a doctor's recommendation to use medical marijuana,\" Benson said in an e-mail. WeGrow founder Dhar Mann, a 27-year-old entrepreneur who opened his first store in Oakland after a small hydroponics store ejected him for asking about medical marijuana cultivation, said his firm still sees an opportunity in Arizona, where he estimates medical pot to be a billion-dollar industry. He has plans for similar superstores in other states with legal marijuana. The Phoenix store, located in an industrial area so as not to upset any homeowners, will be 21,000 square feet. Wednesday's grand opening in Phoenix will feature indoor growing demonstrations with nonmarijuana plants, experts on how to build professional grow rooms, a physician for patient evaluations and classes on how to safely and responsibly cultivate medical marijuana, weGrow officials said. The first weGrow store was a 15,000-square-foot facility that opened in Oakland last year, and in February, a 10,000-square-foot store opened in Sacramento, California, a spokeswoman said. Mann said the Arizona law allows for up to 124 dispensaries, and he took exception with the governor's lawsuit. \"Delaying the process that a majority of the Arizona voters have asked for prevents qualified cannabis patients from receiving medicine they need and causes unfair financial hardship to businesspeople who have invested tens of thousands of dollars in reliance on the process set up by the state,\" Mann said in a statement. \"Medical marijuana cultivating, dispensing, and consuming will exist with or without governmental approval,\" Mann said. \"It's time to tax and regulate this industry so legitimate business people and patients are not criminalized.\"</td>\n",
              "      <td>The weGrow firm will open its third superstore in Phoenix on Wednesday .\\nIt openly sells marijuana-growing equipment but not the drug itself .\\nArizona has asked a federal court to rule on the legality of medical cannabis .\\nArizona voters approved medical marijuana in November .</td>\n",
              "      <td>81f493b60c66aa78de3ec8f1171af1f44adfce85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>(CNN) -- It's called the \"Walmart of Weed.\" This newest big-box store is devoted to selling marijuana-growing equipment. Its biggest outlet yet is to open Wednesday in Phoenix, where Arizona voters approved medical cannabis last fall. The weGrow Store will be the first outside California, where medical weed has long been legalized. None of the chain's three stores sell the drug. Instead, they unabashedly promote medical marijuana by offering \"everything -- from supplies to services -- that cultivators need to grow it,\" the company says. What sets weGrow apart from the mom-and-pop stores selling the growing supplies is that weGrow openly states its hydroponic equipment is intended for pot, a spokeswoman said. The smaller stores often claim the hydroponic equipment is used for growing indoor tomatoes and often kick out a customer if they broach the subject of medical cannabis, said weGrow spokeswoman Melissa DiGianfilippo. While 16 states and the District of Columbia now allow for some sort of medical marijuana, there remains ambiguity about whether U.S. authorities would ever enforce federal laws against marijuana distribution, though 90% of hydroponics revenue is from cannabis growers, DiGianfilippo said. In fact, the state of Arizona filed a federal lawsuit last week asking the courts to determine whether the voter-approved initiative legalizing medical marijuana is, indeed, legal. Calling itself \"the nation's only hydroponics franchise that openly talks about medical marijuana,\" the Oakland, California-based weGrow sees a boom in Arizona and projects 100,000 Arizonans acquiring state medical cannabis cards, DiGianfilippo said. So far, about 4,000 Arizonans became state-authorized marijuana users after the state began issuing the cards on April 15, DiGianfilippo said. WeGrow chose Wednesday to open its superstore because that was supposed to be the day the state was scheduled to begin accepting applications to run a marijuana dispensary. But Arizona Gov. Jan Brewer and Attorney General Tom Horne filed a federal lawsuit Friday seeking a declaratory judgment regarding the legality of Proposition 203. Voters approved the Arizona Medical Marijuana Act last November and Brewer signed it into law in December. Nonetheless, the federal government considers marijuana a controlled substance. In a May 2, 2011, letter, U.S. Attorney Dennis Burke of Arizona warned state officials that \"growing, distributing and possessing marijuana, in any capacity, other than as a federally authorized research program, is a violation of federal law regardless of state laws that purport to permit such activities,\" according to Brewer. Burke declared that his office would \"vigorously prosecute individuals and organizations that participate in unlawful manufacturing, distribution and marketing activity involving marijuana, even if such activities are permitted under state law,\" according to Brewer's office. \"The State of Arizona has worked to follow the wishes of voters,\" Brewer said in a statement. \"But I won't stand aside while state employees and average Arizonans acting in good faith are unwittingly put at risk. In light of the explicit warnings on this issue offered by Arizona's U.S. Attorney, as well as many other federal prosecutors, clarity and judicial direction are in order.\" Brewer has directed that the Arizona Department of Health Services put on hold its process for licensing marijuana dispensaries, pending court action on this issue, Brewer's spokesman, Matthew Benson, told CNN on Tuesday. \"The agency will continue providing registration cards for individuals who receive a doctor's recommendation to use medical marijuana,\" Benson said in an e-mail. WeGrow founder Dhar Mann, a 27-year-old entrepreneur who opened his first store in Oakland after a small hydroponics store ejected him for asking about medical marijuana cultivation, said his firm still sees an opportunity in Arizona, where he estimates medical pot to be a billion-dollar industry. He has plans for similar superstores in other states with legal marijuana. The Phoenix store, located in an industrial area so as not to upset any homeowners, will be 21,000 square feet. Wednesday's grand opening in Phoenix will feature indoor growing demonstrations with nonmarijuana plants, experts on how to build professional grow rooms, a physician for patient evaluations and classes on how to safely and responsibly cultivate medical marijuana, weGrow officials said. The first weGrow store was a 15,000-square-foot facility that opened in Oakland last year, and in February, a 10,000-square-foot store opened in Sacramento, California, a spokeswoman said. Mann said the Arizona law allows for up to 124 dispensaries, and he took exception with the governor's lawsuit. \"Delaying the process that a majority of the Arizona voters have asked for prevents qualified cannabis patients from receiving medicine they need and causes unfair financial hardship to businesspeople who have invested tens of thousands of dollars in reliance on the process set up by the state,\" Mann said in a statement. \"Medical marijuana cultivating, dispensing, and consuming will exist with or without governmental approval,\" Mann said. \"It's time to tax and regulate this industry so legitimate business people and patients are not criminalized.\"</td>\n",
              "      <td>The weGrow firm will open its third superstore in Phoenix on Wednesday .\\nIt openly sells marijuana-growing equipment but not the drug itself .\\nThe state of Arizona has asked a federal court to rule on the legality of medical cannabis .\\nArizona voters approved medical marijuana in November .</td>\n",
              "      <td>5bfcd50db0e8c9a3ebf74c531c30bf7e8bccfa08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Burglars have raided the home of former world champion boxer Ricky Hatton in Manchester stealing Rolex watches, credit cards, and a wad of 500 Euro notes. Police believe the detached property in Hyde was targeted after Mr Hatton tweeted on Saturday that he would be spending the day in London filming for Soccer AM and watching football. Mr Hatton spent the rest of the weekend in the capital before returning home on Sunday to find a window had been broken and the property ransacked. Robbed: Burglars have ransacked the Manchester home of former world champion boxer Ricky Hatton stealing two Rolex watches, a selection of credit cards, and a wad of 500 Euro notes . The thieves took two Rolex watches, a selection of credit cards, and a stack of 500 Euro notes thought to be worth a considerable sum. At 12.30 on Saturday he tweeted that he was going to London to film Soccer AM, before going on to watch his team Manchester City play Chelsea at Stamford Bridge that night. Mr Hatton, who was known as 'the Hitman' during his 15-year career, is estranged from his family and moved to the house recently, meaning a burglar alarm had not been fitted. Targeted: Police in Hyde, Greater Manchester, think Mr Hatton may have been hit after tweeting that he was off to London over the weekend . Raid: Mr Hatton, who is estranged from his family, moved into the property recently and did not have a burglar alarm fitted when the thefts took place . There were workmen at the house today installing a security system. Also at the property were forensic officers from Hyde CID dusting for fingerprints. Mr Hatton, who is now a boxing promoter, said: 'I am devastated about what’s happened. It makes you feel gutted that someone can do this to you. 'Anyone with information should get onto the police as soon as possible because I’ve put up a reward.'</td>\n",
              "      <td>Ricky Hatton tweeted fans on Saturday saying he was going to London .\\nSpent weekend in the capital filming Soccer AM and watching football .\\nWhen he returned to Hyde, Greater Manchester, he found home ransacked .\\nHe had only just moved to property which didn't have burglar alarm fitted .</td>\n",
              "      <td>84458757b948dee87a7dc6316941549a005d2c25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Burglars have raided the home of former world champion boxer Ricky Hatton in Manchester stealing Rolex watches, credit cards, and a wad of 500 Euro notes. Police believe the detached property in Hyde was targeted after Mr Hatton tweeted on Saturday that he would be spending the day in London filming for Soccer AM and watching football. Mr Hatton spent the rest of the weekend in the capital before returning home on Sunday to find a window had been broken and the property ransacked. Robbed: Burglars have ransacked the Manchester home of former world champion boxer Ricky Hatton stealing two Rolex watches, a selection of credit cards, and a wad of 500 Euro notes . The thieves took two Rolex watches, a selection of credit cards, and a stack of 500 Euro notes thought to be worth a considerable sum. At 12.30 on Saturday he tweeted that he was going to London to film Soccer AM, before going on to watch his team Manchester City play Chelsea at Stamford Bridge that night. Mr Hatton, who was known as 'the Hitman' during his 15-year career, is estranged from his family and moved to the house recently, meaning a burglar alarm had not been fitted. Targeted: Police in Hyde, Greater Manchester, think Mr Hatton may have been hit after tweeting that he was off to London over the weekend . Raid: Mr Hatton, who is estranged from his family, moved into the property recently and did not have a burglar alarm fitted when the thefts took place . There were workmen at the house today installing a security system. Also at the property were forensic officers from Hyde CID dusting for fingerprints. Mr Hatton, who is now a boxing promoter, said: 'I am devastated about what’s happened. It makes you feel gutted that someone can do this to you. 'Anyone with information should get onto the police as soon as possible because I’ve put up a reward.'</td>\n",
              "      <td>Ricky Hatton's house was burgled as the former former boxer was away .\\nHatton was in London last weekend and watched Chelsea vs Man City .\\nWhen he returned to Hyde, Greater Manchester, he found home ransacked .\\nHe had only just moved to property which didn't have burglar alarm fitted .</td>\n",
              "      <td>4e88476070bd6b867745b4d904738637d92b16d2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Congressman Jared Polis . (D) Colorado: District 02 . Congressman Jason Chaffetz . (R) Utah: District 03 .</td>\n",
              "      <td>Two freshman representatives document their experience for CNN .\\nRep. Jared Polis is a Democrat representing Colorado's Second district .\\nRep. Jason Chaffetz is a Republican representing Utah's Third district .</td>\n",
              "      <td>86bd905861391cbd3a98de15c83768b6d1400304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Congressman Jared Polis . (D) Colorado: District 02 . Congressman Jason Chaffetz . (R) Utah: District 03 .</td>\n",
              "      <td>Video: Hand-held cams track freshmen moves .\\nTwo freshman representatives document their experience for CNN .\\nRep. Jared Polis is a Democrat representing Colorado's Second district .\\nRep. Jason Chaffetz is a Republican representing Utah's Third district .</td>\n",
              "      <td>fd4bd93f0e11cec9a6c3f50441b6023b1e582581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Everett, Washington (CNN)  -- After more than two years of delays, Boeing's 787 Dreamliner made its maiden flight Tuesday in a three-hour trip that the maker described as a success. \"Today is truly a proud and historic day for the global team who has worked tirelessly to design and build the 787 Dreamliner -- the first all-new jet airplane of the 21st century,\" said Scott Fancher, vice president and general manager of the 787 program, in a news release. \"We look forward to the upcoming flight test program and soon bringing groundbreaking levels of efficiency, technology and passenger comfort to airlines and the flying public.\" More than 12,000 employees and guests watched as the plane took off at 10:27 a.m. from Paine Field in Everett, Washington. It landed more than three hours later and about 40 miles away at Seattle, Washington's Boeing Field after having flown at a speed of 207 mph at 15,000 feet -- typical for a maiden flight, the company said. During their time aloft, the chief pilot and captain tested some of the airplane's systems and structures while flight data were transmitted electronically to engineers at Boeing Field. \"The flight marks the beginning of a flight test program that will see six airplanes flying nearly around the clock and around the globe, with the airplane's first delivery scheduled for fourth quarter 2010,\" Boeing's news release said. Boeing promises passengers \"a better flying experience\" that includes bigger windows, more luggage space and better lighting. It promises airline operators greater efficiency by burning 20 percent less fuel than current models of comparable size and by providing as much as 45 percent more space for cargo. So far, 55 customers have ordered 840 of the planes. The official price of one is $150 million. \"We think this is going to be a very efficient airplane,\" Jim Albaugh, Boeing executive vice president and CEO, told CNN. \"It's going to change the way people travel.\" Despite the delays, Boeing's first new commercial airliner in more than a decade will still be relevant, Albaugh said Monday. \"It's more environmentally friendly, it's more efficient, uses less fuel, it's going to cost the operator less to fly, it's going to allow the passengers to pay less and feel better when they land,\" he said. Boeing's fuel claims are linked to its design. It is the first major airliner to be made mostly of composite materials and, as a result, is lighter. Depending on the configuration, the plane can seat 200 to 300 passengers and can travel more than 2,500 nautical miles. But production delays and technical problems have stolen some of the Dreamliner's luster. Many of the snags in the supply line have been blamed on the army of partners Boeing brought in to help with the construction. \"They did too much outsourcing, too soon, with too little oversight,\" said Scott Hamilton of the aviation consulting firm Leeham Co. \"The customers have been mightily [upset] over the creeping delays.\" Albaugh acknowledged that, \"in hindsight,\" the level of outsourcing may not have been the best strategy. \"There a few things we might have kept inside, yes,\" he said. With 10 months of flight tests ahead, the 787s won't start flying commercially until at least 2011, the company said. \"There's a lot of work to do,\" Albaugh said.</td>\n",
              "      <td>NEW: 787 Dreamliner completes three-hour test flight .\\nBoeing has touted the 787 as more environmentally friendly and fuel efficient .\\nCompany says plane is made of composite materials than are lighter than aluminum .\\nDepending on configuration, Dreamliner can seat 200 to 300 passengers .</td>\n",
              "      <td>2079196ea8cb59b2a75babf1d2c0f3eb3ff620e0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Everett, Washington (CNN)  -- After more than two years of delays, Boeing's 787 Dreamliner made its maiden flight Tuesday in a three-hour trip that the maker described as a success. \"Today is truly a proud and historic day for the global team who has worked tirelessly to design and build the 787 Dreamliner -- the first all-new jet airplane of the 21st century,\" said Scott Fancher, vice president and general manager of the 787 program, in a news release. \"We look forward to the upcoming flight test program and soon bringing groundbreaking levels of efficiency, technology and passenger comfort to airlines and the flying public.\" More than 12,000 employees and guests watched as the plane took off at 10:27 a.m. from Paine Field in Everett, Washington. It landed more than three hours later and about 40 miles away at Seattle, Washington's Boeing Field after having flown at a speed of 207 mph at 15,000 feet -- typical for a maiden flight, the company said. During their time aloft, the chief pilot and captain tested some of the airplane's systems and structures while flight data were transmitted electronically to engineers at Boeing Field. \"The flight marks the beginning of a flight test program that will see six airplanes flying nearly around the clock and around the globe, with the airplane's first delivery scheduled for fourth quarter 2010,\" Boeing's news release said. Boeing promises passengers \"a better flying experience\" that includes bigger windows, more luggage space and better lighting. It promises airline operators greater efficiency by burning 20 percent less fuel than current models of comparable size and by providing as much as 45 percent more space for cargo. So far, 55 customers have ordered 840 of the planes. The official price of one is $150 million. \"We think this is going to be a very efficient airplane,\" Jim Albaugh, Boeing executive vice president and CEO, told CNN. \"It's going to change the way people travel.\" Despite the delays, Boeing's first new commercial airliner in more than a decade will still be relevant, Albaugh said Monday. \"It's more environmentally friendly, it's more efficient, uses less fuel, it's going to cost the operator less to fly, it's going to allow the passengers to pay less and feel better when they land,\" he said. Boeing's fuel claims are linked to its design. It is the first major airliner to be made mostly of composite materials and, as a result, is lighter. Depending on the configuration, the plane can seat 200 to 300 passengers and can travel more than 2,500 nautical miles. But production delays and technical problems have stolen some of the Dreamliner's luster. Many of the snags in the supply line have been blamed on the army of partners Boeing brought in to help with the construction. \"They did too much outsourcing, too soon, with too little oversight,\" said Scott Hamilton of the aviation consulting firm Leeham Co. \"The customers have been mightily [upset] over the creeping delays.\" Albaugh acknowledged that, \"in hindsight,\" the level of outsourcing may not have been the best strategy. \"There a few things we might have kept inside, yes,\" he said. With 10 months of flight tests ahead, the 787s won't start flying commercially until at least 2011, the company said. \"There's a lot of work to do,\" Albaugh said.</td>\n",
              "      <td>NEW: 787 Dreamliner completes three-hour test flight .\\nBoeing has touted the 787 as more environmentally friendly and fuel efficient .\\nCompany says plane is made of composite materials that are lighter than aluminum .\\nDepending on configuration, Dreamliner can seat 200 to 300 passengers .</td>\n",
              "      <td>dbfff7e9eaac5ba8e4d1657efd799e65d4eb4628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Former England footballer Paul Gascoigne looked down in the dumps as he put his bins out in what is his first public appearance since he was sectioned following an alcohol meltdown. The 47-year-old looked downcast when he was spotted wearing a blue dressing gown, tartan pyjama bottoms and slip-on shoes outside his property in Poole, Dorset. The former midfielder, who was sectioned for his own safety last month, was also sporting scars to his face following a recent fall while he carried out the chore at 1pm on Thursday. Scroll down for video . Former England footballer Paul Gascoigne looked down in the dumps as he put his bins out in Poole, Dorset . The former midfielder looked downcast as he carried out the chore in his dressing gown and pyjama bottoms . It is the first time Gascoigne, who retired from football in 2004, has been pictured in public since being sectioned under the Mental Health Act on October 24. The star was placed on a three-day emergency detox after being rushed to hospital in the early hours following an apparent alcohol binge. He is now said to be recovering at home with his new 44-year-old girlfriend Mandy Thomas, who he only recently started seeing. She is said to have vowed to stand by him and is helping him with his recovery. Less than 48 hours prior to his hospitalisation last month, the pair were pictured out and about and onlookers said they had been discussing their plans for the future. They are said to have met after the former Newcastle, Tottenham and Rangers star visited the post office she runs in Bournemouth, Dorset. The star was placed on a three-day emergency detox last month after being rushed to hospital in the early hours following an apparent alcohol binge. He is now said to be recovering at home with his new girlfriend . It is the first time Gascoigne, who retired from football in 2004, has been pictured in public since being sectioned under the Mental Health Act on October 24. He has suffered a lengthy battle with alcoholism . Ms Thomas, who is separated from her husband and has two children, is said to be 'besotted' with the star with whom she has become 'inseparable'. She reportedly helped him move into his new apartment in the expensive area of Sandbanks in Poole in August. It was shortly after he was released from Poole Hospital after another alcoholic breakdown that he was found slumped outside his flat clutching a bottle of gin. Friends said the new couple viewed the move as a fresh start and said that Ms Thomas hopes to help him through the most recent 'setback'. Gascoigne and his new love Mandy Thomas appear to be the  picture of domestic bliss as they pose together in the garden of his new home two days prior to his hospitalisation. Ms Thomas (right) is supporting him . The 47-year-old star was rushed to hospital at 3am on October 24 and sectioned under the Mental Health Act . The football ace has suffered a long battle with alcoholism and was previously sectioned six years ago. The star, who made 57 appearances for his country, has been in rehab seven times and was first sectioned under the Mental Health Act in 2008, following an incident at a Newcastle hotel. His seventh stint in rehab was in January this year when he checked into a £6,000-a-month treatment clinic in Southampton. Last month, the star was pictured cradling a chicken while enjoying a pint and speaking to locals in a pub beer garden. At the time of his hospitalisation, Gascoigne's agent Terry Baker told MailOnline that the star 'is not very well' but refused to comment on his condition any further. Paul Gascoigne leans in for a kiss  with his girlfriend Mandy Thomas who was with him in hospital last month . Gascogine looking weak and frail in August  (left) and in his footballing glory days playing for England (right) Former England star Gascoigne netted two huge fish during an outing with a friend in Hampshire last month .</td>\n",
              "      <td>Gazza spotted for the first time since being sectioned last month .\\n47-year-old looked downcast as he put the bins out outside home in Poole .\\nSpotted in blue dressing gown and pyjama bottoms on Thursday afternoon .\\nFormer footballer put on emergency three-day detox in October after binge .\\nNew girlfriend Mandy Thomas has vowed to stand by him through ordeal .\\nFormer England star has suffered a long battle with alcoholism and drugs .</td>\n",
              "      <td>9ca32ad1aacaadbe94b2398586085c918ac0e2e5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Former England footballer Paul Gascoigne looked down in the dumps as he put his bins out in what is his first public appearance since he was sectioned following an alcohol meltdown. The 47-year-old looked downcast when he was spotted wearing a blue dressing gown, tartan pyjama bottoms and slip-on shoes outside his property in Poole, Dorset. The former midfielder, who was sectioned for his own safety last month, was also sporting scars to his face following a recent fall while he carried out the chore at 1pm on Thursday. Scroll down for video . Former England footballer Paul Gascoigne looked down in the dumps as he put his bins out in Poole, Dorset . The former midfielder looked downcast as he carried out the chore in his dressing gown and pyjama bottoms . It is the first time Gascoigne, who retired from football in 2004, has been pictured in public since being sectioned under the Mental Health Act on October 24. The star was placed on a three-day emergency detox after being rushed to hospital in the early hours following an apparent alcohol binge. He is now said to be recovering at home with his new 44-year-old girlfriend Mandy Thomas, who he only recently started seeing. She is said to have vowed to stand by him and is helping him with his recovery. Less than 48 hours prior to his hospitalisation last month, the pair were pictured out and about and onlookers said they had been discussing their plans for the future. They are said to have met after the former Newcastle, Tottenham and Rangers star visited the post office she runs in Bournemouth, Dorset. The star was placed on a three-day emergency detox last month after being rushed to hospital in the early hours following an apparent alcohol binge. He is now said to be recovering at home with his new girlfriend . It is the first time Gascoigne, who retired from football in 2004, has been pictured in public since being sectioned under the Mental Health Act on October 24. He has suffered a lengthy battle with alcoholism . Ms Thomas, who is separated from her husband and has two children, is said to be 'besotted' with the star with whom she has become 'inseparable'. She reportedly helped him move into his new apartment in the expensive area of Sandbanks in Poole in August. It was shortly after he was released from Poole Hospital after another alcoholic breakdown that he was found slumped outside his flat clutching a bottle of gin. Friends said the new couple viewed the move as a fresh start and said that Ms Thomas hopes to help him through the most recent 'setback'. Gascoigne and his new love Mandy Thomas appear to be the  picture of domestic bliss as they pose together in the garden of his new home two days prior to his hospitalisation. Ms Thomas (right) is supporting him . The 47-year-old star was rushed to hospital at 3am on October 24 and sectioned under the Mental Health Act . The football ace has suffered a long battle with alcoholism and was previously sectioned six years ago. The star, who made 57 appearances for his country, has been in rehab seven times and was first sectioned under the Mental Health Act in 2008, following an incident at a Newcastle hotel. His seventh stint in rehab was in January this year when he checked into a £6,000-a-month treatment clinic in Southampton. Last month, the star was pictured cradling a chicken while enjoying a pint and speaking to locals in a pub beer garden. At the time of his hospitalisation, Gascoigne's agent Terry Baker told MailOnline that the star 'is not very well' but refused to comment on his condition any further. Paul Gascoigne leans in for a kiss  with his girlfriend Mandy Thomas who was with him in hospital last month . Gascogine looking weak and frail in August  (left) and in his footballing glory days playing for England (right) Former England star Gascoigne netted two huge fish during an outing with a friend in Hampshire last month .</td>\n",
              "      <td>Paul Gascoigne spotted for the first time since being sectioned last month .\\n47-year-old looked downcast as he put the bins out outside home in Poole .\\nSpotted in blue dressing gown and pyjama bottoms on Thursday afternoon .\\nFormer footballer put on emergency three-day detox in October after binge .\\nNew girlfriend Mandy Thomas has vowed to stand by him through ordeal .\\nFormer England star has suffered a long battle with alcoholism and drugs .</td>\n",
              "      <td>768fbe852864cdb0f0f5ca3fea025fae16c499a4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Scroll down for video . The Ferguson police officer who was shot in the arm was released from the hospital as the search for the shooter continues, police say. The officer was shot during a routine patrol near the new Ferguson Community Center and away from the protests related to the death of Michael Brown. The body camera worn by the officer was off at the time, though officials do not know why. Officials say that the officer saw the suspect standing behind the community center and gave chase after he fled. The suspect turned and fired at the officer, striking him in the left arm. The New York Times reported that the suspect fled into the woods, but that an hour and a half search by Ferguson and other departments turned up empty. The reports revised earlier statements that there were two shooters and that the officer was responding to a burglary. St Louis County Police spokesperson Sgt Brian Schellman said that the officer was wearing a body camera that was turned off for an unknown reason at the time of the shooting. Ferguson police began wearing body cameras in August that had been donated by two security companies. The officer was treated for non-life threatening injuries and officials believe he will live after being released from the hospital. Another shooting, this of an off-duty St Louis Metropolitan Police officer, happened hours later, though there are no indications it is related to events in Ferguson. An earlier press conference by St Louis County Police Chief Jon Belmar said police are hunting for the suspect and he did not think the shooting was related to protests over Michael Brown’s death at the hands of white police officer Darren Brown. Shooting: Missouri State Highway Patrol troopers stand posted at the corner of Chambers Road and West Florissant Avenue on Saturday in Ferguson as police search for a suspect in the shooting of a police officer . Location: Police have set up a staging area  on West Florissant Road in Ferguson, Missouri, according to local news station KMOV . Crime scene: A police officer takes pictures outside the Ferguson Community Center at 1050 Smith Avenue on Saturday night after the shooting . Police line: St. Louis County Police Chief Jon Belmar, left, and Ferguson Police Chief Thomas Jackson gather together over the search for the suspect  who shot an officer . Show of force: Officers line up with helmets on in front of a crowd which gathered in Ferguson Saturday night . The center is located  away from where the protesters had gathered and Belmar said he did not think the shooting was related to protests over Brown’s death. 'I don’t think it is,' he said. 'It didn’t happen within the proximity of the protest area. This is a fairly secluded area. I wouldn’t have any reason to think that it was linked in any way, shape or form. At the original press conference, Belmar said officers chased two suspects, and as he closed in on one of them, the second suspect pulled out a gun and fired at the officer, who was shot in one arm before returning fire. Officials have not released the officer’s name or said how long he had been on the police force. Mike O'Connell, communications director for the State Emergency Management Agency, said authorities were looking for the suspect. He said: 'A search is underway for the suspect in the shooting of the officer.' Search: Police officers from various forces have gathered in the area to deal with the aftermath of the shooting . Police from various different forces  responded to the scene  and set up a staging area on West Florissant Road, KMOV reported. Reports indicate that the suspect fled on foot into a neighborhood near West Florissant Avenue and Stein Street. Officers and police helicopters canvassed the area in an attempt to find the shooter. There were conflicting reports over the reason for the shooting which appears not to be linked to protests over the death of Michael Brown by a white police officer. NBC News reported an officer interrupted a burglary at a business on Saturday night and was shot in the arm once before the suspect ran off. Crowd: A small crowd of people have gathered near police close to the staging area . Scene: Police from various different forces along with helicopters have responded to the scene . Earlier on Saturday, an eyewitness told NBC News: 'Everybody was fine, they said, police stay back and then all of a sudden a black guy came pushing everybody, when I was right behind the chief.' Antonio French, an Alderman of the 21st ward in St. Louis tweeted: 'Police are being instructed to get in riot gear.' According to several eyewitness accounts on Twitter, Captain Ron Johnson of the Missouri Highway Patrol told protestors: 'I'm going to ask you to leave and I'm only asking you one time.' In a video posted on social media Johnson then told protesters that the officer was the only person who was shot. Meanwhile, a small, peaceful crowd of local residents gathered near the police staging area on West Florissant Road. Crowd: A police car drives past a small peaceful crowd of local residents as officers hunt for the suspect . Eyewitness: This unnamed eyewitness told NBC News everything was calm until 'a black guy came pushing everybody' Suspect: Chief Jackson said police are hunting for the suspect and he did not think the shooting was related to protests over Michael Brown’s death at the hands of white police officer Darren Brown . Vice News reports that a St Louis Metropolitan Police officer was injured when a black sedan crossed two lanes of traffic and fired into his personal vehicle on Interstate 70. An unknown number of shooters produced handguns and fired at the vehicle. The driver of the vehicle was injured by the shattered window. Officials say the officer was wearing the pants of his St Louis Metropolitan Police uniform, but not the shirt or jacket. The incident is being investigated and it is unclear whether it was a targeted attempt on the officer's life. The shootings came just days after Ferguson police chief Tom Jackson issued a video apology to Brown's parents following weeks of heavy criticism and calls for him to resign. In the video released on Thursday Jackson apologized to the parents for taking so long to remove the body of their son from the street. Sorry: Ferguson Police Chief Tom Jackson released this video apology on Thursday to the family of  Michael Brown and apologized for leaving the teenager's body in the street for so long . Dead: Police took more than four hours to remove Brown's body from the street where he was shot August 9 . Shot and killed: Michael Brown, left, was unarmed when he was shot dead by Police Officer Darren Wilson . 'I want to say this to the Brown family: No one who has not experienced the loss of a child can understand what you're feeling. I am truly sorry for the loss of your son,' Jackson said. But his apology was not well-received among some and led to reported protests hours after it was issued. The grieving parents of Michael Brown said they were unmoved by the apology made by the Ferguson police chief - over a month after their unarmed teenage son was shot dead by an officer. Michael Brown's mother, Lesley McSpadden, called for Chief Jackson to be fired and his father, Michael Brown Sr., said rather than an apology, he would like to see Officer Darren Wilson arrested. Michael Brown, an unarmed black teenager was fatally shot by Officer Darren Wilson on August 9, despite several eyewitness accounts which suggested he had his hands up in a 'don't shoot' position. His death sparked days of protests, some violent and many peaceful, in the predominately black neighborhood of Ferguson Missouri where racial tensions reached boiling point. March: Protesters march in front of the police department during a rally in Ferguson, Missouri, September 26, 2014 following Thursday's clashes between policemen and demonstrators in Ferguson . Protesters: Residents said officers were excessively aggressive towards them and unleashed the national guard, tear gas and military equipment during the unrest . Support: The U.S. Justice Department has asked the Ferguson, Missouri, Police Department to order its officers not to wear bracelets in support of the white policeman who shot to death an unarmed black teenager . During the protests, residents said officers were excessively aggressive towards them and unleashed National Guard troops, tear gas and military equipment. In the video apology, Jackson apologized for the heavy handed response from his officers towards the peaceful protesters. But earlier this week, police officers from the St. Louis County Department were spotted wearing ‘I am Darren Wilson’ bracelets whilst on duty. Residents from Ferguson complained about the black bracelets - which had stark white lettering on them with the words 'I am Darren Wilson' emblazoned across them - during a meeting with federal officials.</td>\n",
              "      <td>St. Louis County Police Chief Jon Belmar said he does not believe the shooting was connected with the two protests occurring at the same time .\\nA police spokesperson said the officer was treated and released from the hospital and was expected to survive .\\nBelmar says the officer was wearing a body camera at the time but that it was off for unknown reasons .\\nThe officer was checking on a community center at the time .\\nA small peaceful crowd of local residents were gathered near police on West Florissant Road, away from the community center .\\nPolice reported the suspect was fleeing when he shot the officer .\\nThe officer returned fire though police say no one else was known to be shot and the suspect fled on foot .\\nPolice helicopters are canvassing the area in an attempt to find the shooter .\\nThere were conflicting reports early on over the reason for the shooting, and it is not known that it was linked to the death of Michael Brown .\\nAnother shooting on Interstate 70 involving an off-duty St Louis Metropolitan Police officer happened hours later .\\nThis week police officers brazenly  wore ‘I am Darren Wilson’ bracelets while on duty .\\nThe Justice Department asked the Ferguson Police Department  to order its officers not to wear the bracelets in support of Wilson .</td>\n",
              "      <td>2513b18a974d124277bad294a467b106ef5484cf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Scroll down for video . The Ferguson police officer who was shot in the arm was released from the hospital as the search for the shooter continues, police say. The officer was shot during a routine patrol near the new Ferguson Community Center and away from the protests related to the death of Michael Brown. The body camera worn by the officer was off at the time, though officials do not know why. Officials say that the officer saw the suspect standing behind the community center and gave chase after he fled. The suspect turned and fired at the officer, striking him in the left arm. The New York Times reported that the suspect fled into the woods, but that an hour and a half search by Ferguson and other departments turned up empty. The reports revised earlier statements that there were two shooters and that the officer was responding to a burglary. St Louis County Police spokesperson Sgt Brian Schellman said that the officer was wearing a body camera that was turned off for an unknown reason at the time of the shooting. Ferguson police began wearing body cameras in August that had been donated by two security companies. The officer was treated for non-life threatening injuries and officials believe he will live after being released from the hospital. Another shooting, this of an off-duty St Louis Metropolitan Police officer, happened hours later, though there are no indications it is related to events in Ferguson. An earlier press conference by St Louis County Police Chief Jon Belmar said police are hunting for the suspect and he did not think the shooting was related to protests over Michael Brown’s death at the hands of white police officer Darren Brown. Shooting: Missouri State Highway Patrol troopers stand posted at the corner of Chambers Road and West Florissant Avenue on Saturday in Ferguson as police search for a suspect in the shooting of a police officer . Location: Police have set up a staging area  on West Florissant Road in Ferguson, Missouri, according to local news station KMOV . Crime scene: A police officer takes pictures outside the Ferguson Community Center at 1050 Smith Avenue on Saturday night after the shooting . Police line: St. Louis County Police Chief Jon Belmar, left, and Ferguson Police Chief Thomas Jackson gather together over the search for the suspect  who shot an officer . Show of force: Officers line up with helmets on in front of a crowd which gathered in Ferguson Saturday night . The center is located  away from where the protesters had gathered and Belmar said he did not think the shooting was related to protests over Brown’s death. 'I don’t think it is,' he said. 'It didn’t happen within the proximity of the protest area. This is a fairly secluded area. I wouldn’t have any reason to think that it was linked in any way, shape or form. At the original press conference, Belmar said officers chased two suspects, and as he closed in on one of them, the second suspect pulled out a gun and fired at the officer, who was shot in one arm before returning fire. Officials have not released the officer’s name or said how long he had been on the police force. Mike O'Connell, communications director for the State Emergency Management Agency, said authorities were looking for the suspect. He said: 'A search is underway for the suspect in the shooting of the officer.' Search: Police officers from various forces have gathered in the area to deal with the aftermath of the shooting . Police from various different forces  responded to the scene  and set up a staging area on West Florissant Road, KMOV reported. Reports indicate that the suspect fled on foot into a neighborhood near West Florissant Avenue and Stein Street. Officers and police helicopters canvassed the area in an attempt to find the shooter. There were conflicting reports over the reason for the shooting which appears not to be linked to protests over the death of Michael Brown by a white police officer. NBC News reported an officer interrupted a burglary at a business on Saturday night and was shot in the arm once before the suspect ran off. Crowd: A small crowd of people have gathered near police close to the staging area . Scene: Police from various different forces along with helicopters have responded to the scene . Earlier on Saturday, an eyewitness told NBC News: 'Everybody was fine, they said, police stay back and then all of a sudden a black guy came pushing everybody, when I was right behind the chief.' Antonio French, an Alderman of the 21st ward in St. Louis tweeted: 'Police are being instructed to get in riot gear.' According to several eyewitness accounts on Twitter, Captain Ron Johnson of the Missouri Highway Patrol told protestors: 'I'm going to ask you to leave and I'm only asking you one time.' In a video posted on social media Johnson then told protesters that the officer was the only person who was shot. Meanwhile, a small, peaceful crowd of local residents gathered near the police staging area on West Florissant Road. Crowd: A police car drives past a small peaceful crowd of local residents as officers hunt for the suspect . Eyewitness: This unnamed eyewitness told NBC News everything was calm until 'a black guy came pushing everybody' Suspect: Chief Jackson said police are hunting for the suspect and he did not think the shooting was related to protests over Michael Brown’s death at the hands of white police officer Darren Brown . Vice News reports that a St Louis Metropolitan Police officer was injured when a black sedan crossed two lanes of traffic and fired into his personal vehicle on Interstate 70. An unknown number of shooters produced handguns and fired at the vehicle. The driver of the vehicle was injured by the shattered window. Officials say the officer was wearing the pants of his St Louis Metropolitan Police uniform, but not the shirt or jacket. The incident is being investigated and it is unclear whether it was a targeted attempt on the officer's life. The shootings came just days after Ferguson police chief Tom Jackson issued a video apology to Brown's parents following weeks of heavy criticism and calls for him to resign. In the video released on Thursday Jackson apologized to the parents for taking so long to remove the body of their son from the street. Sorry: Ferguson Police Chief Tom Jackson released this video apology on Thursday to the family of  Michael Brown and apologized for leaving the teenager's body in the street for so long . Dead: Police took more than four hours to remove Brown's body from the street where he was shot August 9 . Shot and killed: Michael Brown, left, was unarmed when he was shot dead by Police Officer Darren Wilson . 'I want to say this to the Brown family: No one who has not experienced the loss of a child can understand what you're feeling. I am truly sorry for the loss of your son,' Jackson said. But his apology was not well-received among some and led to reported protests hours after it was issued. The grieving parents of Michael Brown said they were unmoved by the apology made by the Ferguson police chief - over a month after their unarmed teenage son was shot dead by an officer. Michael Brown's mother, Lesley McSpadden, called for Chief Jackson to be fired and his father, Michael Brown Sr., said rather than an apology, he would like to see Officer Darren Wilson arrested. Michael Brown, an unarmed black teenager was fatally shot by Officer Darren Wilson on August 9, despite several eyewitness accounts which suggested he had his hands up in a 'don't shoot' position. His death sparked days of protests, some violent and many peaceful, in the predominately black neighborhood of Ferguson Missouri where racial tensions reached boiling point. March: Protesters march in front of the police department during a rally in Ferguson, Missouri, September 26, 2014 following Thursday's clashes between policemen and demonstrators in Ferguson . Protesters: Residents said officers were excessively aggressive towards them and unleashed the national guard, tear gas and military equipment during the unrest . Support: The U.S. Justice Department has asked the Ferguson, Missouri, Police Department to order its officers not to wear bracelets in support of the white policeman who shot to death an unarmed black teenager . During the protests, residents said officers were excessively aggressive towards them and unleashed National Guard troops, tear gas and military equipment. In the video apology, Jackson apologized for the heavy handed response from his officers towards the peaceful protesters. But earlier this week, police officers from the St. Louis County Department were spotted wearing ‘I am Darren Wilson’ bracelets whilst on duty. Residents from Ferguson complained about the black bracelets - which had stark white lettering on them with the words 'I am Darren Wilson' emblazoned across them - during a meeting with federal officials.</td>\n",
              "      <td>St. Louis County Police Chief Jon Belmar said he does not believe the shooting was connected with the two protests occurring at the same time .\\nA police spokesperson said the officer was treated and released from the hospital and was expected to survive .\\nBelmar says the officer was wearing a body camera at the time but that it was off for unknown reasons .\\nThere were conflicting reports early on over the reason for the shooting, and it is not known that it was linked to the death of Michael Brown .\\nAnother officer was shot in an incident believed to be unrelated to Ferguson hours later .</td>\n",
              "      <td>f12abf917117d93d51f10e59e87b67d6d1fe879d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>The son of boxing champ Evander Holyfield seems to be following in his father's athletic footsteps. Elijah Holyfield, a junior at Woodward Academy in College Park, Georgia, has just fielded an offer to play college football for the University of Oregon Ducks. That's not all either, as Yahoo Sports reports the running back has also received recruitment letters from Arkansas, Boston College, Cal, Duke, Georgia, Louisville, Michigan, Michigan State, Ohio State, Ole Miss and Wisconsin, to name just a few. Scroll down for video . Athletic genes: Elijah Holyfield (above during a visit to Ohio State) has been recruited to play college football for over 20 major universities . Famous dad: Elijah (second from right) is the son of boxing champ Evander Holyfield (third from left, pictured with his children) In fact, the young Holyfield is so good, he has been ranked as the No. 137 recruit in the country by ESPN. Overall, 22 different colleges have extended offers to the 17-year-old. 'Oregon Just offered #GoDucks!' Elijah wrote on Twitter yesterday. Elijah, Evander's only child with ex-wife Dr. Janice Itson, is also a state champion in track. And not only is Evander proud of his son, he also seems to think he will be a better athlete than he ever was. '[My dad] was saying how different it was for me and him,' Elijah told The Oregonian. 'He was picking up, as far as people knowing him, at a later age. At a younger age, he didn't have a lot of money and was a small kid and wasn't that popular. He was saying how different it is for (me), because I'm so young and all the people on me, talking to me all the time – because of him, because of me playing football. He just said to keep a cool head and just remember the main focus.' Multitalented: Elijah (far right) is also a track star, as his 4 x 100 relay team (above) placed sixth at the Georgia state championship last year . Painful memories: Evander (right) will long be remembered as the only 4-time World Heavyweight Champion, and for having fellow boxer Mike Tyson (left) bite his ear during a fight . Evander made millions over the course of his career, and is still the only 4-time World Heavyweight Champion in the history of boxing. Despite his huge success, he is perhaps best known to many for an incident in 1996 when fellow boxer Mike Tyson bit off part of his ear during his match. As for his son, Elijah says he is excited to visit Oregon later this year, but has made no choice yet on what college he might attend. 'They are always good and looked good against Michigan State this year,' Elijah said of the Ducks. 'I love the way they run the ball. That's the main thing I'm concerned with.'</td>\n",
              "      <td>Elijah Holyfield, the son of boxing champ Evander Holyfield, has received an offer to play for the University of Oregon Ducks .\\nThe high school junior has also fielded offers from over 20 other universities, including Boston College and Michigan .\\nThe running back has been ranked as the No. 137 recruit in the country by ESPN .</td>\n",
              "      <td>f912f04ac3925d228fb6dc43344951bc5c4c91e6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>The son of boxing champ Evander Holyfield seems to be following in his father's athletic footsteps. Elijah Holyfield, a junior at Woodward Academy in College Park, Georgia, has just fielded an offer to play college football for the University of Oregon Ducks. That's not all either, as Yahoo Sports reports the running back has also received recruitment letters from Arkansas, Boston College, Cal, Duke, Georgia, Louisville, Michigan, Michigan State, Ohio State, Ole Miss and Wisconsin, to name just a few. Scroll down for video . Athletic genes: Elijah Holyfield (above during a visit to Ohio State) has been recruited to play college football for over 20 major universities . Famous dad: Elijah (second from right) is the son of boxing champ Evander Holyfield (third from left, pictured with his children) In fact, the young Holyfield is so good, he has been ranked as the No. 137 recruit in the country by ESPN. Overall, 22 different colleges have extended offers to the 17-year-old. 'Oregon Just offered #GoDucks!' Elijah wrote on Twitter yesterday. Elijah, Evander's only child with ex-wife Dr. Janice Itson, is also a state champion in track. And not only is Evander proud of his son, he also seems to think he will be a better athlete than he ever was. '[My dad] was saying how different it was for me and him,' Elijah told The Oregonian. 'He was picking up, as far as people knowing him, at a later age. At a younger age, he didn't have a lot of money and was a small kid and wasn't that popular. He was saying how different it is for (me), because I'm so young and all the people on me, talking to me all the time – because of him, because of me playing football. He just said to keep a cool head and just remember the main focus.' Multitalented: Elijah (far right) is also a track star, as his 4 x 100 relay team (above) placed sixth at the Georgia state championship last year . Painful memories: Evander (right) will long be remembered as the only 4-time World Heavyweight Champion, and for having fellow boxer Mike Tyson (left) bite his ear during a fight . Evander made millions over the course of his career, and is still the only 4-time World Heavyweight Champion in the history of boxing. Despite his huge success, he is perhaps best known to many for an incident in 1996 when fellow boxer Mike Tyson bit off part of his ear during his match. As for his son, Elijah says he is excited to visit Oregon later this year, but has made no choice yet on what college he might attend. 'They are always good and looked good against Michigan State this year,' Elijah said of the Ducks. 'I love the way they run the ball. That's the main thing I'm concerned with.'</td>\n",
              "      <td>Elijah Holyfield, the son of boxing champ Evander Holyfield, has received an offer to play for the University of Oregon Ducks .\\nThe high school junior has also fielded offers from over 20 other universities, including Boston College and Michigan .\\nThe running back ranked as the No.137 recruit in the country by ESPN .</td>\n",
              "      <td>ee44f8ed179218193000cb33e38b3c1a2cc0a034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>The youngest daughter of the crazed Florida father who shot and killed two of his three children last week only survived because her older sister acted as a human shield. Lauren Mohney, nine, is currently in a stable condition only because her sister, Savanna Mohney, 14, put herself in the way of her father, David Mohney's bullets during his shooting spree at the family home in Port Orange. When officers responded to the home they found the girls mother, Cynthia Mohney, 48, curled up between her two daughters and the estranged couple's son, David dead in his bed. Scroll down for video . Happy family: A look at Cynthia Mohney (far right) with her three children Lauren, Savanna and David, who were all shot by their father David Mohney . Still stable: Savanna, 14, (left) saved her little sister, Lauren (right) from their father and the nine-year-old is currently at a local hospital and in a medically induced coma . Court filings showed David Mohney wanted to leave his wife and move with daughter Savanna (pictured) and her siblings to South Dakota . 'We believe Savanna was coming down the stairs because she suspected something bad had happened (to her brother),' said state attorney Zachary Stoumbos to the Journal Online. 'But then changed her mind and went to her younger sister's bedroom to protect her.' Stoumbus said he belives Savanna's selfless actions saved her younger sister from their crazed father, who locked his wife out of the house after telling her he was going to kill their children because she wanted a divorce. 'Savanna was draped over Lauren,' said Stoumbos. 'We believe that because she protected her little sister, Lauren's injuries were not as bad.' The nine-year-old is currently in a coma at the Arnold Palmer Hospital in Orlando. Her mother has not left her side. According to Cynthia Mohney's lawyer, David Mohney told her on the morning of the murders, 'If you don't come back to me and stop the divorce, I will kill our children.' The retired Army sergeant then reportedly punched her in the mouth at their Port Orange, Florida, home, and added, 'If you’re going to take my kids away, you will not have them and you will go through the rest of your life without children.' Killer and a coward: David (above) killed himself after killing his children . Zachary Stoumbos, the attorney for grieving mother Cynthia, says her husband woke her up early that morning saying that something was wrong with their son, before taking her to the kitchen and showing that he had a gun. 'He continued to tell her, \"You will be back with me,\" and, \"This divorce was going to stop,\"' Stoumbos told WFTV. Then, when Cynthia ran to get help, she heard shots fired while at a neighbor's home. She immediately ran back to the house and was found lying between her two daughter when authorities arrived. 'His hate took over his love for his children,' added Stoumbos. 'I can't imagine that level of evil, that level of hate.' David had claimed his wife Cynthia was a violent, abusive alcoholic in court papers filed just two months before he gunned down his children Friday in their quiet, middle-class neighborhood just south of Daytona Beach. The girls: Cynthia, with daughter Savanna and Lauren, has not left the side of her only surviving child, Lauren, since the incident occurred . Smiling girls: The two sisters, Lauren and Savanna and their brother, David, before their father decided to try and kill them . However, neighbors said both parents were troubled. In a 911 call before the shooting, a neighbor told a dispatcher that neither parent should have had children because they were 'a little bit selfish and self-centered' and said 'you can't believe either one of them.' He had recently finished chiropractic school while she supported the family working as a physician's assistant, making $220,000. Nearly a month after he filed for divorce, the husband sought a protective injunction against his wife June 3. He said in court papers his wife had been drinking heavily and slapping him and their children on their chests, backs and arms. Florida's child welfare agency said Cynthia had recently been treated for substance abuse. The slain children were 11-year-old David Mohney and 14-year-old Savanna Mohney, Volusia County Sheriff Ben Johnson said. Nine-year-old Lauren Mohney was also was shot and in stable condition at a hospital. Cynthia  wasn't injured. 'If he wants to commit suicide, let him commit suicide. To shoot the children, that's cowardly,' Johnson said at a news conference outside the family's home. Deputies received a call at 5:11am Friday. Arriving deputies found the two girls in an upstairs bedroom and the boy in a bedroom on the first floor. Their father was in the kitchen with a handgun next to him. Survivor: The wife was at the house of a neighbor trying to get help in their Port Orange, Florida, neighborhood when the shooting occurred while the children were asleep at the home . In court papers, David Mohney said he and his children 'prefer living in a climate with snow to celebrate Christmas and other holidays.' Pictured: victim Savanna Mohney, 14 . Little guy: Young David was pronounced dead on the scene . In his divorce filing, David said his family moved to Florida in 2010 so he could study at Palmer College of Chiropractic's campus in Port Orange. He graduated in September 2013, but failed his first time taking the board exam and hoped to try again soon. He said in the court papers that returning to Rapid City, South Dakota, had been his family's plan all along, but his wife had become opposed to it. He said his children also wanted to make the 1,900-mile move to get relief from their allergies and because they found the Midwest region better suited their 'morals and values.' David Mohney said he and his children also 'prefer living in a climate with snow to celebrate Christmas and other holidays.' When he filed for divorce, David Mohney said little about problems with his wife. He made more specific accusations when he asked a Volusia County court for a protective injunction June 3. In that filing, David Mohney cited four instances between April 4 and May 29 in which he said his wife had slapped one or more of their children. He said his wife would hit the children — sometimes several times — for arguing with each other, talking back to her, or for not picking up their toys and clothes when she told them to. 'The \"hits\" described in the examples are loud and hard, beyond corporal punishment,' David Mohney wrote. 'My children were crying, afraid and trying to duck or get away from Cynthia.' However, he dismissed the request for protection two weeks after filing it. The state's child abuse hotline was notified in June that Cynthia Mohney had a substance abuse problem after an incident at a restaurant. Cynthia Mohney followed through on treatment, according to the Department of Children and Families.</td>\n",
              "      <td>David Mohney shot his three children and then himself on Friday .\\nHis daughter, Lauren, who is just 9, survived and is in a stable condition .\\nHer sister, Savanna, 14, shielded Lauren from her father's attack .\\nTheir brother David, 11, was shot dead in his bed while he was asleep .\\nHe reportedly told his wife to call off the divorce or he would kill their children .\\nCourt documents David Mohney had long complained that his wife was an alcoholic who hit their children .\\nWife Cynthia was at a neighbor's house to get away from the man and get help for their kids, whom she said was threatening her with a gun .</td>\n",
              "      <td>8fb70c01ee0201889b925865ea0906c1e27ce0f6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>The youngest daughter of the crazed Florida father who shot and killed two of his three children last week only survived because her older sister acted as a human shield. Lauren Mohney, nine, is currently in a stable condition only because her sister, Savanna Mohney, 14, put herself in the way of her father, David Mohney's bullets during his shooting spree at the family home in Port Orange. When officers responded to the home they found the girls mother, Cynthia Mohney, 48, curled up between her two daughters and the estranged couple's son, David dead in his bed. Scroll down for video . Happy family: A look at Cynthia Mohney (far right) with her three children Lauren, Savanna and David, who were all shot by their father David Mohney . Still stable: Savanna, 14, (left) saved her little sister, Lauren (right) from their father and the nine-year-old is currently at a local hospital and in a medically induced coma . Court filings showed David Mohney wanted to leave his wife and move with daughter Savanna (pictured) and her siblings to South Dakota . 'We believe Savanna was coming down the stairs because she suspected something bad had happened (to her brother),' said state attorney Zachary Stoumbos to the Journal Online. 'But then changed her mind and went to her younger sister's bedroom to protect her.' Stoumbus said he belives Savanna's selfless actions saved her younger sister from their crazed father, who locked his wife out of the house after telling her he was going to kill their children because she wanted a divorce. 'Savanna was draped over Lauren,' said Stoumbos. 'We believe that because she protected her little sister, Lauren's injuries were not as bad.' The nine-year-old is currently in a coma at the Arnold Palmer Hospital in Orlando. Her mother has not left her side. According to Cynthia Mohney's lawyer, David Mohney told her on the morning of the murders, 'If you don't come back to me and stop the divorce, I will kill our children.' The retired Army sergeant then reportedly punched her in the mouth at their Port Orange, Florida, home, and added, 'If you’re going to take my kids away, you will not have them and you will go through the rest of your life without children.' Killer and a coward: David (above) killed himself after killing his children . Zachary Stoumbos, the attorney for grieving mother Cynthia, says her husband woke her up early that morning saying that something was wrong with their son, before taking her to the kitchen and showing that he had a gun. 'He continued to tell her, \"You will be back with me,\" and, \"This divorce was going to stop,\"' Stoumbos told WFTV. Then, when Cynthia ran to get help, she heard shots fired while at a neighbor's home. She immediately ran back to the house and was found lying between her two daughter when authorities arrived. 'His hate took over his love for his children,' added Stoumbos. 'I can't imagine that level of evil, that level of hate.' David had claimed his wife Cynthia was a violent, abusive alcoholic in court papers filed just two months before he gunned down his children Friday in their quiet, middle-class neighborhood just south of Daytona Beach. The girls: Cynthia, with daughter Savanna and Lauren, has not left the side of her only surviving child, Lauren, since the incident occurred . Smiling girls: The two sisters, Lauren and Savanna and their brother, David, before their father decided to try and kill them . However, neighbors said both parents were troubled. In a 911 call before the shooting, a neighbor told a dispatcher that neither parent should have had children because they were 'a little bit selfish and self-centered' and said 'you can't believe either one of them.' He had recently finished chiropractic school while she supported the family working as a physician's assistant, making $220,000. Nearly a month after he filed for divorce, the husband sought a protective injunction against his wife June 3. He said in court papers his wife had been drinking heavily and slapping him and their children on their chests, backs and arms. Florida's child welfare agency said Cynthia had recently been treated for substance abuse. The slain children were 11-year-old David Mohney and 14-year-old Savanna Mohney, Volusia County Sheriff Ben Johnson said. Nine-year-old Lauren Mohney was also was shot and in stable condition at a hospital. Cynthia  wasn't injured. 'If he wants to commit suicide, let him commit suicide. To shoot the children, that's cowardly,' Johnson said at a news conference outside the family's home. Deputies received a call at 5:11am Friday. Arriving deputies found the two girls in an upstairs bedroom and the boy in a bedroom on the first floor. Their father was in the kitchen with a handgun next to him. Survivor: The wife was at the house of a neighbor trying to get help in their Port Orange, Florida, neighborhood when the shooting occurred while the children were asleep at the home . In court papers, David Mohney said he and his children 'prefer living in a climate with snow to celebrate Christmas and other holidays.' Pictured: victim Savanna Mohney, 14 . Little guy: Young David was pronounced dead on the scene . In his divorce filing, David said his family moved to Florida in 2010 so he could study at Palmer College of Chiropractic's campus in Port Orange. He graduated in September 2013, but failed his first time taking the board exam and hoped to try again soon. He said in the court papers that returning to Rapid City, South Dakota, had been his family's plan all along, but his wife had become opposed to it. He said his children also wanted to make the 1,900-mile move to get relief from their allergies and because they found the Midwest region better suited their 'morals and values.' David Mohney said he and his children also 'prefer living in a climate with snow to celebrate Christmas and other holidays.' When he filed for divorce, David Mohney said little about problems with his wife. He made more specific accusations when he asked a Volusia County court for a protective injunction June 3. In that filing, David Mohney cited four instances between April 4 and May 29 in which he said his wife had slapped one or more of their children. He said his wife would hit the children — sometimes several times — for arguing with each other, talking back to her, or for not picking up their toys and clothes when she told them to. 'The \"hits\" described in the examples are loud and hard, beyond corporal punishment,' David Mohney wrote. 'My children were crying, afraid and trying to duck or get away from Cynthia.' However, he dismissed the request for protection two weeks after filing it. The state's child abuse hotline was notified in June that Cynthia Mohney had a substance abuse problem after an incident at a restaurant. Cynthia Mohney followed through on treatment, according to the Department of Children and Families.</td>\n",
              "      <td>David Mohney shot his three children and then killed himself on Friday .\\nHis daughter, Lauren, who is just 9, survived and is in a stable condition .\\nHer sister, Savanna, 14, shielded Lauren from her father's attack .\\nTheir brother David, 11, was shot dead in his bed while he was asleep .\\nHe reportedly told his wife to call off the divorce or he would kill their children .\\nCourt documents David Mohney had long complained that his wife was an alcoholic who hit their children .\\nWife Cynthia was at a neighbor's house to get away from the man and get help for their kids, whom she said was threatening her with a gun .</td>\n",
              "      <td>2a0098b2167126a1fd5e119e68d83cd01a11d8df</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-29cba5d9-4584-4e6c-bc52-a831f379c65d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-29cba5d9-4584-4e6c-bc52-a831f379c65d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-29cba5d9-4584-4e6c-bc52-a831f379c65d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-bf602fac-c05a-4545-a5f0-368b78ded68d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bf602fac-c05a-4545-a5f0-368b78ded68d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-bf602fac-c05a-4545-a5f0-368b78ded68d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_70ad9083-6fd2-4f66-8d43-671785216611\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_10')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_70ad9083-6fd2-4f66-8d43-671785216611 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_10');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               article  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               (CNN) -- A Florida exterminator whose dead daughter and injured son were found in his truck has been charged with attempted murder, and police were searching his Miami home Thursday night, police said. Chase Scott, spokesman for West Palm Beach Police, told CNN that officers were executing a search warrant for evidence in the home of Jorge and Carmen Barahona. Jorge Barahona, 53, was found unconscious beside his pest-control truck early Monday along a south Florida interstate by a road assistance ranger, along with his 10-year-old adopted son, who was inside the vehicle next to an open gas can, according to a probable-cause affidavit filed by detectives. Hours later, crews removing toxic chemicals from the truck discovered the boy's twin sister dead in a plastic bag. Earlier Thursday, Barahona was taken to a hospital Thursday after he \"attempted to harm himself,\" police said. Barahona, who was in custody in the Palm Beach County Jail, suffered a self-inflicted injury after deputies told him to get ready to go to a court hearing Thursday morning, West Palm Beach Police spokesman Scott Chase said. \"He immediately attempted to harm himself by thrusting himself backwards, causing an injury to his head,\" Chase said. \"He was immediately checked by emergency personnel and it was decided he was OK to appear in court.\" However, Barahona \"refused to cooperate\" by not speaking and the judge decided to delay the hearing until another date, Chase said. Authorities later decided to take Barahona to Wellington Regional Medical Center for observation, he said. Meanwhile, a medical examiner has determined a cause of death for the girl, but it will not be made public until investigators review the findings, Chase said Thursday. Authorities likely will decide whether and how to charge Barahona in her death based on the autopsy results, Police Capt. Mary Olsen said Wednesday. The children were among the four the Barahonas adopted from Florida's foster care system. When the boy was found Monday by the roadside assistance ranger, he \"appeared to be in respiratory distress and (was) trembling\" and his clothing \"was soaked with an unknown chemical,\" the probable-cause affidavit said. The ranger then found Barahona on the ground beside the truck and called for help. The boy was hospitalized in intensive care with severe burns to his abdomen, upper thighs and buttocks, the affidavit said. While examining the boy, doctors noted he had sustained previous injuries, including a broken collarbone, a broken arm, scarring to his buttocks and lower abdomen, and ligature marks on both wrists, police said. After Barahona and his son were taken to a hospital, a worker decontaminating the truck discovered the body of the girl, wrapped in a plastic bag, the document said. Barahona told police he was distraught over the death of his daughter, and had intended to commit suicide by dousing himself with gasoline and setting himself afire, the affidavit said. Barahona said he didn't go through with his suicide plan because his son was with him, the document added. \"Basically, to paraphrase, he was stating that he placed his daughter in a plastic bag being distraught over her death,\" Chase told reporters Wednesday. \"He drove here from South Florida accompanied by his son. \"He then pulled off to the side of the road saying that he poured gas on his self, intending to light himself on fire. His son's head was in his lap and he decided, after giving his son some sleeping pills, that he wasn't going to do that.\" Barahona told police that in dousing himself with gasoline, he inadvertently got some on the boy, Olsen said Wednesday. But his story doesn't add up, because there was no gasoline on the boy, she said. Instead, he was covered with another chemical whose composition had yet to be determined. \"That's why we're still treating this as a hazmat (hazardous materials case),\" Olsen said. The truck in which the children were found was taken to a secure location, Chase said, where an FBI evidence recovery team is going through the vehicle. Authorities are waiting for test results on the chemicals found in the truck. Chase said the substance on the boy's body and clothes was so potent that staff caring for the boy at the hospital became ill as well. The boy, who was transferred Wednesday morning to a specialized burn unit at Miami's Jackson Memorial Hospital, has not been able to talk to investigators because he is on a breathing tube, Olsen explained. Asked whether Barahona has expressed remorse, she said, \"He feels remorse, but we're not getting consistent statements with what we're seeing in our evidence.\" \"It's a complex case,\" she added. At a hearing Wednesday in Miami attended by Barahona's wife, Carmen, a judge ordered that the remaining two children in the home be placed in foster care. Florida's Department of Children and Families had opened a child protection investigation within the past few days to look into a complaint involving the Barahona family, and it wasn't the first such complaint, spokesman Mark Riordan said. Reporters in the courtroom Wednesday heard tales of abuse, mainly concerning the twins, from state officials and experts. The caller to the child protection hotline in the latest case reported that the twins were routinely locked in a bathroom for long periods of time and had been bound with tape, the court heard. The story was corroborated by interviews with the other two children in the home, officials said in court. An investigator told the court that she went to the family's home on February 11 but had not seen the children. Instead, she said, she left after speaking with Carmen Barahona, planning to return on Monday. Asked why she did not return sooner, she said, \"I'm not allowed to do investigations on a weekend.\" However, a spokesman for the department, John Harrell, said it is the job of investigators to follow through immediately or refer to someone else in the department to follow through when a matter is urgent. CNN's Kim Segal contributed to this report.   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               (CNN) -- A Florida exterminator whose dead daughter and injured son were found in his truck has been charged with attempted murder, and police were searching his Miami home Thursday night, police said. Chase Scott, spokesman for West Palm Beach Police, told CNN that officers were executing a search warrant for evidence in the home of Jorge and Carmen Barahona. Jorge Barahona, 53, was found unconscious beside his pest-control truck early Monday along a south Florida interstate by a road assistance ranger, along with his 10-year-old adopted son, who was inside the vehicle next to an open gas can, according to a probable-cause affidavit filed by detectives. Hours later, crews removing toxic chemicals from the truck discovered the boy's twin sister dead in a plastic bag. Earlier Thursday, Barahona was taken to a hospital Thursday after he \"attempted to harm himself,\" police said. Barahona, who was in custody in the Palm Beach County Jail, suffered a self-inflicted injury after deputies told him to get ready to go to a court hearing Thursday morning, West Palm Beach Police spokesman Scott Chase said. \"He immediately attempted to harm himself by thrusting himself backwards, causing an injury to his head,\" Chase said. \"He was immediately checked by emergency personnel and it was decided he was OK to appear in court.\" However, Barahona \"refused to cooperate\" by not speaking and the judge decided to delay the hearing until another date, Chase said. Authorities later decided to take Barahona to Wellington Regional Medical Center for observation, he said. Meanwhile, a medical examiner has determined a cause of death for the girl, but it will not be made public until investigators review the findings, Chase said Thursday. Authorities likely will decide whether and how to charge Barahona in her death based on the autopsy results, Police Capt. Mary Olsen said Wednesday. The children were among the four the Barahonas adopted from Florida's foster care system. When the boy was found Monday by the roadside assistance ranger, he \"appeared to be in respiratory distress and (was) trembling\" and his clothing \"was soaked with an unknown chemical,\" the probable-cause affidavit said. The ranger then found Barahona on the ground beside the truck and called for help. The boy was hospitalized in intensive care with severe burns to his abdomen, upper thighs and buttocks, the affidavit said. While examining the boy, doctors noted he had sustained previous injuries, including a broken collarbone, a broken arm, scarring to his buttocks and lower abdomen, and ligature marks on both wrists, police said. After Barahona and his son were taken to a hospital, a worker decontaminating the truck discovered the body of the girl, wrapped in a plastic bag, the document said. Barahona told police he was distraught over the death of his daughter, and had intended to commit suicide by dousing himself with gasoline and setting himself afire, the affidavit said. Barahona said he didn't go through with his suicide plan because his son was with him, the document added. \"Basically, to paraphrase, he was stating that he placed his daughter in a plastic bag being distraught over her death,\" Chase told reporters Wednesday. \"He drove here from South Florida accompanied by his son. \"He then pulled off to the side of the road saying that he poured gas on his self, intending to light himself on fire. His son's head was in his lap and he decided, after giving his son some sleeping pills, that he wasn't going to do that.\" Barahona told police that in dousing himself with gasoline, he inadvertently got some on the boy, Olsen said Wednesday. But his story doesn't add up, because there was no gasoline on the boy, she said. Instead, he was covered with another chemical whose composition had yet to be determined. \"That's why we're still treating this as a hazmat (hazardous materials case),\" Olsen said. The truck in which the children were found was taken to a secure location, Chase said, where an FBI evidence recovery team is going through the vehicle. Authorities are waiting for test results on the chemicals found in the truck. Chase said the substance on the boy's body and clothes was so potent that staff caring for the boy at the hospital became ill as well. The boy, who was transferred Wednesday morning to a specialized burn unit at Miami's Jackson Memorial Hospital, has not been able to talk to investigators because he is on a breathing tube, Olsen explained. Asked whether Barahona has expressed remorse, she said, \"He feels remorse, but we're not getting consistent statements with what we're seeing in our evidence.\" \"It's a complex case,\" she added. At a hearing Wednesday in Miami attended by Barahona's wife, Carmen, a judge ordered that the remaining two children in the home be placed in foster care. Florida's Department of Children and Families had opened a child protection investigation within the past few days to look into a complaint involving the Barahona family, and it wasn't the first such complaint, spokesman Mark Riordan said. Reporters in the courtroom Wednesday heard tales of abuse, mainly concerning the twins, from state officials and experts. The caller to the child protection hotline in the latest case reported that the twins were routinely locked in a bathroom for long periods of time and had been bound with tape, the court heard. The story was corroborated by interviews with the other two children in the home, officials said in court. An investigator told the court that she went to the family's home on February 11 but had not seen the children. Instead, she said, she left after speaking with Carmen Barahona, planning to return on Monday. Asked why she did not return sooner, she said, \"I'm not allowed to do investigations on a weekend.\" However, a spokesman for the department, John Harrell, said it is the job of investigators to follow through immediately or refer to someone else in the department to follow through when a matter is urgent. CNN's Kim Segal contributed to this report.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (CNN) -- Freed Iranian-American journalist Roxana Saberi thanked friends and family Tuesday for their support during her ordeal in an Iranian prison, and said she plans to spend the next few days relaxing. Roxana Saberi smiles ouside her home in Tehran, Iran, on Tuesday. \"I am, of course, very happy to be free and to be with my parents again,\" a smiling Saberi, 32, told reporters. Saberi, who was dressed in a black tunic and a blue headscarf, said she was only now learning of a global support campaign on her behalf. \"I want to thank all the people all over the world, who, whether they knew me or not, helped me and my family during this period,\" she said. \"I don't have any specific plans for the moment. I just want to be with my parents and my friends and to relax.\" Reza Saberi, her father, said they plan to leave Iran soon. Saberi was convicted last month on espionage charges in a one-day trial that was closed to the public. She was sentenced to eight years in prison after being accused of spying for the United States. A judge changed Saberi's sentence during an appeal hearing Monday. The court agreed with her lawyers that, because Iran is not at war with the United States, Saberi cannot be punished for cooperating with agents of a hostile nation, according to Saberi's spokesman, Abdolsamad Khorramshahi. Her sentence was changed to a two-year jail term, suspended for five years, Iran's state-run news agency IRNA reported. Saberi was detained in January after initially being accused of buying a bottle of wine and working as a journalist without proper accreditation, according to the Committee to Protect Journalists, an advocacy group. She was soon charged with espionage. Saberi went on a hunger strike while imprisoned, but her father said she has since put on some weight.   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (CNN) -- Freed Iranian-American journalist Roxana Saberi thanked friends and family Tuesday for their support during her ordeal in an Iranian prison, and said she plans to spend the next few days relaxing. Roxana Saberi smiles ouside her home in Tehran, Iran, on Tuesday. \"I am, of course, very happy to be free and to be with my parents again,\" a smiling Saberi, 32, told reporters. Saberi, who was dressed in a black tunic and a blue headscarf, said she was only now learning of a global support campaign on her behalf. \"I want to thank all the people all over the world, who, whether they knew me or not, helped me and my family during this period,\" she said. \"I don't have any specific plans for the moment. I just want to be with my parents and my friends and to relax.\" Reza Saberi, her father, said they plan to leave Iran soon. Saberi was convicted last month on espionage charges in a one-day trial that was closed to the public. She was sentenced to eight years in prison after being accused of spying for the United States. A judge changed Saberi's sentence during an appeal hearing Monday. The court agreed with her lawyers that, because Iran is not at war with the United States, Saberi cannot be punished for cooperating with agents of a hostile nation, according to Saberi's spokesman, Abdolsamad Khorramshahi. Her sentence was changed to a two-year jail term, suspended for five years, Iran's state-run news agency IRNA reported. Saberi was detained in January after initially being accused of buying a bottle of wine and working as a journalist without proper accreditation, according to the Committee to Protect Journalists, an advocacy group. She was soon charged with espionage. Saberi went on a hunger strike while imprisoned, but her father said she has since put on some weight.   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (CNN) -- It's called the \"Walmart of Weed.\" This newest big-box store is devoted to selling marijuana-growing equipment. Its biggest outlet yet is to open Wednesday in Phoenix, where Arizona voters approved medical cannabis last fall. The weGrow Store will be the first outside California, where medical weed has long been legalized. None of the chain's three stores sell the drug. Instead, they unabashedly promote medical marijuana by offering \"everything -- from supplies to services -- that cultivators need to grow it,\" the company says. What sets weGrow apart from the mom-and-pop stores selling the growing supplies is that weGrow openly states its hydroponic equipment is intended for pot, a spokeswoman said. The smaller stores often claim the hydroponic equipment is used for growing indoor tomatoes and often kick out a customer if they broach the subject of medical cannabis, said weGrow spokeswoman Melissa DiGianfilippo. While 16 states and the District of Columbia now allow for some sort of medical marijuana, there remains ambiguity about whether U.S. authorities would ever enforce federal laws against marijuana distribution, though 90% of hydroponics revenue is from cannabis growers, DiGianfilippo said. In fact, the state of Arizona filed a federal lawsuit last week asking the courts to determine whether the voter-approved initiative legalizing medical marijuana is, indeed, legal. Calling itself \"the nation's only hydroponics franchise that openly talks about medical marijuana,\" the Oakland, California-based weGrow sees a boom in Arizona and projects 100,000 Arizonans acquiring state medical cannabis cards, DiGianfilippo said. So far, about 4,000 Arizonans became state-authorized marijuana users after the state began issuing the cards on April 15, DiGianfilippo said. WeGrow chose Wednesday to open its superstore because that was supposed to be the day the state was scheduled to begin accepting applications to run a marijuana dispensary. But Arizona Gov. Jan Brewer and Attorney General Tom Horne filed a federal lawsuit Friday seeking a declaratory judgment regarding the legality of Proposition 203. Voters approved the Arizona Medical Marijuana Act last November and Brewer signed it into law in December. Nonetheless, the federal government considers marijuana a controlled substance. In a May 2, 2011, letter, U.S. Attorney Dennis Burke of Arizona warned state officials that \"growing, distributing and possessing marijuana, in any capacity, other than as a federally authorized research program, is a violation of federal law regardless of state laws that purport to permit such activities,\" according to Brewer. Burke declared that his office would \"vigorously prosecute individuals and organizations that participate in unlawful manufacturing, distribution and marketing activity involving marijuana, even if such activities are permitted under state law,\" according to Brewer's office. \"The State of Arizona has worked to follow the wishes of voters,\" Brewer said in a statement. \"But I won't stand aside while state employees and average Arizonans acting in good faith are unwittingly put at risk. In light of the explicit warnings on this issue offered by Arizona's U.S. Attorney, as well as many other federal prosecutors, clarity and judicial direction are in order.\" Brewer has directed that the Arizona Department of Health Services put on hold its process for licensing marijuana dispensaries, pending court action on this issue, Brewer's spokesman, Matthew Benson, told CNN on Tuesday. \"The agency will continue providing registration cards for individuals who receive a doctor's recommendation to use medical marijuana,\" Benson said in an e-mail. WeGrow founder Dhar Mann, a 27-year-old entrepreneur who opened his first store in Oakland after a small hydroponics store ejected him for asking about medical marijuana cultivation, said his firm still sees an opportunity in Arizona, where he estimates medical pot to be a billion-dollar industry. He has plans for similar superstores in other states with legal marijuana. The Phoenix store, located in an industrial area so as not to upset any homeowners, will be 21,000 square feet. Wednesday's grand opening in Phoenix will feature indoor growing demonstrations with nonmarijuana plants, experts on how to build professional grow rooms, a physician for patient evaluations and classes on how to safely and responsibly cultivate medical marijuana, weGrow officials said. The first weGrow store was a 15,000-square-foot facility that opened in Oakland last year, and in February, a 10,000-square-foot store opened in Sacramento, California, a spokeswoman said. Mann said the Arizona law allows for up to 124 dispensaries, and he took exception with the governor's lawsuit. \"Delaying the process that a majority of the Arizona voters have asked for prevents qualified cannabis patients from receiving medicine they need and causes unfair financial hardship to businesspeople who have invested tens of thousands of dollars in reliance on the process set up by the state,\" Mann said in a statement. \"Medical marijuana cultivating, dispensing, and consuming will exist with or without governmental approval,\" Mann said. \"It's time to tax and regulate this industry so legitimate business people and patients are not criminalized.\"   \n",
              "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (CNN) -- It's called the \"Walmart of Weed.\" This newest big-box store is devoted to selling marijuana-growing equipment. Its biggest outlet yet is to open Wednesday in Phoenix, where Arizona voters approved medical cannabis last fall. The weGrow Store will be the first outside California, where medical weed has long been legalized. None of the chain's three stores sell the drug. Instead, they unabashedly promote medical marijuana by offering \"everything -- from supplies to services -- that cultivators need to grow it,\" the company says. What sets weGrow apart from the mom-and-pop stores selling the growing supplies is that weGrow openly states its hydroponic equipment is intended for pot, a spokeswoman said. The smaller stores often claim the hydroponic equipment is used for growing indoor tomatoes and often kick out a customer if they broach the subject of medical cannabis, said weGrow spokeswoman Melissa DiGianfilippo. While 16 states and the District of Columbia now allow for some sort of medical marijuana, there remains ambiguity about whether U.S. authorities would ever enforce federal laws against marijuana distribution, though 90% of hydroponics revenue is from cannabis growers, DiGianfilippo said. In fact, the state of Arizona filed a federal lawsuit last week asking the courts to determine whether the voter-approved initiative legalizing medical marijuana is, indeed, legal. Calling itself \"the nation's only hydroponics franchise that openly talks about medical marijuana,\" the Oakland, California-based weGrow sees a boom in Arizona and projects 100,000 Arizonans acquiring state medical cannabis cards, DiGianfilippo said. So far, about 4,000 Arizonans became state-authorized marijuana users after the state began issuing the cards on April 15, DiGianfilippo said. WeGrow chose Wednesday to open its superstore because that was supposed to be the day the state was scheduled to begin accepting applications to run a marijuana dispensary. But Arizona Gov. Jan Brewer and Attorney General Tom Horne filed a federal lawsuit Friday seeking a declaratory judgment regarding the legality of Proposition 203. Voters approved the Arizona Medical Marijuana Act last November and Brewer signed it into law in December. Nonetheless, the federal government considers marijuana a controlled substance. In a May 2, 2011, letter, U.S. Attorney Dennis Burke of Arizona warned state officials that \"growing, distributing and possessing marijuana, in any capacity, other than as a federally authorized research program, is a violation of federal law regardless of state laws that purport to permit such activities,\" according to Brewer. Burke declared that his office would \"vigorously prosecute individuals and organizations that participate in unlawful manufacturing, distribution and marketing activity involving marijuana, even if such activities are permitted under state law,\" according to Brewer's office. \"The State of Arizona has worked to follow the wishes of voters,\" Brewer said in a statement. \"But I won't stand aside while state employees and average Arizonans acting in good faith are unwittingly put at risk. In light of the explicit warnings on this issue offered by Arizona's U.S. Attorney, as well as many other federal prosecutors, clarity and judicial direction are in order.\" Brewer has directed that the Arizona Department of Health Services put on hold its process for licensing marijuana dispensaries, pending court action on this issue, Brewer's spokesman, Matthew Benson, told CNN on Tuesday. \"The agency will continue providing registration cards for individuals who receive a doctor's recommendation to use medical marijuana,\" Benson said in an e-mail. WeGrow founder Dhar Mann, a 27-year-old entrepreneur who opened his first store in Oakland after a small hydroponics store ejected him for asking about medical marijuana cultivation, said his firm still sees an opportunity in Arizona, where he estimates medical pot to be a billion-dollar industry. He has plans for similar superstores in other states with legal marijuana. The Phoenix store, located in an industrial area so as not to upset any homeowners, will be 21,000 square feet. Wednesday's grand opening in Phoenix will feature indoor growing demonstrations with nonmarijuana plants, experts on how to build professional grow rooms, a physician for patient evaluations and classes on how to safely and responsibly cultivate medical marijuana, weGrow officials said. The first weGrow store was a 15,000-square-foot facility that opened in Oakland last year, and in February, a 10,000-square-foot store opened in Sacramento, California, a spokeswoman said. Mann said the Arizona law allows for up to 124 dispensaries, and he took exception with the governor's lawsuit. \"Delaying the process that a majority of the Arizona voters have asked for prevents qualified cannabis patients from receiving medicine they need and causes unfair financial hardship to businesspeople who have invested tens of thousands of dollars in reliance on the process set up by the state,\" Mann said in a statement. \"Medical marijuana cultivating, dispensing, and consuming will exist with or without governmental approval,\" Mann said. \"It's time to tax and regulate this industry so legitimate business people and patients are not criminalized.\"   \n",
              "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Burglars have raided the home of former world champion boxer Ricky Hatton in Manchester stealing Rolex watches, credit cards, and a wad of 500 Euro notes. Police believe the detached property in Hyde was targeted after Mr Hatton tweeted on Saturday that he would be spending the day in London filming for Soccer AM and watching football. Mr Hatton spent the rest of the weekend in the capital before returning home on Sunday to find a window had been broken and the property ransacked. Robbed: Burglars have ransacked the Manchester home of former world champion boxer Ricky Hatton stealing two Rolex watches, a selection of credit cards, and a wad of 500 Euro notes . The thieves took two Rolex watches, a selection of credit cards, and a stack of 500 Euro notes thought to be worth a considerable sum. At 12.30 on Saturday he tweeted that he was going to London to film Soccer AM, before going on to watch his team Manchester City play Chelsea at Stamford Bridge that night. Mr Hatton, who was known as 'the Hitman' during his 15-year career, is estranged from his family and moved to the house recently, meaning a burglar alarm had not been fitted. Targeted: Police in Hyde, Greater Manchester, think Mr Hatton may have been hit after tweeting that he was off to London over the weekend . Raid: Mr Hatton, who is estranged from his family, moved into the property recently and did not have a burglar alarm fitted when the thefts took place . There were workmen at the house today installing a security system. Also at the property were forensic officers from Hyde CID dusting for fingerprints. Mr Hatton, who is now a boxing promoter, said: 'I am devastated about what’s happened. It makes you feel gutted that someone can do this to you. 'Anyone with information should get onto the police as soon as possible because I’ve put up a reward.'   \n",
              "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Burglars have raided the home of former world champion boxer Ricky Hatton in Manchester stealing Rolex watches, credit cards, and a wad of 500 Euro notes. Police believe the detached property in Hyde was targeted after Mr Hatton tweeted on Saturday that he would be spending the day in London filming for Soccer AM and watching football. Mr Hatton spent the rest of the weekend in the capital before returning home on Sunday to find a window had been broken and the property ransacked. Robbed: Burglars have ransacked the Manchester home of former world champion boxer Ricky Hatton stealing two Rolex watches, a selection of credit cards, and a wad of 500 Euro notes . The thieves took two Rolex watches, a selection of credit cards, and a stack of 500 Euro notes thought to be worth a considerable sum. At 12.30 on Saturday he tweeted that he was going to London to film Soccer AM, before going on to watch his team Manchester City play Chelsea at Stamford Bridge that night. Mr Hatton, who was known as 'the Hitman' during his 15-year career, is estranged from his family and moved to the house recently, meaning a burglar alarm had not been fitted. Targeted: Police in Hyde, Greater Manchester, think Mr Hatton may have been hit after tweeting that he was off to London over the weekend . Raid: Mr Hatton, who is estranged from his family, moved into the property recently and did not have a burglar alarm fitted when the thefts took place . There were workmen at the house today installing a security system. Also at the property were forensic officers from Hyde CID dusting for fingerprints. Mr Hatton, who is now a boxing promoter, said: 'I am devastated about what’s happened. It makes you feel gutted that someone can do this to you. 'Anyone with information should get onto the police as soon as possible because I’ve put up a reward.'   \n",
              "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Congressman Jared Polis . (D) Colorado: District 02 . Congressman Jason Chaffetz . (R) Utah: District 03 .   \n",
              "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Congressman Jared Polis . (D) Colorado: District 02 . Congressman Jason Chaffetz . (R) Utah: District 03 .   \n",
              "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Everett, Washington (CNN)  -- After more than two years of delays, Boeing's 787 Dreamliner made its maiden flight Tuesday in a three-hour trip that the maker described as a success. \"Today is truly a proud and historic day for the global team who has worked tirelessly to design and build the 787 Dreamliner -- the first all-new jet airplane of the 21st century,\" said Scott Fancher, vice president and general manager of the 787 program, in a news release. \"We look forward to the upcoming flight test program and soon bringing groundbreaking levels of efficiency, technology and passenger comfort to airlines and the flying public.\" More than 12,000 employees and guests watched as the plane took off at 10:27 a.m. from Paine Field in Everett, Washington. It landed more than three hours later and about 40 miles away at Seattle, Washington's Boeing Field after having flown at a speed of 207 mph at 15,000 feet -- typical for a maiden flight, the company said. During their time aloft, the chief pilot and captain tested some of the airplane's systems and structures while flight data were transmitted electronically to engineers at Boeing Field. \"The flight marks the beginning of a flight test program that will see six airplanes flying nearly around the clock and around the globe, with the airplane's first delivery scheduled for fourth quarter 2010,\" Boeing's news release said. Boeing promises passengers \"a better flying experience\" that includes bigger windows, more luggage space and better lighting. It promises airline operators greater efficiency by burning 20 percent less fuel than current models of comparable size and by providing as much as 45 percent more space for cargo. So far, 55 customers have ordered 840 of the planes. The official price of one is $150 million. \"We think this is going to be a very efficient airplane,\" Jim Albaugh, Boeing executive vice president and CEO, told CNN. \"It's going to change the way people travel.\" Despite the delays, Boeing's first new commercial airliner in more than a decade will still be relevant, Albaugh said Monday. \"It's more environmentally friendly, it's more efficient, uses less fuel, it's going to cost the operator less to fly, it's going to allow the passengers to pay less and feel better when they land,\" he said. Boeing's fuel claims are linked to its design. It is the first major airliner to be made mostly of composite materials and, as a result, is lighter. Depending on the configuration, the plane can seat 200 to 300 passengers and can travel more than 2,500 nautical miles. But production delays and technical problems have stolen some of the Dreamliner's luster. Many of the snags in the supply line have been blamed on the army of partners Boeing brought in to help with the construction. \"They did too much outsourcing, too soon, with too little oversight,\" said Scott Hamilton of the aviation consulting firm Leeham Co. \"The customers have been mightily [upset] over the creeping delays.\" Albaugh acknowledged that, \"in hindsight,\" the level of outsourcing may not have been the best strategy. \"There a few things we might have kept inside, yes,\" he said. With 10 months of flight tests ahead, the 787s won't start flying commercially until at least 2011, the company said. \"There's a lot of work to do,\" Albaugh said.   \n",
              "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Everett, Washington (CNN)  -- After more than two years of delays, Boeing's 787 Dreamliner made its maiden flight Tuesday in a three-hour trip that the maker described as a success. \"Today is truly a proud and historic day for the global team who has worked tirelessly to design and build the 787 Dreamliner -- the first all-new jet airplane of the 21st century,\" said Scott Fancher, vice president and general manager of the 787 program, in a news release. \"We look forward to the upcoming flight test program and soon bringing groundbreaking levels of efficiency, technology and passenger comfort to airlines and the flying public.\" More than 12,000 employees and guests watched as the plane took off at 10:27 a.m. from Paine Field in Everett, Washington. It landed more than three hours later and about 40 miles away at Seattle, Washington's Boeing Field after having flown at a speed of 207 mph at 15,000 feet -- typical for a maiden flight, the company said. During their time aloft, the chief pilot and captain tested some of the airplane's systems and structures while flight data were transmitted electronically to engineers at Boeing Field. \"The flight marks the beginning of a flight test program that will see six airplanes flying nearly around the clock and around the globe, with the airplane's first delivery scheduled for fourth quarter 2010,\" Boeing's news release said. Boeing promises passengers \"a better flying experience\" that includes bigger windows, more luggage space and better lighting. It promises airline operators greater efficiency by burning 20 percent less fuel than current models of comparable size and by providing as much as 45 percent more space for cargo. So far, 55 customers have ordered 840 of the planes. The official price of one is $150 million. \"We think this is going to be a very efficient airplane,\" Jim Albaugh, Boeing executive vice president and CEO, told CNN. \"It's going to change the way people travel.\" Despite the delays, Boeing's first new commercial airliner in more than a decade will still be relevant, Albaugh said Monday. \"It's more environmentally friendly, it's more efficient, uses less fuel, it's going to cost the operator less to fly, it's going to allow the passengers to pay less and feel better when they land,\" he said. Boeing's fuel claims are linked to its design. It is the first major airliner to be made mostly of composite materials and, as a result, is lighter. Depending on the configuration, the plane can seat 200 to 300 passengers and can travel more than 2,500 nautical miles. But production delays and technical problems have stolen some of the Dreamliner's luster. Many of the snags in the supply line have been blamed on the army of partners Boeing brought in to help with the construction. \"They did too much outsourcing, too soon, with too little oversight,\" said Scott Hamilton of the aviation consulting firm Leeham Co. \"The customers have been mightily [upset] over the creeping delays.\" Albaugh acknowledged that, \"in hindsight,\" the level of outsourcing may not have been the best strategy. \"There a few things we might have kept inside, yes,\" he said. With 10 months of flight tests ahead, the 787s won't start flying commercially until at least 2011, the company said. \"There's a lot of work to do,\" Albaugh said.   \n",
              "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Former England footballer Paul Gascoigne looked down in the dumps as he put his bins out in what is his first public appearance since he was sectioned following an alcohol meltdown. The 47-year-old looked downcast when he was spotted wearing a blue dressing gown, tartan pyjama bottoms and slip-on shoes outside his property in Poole, Dorset. The former midfielder, who was sectioned for his own safety last month, was also sporting scars to his face following a recent fall while he carried out the chore at 1pm on Thursday. Scroll down for video . Former England footballer Paul Gascoigne looked down in the dumps as he put his bins out in Poole, Dorset . The former midfielder looked downcast as he carried out the chore in his dressing gown and pyjama bottoms . It is the first time Gascoigne, who retired from football in 2004, has been pictured in public since being sectioned under the Mental Health Act on October 24. The star was placed on a three-day emergency detox after being rushed to hospital in the early hours following an apparent alcohol binge. He is now said to be recovering at home with his new 44-year-old girlfriend Mandy Thomas, who he only recently started seeing. She is said to have vowed to stand by him and is helping him with his recovery. Less than 48 hours prior to his hospitalisation last month, the pair were pictured out and about and onlookers said they had been discussing their plans for the future. They are said to have met after the former Newcastle, Tottenham and Rangers star visited the post office she runs in Bournemouth, Dorset. The star was placed on a three-day emergency detox last month after being rushed to hospital in the early hours following an apparent alcohol binge. He is now said to be recovering at home with his new girlfriend . It is the first time Gascoigne, who retired from football in 2004, has been pictured in public since being sectioned under the Mental Health Act on October 24. He has suffered a lengthy battle with alcoholism . Ms Thomas, who is separated from her husband and has two children, is said to be 'besotted' with the star with whom she has become 'inseparable'. She reportedly helped him move into his new apartment in the expensive area of Sandbanks in Poole in August. It was shortly after he was released from Poole Hospital after another alcoholic breakdown that he was found slumped outside his flat clutching a bottle of gin. Friends said the new couple viewed the move as a fresh start and said that Ms Thomas hopes to help him through the most recent 'setback'. Gascoigne and his new love Mandy Thomas appear to be the  picture of domestic bliss as they pose together in the garden of his new home two days prior to his hospitalisation. Ms Thomas (right) is supporting him . The 47-year-old star was rushed to hospital at 3am on October 24 and sectioned under the Mental Health Act . The football ace has suffered a long battle with alcoholism and was previously sectioned six years ago. The star, who made 57 appearances for his country, has been in rehab seven times and was first sectioned under the Mental Health Act in 2008, following an incident at a Newcastle hotel. His seventh stint in rehab was in January this year when he checked into a £6,000-a-month treatment clinic in Southampton. Last month, the star was pictured cradling a chicken while enjoying a pint and speaking to locals in a pub beer garden. At the time of his hospitalisation, Gascoigne's agent Terry Baker told MailOnline that the star 'is not very well' but refused to comment on his condition any further. Paul Gascoigne leans in for a kiss  with his girlfriend Mandy Thomas who was with him in hospital last month . Gascogine looking weak and frail in August  (left) and in his footballing glory days playing for England (right) Former England star Gascoigne netted two huge fish during an outing with a friend in Hampshire last month .   \n",
              "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Former England footballer Paul Gascoigne looked down in the dumps as he put his bins out in what is his first public appearance since he was sectioned following an alcohol meltdown. The 47-year-old looked downcast when he was spotted wearing a blue dressing gown, tartan pyjama bottoms and slip-on shoes outside his property in Poole, Dorset. The former midfielder, who was sectioned for his own safety last month, was also sporting scars to his face following a recent fall while he carried out the chore at 1pm on Thursday. Scroll down for video . Former England footballer Paul Gascoigne looked down in the dumps as he put his bins out in Poole, Dorset . The former midfielder looked downcast as he carried out the chore in his dressing gown and pyjama bottoms . It is the first time Gascoigne, who retired from football in 2004, has been pictured in public since being sectioned under the Mental Health Act on October 24. The star was placed on a three-day emergency detox after being rushed to hospital in the early hours following an apparent alcohol binge. He is now said to be recovering at home with his new 44-year-old girlfriend Mandy Thomas, who he only recently started seeing. She is said to have vowed to stand by him and is helping him with his recovery. Less than 48 hours prior to his hospitalisation last month, the pair were pictured out and about and onlookers said they had been discussing their plans for the future. They are said to have met after the former Newcastle, Tottenham and Rangers star visited the post office she runs in Bournemouth, Dorset. The star was placed on a three-day emergency detox last month after being rushed to hospital in the early hours following an apparent alcohol binge. He is now said to be recovering at home with his new girlfriend . It is the first time Gascoigne, who retired from football in 2004, has been pictured in public since being sectioned under the Mental Health Act on October 24. He has suffered a lengthy battle with alcoholism . Ms Thomas, who is separated from her husband and has two children, is said to be 'besotted' with the star with whom she has become 'inseparable'. She reportedly helped him move into his new apartment in the expensive area of Sandbanks in Poole in August. It was shortly after he was released from Poole Hospital after another alcoholic breakdown that he was found slumped outside his flat clutching a bottle of gin. Friends said the new couple viewed the move as a fresh start and said that Ms Thomas hopes to help him through the most recent 'setback'. Gascoigne and his new love Mandy Thomas appear to be the  picture of domestic bliss as they pose together in the garden of his new home two days prior to his hospitalisation. Ms Thomas (right) is supporting him . The 47-year-old star was rushed to hospital at 3am on October 24 and sectioned under the Mental Health Act . The football ace has suffered a long battle with alcoholism and was previously sectioned six years ago. The star, who made 57 appearances for his country, has been in rehab seven times and was first sectioned under the Mental Health Act in 2008, following an incident at a Newcastle hotel. His seventh stint in rehab was in January this year when he checked into a £6,000-a-month treatment clinic in Southampton. Last month, the star was pictured cradling a chicken while enjoying a pint and speaking to locals in a pub beer garden. At the time of his hospitalisation, Gascoigne's agent Terry Baker told MailOnline that the star 'is not very well' but refused to comment on his condition any further. Paul Gascoigne leans in for a kiss  with his girlfriend Mandy Thomas who was with him in hospital last month . Gascogine looking weak and frail in August  (left) and in his footballing glory days playing for England (right) Former England star Gascoigne netted two huge fish during an outing with a friend in Hampshire last month .   \n",
              "14  Scroll down for video . The Ferguson police officer who was shot in the arm was released from the hospital as the search for the shooter continues, police say. The officer was shot during a routine patrol near the new Ferguson Community Center and away from the protests related to the death of Michael Brown. The body camera worn by the officer was off at the time, though officials do not know why. Officials say that the officer saw the suspect standing behind the community center and gave chase after he fled. The suspect turned and fired at the officer, striking him in the left arm. The New York Times reported that the suspect fled into the woods, but that an hour and a half search by Ferguson and other departments turned up empty. The reports revised earlier statements that there were two shooters and that the officer was responding to a burglary. St Louis County Police spokesperson Sgt Brian Schellman said that the officer was wearing a body camera that was turned off for an unknown reason at the time of the shooting. Ferguson police began wearing body cameras in August that had been donated by two security companies. The officer was treated for non-life threatening injuries and officials believe he will live after being released from the hospital. Another shooting, this of an off-duty St Louis Metropolitan Police officer, happened hours later, though there are no indications it is related to events in Ferguson. An earlier press conference by St Louis County Police Chief Jon Belmar said police are hunting for the suspect and he did not think the shooting was related to protests over Michael Brown’s death at the hands of white police officer Darren Brown. Shooting: Missouri State Highway Patrol troopers stand posted at the corner of Chambers Road and West Florissant Avenue on Saturday in Ferguson as police search for a suspect in the shooting of a police officer . Location: Police have set up a staging area  on West Florissant Road in Ferguson, Missouri, according to local news station KMOV . Crime scene: A police officer takes pictures outside the Ferguson Community Center at 1050 Smith Avenue on Saturday night after the shooting . Police line: St. Louis County Police Chief Jon Belmar, left, and Ferguson Police Chief Thomas Jackson gather together over the search for the suspect  who shot an officer . Show of force: Officers line up with helmets on in front of a crowd which gathered in Ferguson Saturday night . The center is located  away from where the protesters had gathered and Belmar said he did not think the shooting was related to protests over Brown’s death. 'I don’t think it is,' he said. 'It didn’t happen within the proximity of the protest area. This is a fairly secluded area. I wouldn’t have any reason to think that it was linked in any way, shape or form. At the original press conference, Belmar said officers chased two suspects, and as he closed in on one of them, the second suspect pulled out a gun and fired at the officer, who was shot in one arm before returning fire. Officials have not released the officer’s name or said how long he had been on the police force. Mike O'Connell, communications director for the State Emergency Management Agency, said authorities were looking for the suspect. He said: 'A search is underway for the suspect in the shooting of the officer.' Search: Police officers from various forces have gathered in the area to deal with the aftermath of the shooting . Police from various different forces  responded to the scene  and set up a staging area on West Florissant Road, KMOV reported. Reports indicate that the suspect fled on foot into a neighborhood near West Florissant Avenue and Stein Street. Officers and police helicopters canvassed the area in an attempt to find the shooter. There were conflicting reports over the reason for the shooting which appears not to be linked to protests over the death of Michael Brown by a white police officer. NBC News reported an officer interrupted a burglary at a business on Saturday night and was shot in the arm once before the suspect ran off. Crowd: A small crowd of people have gathered near police close to the staging area . Scene: Police from various different forces along with helicopters have responded to the scene . Earlier on Saturday, an eyewitness told NBC News: 'Everybody was fine, they said, police stay back and then all of a sudden a black guy came pushing everybody, when I was right behind the chief.' Antonio French, an Alderman of the 21st ward in St. Louis tweeted: 'Police are being instructed to get in riot gear.' According to several eyewitness accounts on Twitter, Captain Ron Johnson of the Missouri Highway Patrol told protestors: 'I'm going to ask you to leave and I'm only asking you one time.' In a video posted on social media Johnson then told protesters that the officer was the only person who was shot. Meanwhile, a small, peaceful crowd of local residents gathered near the police staging area on West Florissant Road. Crowd: A police car drives past a small peaceful crowd of local residents as officers hunt for the suspect . Eyewitness: This unnamed eyewitness told NBC News everything was calm until 'a black guy came pushing everybody' Suspect: Chief Jackson said police are hunting for the suspect and he did not think the shooting was related to protests over Michael Brown’s death at the hands of white police officer Darren Brown . Vice News reports that a St Louis Metropolitan Police officer was injured when a black sedan crossed two lanes of traffic and fired into his personal vehicle on Interstate 70. An unknown number of shooters produced handguns and fired at the vehicle. The driver of the vehicle was injured by the shattered window. Officials say the officer was wearing the pants of his St Louis Metropolitan Police uniform, but not the shirt or jacket. The incident is being investigated and it is unclear whether it was a targeted attempt on the officer's life. The shootings came just days after Ferguson police chief Tom Jackson issued a video apology to Brown's parents following weeks of heavy criticism and calls for him to resign. In the video released on Thursday Jackson apologized to the parents for taking so long to remove the body of their son from the street. Sorry: Ferguson Police Chief Tom Jackson released this video apology on Thursday to the family of  Michael Brown and apologized for leaving the teenager's body in the street for so long . Dead: Police took more than four hours to remove Brown's body from the street where he was shot August 9 . Shot and killed: Michael Brown, left, was unarmed when he was shot dead by Police Officer Darren Wilson . 'I want to say this to the Brown family: No one who has not experienced the loss of a child can understand what you're feeling. I am truly sorry for the loss of your son,' Jackson said. But his apology was not well-received among some and led to reported protests hours after it was issued. The grieving parents of Michael Brown said they were unmoved by the apology made by the Ferguson police chief - over a month after their unarmed teenage son was shot dead by an officer. Michael Brown's mother, Lesley McSpadden, called for Chief Jackson to be fired and his father, Michael Brown Sr., said rather than an apology, he would like to see Officer Darren Wilson arrested. Michael Brown, an unarmed black teenager was fatally shot by Officer Darren Wilson on August 9, despite several eyewitness accounts which suggested he had his hands up in a 'don't shoot' position. His death sparked days of protests, some violent and many peaceful, in the predominately black neighborhood of Ferguson Missouri where racial tensions reached boiling point. March: Protesters march in front of the police department during a rally in Ferguson, Missouri, September 26, 2014 following Thursday's clashes between policemen and demonstrators in Ferguson . Protesters: Residents said officers were excessively aggressive towards them and unleashed the national guard, tear gas and military equipment during the unrest . Support: The U.S. Justice Department has asked the Ferguson, Missouri, Police Department to order its officers not to wear bracelets in support of the white policeman who shot to death an unarmed black teenager . During the protests, residents said officers were excessively aggressive towards them and unleashed National Guard troops, tear gas and military equipment. In the video apology, Jackson apologized for the heavy handed response from his officers towards the peaceful protesters. But earlier this week, police officers from the St. Louis County Department were spotted wearing ‘I am Darren Wilson’ bracelets whilst on duty. Residents from Ferguson complained about the black bracelets - which had stark white lettering on them with the words 'I am Darren Wilson' emblazoned across them - during a meeting with federal officials.   \n",
              "15  Scroll down for video . The Ferguson police officer who was shot in the arm was released from the hospital as the search for the shooter continues, police say. The officer was shot during a routine patrol near the new Ferguson Community Center and away from the protests related to the death of Michael Brown. The body camera worn by the officer was off at the time, though officials do not know why. Officials say that the officer saw the suspect standing behind the community center and gave chase after he fled. The suspect turned and fired at the officer, striking him in the left arm. The New York Times reported that the suspect fled into the woods, but that an hour and a half search by Ferguson and other departments turned up empty. The reports revised earlier statements that there were two shooters and that the officer was responding to a burglary. St Louis County Police spokesperson Sgt Brian Schellman said that the officer was wearing a body camera that was turned off for an unknown reason at the time of the shooting. Ferguson police began wearing body cameras in August that had been donated by two security companies. The officer was treated for non-life threatening injuries and officials believe he will live after being released from the hospital. Another shooting, this of an off-duty St Louis Metropolitan Police officer, happened hours later, though there are no indications it is related to events in Ferguson. An earlier press conference by St Louis County Police Chief Jon Belmar said police are hunting for the suspect and he did not think the shooting was related to protests over Michael Brown’s death at the hands of white police officer Darren Brown. Shooting: Missouri State Highway Patrol troopers stand posted at the corner of Chambers Road and West Florissant Avenue on Saturday in Ferguson as police search for a suspect in the shooting of a police officer . Location: Police have set up a staging area  on West Florissant Road in Ferguson, Missouri, according to local news station KMOV . Crime scene: A police officer takes pictures outside the Ferguson Community Center at 1050 Smith Avenue on Saturday night after the shooting . Police line: St. Louis County Police Chief Jon Belmar, left, and Ferguson Police Chief Thomas Jackson gather together over the search for the suspect  who shot an officer . Show of force: Officers line up with helmets on in front of a crowd which gathered in Ferguson Saturday night . The center is located  away from where the protesters had gathered and Belmar said he did not think the shooting was related to protests over Brown’s death. 'I don’t think it is,' he said. 'It didn’t happen within the proximity of the protest area. This is a fairly secluded area. I wouldn’t have any reason to think that it was linked in any way, shape or form. At the original press conference, Belmar said officers chased two suspects, and as he closed in on one of them, the second suspect pulled out a gun and fired at the officer, who was shot in one arm before returning fire. Officials have not released the officer’s name or said how long he had been on the police force. Mike O'Connell, communications director for the State Emergency Management Agency, said authorities were looking for the suspect. He said: 'A search is underway for the suspect in the shooting of the officer.' Search: Police officers from various forces have gathered in the area to deal with the aftermath of the shooting . Police from various different forces  responded to the scene  and set up a staging area on West Florissant Road, KMOV reported. Reports indicate that the suspect fled on foot into a neighborhood near West Florissant Avenue and Stein Street. Officers and police helicopters canvassed the area in an attempt to find the shooter. There were conflicting reports over the reason for the shooting which appears not to be linked to protests over the death of Michael Brown by a white police officer. NBC News reported an officer interrupted a burglary at a business on Saturday night and was shot in the arm once before the suspect ran off. Crowd: A small crowd of people have gathered near police close to the staging area . Scene: Police from various different forces along with helicopters have responded to the scene . Earlier on Saturday, an eyewitness told NBC News: 'Everybody was fine, they said, police stay back and then all of a sudden a black guy came pushing everybody, when I was right behind the chief.' Antonio French, an Alderman of the 21st ward in St. Louis tweeted: 'Police are being instructed to get in riot gear.' According to several eyewitness accounts on Twitter, Captain Ron Johnson of the Missouri Highway Patrol told protestors: 'I'm going to ask you to leave and I'm only asking you one time.' In a video posted on social media Johnson then told protesters that the officer was the only person who was shot. Meanwhile, a small, peaceful crowd of local residents gathered near the police staging area on West Florissant Road. Crowd: A police car drives past a small peaceful crowd of local residents as officers hunt for the suspect . Eyewitness: This unnamed eyewitness told NBC News everything was calm until 'a black guy came pushing everybody' Suspect: Chief Jackson said police are hunting for the suspect and he did not think the shooting was related to protests over Michael Brown’s death at the hands of white police officer Darren Brown . Vice News reports that a St Louis Metropolitan Police officer was injured when a black sedan crossed two lanes of traffic and fired into his personal vehicle on Interstate 70. An unknown number of shooters produced handguns and fired at the vehicle. The driver of the vehicle was injured by the shattered window. Officials say the officer was wearing the pants of his St Louis Metropolitan Police uniform, but not the shirt or jacket. The incident is being investigated and it is unclear whether it was a targeted attempt on the officer's life. The shootings came just days after Ferguson police chief Tom Jackson issued a video apology to Brown's parents following weeks of heavy criticism and calls for him to resign. In the video released on Thursday Jackson apologized to the parents for taking so long to remove the body of their son from the street. Sorry: Ferguson Police Chief Tom Jackson released this video apology on Thursday to the family of  Michael Brown and apologized for leaving the teenager's body in the street for so long . Dead: Police took more than four hours to remove Brown's body from the street where he was shot August 9 . Shot and killed: Michael Brown, left, was unarmed when he was shot dead by Police Officer Darren Wilson . 'I want to say this to the Brown family: No one who has not experienced the loss of a child can understand what you're feeling. I am truly sorry for the loss of your son,' Jackson said. But his apology was not well-received among some and led to reported protests hours after it was issued. The grieving parents of Michael Brown said they were unmoved by the apology made by the Ferguson police chief - over a month after their unarmed teenage son was shot dead by an officer. Michael Brown's mother, Lesley McSpadden, called for Chief Jackson to be fired and his father, Michael Brown Sr., said rather than an apology, he would like to see Officer Darren Wilson arrested. Michael Brown, an unarmed black teenager was fatally shot by Officer Darren Wilson on August 9, despite several eyewitness accounts which suggested he had his hands up in a 'don't shoot' position. His death sparked days of protests, some violent and many peaceful, in the predominately black neighborhood of Ferguson Missouri where racial tensions reached boiling point. March: Protesters march in front of the police department during a rally in Ferguson, Missouri, September 26, 2014 following Thursday's clashes between policemen and demonstrators in Ferguson . Protesters: Residents said officers were excessively aggressive towards them and unleashed the national guard, tear gas and military equipment during the unrest . Support: The U.S. Justice Department has asked the Ferguson, Missouri, Police Department to order its officers not to wear bracelets in support of the white policeman who shot to death an unarmed black teenager . During the protests, residents said officers were excessively aggressive towards them and unleashed National Guard troops, tear gas and military equipment. In the video apology, Jackson apologized for the heavy handed response from his officers towards the peaceful protesters. But earlier this week, police officers from the St. Louis County Department were spotted wearing ‘I am Darren Wilson’ bracelets whilst on duty. Residents from Ferguson complained about the black bracelets - which had stark white lettering on them with the words 'I am Darren Wilson' emblazoned across them - during a meeting with federal officials.   \n",
              "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The son of boxing champ Evander Holyfield seems to be following in his father's athletic footsteps. Elijah Holyfield, a junior at Woodward Academy in College Park, Georgia, has just fielded an offer to play college football for the University of Oregon Ducks. That's not all either, as Yahoo Sports reports the running back has also received recruitment letters from Arkansas, Boston College, Cal, Duke, Georgia, Louisville, Michigan, Michigan State, Ohio State, Ole Miss and Wisconsin, to name just a few. Scroll down for video . Athletic genes: Elijah Holyfield (above during a visit to Ohio State) has been recruited to play college football for over 20 major universities . Famous dad: Elijah (second from right) is the son of boxing champ Evander Holyfield (third from left, pictured with his children) In fact, the young Holyfield is so good, he has been ranked as the No. 137 recruit in the country by ESPN. Overall, 22 different colleges have extended offers to the 17-year-old. 'Oregon Just offered #GoDucks!' Elijah wrote on Twitter yesterday. Elijah, Evander's only child with ex-wife Dr. Janice Itson, is also a state champion in track. And not only is Evander proud of his son, he also seems to think he will be a better athlete than he ever was. '[My dad] was saying how different it was for me and him,' Elijah told The Oregonian. 'He was picking up, as far as people knowing him, at a later age. At a younger age, he didn't have a lot of money and was a small kid and wasn't that popular. He was saying how different it is for (me), because I'm so young and all the people on me, talking to me all the time – because of him, because of me playing football. He just said to keep a cool head and just remember the main focus.' Multitalented: Elijah (far right) is also a track star, as his 4 x 100 relay team (above) placed sixth at the Georgia state championship last year . Painful memories: Evander (right) will long be remembered as the only 4-time World Heavyweight Champion, and for having fellow boxer Mike Tyson (left) bite his ear during a fight . Evander made millions over the course of his career, and is still the only 4-time World Heavyweight Champion in the history of boxing. Despite his huge success, he is perhaps best known to many for an incident in 1996 when fellow boxer Mike Tyson bit off part of his ear during his match. As for his son, Elijah says he is excited to visit Oregon later this year, but has made no choice yet on what college he might attend. 'They are always good and looked good against Michigan State this year,' Elijah said of the Ducks. 'I love the way they run the ball. That's the main thing I'm concerned with.'   \n",
              "17                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The son of boxing champ Evander Holyfield seems to be following in his father's athletic footsteps. Elijah Holyfield, a junior at Woodward Academy in College Park, Georgia, has just fielded an offer to play college football for the University of Oregon Ducks. That's not all either, as Yahoo Sports reports the running back has also received recruitment letters from Arkansas, Boston College, Cal, Duke, Georgia, Louisville, Michigan, Michigan State, Ohio State, Ole Miss and Wisconsin, to name just a few. Scroll down for video . Athletic genes: Elijah Holyfield (above during a visit to Ohio State) has been recruited to play college football for over 20 major universities . Famous dad: Elijah (second from right) is the son of boxing champ Evander Holyfield (third from left, pictured with his children) In fact, the young Holyfield is so good, he has been ranked as the No. 137 recruit in the country by ESPN. Overall, 22 different colleges have extended offers to the 17-year-old. 'Oregon Just offered #GoDucks!' Elijah wrote on Twitter yesterday. Elijah, Evander's only child with ex-wife Dr. Janice Itson, is also a state champion in track. And not only is Evander proud of his son, he also seems to think he will be a better athlete than he ever was. '[My dad] was saying how different it was for me and him,' Elijah told The Oregonian. 'He was picking up, as far as people knowing him, at a later age. At a younger age, he didn't have a lot of money and was a small kid and wasn't that popular. He was saying how different it is for (me), because I'm so young and all the people on me, talking to me all the time – because of him, because of me playing football. He just said to keep a cool head and just remember the main focus.' Multitalented: Elijah (far right) is also a track star, as his 4 x 100 relay team (above) placed sixth at the Georgia state championship last year . Painful memories: Evander (right) will long be remembered as the only 4-time World Heavyweight Champion, and for having fellow boxer Mike Tyson (left) bite his ear during a fight . Evander made millions over the course of his career, and is still the only 4-time World Heavyweight Champion in the history of boxing. Despite his huge success, he is perhaps best known to many for an incident in 1996 when fellow boxer Mike Tyson bit off part of his ear during his match. As for his son, Elijah says he is excited to visit Oregon later this year, but has made no choice yet on what college he might attend. 'They are always good and looked good against Michigan State this year,' Elijah said of the Ducks. 'I love the way they run the ball. That's the main thing I'm concerned with.'   \n",
              "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   The youngest daughter of the crazed Florida father who shot and killed two of his three children last week only survived because her older sister acted as a human shield. Lauren Mohney, nine, is currently in a stable condition only because her sister, Savanna Mohney, 14, put herself in the way of her father, David Mohney's bullets during his shooting spree at the family home in Port Orange. When officers responded to the home they found the girls mother, Cynthia Mohney, 48, curled up between her two daughters and the estranged couple's son, David dead in his bed. Scroll down for video . Happy family: A look at Cynthia Mohney (far right) with her three children Lauren, Savanna and David, who were all shot by their father David Mohney . Still stable: Savanna, 14, (left) saved her little sister, Lauren (right) from their father and the nine-year-old is currently at a local hospital and in a medically induced coma . Court filings showed David Mohney wanted to leave his wife and move with daughter Savanna (pictured) and her siblings to South Dakota . 'We believe Savanna was coming down the stairs because she suspected something bad had happened (to her brother),' said state attorney Zachary Stoumbos to the Journal Online. 'But then changed her mind and went to her younger sister's bedroom to protect her.' Stoumbus said he belives Savanna's selfless actions saved her younger sister from their crazed father, who locked his wife out of the house after telling her he was going to kill their children because she wanted a divorce. 'Savanna was draped over Lauren,' said Stoumbos. 'We believe that because she protected her little sister, Lauren's injuries were not as bad.' The nine-year-old is currently in a coma at the Arnold Palmer Hospital in Orlando. Her mother has not left her side. According to Cynthia Mohney's lawyer, David Mohney told her on the morning of the murders, 'If you don't come back to me and stop the divorce, I will kill our children.' The retired Army sergeant then reportedly punched her in the mouth at their Port Orange, Florida, home, and added, 'If you’re going to take my kids away, you will not have them and you will go through the rest of your life without children.' Killer and a coward: David (above) killed himself after killing his children . Zachary Stoumbos, the attorney for grieving mother Cynthia, says her husband woke her up early that morning saying that something was wrong with their son, before taking her to the kitchen and showing that he had a gun. 'He continued to tell her, \"You will be back with me,\" and, \"This divorce was going to stop,\"' Stoumbos told WFTV. Then, when Cynthia ran to get help, she heard shots fired while at a neighbor's home. She immediately ran back to the house and was found lying between her two daughter when authorities arrived. 'His hate took over his love for his children,' added Stoumbos. 'I can't imagine that level of evil, that level of hate.' David had claimed his wife Cynthia was a violent, abusive alcoholic in court papers filed just two months before he gunned down his children Friday in their quiet, middle-class neighborhood just south of Daytona Beach. The girls: Cynthia, with daughter Savanna and Lauren, has not left the side of her only surviving child, Lauren, since the incident occurred . Smiling girls: The two sisters, Lauren and Savanna and their brother, David, before their father decided to try and kill them . However, neighbors said both parents were troubled. In a 911 call before the shooting, a neighbor told a dispatcher that neither parent should have had children because they were 'a little bit selfish and self-centered' and said 'you can't believe either one of them.' He had recently finished chiropractic school while she supported the family working as a physician's assistant, making $220,000. Nearly a month after he filed for divorce, the husband sought a protective injunction against his wife June 3. He said in court papers his wife had been drinking heavily and slapping him and their children on their chests, backs and arms. Florida's child welfare agency said Cynthia had recently been treated for substance abuse. The slain children were 11-year-old David Mohney and 14-year-old Savanna Mohney, Volusia County Sheriff Ben Johnson said. Nine-year-old Lauren Mohney was also was shot and in stable condition at a hospital. Cynthia  wasn't injured. 'If he wants to commit suicide, let him commit suicide. To shoot the children, that's cowardly,' Johnson said at a news conference outside the family's home. Deputies received a call at 5:11am Friday. Arriving deputies found the two girls in an upstairs bedroom and the boy in a bedroom on the first floor. Their father was in the kitchen with a handgun next to him. Survivor: The wife was at the house of a neighbor trying to get help in their Port Orange, Florida, neighborhood when the shooting occurred while the children were asleep at the home . In court papers, David Mohney said he and his children 'prefer living in a climate with snow to celebrate Christmas and other holidays.' Pictured: victim Savanna Mohney, 14 . Little guy: Young David was pronounced dead on the scene . In his divorce filing, David said his family moved to Florida in 2010 so he could study at Palmer College of Chiropractic's campus in Port Orange. He graduated in September 2013, but failed his first time taking the board exam and hoped to try again soon. He said in the court papers that returning to Rapid City, South Dakota, had been his family's plan all along, but his wife had become opposed to it. He said his children also wanted to make the 1,900-mile move to get relief from their allergies and because they found the Midwest region better suited their 'morals and values.' David Mohney said he and his children also 'prefer living in a climate with snow to celebrate Christmas and other holidays.' When he filed for divorce, David Mohney said little about problems with his wife. He made more specific accusations when he asked a Volusia County court for a protective injunction June 3. In that filing, David Mohney cited four instances between April 4 and May 29 in which he said his wife had slapped one or more of their children. He said his wife would hit the children — sometimes several times — for arguing with each other, talking back to her, or for not picking up their toys and clothes when she told them to. 'The \"hits\" described in the examples are loud and hard, beyond corporal punishment,' David Mohney wrote. 'My children were crying, afraid and trying to duck or get away from Cynthia.' However, he dismissed the request for protection two weeks after filing it. The state's child abuse hotline was notified in June that Cynthia Mohney had a substance abuse problem after an incident at a restaurant. Cynthia Mohney followed through on treatment, according to the Department of Children and Families.   \n",
              "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   The youngest daughter of the crazed Florida father who shot and killed two of his three children last week only survived because her older sister acted as a human shield. Lauren Mohney, nine, is currently in a stable condition only because her sister, Savanna Mohney, 14, put herself in the way of her father, David Mohney's bullets during his shooting spree at the family home in Port Orange. When officers responded to the home they found the girls mother, Cynthia Mohney, 48, curled up between her two daughters and the estranged couple's son, David dead in his bed. Scroll down for video . Happy family: A look at Cynthia Mohney (far right) with her three children Lauren, Savanna and David, who were all shot by their father David Mohney . Still stable: Savanna, 14, (left) saved her little sister, Lauren (right) from their father and the nine-year-old is currently at a local hospital and in a medically induced coma . Court filings showed David Mohney wanted to leave his wife and move with daughter Savanna (pictured) and her siblings to South Dakota . 'We believe Savanna was coming down the stairs because she suspected something bad had happened (to her brother),' said state attorney Zachary Stoumbos to the Journal Online. 'But then changed her mind and went to her younger sister's bedroom to protect her.' Stoumbus said he belives Savanna's selfless actions saved her younger sister from their crazed father, who locked his wife out of the house after telling her he was going to kill their children because she wanted a divorce. 'Savanna was draped over Lauren,' said Stoumbos. 'We believe that because she protected her little sister, Lauren's injuries were not as bad.' The nine-year-old is currently in a coma at the Arnold Palmer Hospital in Orlando. Her mother has not left her side. According to Cynthia Mohney's lawyer, David Mohney told her on the morning of the murders, 'If you don't come back to me and stop the divorce, I will kill our children.' The retired Army sergeant then reportedly punched her in the mouth at their Port Orange, Florida, home, and added, 'If you’re going to take my kids away, you will not have them and you will go through the rest of your life without children.' Killer and a coward: David (above) killed himself after killing his children . Zachary Stoumbos, the attorney for grieving mother Cynthia, says her husband woke her up early that morning saying that something was wrong with their son, before taking her to the kitchen and showing that he had a gun. 'He continued to tell her, \"You will be back with me,\" and, \"This divorce was going to stop,\"' Stoumbos told WFTV. Then, when Cynthia ran to get help, she heard shots fired while at a neighbor's home. She immediately ran back to the house and was found lying between her two daughter when authorities arrived. 'His hate took over his love for his children,' added Stoumbos. 'I can't imagine that level of evil, that level of hate.' David had claimed his wife Cynthia was a violent, abusive alcoholic in court papers filed just two months before he gunned down his children Friday in their quiet, middle-class neighborhood just south of Daytona Beach. The girls: Cynthia, with daughter Savanna and Lauren, has not left the side of her only surviving child, Lauren, since the incident occurred . Smiling girls: The two sisters, Lauren and Savanna and their brother, David, before their father decided to try and kill them . However, neighbors said both parents were troubled. In a 911 call before the shooting, a neighbor told a dispatcher that neither parent should have had children because they were 'a little bit selfish and self-centered' and said 'you can't believe either one of them.' He had recently finished chiropractic school while she supported the family working as a physician's assistant, making $220,000. Nearly a month after he filed for divorce, the husband sought a protective injunction against his wife June 3. He said in court papers his wife had been drinking heavily and slapping him and their children on their chests, backs and arms. Florida's child welfare agency said Cynthia had recently been treated for substance abuse. The slain children were 11-year-old David Mohney and 14-year-old Savanna Mohney, Volusia County Sheriff Ben Johnson said. Nine-year-old Lauren Mohney was also was shot and in stable condition at a hospital. Cynthia  wasn't injured. 'If he wants to commit suicide, let him commit suicide. To shoot the children, that's cowardly,' Johnson said at a news conference outside the family's home. Deputies received a call at 5:11am Friday. Arriving deputies found the two girls in an upstairs bedroom and the boy in a bedroom on the first floor. Their father was in the kitchen with a handgun next to him. Survivor: The wife was at the house of a neighbor trying to get help in their Port Orange, Florida, neighborhood when the shooting occurred while the children were asleep at the home . In court papers, David Mohney said he and his children 'prefer living in a climate with snow to celebrate Christmas and other holidays.' Pictured: victim Savanna Mohney, 14 . Little guy: Young David was pronounced dead on the scene . In his divorce filing, David said his family moved to Florida in 2010 so he could study at Palmer College of Chiropractic's campus in Port Orange. He graduated in September 2013, but failed his first time taking the board exam and hoped to try again soon. He said in the court papers that returning to Rapid City, South Dakota, had been his family's plan all along, but his wife had become opposed to it. He said his children also wanted to make the 1,900-mile move to get relief from their allergies and because they found the Midwest region better suited their 'morals and values.' David Mohney said he and his children also 'prefer living in a climate with snow to celebrate Christmas and other holidays.' When he filed for divorce, David Mohney said little about problems with his wife. He made more specific accusations when he asked a Volusia County court for a protective injunction June 3. In that filing, David Mohney cited four instances between April 4 and May 29 in which he said his wife had slapped one or more of their children. He said his wife would hit the children — sometimes several times — for arguing with each other, talking back to her, or for not picking up their toys and clothes when she told them to. 'The \"hits\" described in the examples are loud and hard, beyond corporal punishment,' David Mohney wrote. 'My children were crying, afraid and trying to duck or get away from Cynthia.' However, he dismissed the request for protection two weeks after filing it. The state's child abuse hotline was notified in June that Cynthia Mohney had a substance abuse problem after an incident at a restaurant. Cynthia Mohney followed through on treatment, according to the Department of Children and Families.   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      highlights  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            NEW: Jorge Barahona is charged with attempted murder; home is being searched .\\nBarahona \"attempted to harm himself\" Thursday morning, police say .\\nHe \"refused to cooperate\" by not speaking,  which postponed his hearing .\\nCause of daughter's death has been determined but not made public .   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             NEW: Jorge Barahona is charged with attempted murder; home is being searched .\\nBarahona \"attempted to harm himself\" Thursday morning, police say .\\nHe \"refused to cooperate\" by not speaking, which postponed his hearing .\\nCause of daughter's death has been determined but not made public .   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Father of Roxana Saberi says they plan to leave Iran soon .\\nSaberi, 32, was convicted last month on espionage charges .\\nHer sentence was changed to a two-year jail term, suspended for five years .\\nShe has lived in Iran since 2003, reporting for international news organizations .   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Father of Roxana Saberi says they plan to leave Iran soon .\\nSaberi, 33, was convicted last month on espionage charges .\\nHer sentence was changed to a two-year jail term, suspended for five years .\\nShe has lived in Iran since 2003, reporting for international news organizations .   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      The weGrow firm will open its third superstore in Phoenix on Wednesday .\\nIt openly sells marijuana-growing equipment but not the drug itself .\\nArizona has asked a federal court to rule on the legality of medical cannabis .\\nArizona voters approved medical marijuana in November .   \n",
              "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The weGrow firm will open its third superstore in Phoenix on Wednesday .\\nIt openly sells marijuana-growing equipment but not the drug itself .\\nThe state of Arizona has asked a federal court to rule on the legality of medical cannabis .\\nArizona voters approved medical marijuana in November .   \n",
              "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Ricky Hatton tweeted fans on Saturday saying he was going to London .\\nSpent weekend in the capital filming Soccer AM and watching football .\\nWhen he returned to Hyde, Greater Manchester, he found home ransacked .\\nHe had only just moved to property which didn't have burglar alarm fitted .   \n",
              "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Ricky Hatton's house was burgled as the former former boxer was away .\\nHatton was in London last weekend and watched Chelsea vs Man City .\\nWhen he returned to Hyde, Greater Manchester, he found home ransacked .\\nHe had only just moved to property which didn't have burglar alarm fitted .   \n",
              "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Two freshman representatives document their experience for CNN .\\nRep. Jared Polis is a Democrat representing Colorado's Second district .\\nRep. Jason Chaffetz is a Republican representing Utah's Third district .   \n",
              "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Video: Hand-held cams track freshmen moves .\\nTwo freshman representatives document their experience for CNN .\\nRep. Jared Polis is a Democrat representing Colorado's Second district .\\nRep. Jason Chaffetz is a Republican representing Utah's Third district .   \n",
              "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          NEW: 787 Dreamliner completes three-hour test flight .\\nBoeing has touted the 787 as more environmentally friendly and fuel efficient .\\nCompany says plane is made of composite materials than are lighter than aluminum .\\nDepending on configuration, Dreamliner can seat 200 to 300 passengers .   \n",
              "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          NEW: 787 Dreamliner completes three-hour test flight .\\nBoeing has touted the 787 as more environmentally friendly and fuel efficient .\\nCompany says plane is made of composite materials that are lighter than aluminum .\\nDepending on configuration, Dreamliner can seat 200 to 300 passengers .   \n",
              "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Gazza spotted for the first time since being sectioned last month .\\n47-year-old looked downcast as he put the bins out outside home in Poole .\\nSpotted in blue dressing gown and pyjama bottoms on Thursday afternoon .\\nFormer footballer put on emergency three-day detox in October after binge .\\nNew girlfriend Mandy Thomas has vowed to stand by him through ordeal .\\nFormer England star has suffered a long battle with alcoholism and drugs .   \n",
              "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Paul Gascoigne spotted for the first time since being sectioned last month .\\n47-year-old looked downcast as he put the bins out outside home in Poole .\\nSpotted in blue dressing gown and pyjama bottoms on Thursday afternoon .\\nFormer footballer put on emergency three-day detox in October after binge .\\nNew girlfriend Mandy Thomas has vowed to stand by him through ordeal .\\nFormer England star has suffered a long battle with alcoholism and drugs .   \n",
              "14  St. Louis County Police Chief Jon Belmar said he does not believe the shooting was connected with the two protests occurring at the same time .\\nA police spokesperson said the officer was treated and released from the hospital and was expected to survive .\\nBelmar says the officer was wearing a body camera at the time but that it was off for unknown reasons .\\nThe officer was checking on a community center at the time .\\nA small peaceful crowd of local residents were gathered near police on West Florissant Road, away from the community center .\\nPolice reported the suspect was fleeing when he shot the officer .\\nThe officer returned fire though police say no one else was known to be shot and the suspect fled on foot .\\nPolice helicopters are canvassing the area in an attempt to find the shooter .\\nThere were conflicting reports early on over the reason for the shooting, and it is not known that it was linked to the death of Michael Brown .\\nAnother shooting on Interstate 70 involving an off-duty St Louis Metropolitan Police officer happened hours later .\\nThis week police officers brazenly  wore ‘I am Darren Wilson’ bracelets while on duty .\\nThe Justice Department asked the Ferguson Police Department  to order its officers not to wear the bracelets in support of Wilson .   \n",
              "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       St. Louis County Police Chief Jon Belmar said he does not believe the shooting was connected with the two protests occurring at the same time .\\nA police spokesperson said the officer was treated and released from the hospital and was expected to survive .\\nBelmar says the officer was wearing a body camera at the time but that it was off for unknown reasons .\\nThere were conflicting reports early on over the reason for the shooting, and it is not known that it was linked to the death of Michael Brown .\\nAnother officer was shot in an incident believed to be unrelated to Ferguson hours later .   \n",
              "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Elijah Holyfield, the son of boxing champ Evander Holyfield, has received an offer to play for the University of Oregon Ducks .\\nThe high school junior has also fielded offers from over 20 other universities, including Boston College and Michigan .\\nThe running back has been ranked as the No. 137 recruit in the country by ESPN .   \n",
              "17                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Elijah Holyfield, the son of boxing champ Evander Holyfield, has received an offer to play for the University of Oregon Ducks .\\nThe high school junior has also fielded offers from over 20 other universities, including Boston College and Michigan .\\nThe running back ranked as the No.137 recruit in the country by ESPN .   \n",
              "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   David Mohney shot his three children and then himself on Friday .\\nHis daughter, Lauren, who is just 9, survived and is in a stable condition .\\nHer sister, Savanna, 14, shielded Lauren from her father's attack .\\nTheir brother David, 11, was shot dead in his bed while he was asleep .\\nHe reportedly told his wife to call off the divorce or he would kill their children .\\nCourt documents David Mohney had long complained that his wife was an alcoholic who hit their children .\\nWife Cynthia was at a neighbor's house to get away from the man and get help for their kids, whom she said was threatening her with a gun .   \n",
              "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            David Mohney shot his three children and then killed himself on Friday .\\nHis daughter, Lauren, who is just 9, survived and is in a stable condition .\\nHer sister, Savanna, 14, shielded Lauren from her father's attack .\\nTheir brother David, 11, was shot dead in his bed while he was asleep .\\nHe reportedly told his wife to call off the divorce or he would kill their children .\\nCourt documents David Mohney had long complained that his wife was an alcoholic who hit their children .\\nWife Cynthia was at a neighbor's house to get away from the man and get help for their kids, whom she said was threatening her with a gun .   \n",
              "\n",
              "                                          id  \n",
              "0   202f7a6543cc6b750304b9552d67e341287909ad  \n",
              "1   c812abf550e08e3e91fa5961f30aa888ab92f249  \n",
              "2   57aad99a31217dcf1ce2086c1cdc70981f7db28b  \n",
              "3   edffbeec92ef3fb2994768405c0b78b23766e5e6  \n",
              "4   81f493b60c66aa78de3ec8f1171af1f44adfce85  \n",
              "5   5bfcd50db0e8c9a3ebf74c531c30bf7e8bccfa08  \n",
              "6   84458757b948dee87a7dc6316941549a005d2c25  \n",
              "7   4e88476070bd6b867745b4d904738637d92b16d2  \n",
              "8   86bd905861391cbd3a98de15c83768b6d1400304  \n",
              "9   fd4bd93f0e11cec9a6c3f50441b6023b1e582581  \n",
              "10  2079196ea8cb59b2a75babf1d2c0f3eb3ff620e0  \n",
              "11  dbfff7e9eaac5ba8e4d1657efd799e65d4eb4628  \n",
              "12  9ca32ad1aacaadbe94b2398586085c918ac0e2e5  \n",
              "13  768fbe852864cdb0f0f5ca3fea025fae16c499a4  \n",
              "14  2513b18a974d124277bad294a467b106ef5484cf  \n",
              "15  f12abf917117d93d51f10e59e87b67d6d1fe879d  \n",
              "16  f912f04ac3925d228fb6dc43344951bc5c4c91e6  \n",
              "17  ee44f8ed179218193000cb33e38b3c1a2cc0a034  \n",
              "18  8fb70c01ee0201889b925865ea0906c1e27ce0f6  \n",
              "19  2a0098b2167126a1fd5e119e68d83cd01a11d8df  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "highlight_counts = df_train.groupby(\"article\")[\"highlights\"].nunique()\n",
        "articles_with_diff_highlights = highlight_counts[highlight_counts > 1].index\n",
        "print(f\"Number of articles with different highlights: {len(articles_with_diff_highlights)}\")\n",
        "\n",
        "df_10 = df_train[df_train[\"article\"].isin(articles_with_diff_highlights)]\n",
        "\n",
        "df_10 = df_10.sort_values(by=[\"article\", \"highlights\"]).reset_index(drop=True)\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "display(df_10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szFfFL-7utEe",
        "outputId": "c0d73e61-5d66-4ab1-ebd2-6cbce9ab0b9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dropped 10 rows from df_train.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(284005, 3)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Rows in df_10 that should be removed\n",
        "rows_to_remove_in_df10 = [0, 3, 5,6, 8, 9, 11, 12, 16,19]\n",
        "\n",
        "# Get 'article' and 'highlights' for these rows\n",
        "rows_to_remove = df_10.loc[rows_to_remove_in_df10, [\"article\", \"highlights\"]]\n",
        "\n",
        "# Remove them from df_train by matching both columns\n",
        "df_train_cleaned = df_train.merge(rows_to_remove, on=[\"article\", \"highlights\"], how=\"left\", indicator=True)\n",
        "df_train_cleaned = df_train_cleaned[df_train_cleaned[\"_merge\"] == \"left_only\"].drop(columns=\"_merge\")\n",
        "\n",
        "print(f\"Dropped 10 rows from df_train.\")\n",
        "\n",
        "# Optional: overwrite df_train if you want to keep working with it\n",
        "df_train = df_train_cleaned\n",
        "df_train_cleaned.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAAeoM9t6i27"
      },
      "source": [
        "These dups were closely studied and only the ones that were truly paraphrased in a significant form, were preserved. The rest with trivial changes (one word difference) or factual errors were dropped to avoid data memorization and lowering the learning rate.\n",
        "In past work (Lee et al., 2022, Sec. 4.2) near duplicates above a high similarity threshold are removed even if they’re not exact matches, because the gain in true diversity outweighs the loss of minor variants."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hVA91cFgMW3"
      },
      "source": [
        "\n",
        "\n",
        "> **train and val overlap**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbMnAjdeIFby",
        "outputId": "72c5f7fb-9225-465d-d851-7f0b3921271d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of overlapping articles (train-val): 1\n",
            "\n",
            "--- Overlapping article ---\n",
            "\n",
            "The Irishman left fighting for life by his brother in an alleged one-punch attack has called for the charges against his sibling to be dropped. Patrick Lyttle today accompanied his elder brother Barry to court for the first time, wearing a dark beanie. 'This should not be going ahead, it's not a matter for the court,' Patrick told reporters following a brief court mention on Thursday. Barry is facing reckless grievous bodily harm charges after he allegedly struck his brother in Kings Cross on January 3. Scroll down for video . Patrick Lyttle (pictured) appeared at Downing Centre Local Court in Sydney today wearing a beanie . Patrick was accompanying his brother, Barry (left), who has been charged with reckless grievous bodily harm for the alleged assault on his brother . Patrick, Barry and father Oliver appeared in good spirits at the trial today, with Patrick saying the family were hoping for a resolution soon . Patrick, who said he was doing well, explained his appearance at Downing Centre Local Court with: 'Of course I'm here, he's my brother. 'We love each other.' Barry told assembled media the family were hoping for a resolution to the matter shortly, so 'we can all go home as a family'. Patrick had not previously appeared in public since he woke up from an induced coma five days following the alleged attack. Patrick Lyttle has walked out of hospital following a one-punch attack by his brother Barry on January 3 . Patrick stood side-by-side with his brother Barry and parents in another shot, taken a month after his injuries . Before the attack: Barry uploaded this photo with Patrick, right, and their father Oliver, centre, hours after his brother woke up . The brothers' father, Oliver, has stood by Barry at each court hearing so far. A court  loosened Barry's bail conditions in February, allowing him to move closer to the St Vincent's Hospital in Sydney's inner east, where Patrick was recovering. Daily Mail Australia exclusively broke the news had woken up, to the relief of his relatives, on January 8. The matter has been adjourned until March 19. Barry Lyttle, 33, was charged with reckless grievous bodily harm and was granted bail on January 5 . The brothers' father, Oliver, has been a constant presence beside his son, Barry, at court from the beginning (pictured) The brothers from Belfast in Ireland (pictured here as children) went on a night out on January 3 in Kings Cross - in Sydney's inner-city . Patrick Lyttle was treated by paramedics at the scene in Sydney's inner city, where he was found in a pool of blood and vomit . Two Irish brothers were involved in a confrontation outside a nightclub in Kings Cross in Sydney's inner city which has left the younger brother fighting for his life . Sorry we are not currently accepting comments on this article.\n"
          ]
        }
      ],
      "source": [
        "# Find overlap between cleaned train and untouched val\n",
        "train_set = set(df_train_cleaned[\"article\"])\n",
        "val_set = set(df_val[\"article\"])\n",
        "\n",
        "# Intersection\n",
        "overlap_articles = train_set.intersection(val_set)\n",
        "\n",
        "print(f\"Number of overlapping articles (train-val): {len(overlap_articles)}\")\n",
        "\n",
        "# Show the overlapping article(s)\n",
        "if overlap_articles:\n",
        "    for art in overlap_articles:\n",
        "        print(\"\\n--- Overlapping article ---\\n\")\n",
        "        print(art)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IG4SYPw7gZr1",
        "outputId": "bac2c442-269e-4bfc-8d2c-068d38223f04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New train size: 284004\n"
          ]
        }
      ],
      "source": [
        "# Drop overlapping articles from train only\n",
        "df_train_cleaned = df_train_cleaned[~df_train_cleaned[\"article\"].isin(overlap_articles)]\n",
        "\n",
        "print(f\"New train size: {len(df_train_cleaned)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfSmauZFgrRQ"
      },
      "source": [
        "During dataset preparation, one article was found to occur verbatim in both the training and validation splits.\n",
        "Following the recommendations in Deduplicating Training Data Makes Language Models Better (Lee et al., 2022), such overlaps were treated as train–eval contamination, which can cause the model to artificially inflate evaluation scores by memorizing examples seen during training.\n",
        "\n",
        "To mitigate this risk:\n",
        "\n",
        "    The overlapping instance was removed from the training set only, preserving the validation set’s original composition.\n",
        "\n",
        "    This ensures that validation performance remains a reliable measure of model generalization without altering the evaluation distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMleRKn3wMS3"
      },
      "source": [
        "# Structural cleaning before removing near-dups and after the removal of exact dups\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxvlTxBK0xrs"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import html\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "\n",
        "# --- compile once, reuse ---\n",
        "_PATS = {\n",
        "    # A. excessive punctuation\n",
        "    \"repeated_punct\": re.compile(r\"([?!\\.,;:])\\1{2,}\"),\n",
        "    \"mixed_exclaim_q\": re.compile(r\"([?!]){3,}\"),\n",
        "\n",
        "    # B. whitespace / invisibles\n",
        "    \"trailing_ws\": re.compile(r\"[ \\t]+$\", re.MULTILINE),\n",
        "    \"multi_space\": re.compile(r\"(\\S)(\\s{2,})(\\S)\"),\n",
        "    \"nb_zero_width\": re.compile(r\"[\\u00A0\\u200B-\\u200D\\u2060]\"),\n",
        "\n",
        "    # C. html / entities / linky stuff\n",
        "    \"html_tags\": re.compile(r\"<[^>]+>\"),\n",
        "    \"html_entity\": re.compile(r\"&[a-zA-Z]+;\"),\n",
        "    \"urlish\": re.compile(r\"(https?://|<a\\s|<script|<style)\", re.IGNORECASE),\n",
        "\n",
        "    # D. unicode oddities\n",
        "    \"smart_quotes\": re.compile(r\"[“”]\"),\n",
        "    \"smart_squotes\": re.compile(r\"[‘’]\"),\n",
        "    \"emdash_endash\": re.compile(r\"[–—]\"),\n",
        "\n",
        "    # E1. tokenization artifacts\n",
        "    \"at_hyphen\": re.compile(r\"\\s*@-@\\s*\"),\n",
        "    \"lrb\": re.compile(r\"\\s*-LRB-\\s*\"),\n",
        "    \"rrb\": re.compile(r\"\\s*-RRB-\\s*\"),\n",
        "    \"lsb\": re.compile(r\"\\s*-LSB-\\s*\"),\n",
        "    \"rsb\": re.compile(r\"\\s*-RSB-\\s*\"),\n",
        "    \"bq\": re.compile(r\"``\"),\n",
        "    \"eq\": re.compile(r\"''\"),\n",
        "\n",
        "    # E2/E3. spacing anomalies in numbers & ellipses\n",
        "    \"spaced_num_punct\": re.compile(r\"(?<=\\d)\\s+[.,]\\s+(?=\\d)\"),\n",
        "    \"artifact_num_dot\": re.compile(r\"(?P<a>\\d+)\\s*@\\.@\\s*(?P<b>\\d+)\"),\n",
        "    \"artifact_num_com\": re.compile(r\"(?P<a>\\d+)\\s*@,@\\s*(?P<b>\\d+)\"),\n",
        "    \"artifact_num_any\": re.compile(r\"(?P<a>\\d+)\\s*@[@.,]\\s*(?P<b>\\d+)\"),\n",
        "    \"spaced_ellipsis\": re.compile(r\"(?:\\s*\\.\\s*){3,}\"),   # . . . → ...\n",
        "}\n",
        "\n",
        "def _normalize_unicode(s: str) -> str:\n",
        "    # NFKC, then normalize common typography to ASCII\n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "    s = _PATS[\"smart_quotes\"].sub('\"', s)\n",
        "    s = _PATS[\"smart_squotes\"].sub(\"'\", s)\n",
        "    s = _PATS[\"emdash_endash\"].sub(\"-\", s)\n",
        "    return s\n",
        "\n",
        "def _fix_tokenization_artifacts(s: str) -> str:\n",
        "    s = _PATS[\"at_hyphen\"].sub(\"-\", s)\n",
        "    s = _PATS[\"lrb\"].sub(\"(\", s)\n",
        "    s = _PATS[\"rrb\"].sub(\")\", s)\n",
        "    s = _PATS[\"lsb\"].sub(\"[\", s)\n",
        "    s = _PATS[\"rsb\"].sub(\"]\", s)\n",
        "    s = _PATS[\"bq\"].sub('\"', s)\n",
        "    s = _PATS[\"eq\"].sub('\"', s)\n",
        "    return s\n",
        "\n",
        "def _fix_number_spacing(s: str) -> str:\n",
        "    # 1 . 000 → 1.000\n",
        "    s = _PATS[\"spaced_num_punct\"].sub(lambda m: \"\", s)\n",
        "    # 0 @.@ 59 → 0.59 ; 4 @,@ 386 → 4,386 ; generic @X@ fallbacks\n",
        "    s = _PATS[\"artifact_num_dot\"].sub(lambda m: f\"{m.group('a')}.{m.group('b')}\", s)\n",
        "    s = _PATS[\"artifact_num_com\"].sub(lambda m: f\"{m.group('a')},{m.group('b')}\", s)\n",
        "    s = _PATS[\"artifact_num_any\"].sub(lambda m: f\"{m.group('a')}{m.group('b')}\", s)\n",
        "    # . . . / ..... with spaces → ...\n",
        "    s = _PATS[\"spaced_ellipsis\"].sub(\"...\", s)\n",
        "    return s\n",
        "\n",
        "def _collapse_punct_and_ws(s: str) -> str:\n",
        "    s = _PATS[\"repeated_punct\"].sub(lambda m: m.group(1), s)\n",
        "    s = _PATS[\"mixed_exclaim_q\"].sub(\"?\", s)  # normalize long ?!/!? runs to single '?'\n",
        "    s = _PATS[\"trailing_ws\"].sub(\"\", s)\n",
        "    s = _PATS[\"multi_space\"].sub(r\"\\1 \\3\", s)\n",
        "    return s\n",
        "\n",
        "def _strip_html_and_entities(s: str) -> str:\n",
        "    # unescape named entities first (e.g., &nbsp;), then drop residual tags\n",
        "    if _PATS[\"html_entity\"].search(s):\n",
        "        s = html.unescape(s)\n",
        "    if _PATS[\"html_tags\"].search(s):\n",
        "        s = _PATS[\"html_tags\"].sub(\" \", s)\n",
        "    return s\n",
        "\n",
        "def clean_text(s: str, stats: dict | None = None) -> str:\n",
        "    if not isinstance(s, str) or not s:\n",
        "        return s\n",
        "\n",
        "    orig = s\n",
        "\n",
        "    # --- ADDITION: Replace all newlines with a space ---\n",
        "    s = s.replace(\"\\r\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\n\", \" \")\n",
        "\n",
        "    if stats is not None:\n",
        "        # D. non-ascii & normalization drift (log only)\n",
        "        stats[\"non_ascii_before\"] += bool(re.search(r\"[^\\x00-\\x7F]\", s))\n",
        "        stats[\"nfkc_drift\"] += (s != unicodedata.normalize(\"NFKC\", s))\n",
        "        stats[\"urlish_hits\"] += bool(_PATS[\"urlish\"].search(s))\n",
        "\n",
        "    # order matters: normalize → strip invisibles → html/entity → artifacts → numbers → punctuation/whitespace\n",
        "    s = _normalize_unicode(s)\n",
        "    s = _PATS[\"nb_zero_width\"].sub(\" \", s)\n",
        "    s = _strip_html_and_entities(s)\n",
        "    s = _fix_tokenization_artifacts(s)\n",
        "    s = _fix_number_spacing(s)\n",
        "    s = _collapse_punct_and_ws(s)\n",
        "\n",
        "    # final tidy: collapse leftover 2+ spaces, strip\n",
        "    s = re.sub(r\"[ \\t]{2,}\", \" \", s).strip()\n",
        "\n",
        "    if stats is not None:\n",
        "        stats[\"changed\"] += (s != orig)\n",
        "    return s\n",
        "\n",
        "def clean_df(df: pd.DataFrame, text_cols=(\"text\",), return_stats=True):\n",
        "    \"\"\"\n",
        "    Apply cleaning to the given text columns of a DataFrame.\n",
        "    \"\"\"\n",
        "    stats = {k: 0 for k in [\"changed\", \"non_ascii_before\", \"nfkc_drift\", \"urlish_hits\"]}\n",
        "    out = df.copy()\n",
        "    for col in text_cols:\n",
        "        if col not in out.columns:\n",
        "            continue\n",
        "        out[col] = out[col].map(lambda x: clean_text(x, stats))\n",
        "    return (out, stats) if return_stats else out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wlCGxVZVty6",
        "outputId": "67cb5b7c-7d69-4449-d3e7-d4c1b2025d84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(              text\n",
              " 0     Hello world!\n",
              " 1  This is a test.\n",
              " 2   Multiline text\n",
              " 3  No newline here,\n",
              " {'changed': 3, 'non_ascii_before': 0, 'nfkc_drift': 0, 'urlish_hits': 0})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cleansanitycheck = pd.DataFrame({\n",
        "    \"text\": [\"Hello\\nworld!\", \"This is\\r\\na test.\", \"Multiline\\rtext\", \"No newline here\"]\n",
        "})\n",
        "clean_df(cleansanitycheck, text_cols=(\"text\",))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVydTPZy1T1M",
        "outputId": "8e516f9f-d973-4b29-cc50-0527e6a154aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'changed': 464987, 'non_ascii_before': 207154, 'nfkc_drift': 93279, 'urlish_hits': 917} {'changed': 24008, 'non_ascii_before': 13889, 'nfkc_drift': 9766, 'urlish_hits': 27} {'changed': 20598, 'non_ascii_before': 11698, 'nfkc_drift': 8382, 'urlish_hits': 17}\n"
          ]
        }
      ],
      "source": [
        "df_train_cleaned, train_stats = clean_df(df_train_cleaned, text_cols=(\"article\", \"highlights\"))\n",
        "df_val,          val_stats   = clean_df(df_val,          text_cols=(\"article\", \"highlights\"))\n",
        "df_test_cleaned, test_stats  = clean_df(df_test, text_cols=(\"article\", \"highlights\"))\n",
        "print(train_stats, val_stats, test_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlORffjeXPar"
      },
      "outputs": [],
      "source": [
        "# ---- Safe CSV save + strict verify (RFC-4180 compliant) ----\n",
        "import os, csv, hashlib, tempfile\n",
        "import pandas as pd\n",
        "\n",
        "def _atomic_save_csv(df: pd.DataFrame, path: str):\n",
        "    d = os.path.dirname(path) or \".\"\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "    fd, tmp = tempfile.mkstemp(prefix=\".tmp_\", dir=d, text=True)\n",
        "    os.close(fd)\n",
        "    try:\n",
        "        # Try modern param name, fall back if not supported\n",
        "        try:\n",
        "            df.to_csv(\n",
        "                tmp,\n",
        "                index=False,\n",
        "                encoding=\"utf-8\",\n",
        "                quoting=csv.QUOTE_ALL,\n",
        "                quotechar='\"',\n",
        "                doublequote=False,         # MUST BE FALSE\n",
        "                escapechar=\"\\\\\",\n",
        "                line_terminator=\"\\n\",      # Newer pandas\n",
        "            )\n",
        "        except TypeError:\n",
        "            df.to_csv(\n",
        "                tmp,\n",
        "                index=False,\n",
        "                encoding=\"utf-8\",\n",
        "                quoting=csv.QUOTE_ALL,\n",
        "                quotechar='\"',\n",
        "                doublequote=False,         # STILL FALSE here\n",
        "                escapechar=\"\\\\\",\n",
        "                lineterminator=\"\\n\",       # Older pandas fallback\n",
        "            )\n",
        "\n",
        "        # strict re-read to verify\n",
        "        df2 = pd.read_csv(\n",
        "            tmp,\n",
        "            encoding=\"utf-8\",\n",
        "            quotechar='\"',\n",
        "            doublequote=False,            # MUST MATCH\n",
        "            escapechar=\"\\\\\",\n",
        "            keep_default_na=False,\n",
        "            on_bad_lines=\"error\",\n",
        "        )\n",
        "        if len(df2) != len(df):\n",
        "            raise RuntimeError(\n",
        "                f\"Row count changed after save+read: wrote {len(df)}, re-read {len(df2)} ({path})\"\n",
        "            )\n",
        "        os.replace(tmp, path)\n",
        "    except Exception:\n",
        "        try:\n",
        "            os.remove(tmp)\n",
        "        except:\n",
        "            pass\n",
        "        raise\n",
        "\n",
        "\n",
        "def save_and_verify(df: pd.DataFrame, out_path: str):\n",
        "    _atomic_save_csv(df, out_path)\n",
        "    df2 = pd.read_csv(\n",
        "        out_path,\n",
        "        encoding=\"utf-8\",\n",
        "        quotechar='\"',\n",
        "        doublequote=False,            # MUST MATCH\n",
        "        escapechar=\"\\\\\",\n",
        "        keep_default_na=False,\n",
        "        on_bad_lines=\"error\",\n",
        "    )\n",
        "    md5 = hashlib.md5(open(out_path, \"rb\").read()).hexdigest()\n",
        "    print(f\"OK: {out_path} | rows={len(df2)} | md5={md5}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RnLwb9falvm",
        "outputId": "2d927a9b-7ac2-4a1c-97b2-b887a611e451"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: /content/cleaned_outputs/train_cleaned_safe.csv | rows=284004 | md5=019bb02f093699cca58f9d1a1565b408\n",
            "OK: /content/cleaned_outputs/val_cleaned_safe.csv | rows=13368 | md5=f5c07261674f0d72f0be40f2da1bfe48\n",
            "OK: /content/cleaned_outputs/test_cleaned_safe.csv | rows=11488 | md5=36bc75219389258aa7245553aee3f7f5\n"
          ]
        }
      ],
      "source": [
        "# --- Make sure output directory exists ---\n",
        "OUT_DIR = \"/content/cleaned_outputs\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# --- TRAIN ---\n",
        "save_and_verify(df_train_cleaned, f\"{OUT_DIR}/train_cleaned_safe.csv\")\n",
        "\n",
        "# --- VAL ---\n",
        "save_and_verify(df_val, f\"{OUT_DIR}/val_cleaned_safe.csv\")\n",
        "\n",
        "# --- TEST ---\n",
        "save_and_verify(df_test_cleaned, f\"{OUT_DIR}/test_cleaned_safe.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVIAHTVHhR8L",
        "outputId": "99ab577f-f30e-44fc-bedf-7b8542b331bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== VERIFY: TRAIN ===\n",
            "File: /content/cleaned_outputs/train_cleaned_safe.csv\n",
            "MD5: 019bb02f093699cca58f9d1a1565b408\n",
            "Rows (mem/C/Py): 284004 / 284004 / 284004\n",
            "  ✓ 'article' content matches (digest 4587c827…)\n",
            "  ✓ 'highlights' content matches (digest a33f599f…)\n",
            "All checks passed.\n",
            "\n",
            "=== VERIFY: VAL ===\n",
            "File: /content/cleaned_outputs/val_cleaned_safe.csv\n",
            "MD5: f5c07261674f0d72f0be40f2da1bfe48\n",
            "Rows (mem/C/Py): 13368 / 13368 / 13368\n",
            "  ✓ 'article' content matches (digest b7125ef9…)\n",
            "  ✓ 'highlights' content matches (digest 6915f3b6…)\n",
            "All checks passed.\n",
            "\n",
            "=== VERIFY: TEST ===\n",
            "File: /content/cleaned_outputs/test_cleaned_safe.csv\n",
            "MD5: 36bc75219389258aa7245553aee3f7f5\n",
            "Rows (mem/C/Py): 11488 / 11488 / 11488\n",
            "  ✓ 'article' content matches (digest 19b18653…)\n",
            "  ✓ 'highlights' content matches (digest 4b36a5b0…)\n",
            "All checks passed.\n"
          ]
        }
      ],
      "source": [
        "# ===============================\n",
        "# Post-save verification (TRAIN/VAL/TEST)\n",
        "# - Strict re-read with both C and Python engines\n",
        "# - Schema & shape checks against in-memory DataFrames\n",
        "# - Exact content equality on key text columns\n",
        "# - Extra smoke tests (chunked reads)\n",
        "# ===============================\n",
        "import os, hashlib, pandas as pd\n",
        "\n",
        "OUT_DIR = \"/content/cleaned_outputs\"\n",
        "PATHS = {\n",
        "    \"train\": f\"{OUT_DIR}/train_cleaned_safe.csv\",\n",
        "    \"val\":   f\"{OUT_DIR}/val_cleaned_safe.csv\",\n",
        "    \"test\":  f\"{OUT_DIR}/test_cleaned_safe.csv\",\n",
        "}\n",
        "\n",
        "# Map split name to in-memory variable\n",
        "INMEM = {\n",
        "    \"train\": \"df_train_cleaned\",\n",
        "    \"val\":   \"df_val\",\n",
        "    \"test\":  \"df_test_cleaned\",\n",
        "}\n",
        "\n",
        "TEXT_COLS = [\"article\", \"highlights\"]  # columns to deeply compare\n",
        "\n",
        "def md5(path):\n",
        "    h = hashlib.md5()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(1 << 20), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def strict_read_c(path):\n",
        "    return pd.read_csv(\n",
        "        path,\n",
        "        encoding=\"utf-8\",\n",
        "        on_bad_lines=\"error\",\n",
        "        keep_default_na=False,\n",
        "        quotechar='\"',\n",
        "        escapechar=\"\\\\\",\n",
        "        doublequote=False,\n",
        "    )\n",
        "\n",
        "def strict_read_py(path):\n",
        "    return pd.read_csv(\n",
        "        path,\n",
        "        engine=\"python\",\n",
        "        encoding=\"utf-8\",\n",
        "        on_bad_lines=\"error\",\n",
        "        keep_default_na=False,\n",
        "        sep=\",\",\n",
        "        quotechar='\"',\n",
        "        escapechar=\"\\\\\",\n",
        "        doublequote=False,\n",
        "    )\n",
        "\n",
        "def series_digest(s: pd.Series) -> str:\n",
        "    t = s.astype(str).str.replace(\"\\r\\n\", \"\\n\", regex=False)\n",
        "    h = hashlib.md5()\n",
        "    for v in t:\n",
        "        h.update(v.encode(\"utf-8\"))\n",
        "        h.update(b\"\\x1f\")\n",
        "    return h.hexdigest()\n",
        "\n",
        "def verify_split(tag, path, df_mem):\n",
        "    print(f\"\\n=== VERIFY: {tag.upper()} ===\")\n",
        "    print(f\"File: {path}\")\n",
        "    assert os.path.exists(path), f\"Missing file: {path}\"\n",
        "    print(f\"MD5: {md5(path)}\")\n",
        "\n",
        "    # Read both engines\n",
        "    df_c  = strict_read_c(path)\n",
        "    df_py = strict_read_py(path)\n",
        "\n",
        "    # Shape checks\n",
        "    n_mem, n_c, n_py = len(df_mem), len(df_c), len(df_py)\n",
        "    print(f\"Rows (mem/C/Py): {n_mem} / {n_c} / {n_py}\")\n",
        "    assert n_c  == n_mem, f\"C-engine row mismatch: mem={n_mem}, c={n_c}\"\n",
        "    assert n_py == n_mem, f\"Python-engine row mismatch: mem={n_mem}, py={n_py}\"\n",
        "\n",
        "    # Column structure checks\n",
        "    assert list(df_c.columns)  == list(df_mem.columns),  \"Column order/names differ (C)\"\n",
        "    assert list(df_py.columns) == list(df_mem.columns),  \"Column order/names differ (Py)\"\n",
        "\n",
        "    # Nulls consistency\n",
        "    for col in df_mem.columns:\n",
        "        na_mem = df_mem[col].isna().sum()\n",
        "        na_c   = df_c[col].isna().sum()\n",
        "        na_py  = df_py[col].isna().sum()\n",
        "        assert na_c  == na_mem, f\"NA drift in {col} (C): {na_mem} -> {na_c}\"\n",
        "        assert na_py == na_mem, f\"NA drift in {col} (Py): {na_mem} -> {na_py}\"\n",
        "\n",
        "    # Deep content match for key text columns\n",
        "    for col in TEXT_COLS:\n",
        "        if col in df_mem.columns:\n",
        "            d_mem = series_digest(df_mem[col])\n",
        "            d_c   = series_digest(df_c[col])\n",
        "            d_py  = series_digest(df_py[col])\n",
        "            assert d_c  == d_mem, f\"Content digest mismatch in '{col}' (C)\"\n",
        "            assert d_py == d_mem, f\"Content digest mismatch in '{col}' (Py)\"\n",
        "            print(f\"  ✓ '{col}' content matches (digest {d_mem[:8]}…)\")\n",
        "\n",
        "    # Smoke test: chunked streaming read\n",
        "    total = 0\n",
        "    for chunk in pd.read_csv(\n",
        "        path,\n",
        "        encoding=\"utf-8\",\n",
        "        quotechar='\"',\n",
        "        escapechar=\"\\\\\",\n",
        "        doublequote=False,\n",
        "        keep_default_na=False,\n",
        "        on_bad_lines=\"error\",\n",
        "        chunksize=100_000,\n",
        "    ):\n",
        "        total += len(chunk)\n",
        "    assert total == n_mem, f\"Chunked read mismatch: {total} vs {n_mem}\"\n",
        "\n",
        "    print(\"All checks passed.\")\n",
        "\n",
        "# Run for each split\n",
        "for tag, path in PATHS.items():\n",
        "    df_name = INMEM[tag]\n",
        "    if df_name not in globals():\n",
        "        raise NameError(f\"In-memory DataFrame '{df_name}' not found in session.\")\n",
        "    verify_split(tag, path, globals()[df_name])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR_K5N8phs6U"
      },
      "source": [
        "# Double Checking Exact Dups after cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFQ-SNNLi8Om"
      },
      "source": [
        "to avoid overwriting the train file, the second appearance ot the dup was removed in boiler plates to do all at once"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d8knqlbp9ZZ"
      },
      "source": [
        "# Detecting and Removing Boilerplates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDaKHyvnkCgg"
      },
      "outputs": [],
      "source": [
        "df_train_cleaned = pd.read_csv(\n",
        "    \"/content/cleaned_outputs/train_cleaned_safe.csv\",\n",
        "    engine=\"python\",\n",
        "    quotechar='\"',\n",
        "    escapechar=\"\\\\\",\n",
        "    doublequote=False,\n",
        "    sep=\",\",\n",
        "    encoding=\"utf-8\",\n",
        "    keep_default_na=False,\n",
        "    on_bad_lines=\"error\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NO2VmBqilgLz",
        "outputId": "d7839231-851c-4c8b-e602-779306bc1524"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dropped 1 exact duplicate rows.\n"
          ]
        }
      ],
      "source": [
        "# Drop duplicates using only article + highlights\n",
        "before = len(df_train_cleaned)\n",
        "df_train_cleaned = df_train_cleaned.drop_duplicates(subset=[\"article\", \"highlights\"], keep=\"first\").reset_index(drop=True)\n",
        "after = len(df_train_cleaned)\n",
        "print(f\"Dropped {before - after} exact duplicate rows.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        },
        "id": "u1b9I3f_qBeS",
        "outputId": "d61093db-e0f3-43aa-c8a0-276ae3096f07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming sentence statistics...\n",
            "Streaming n-gram frequencies...\n",
            "Cross-split shortlists...\n",
            "\n",
            "=== Boilerplate Summary ===\n",
            "Top Sentence Candidates: 99\n",
            "Top N-gram Candidates:   400\n",
            "\n",
            "--- Most Frequent Sentences ---\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"print(\\\"Done\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"A U.S.\",\n          \"The .\",\n          \"All rights reserved.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc_freq\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11832,\n        \"min\": 789,\n        \"max\": 38578,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          856,\n          14210,\n          1946\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"edge_bias\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 274.4060252010999,\n        \"min\": 21.0,\n        \"max\": 762.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          424.0,\n          191.0,\n          188.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"count_total\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12340,\n        \"min\": 793,\n        \"max\": 38624,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          907,\n          19913,\n          1953\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-dbf6a216-625d-4e19-95d1-cf18d29c65d0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>doc_freq</th>\n",
              "      <th>edge_bias</th>\n",
              "      <th>count_total</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Scroll down for video .</td>\n",
              "      <td>38578</td>\n",
              "      <td>424.0</td>\n",
              "      <td>38624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The .</td>\n",
              "      <td>14210</td>\n",
              "      <td>191.0</td>\n",
              "      <td>19913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The U.S.</td>\n",
              "      <td>4477</td>\n",
              "      <td>762.0</td>\n",
              "      <td>5121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sorry we are not currently accepting comments on this article.</td>\n",
              "      <td>3391</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SCROLL DOWN FOR VIDEO .</td>\n",
              "      <td>2282</td>\n",
              "      <td>21.0</td>\n",
              "      <td>2284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>All rights reserved.</td>\n",
              "      <td>1946</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>said.</td>\n",
              "      <td>1305</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Scroll down for videos .</td>\n",
              "      <td>1070</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>A U.S.</td>\n",
              "      <td>856</td>\n",
              "      <td>188.0</td>\n",
              "      <td>907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>The trial continues.</td>\n",
              "      <td>789</td>\n",
              "      <td>NaN</td>\n",
              "      <td>793</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dbf6a216-625d-4e19-95d1-cf18d29c65d0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dbf6a216-625d-4e19-95d1-cf18d29c65d0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dbf6a216-625d-4e19-95d1-cf18d29c65d0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-891c1aa2-0436-4b5b-a73f-22204c788c5c\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-891c1aa2-0436-4b5b-a73f-22204c788c5c')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-891c1aa2-0436-4b5b-a73f-22204c788c5c button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                         sentence  doc_freq  \\\n",
              "0                                         Scroll down for video .     38578   \n",
              "1                                                           The .     14210   \n",
              "2                                                        The U.S.      4477   \n",
              "3  Sorry we are not currently accepting comments on this article.      3391   \n",
              "4                                         SCROLL DOWN FOR VIDEO .      2282   \n",
              "5                                            All rights reserved.      1946   \n",
              "6                                                           said.      1305   \n",
              "7                                        Scroll down for videos .      1070   \n",
              "8                                                          A U.S.       856   \n",
              "9                                            The trial continues.       789   \n",
              "\n",
              "   edge_bias  count_total  \n",
              "0      424.0        38624  \n",
              "1      191.0        19913  \n",
              "2      762.0         5121  \n",
              "3        NaN         3391  \n",
              "4       21.0         2284  \n",
              "5        NaN         1953  \n",
              "6       80.0         1374  \n",
              "7        NaN         1071  \n",
              "8      188.0          907  \n",
              "9        NaN          793  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Most Frequent N-grams ---\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"print(\\\"Done\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"ngram\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"the end of\",\n          \"to this report\",\n          \"the United States\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc_freq\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5894,\n        \"min\": 5503,\n        \"max\": 19181,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          5702,\n          18331,\n          6834\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-488e280a-6a9a-45c6-86ee-a00c56e93401\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ngram</th>\n",
              "      <th>doc_freq</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>contributed to this</td>\n",
              "      <td>19181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>to this report</td>\n",
              "      <td>18331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>contributed to this report</td>\n",
              "      <td>18308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>one of the</td>\n",
              "      <td>12349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>the U S</td>\n",
              "      <td>7216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>the United States</td>\n",
              "      <td>6834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Sorry we are</td>\n",
              "      <td>6365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>as well as</td>\n",
              "      <td>5777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>the end of</td>\n",
              "      <td>5702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>according to the</td>\n",
              "      <td>5503</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-488e280a-6a9a-45c6-86ee-a00c56e93401')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-488e280a-6a9a-45c6-86ee-a00c56e93401 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-488e280a-6a9a-45c6-86ee-a00c56e93401');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-003216ff-88e6-414a-955e-b07613a532b2\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-003216ff-88e6-414a-955e-b07613a532b2')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-003216ff-88e6-414a-955e-b07613a532b2 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                        ngram  doc_freq\n",
              "0         contributed to this     19181\n",
              "1              to this report     18331\n",
              "2  contributed to this report     18308\n",
              "3                  one of the     12349\n",
              "4                     the U S      7216\n",
              "5           the United States      6834\n",
              "6                Sorry we are      6365\n",
              "7                  as well as      5777\n",
              "8                  the end of      5702\n",
              "9            according to the      5503"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done. Files written to: /content/boilerplate_outputs\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# Recompute Boilerplate Candidates (deduped train + safe val/test)\n",
        "# ==============================================================\n",
        "\n",
        "import os, re, pandas as pd\n",
        "from collections import Counter\n",
        "from IPython.display import display\n",
        "\n",
        "# -------------------------------\n",
        "# Config\n",
        "# -------------------------------\n",
        "CHUNK = 5000\n",
        "K_EDGE_SENTS = 3\n",
        "MIN_SENT_CHARS = 5\n",
        "N_RANGE = (3, 6)\n",
        "EDGE_ONLY_NGRAMS = True\n",
        "TEXT_COL = \"article\"\n",
        "OUTDIR = \"/content/boilerplate_outputs\"\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "\n",
        "# These two are still safe gold files\n",
        "VAL_PATH  = \"/content/cleaned_outputs/val_cleaned_safe.csv\"\n",
        "TEST_PATH = \"/content/cleaned_outputs/test_cleaned_safe.csv\"\n",
        "\n",
        "# -------------------------------\n",
        "# Regexes\n",
        "# -------------------------------\n",
        "SENT_SPLIT_RE = re.compile(r'(?<=[.!?])\\s+')\n",
        "WORD_RE = re.compile(r\"[A-Za-z0-9]+(?:['’-][A-Za-z0-9]+)*\")\n",
        "\n",
        "# -------------------------------\n",
        "# Sentence & n-gram utilities\n",
        "# -------------------------------\n",
        "def split_sents_raw(text: str):\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return []\n",
        "    return [s.strip() for s in re.split(SENT_SPLIT_RE, text) if s.strip()]\n",
        "\n",
        "def read_stream(source, text_col=TEXT_COL, chunksize=5000):\n",
        "    if isinstance(source, pd.DataFrame):\n",
        "        yield source[text_col].astype(str)\n",
        "    else:\n",
        "        for df in pd.read_csv(source, usecols=[text_col], chunksize=chunksize,\n",
        "                              encoding=\"utf-8\", quotechar='\"', escapechar=\"\\\\\",\n",
        "                              doublequote=False, engine=\"python\"):\n",
        "            yield df[text_col].astype(str)\n",
        "\n",
        "def frequent_sentences_stream(source, text_col=TEXT_COL, k_edge=3, min_chars=5, chunksize=5000):\n",
        "    count_total, count_first, count_last, doc_freq, last_seen_docid = Counter(), Counter(), Counter(), Counter(), {}\n",
        "    global_docid = 0\n",
        "    for series in read_stream(source, text_col, chunksize):\n",
        "        for art in series:\n",
        "            sents = split_sents_raw(art)\n",
        "            if not sents:\n",
        "                continue\n",
        "            fk, lk = sents[:k_edge], sents[-k_edge:]\n",
        "            seen_in_this_doc = set()\n",
        "            for s in sents:\n",
        "                if len(s) < min_chars:\n",
        "                    continue\n",
        "                count_total[s] += 1\n",
        "                if s not in seen_in_this_doc and last_seen_docid.get(s) != global_docid:\n",
        "                    doc_freq[s] += 1\n",
        "                    last_seen_docid[s] = global_docid\n",
        "                    seen_in_this_doc.add(s)\n",
        "            for s in fk:\n",
        "                if len(s) >= min_chars:\n",
        "                    count_first[s] += 1\n",
        "            for s in lk:\n",
        "                if len(s) >= min_chars:\n",
        "                    count_last[s] += 1\n",
        "            global_docid += 1\n",
        "    df = pd.DataFrame({\"sentence\": list(count_total)})\n",
        "    df[\"count_total\"] = df[\"sentence\"].map(count_total.get)\n",
        "    df[\"doc_freq\"]    = df[\"sentence\"].map(doc_freq.get)\n",
        "    df[\"count_firstk\"]= df[\"sentence\"].map(count_first.get)\n",
        "    df[\"count_lastk\"] = df[\"sentence\"].map(count_last.get)\n",
        "    df[\"edge_bias\"]   = df[\"count_firstk\"] + df[\"count_lastk\"]\n",
        "    df.sort_values([\"doc_freq\", \"edge_bias\", \"count_total\"], ascending=False, inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    return df\n",
        "\n",
        "def ngram_docfreq_stream(source, text_col=TEXT_COL, n_range=(3,6), chunksize=4000,\n",
        "                         max_sents_edge=3, edge_only=True, max_tokens_per_side=400):\n",
        "    df_counts, last_seen_docid = Counter(), {}\n",
        "    global_docid = 0\n",
        "    for series in read_stream(source, text_col, chunksize):\n",
        "        for art in series:\n",
        "            toks = []\n",
        "            sents = split_sents_raw(art) if edge_only else None\n",
        "            if sents:\n",
        "                for s in sents[:max_sents_edge] + sents[-max_sents_edge:]:\n",
        "                    toks.extend(WORD_RE.findall(s))\n",
        "            else:\n",
        "                all_toks = WORD_RE.findall(art)\n",
        "                toks = all_toks[:max_tokens_per_side] + all_toks[-max_tokens_per_side:]\n",
        "            if not toks:\n",
        "                continue\n",
        "            credited = set()\n",
        "            for n in range(n_range[0], n_range[1] + 1):\n",
        "                for i in range(len(toks) - n + 1):\n",
        "                    ng = tuple(toks[i:i+n])\n",
        "                    if ng in credited:\n",
        "                        continue\n",
        "                    if last_seen_docid.get(ng) != global_docid:\n",
        "                        df_counts[ng] += 1\n",
        "                        last_seen_docid[ng] = global_docid\n",
        "                        credited.add(ng)\n",
        "            global_docid += 1\n",
        "    df = pd.DataFrame([(\" \".join(k), v) for k, v in df_counts.items()],\n",
        "                      columns=[\"ngram\", \"doc_freq\"])\n",
        "    df.sort_values(\"doc_freq\", ascending=False, inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    return df\n",
        "\n",
        "def shortlist_sentences_cross(s_train, s_val, s_test, top_probe=800, final_top=200):\n",
        "    s1 = set(s_train.head(top_probe)[\"sentence\"])\n",
        "    s2 = set(s_val.head(top_probe)[\"sentence\"])\n",
        "    s3 = set(s_test.head(top_probe)[\"sentence\"])\n",
        "    cand = (s1 & s2) | (s1 & s3) | (s2 & s3)\n",
        "    out = s_train[s_train[\"sentence\"].isin(cand)].copy()\n",
        "    out.sort_values([\"doc_freq\", \"edge_bias\", \"count_total\"], ascending=False, inplace=True)\n",
        "    return out.head(final_top).reset_index(drop=True)\n",
        "\n",
        "def shortlist_ngrams_cross(ng_train, ng_val, ng_test, top_probe=3000, final_top=400):\n",
        "    n1 = set(ng_train.head(top_probe)[\"ngram\"])\n",
        "    n2 = set(ng_val.head(top_probe)[\"ngram\"])\n",
        "    n3 = set(ng_test.head(top_probe)[\"ngram\"])\n",
        "    cand = (n1 & n2) | (n1 & n3) | (n2 & n3)\n",
        "    out = ng_train[ng_train[\"ngram\"].isin(cand)].copy()\n",
        "    out.sort_values(\"doc_freq\", ascending=False, inplace=True)\n",
        "    return out.head(final_top).reset_index(drop=True)\n",
        "\n",
        "# -------------------------------\n",
        "# Run\n",
        "# -------------------------------\n",
        "print(\"Streaming sentence statistics...\")\n",
        "s_train = frequent_sentences_stream(df_train_cleaned)\n",
        "s_val   = frequent_sentences_stream(VAL_PATH)\n",
        "s_test  = frequent_sentences_stream(TEST_PATH)\n",
        "\n",
        "print(\"Streaming n-gram frequencies...\")\n",
        "ng_train = ngram_docfreq_stream(df_train_cleaned)\n",
        "ng_val   = ngram_docfreq_stream(VAL_PATH)\n",
        "ng_test  = ngram_docfreq_stream(TEST_PATH)\n",
        "\n",
        "print(\"Cross-split shortlists...\")\n",
        "sent_candidates = shortlist_sentences_cross(s_train, s_val, s_test)\n",
        "ng_candidates   = shortlist_ngrams_cross(ng_train, ng_val, ng_test)\n",
        "\n",
        "# Save all outputs\n",
        "s_train.to_parquet(f\"{OUTDIR}/sentences_train.parquet\")\n",
        "s_val.to_parquet(f\"{OUTDIR}/sentences_val.parquet\")\n",
        "s_test.to_parquet(f\"{OUTDIR}/sentences_test.parquet\")\n",
        "ng_train.to_parquet(f\"{OUTDIR}/ngrams_train.parquet\")\n",
        "ng_val.to_parquet(f\"{OUTDIR}/ngrams_val.parquet\")\n",
        "ng_test.to_parquet(f\"{OUTDIR}/ngrams_test.parquet\")\n",
        "sent_candidates.to_csv(f\"{OUTDIR}/candidates_sentences.csv\", index=False)\n",
        "ng_candidates.to_csv(f\"{OUTDIR}/candidates_ngrams.csv\", index=False)\n",
        "\n",
        "# -------------------------------\n",
        "# Summary\n",
        "# -------------------------------\n",
        "print(\"\\n=== Boilerplate Summary ===\")\n",
        "print(f\"Top Sentence Candidates: {len(sent_candidates)}\")\n",
        "print(f\"Top N-gram Candidates:   {len(ng_candidates)}\")\n",
        "\n",
        "print(\"\\n--- Most Frequent Sentences ---\")\n",
        "display(sent_candidates[[\"sentence\", \"doc_freq\", \"edge_bias\", \"count_total\"]].head(10))\n",
        "\n",
        "print(\"\\n--- Most Frequent N-grams ---\")\n",
        "display(ng_candidates.head(10))\n",
        "\n",
        "print(\"Done. Files written to:\", OUTDIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSD_EYVRAvo3",
        "outputId": "02abc673-e248-49c0-fc1d-fe78c1740b1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Cleaning: [in-memory] df_train_cleaned\n",
            "OK: /content/cleaned_outputs/train_cleaned_safe_boilerfree.csv | rows=284003 | md5=b113c390f47a53cfd157a3f94657ce84\n",
            "\n",
            "Cleaning: /content/cleaned_outputs/val_cleaned_safe.csv\n",
            "OK: /content/cleaned_outputs/val_cleaned_safe_boilerfree.csv | rows=13368 | md5=6cdc32ddc64126e133107aabbd6e08b2\n",
            "\n",
            "Cleaning: /content/cleaned_outputs/test_cleaned_safe.csv\n",
            "OK: /content/cleaned_outputs/test_cleaned_safe_boilerfree.csv | rows=11488 | md5=2532fdd53f9048912ec564f7ba816252\n",
            "\n",
            " All done. New files:\n",
            " - /content/cleaned_outputs/train_cleaned_safe_boilerfree.csv\n",
            " - /content/cleaned_outputs/val_cleaned_safe_boilerfree.csv\n",
            " - /content/cleaned_outputs/test_cleaned_safe_boilerfree.csv\n"
          ]
        }
      ],
      "source": [
        "# =============================================================\n",
        "# Boilerplate Cleaner with Strict Save (deduped train + safe val/test)\n",
        "# =============================================================\n",
        "\n",
        "import os, re, csv, tempfile, hashlib, pandas as pd\n",
        "from typing import Optional\n",
        "\n",
        "# -------------------------------\n",
        "# CONFIG\n",
        "# -------------------------------\n",
        "TEXT_COL   = \"article\"\n",
        "OUT_DIR    = \"/content/cleaned_outputs\"\n",
        "OUT_SUFFIX = \"_boilerfree\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# -------------------------------\n",
        "# Phrase Patterns (single list)\n",
        "# -------------------------------\n",
        "SAFE_PHRASES = [\n",
        "    \"scroll down for video\",\n",
        "    \"scroll down for videos\",\n",
        "    \"see below for video\",\n",
        "    \"transcript\",\n",
        "    \"all rights reserved\",\n",
        "    \"odds (subject to change)\",\n",
        "    \"source: nhs choices\",\n",
        "    \"sorry we are not currently accepting comments\",\n",
        "    \"thank you for using cnn student news\",\n",
        "    \"the trial continues\",\n",
        "    \"the hearing continues\",\n",
        "    \"the case continues\",\n",
        "    \"click here to access the transcript of today's\",\n",
        "    \"please note that there may be a delay between\",\n",
        "    \"contributed to this\",\n",
        "    \"to this report\",\n",
        "    \"contributed to this report\",\n",
        "    \"capt\",\n",
        "    \"the rev\",\n",
        "]\n",
        "\n",
        "# Compile patterns\n",
        "def phrase_to_regex(p): return re.compile(rf\"^\\s*{re.escape(p)}\\s*[.:!]*\\s*$\", re.IGNORECASE)\n",
        "SAFE_REGEXES = [phrase_to_regex(p) for p in SAFE_PHRASES]\n",
        "SAFE_REGEXES.append(re.compile(r\"^\\s*scroll\\s+down\\s+for\\s+video(s)?\\s*[.:!]*\\s*$\", re.IGNORECASE))\n",
        "SENT_SPLIT_RE = re.compile(r\"(?<=[.!?])\\s+\")\n",
        "\n",
        "# -------------------------------\n",
        "# Cleaning Logic\n",
        "# -------------------------------\n",
        "def should_drop_sentence(sent: str) -> bool:\n",
        "    s = sent.strip()\n",
        "    if not s or len(s) <= 3:\n",
        "        return True\n",
        "    return any(rx.match(s) for rx in SAFE_REGEXES)\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    if not isinstance(text, str): return text\n",
        "    parts = SENT_SPLIT_RE.split(text)\n",
        "    kept = []\n",
        "    for sent in parts:\n",
        "        for line in sent.splitlines():\n",
        "            if should_drop_sentence(line): continue\n",
        "            kept.append(line)\n",
        "    cleaned = \" \".join(kept).strip()\n",
        "    return re.sub(r\"\\s+\", \" \", cleaned)\n",
        "\n",
        "# -------------------------------\n",
        "# RFC-4180 Safe Saver\n",
        "# -------------------------------\n",
        "def _atomic_save_csv(df: pd.DataFrame, path: str):\n",
        "    d = os.path.dirname(path) or \".\"\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "    fd, tmp = tempfile.mkstemp(prefix=\".tmp_\", dir=d, text=True)\n",
        "    os.close(fd)\n",
        "    try:\n",
        "        try:\n",
        "            df.to_csv(\n",
        "                tmp,\n",
        "                index=False,\n",
        "                encoding=\"utf-8\",\n",
        "                quoting=csv.QUOTE_ALL,\n",
        "                quotechar='\"',\n",
        "                doublequote=False,\n",
        "                escapechar=\"\\\\\",\n",
        "                line_terminator=\"\\n\",\n",
        "            )\n",
        "        except TypeError:\n",
        "            df.to_csv(\n",
        "                tmp,\n",
        "                index=False,\n",
        "                encoding=\"utf-8\",\n",
        "                quoting=csv.QUOTE_ALL,\n",
        "                quotechar='\"',\n",
        "                doublequote=False,\n",
        "                escapechar=\"\\\\\",\n",
        "                lineterminator=\"\\n\",\n",
        "            )\n",
        "        df2 = pd.read_csv(\n",
        "            tmp,\n",
        "            encoding=\"utf-8\",\n",
        "            quotechar='\"',\n",
        "            doublequote=False,\n",
        "            escapechar=\"\\\\\",\n",
        "            keep_default_na=False,\n",
        "            on_bad_lines=\"error\",\n",
        "        )\n",
        "        if len(df2) != len(df):\n",
        "            raise RuntimeError(f\"Row count changed: {len(df)} → {len(df2)}\")\n",
        "        os.replace(tmp, path)\n",
        "    except Exception:\n",
        "        try: os.remove(tmp)\n",
        "        except: pass\n",
        "        raise\n",
        "\n",
        "def save_and_verify(df: pd.DataFrame, out_path: str):\n",
        "    _atomic_save_csv(df, out_path)\n",
        "    df2 = pd.read_csv(\n",
        "        out_path,\n",
        "        encoding=\"utf-8\",\n",
        "        quotechar='\"',\n",
        "        doublequote=False,\n",
        "        escapechar=\"\\\\\",\n",
        "        keep_default_na=False,\n",
        "        on_bad_lines=\"error\",\n",
        "    )\n",
        "    md5 = hashlib.md5(open(out_path, \"rb\").read()).hexdigest()\n",
        "    print(f\"OK: {out_path} | rows={len(df2)} | md5={md5}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Clean and Save One File\n",
        "# -------------------------------\n",
        "def clean_and_save_file(path: str, text_col: str) -> str:\n",
        "    print(f\"\\nCleaning: {path}\")\n",
        "    df = pd.read_csv(path, engine=\"python\", quotechar='\"', escapechar=\"\\\\\", doublequote=False)\n",
        "    df[text_col] = df[text_col].astype(str).map(clean_text)\n",
        "    before = len(df)\n",
        "    df = df[df[text_col].str.strip().astype(bool)].copy()\n",
        "    after = len(df)\n",
        "    if after < before:\n",
        "        print(f\" - Dropped {before - after} empty rows.\")\n",
        "    stem, ext = os.path.splitext(os.path.basename(path))\n",
        "    out_path = os.path.join(OUT_DIR, f\"{stem}{OUT_SUFFIX}{ext or '.csv'}\")\n",
        "    save_and_verify(df, out_path)\n",
        "    return out_path\n",
        "\n",
        "# -------------------------------\n",
        "# Process All 3 Splits\n",
        "# -------------------------------\n",
        "final_outputs = []\n",
        "\n",
        "# 1. TRAIN (deduped, in-memory)\n",
        "print(\"\\nCleaning: [in-memory] df_train_cleaned\")\n",
        "df_train_cleaned[TEXT_COL] = df_train_cleaned[TEXT_COL].astype(str).map(clean_text)\n",
        "before = len(df_train_cleaned)\n",
        "df_train_cleaned = df_train_cleaned[df_train_cleaned[TEXT_COL].str.strip().astype(bool)].copy()\n",
        "after = len(df_train_cleaned)\n",
        "if after < before:\n",
        "    print(f\" - Dropped {before - after} empty rows.\")\n",
        "out_train = os.path.join(OUT_DIR, \"train_cleaned_safe_boilerfree.csv\")\n",
        "save_and_verify(df_train_cleaned, out_train)\n",
        "final_outputs.append(out_train)\n",
        "\n",
        "# 2–3. VAL + TEST (from disk)\n",
        "for tag in [\"val\", \"test\"]:\n",
        "    in_path = f\"/content/cleaned_outputs/{tag}_cleaned_safe.csv\"\n",
        "    out = clean_and_save_file(in_path, TEXT_COL)\n",
        "    final_outputs.append(out)\n",
        "\n",
        "# -------------------------------\n",
        "# Summary\n",
        "# -------------------------------\n",
        "print(\"\\n All done. New files:\")\n",
        "for p in final_outputs:\n",
        "    print(\" -\", p)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAqxIG3JthmQ"
      },
      "source": [
        "**Double checking the size, empty lines, dups**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU2nqsk6sBwU",
        "outputId": "2e06eaae-ac88-40b9-a572-aa0021b906bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== TRAIN ===\n",
            "Path: /content/cleaned_outputs/train_cleaned_safe_boilerfree.csv\n",
            "MD5 : b113c390f47a53cfd157a3f94657ce84\n",
            "Shape: (284003, 3)\n",
            " All expected columns present.\n",
            "Empty 'article' rows: 0\n",
            " No empty articles.\n",
            "Exact duplicate (article+highlight) rows: 2\n",
            "\n",
            "=== VAL ===\n",
            "Path: /content/cleaned_outputs/val_cleaned_safe_boilerfree.csv\n",
            "MD5 : 6cdc32ddc64126e133107aabbd6e08b2\n",
            "Shape: (13368, 3)\n",
            " All expected columns present.\n",
            "Empty 'article' rows: 0\n",
            " No empty articles.\n",
            "Exact duplicate (article+highlight) rows: 0\n",
            "\n",
            "=== TEST ===\n",
            "Path: /content/cleaned_outputs/test_cleaned_safe_boilerfree.csv\n",
            "MD5 : 2532fdd53f9048912ec564f7ba816252\n",
            "Shape: (11488, 3)\n",
            " All expected columns present.\n",
            "Empty 'article' rows: 0\n",
            " No empty articles.\n",
            "Exact duplicate (article+highlight) rows: 0\n"
          ]
        }
      ],
      "source": [
        "import os, hashlib\n",
        "import pandas as pd\n",
        "\n",
        "BOILERFREE_PATHS = {\n",
        "    \"train\": \"/content/cleaned_outputs/train_cleaned_safe_boilerfree.csv\",\n",
        "    \"val\":   \"/content/cleaned_outputs/val_cleaned_safe_boilerfree.csv\",\n",
        "    \"test\":  \"/content/cleaned_outputs/test_cleaned_safe_boilerfree.csv\",\n",
        "}\n",
        "\n",
        "# --- Core config ---\n",
        "TEXT_COLS = [\"article\", \"highlights\"]  # expect both present\n",
        "EXPECTED_COLS = set(TEXT_COLS)\n",
        "\n",
        "# --- MD5 calculator ---\n",
        "def md5(path):\n",
        "    h = hashlib.md5()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(1 << 20), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "# --- Run checks ---\n",
        "for split, path in BOILERFREE_PATHS.items():\n",
        "    print(f\"\\n=== {split.upper()} ===\")\n",
        "    assert os.path.exists(path), f\"Missing file: {path}\"\n",
        "\n",
        "    df = pd.read_csv(\n",
        "        path,\n",
        "        engine=\"python\",\n",
        "        quotechar='\"',\n",
        "        escapechar=\"\\\\\",\n",
        "        doublequote=False,\n",
        "        keep_default_na=False,\n",
        "        on_bad_lines=\"error\",\n",
        "    )\n",
        "\n",
        "    print(f\"Path: {path}\")\n",
        "    print(f\"MD5 : {md5(path)}\")\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "\n",
        "    # Check expected columns exist\n",
        "    missing = EXPECTED_COLS - set(df.columns)\n",
        "    if missing:\n",
        "        print(f\" Missing columns: {missing}\")\n",
        "    else:\n",
        "        print(\" All expected columns present.\")\n",
        "\n",
        "    # Check non-empty article text\n",
        "    empty_rows = df[TEXT_COLS[0]].str.strip().eq(\"\").sum()\n",
        "    print(f\"Empty '{TEXT_COLS[0]}' rows: {empty_rows}\")\n",
        "    if empty_rows == 0:\n",
        "        print(\" No empty articles.\")\n",
        "\n",
        "    # Check uniqueness\n",
        "    dup_count = df.duplicated(subset=TEXT_COLS, keep=False).sum()\n",
        "    print(f\"Exact duplicate (article+highlight) rows: {dup_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "YAXMH4d8sf82",
        "outputId": "5f52accb-fb58-4a13-d75f-6374b3952a40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total duplicates: 2\n",
            "\n",
            "Grouped duplicate pairs:\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"dup_rows_grouped\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"article\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"An Islamic State propaganda magazine has lavished Sydney cafe gunman Man Haron Monis with praise for the siege, labelling the hostage crisis a 'daring raid'. It's also reissued a call for lone wolf attacks in Australia and taunted Western helplessness in the face of the threat. Monis and two of his hostages, Tori Johnson and Katrina Dawson, were killed in the bloody conclusion to the siege in the early hours of December 16. Gunman: Man Haron Monis was killed during the siege's violent conclusion in the early hours of December 16 . Gunned down: A spray of police bullets killed Man Haron Monis on December 16 . In the foreword to its sixth edition, the IS magazine Dabiq described Monis as a brother and said he was fighting in the path of Allah. He did so 'by acting alone and striking the kuffar (non-believers) where it would hurt them most - in their own lands and on the very streets that they presumptively walk in safety,' the magazine states. 'It didn't take much; he got hold of a gun and stormed a cafe taking everyone inside hostage. 'Yet in doing so, he prompted mass panic, brought terror to the entire nation.The blessings in his efforts were apparent from the very outset.' At the start of the siege, a black flag with Arabic writing was visible in a window of the Lindt cafe in Sydney's Martin Place. Dabiq declared this was 'a testament to his sincerity'. Terror on the streets of Sydney: Lindt cafe worker Jieun Bae was the terrified face of the Sydney siege . It said Monis joined other Muslims who answered a jihadist call 'to strike those waging war against the Islamic State wherever they may be'. In October, Dabiq published calls for lone wolf attacks across western nations, especially US, UK, France, Australia and Germany. It repeated that statement in the latest edition. 'There will be others who follow the examples set by Man Haron Monis and Numan Haider in Australia,' it said. Melbourne teenager Haider was shot dead after stabbing two police officers. 'All that the West will be able to do is to anxiously await the next round of slaughter and then issue the same tired, cliche statements in condemnation of it when it occurs.' A week ago Prime Minister Tony Abbott warned there had been a heightened level of 'terrorist chatter' in the aftermath of the Sydney siege tragedy. However, Australia's terror threat level hasn't been lifted - remaining at high, which means an attack is likely.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"highlights\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"The Islamic State's Dabiq propaganda magazine praised Man Haron Monis for 'daring' raid . Two hostages, Tori Johnson and Katrina Dawson, were killed, as was the gunman . It also reissued a call for lone wolf attacks in Australia .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 2,\n        \"max\": 2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "dup_rows_grouped"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-be723ab1-6185-45d4-b13a-2fbbe957e5d6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article</th>\n",
              "      <th>highlights</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>An Islamic State propaganda magazine has lavished Sydney cafe gunman Man Haron Monis with praise for the siege, labelling the hostage crisis a 'daring raid'. It's also reissued a call for lone wolf attacks in Australia and taunted Western helplessness in the face of the threat. Monis and two of his hostages, Tori Johnson and Katrina Dawson, were killed in the bloody conclusion to the siege in the early hours of December 16. Gunman: Man Haron Monis was killed during the siege's violent conclusion in the early hours of December 16 . Gunned down: A spray of police bullets killed Man Haron Monis on December 16 . In the foreword to its sixth edition, the IS magazine Dabiq described Monis as a brother and said he was fighting in the path of Allah. He did so 'by acting alone and striking the kuffar (non-believers) where it would hurt them most - in their own lands and on the very streets that they presumptively walk in safety,' the magazine states. 'It didn't take much; he got hold of a gun and stormed a cafe taking everyone inside hostage. 'Yet in doing so, he prompted mass panic, brought terror to the entire nation.The blessings in his efforts were apparent from the very outset.' At the start of the siege, a black flag with Arabic writing was visible in a window of the Lindt cafe in Sydney's Martin Place. Dabiq declared this was 'a testament to his sincerity'. Terror on the streets of Sydney: Lindt cafe worker Jieun Bae was the terrified face of the Sydney siege . It said Monis joined other Muslims who answered a jihadist call 'to strike those waging war against the Islamic State wherever they may be'. In October, Dabiq published calls for lone wolf attacks across western nations, especially US, UK, France, Australia and Germany. It repeated that statement in the latest edition. 'There will be others who follow the examples set by Man Haron Monis and Numan Haider in Australia,' it said. Melbourne teenager Haider was shot dead after stabbing two police officers. 'All that the West will be able to do is to anxiously await the next round of slaughter and then issue the same tired, cliche statements in condemnation of it when it occurs.' A week ago Prime Minister Tony Abbott warned there had been a heightened level of 'terrorist chatter' in the aftermath of the Sydney siege tragedy. However, Australia's terror threat level hasn't been lifted - remaining at high, which means an attack is likely.</td>\n",
              "      <td>The Islamic State's Dabiq propaganda magazine praised Man Haron Monis for 'daring' raid . Two hostages, Tori Johnson and Katrina Dawson, were killed, as was the gunman . It also reissued a call for lone wolf attacks in Australia .</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-be723ab1-6185-45d4-b13a-2fbbe957e5d6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-be723ab1-6185-45d4-b13a-2fbbe957e5d6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-be723ab1-6185-45d4-b13a-2fbbe957e5d6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_6585133e-5ac3-4fa4-bb9b-9648e06545c3\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('dup_rows_grouped')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_6585133e-5ac3-4fa4-bb9b-9648e06545c3 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('dup_rows_grouped');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        article  \\\n",
              "0  An Islamic State propaganda magazine has lavished Sydney cafe gunman Man Haron Monis with praise for the siege, labelling the hostage crisis a 'daring raid'. It's also reissued a call for lone wolf attacks in Australia and taunted Western helplessness in the face of the threat. Monis and two of his hostages, Tori Johnson and Katrina Dawson, were killed in the bloody conclusion to the siege in the early hours of December 16. Gunman: Man Haron Monis was killed during the siege's violent conclusion in the early hours of December 16 . Gunned down: A spray of police bullets killed Man Haron Monis on December 16 . In the foreword to its sixth edition, the IS magazine Dabiq described Monis as a brother and said he was fighting in the path of Allah. He did so 'by acting alone and striking the kuffar (non-believers) where it would hurt them most - in their own lands and on the very streets that they presumptively walk in safety,' the magazine states. 'It didn't take much; he got hold of a gun and stormed a cafe taking everyone inside hostage. 'Yet in doing so, he prompted mass panic, brought terror to the entire nation.The blessings in his efforts were apparent from the very outset.' At the start of the siege, a black flag with Arabic writing was visible in a window of the Lindt cafe in Sydney's Martin Place. Dabiq declared this was 'a testament to his sincerity'. Terror on the streets of Sydney: Lindt cafe worker Jieun Bae was the terrified face of the Sydney siege . It said Monis joined other Muslims who answered a jihadist call 'to strike those waging war against the Islamic State wherever they may be'. In October, Dabiq published calls for lone wolf attacks across western nations, especially US, UK, France, Australia and Germany. It repeated that statement in the latest edition. 'There will be others who follow the examples set by Man Haron Monis and Numan Haider in Australia,' it said. Melbourne teenager Haider was shot dead after stabbing two police officers. 'All that the West will be able to do is to anxiously await the next round of slaughter and then issue the same tired, cliche statements in condemnation of it when it occurs.' A week ago Prime Minister Tony Abbott warned there had been a heightened level of 'terrorist chatter' in the aftermath of the Sydney siege tragedy. However, Australia's terror threat level hasn't been lifted - remaining at high, which means an attack is likely.   \n",
              "\n",
              "                                                                                                                                                                                                                               highlights  \\\n",
              "0  The Islamic State's Dabiq propaganda magazine praised Man Haron Monis for 'daring' raid . Two hostages, Tori Johnson and Katrina Dawson, were killed, as was the gunman . It also reissued a call for lone wolf attacks in Australia .   \n",
              "\n",
              "   count  \n",
              "0      2  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Full duplicate entries with index:\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"dup_rows\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"article\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"An Islamic State propaganda magazine has lavished Sydney cafe gunman Man Haron Monis with praise for the siege, labelling the hostage crisis a 'daring raid'. It's also reissued a call for lone wolf attacks in Australia and taunted Western helplessness in the face of the threat. Monis and two of his hostages, Tori Johnson and Katrina Dawson, were killed in the bloody conclusion to the siege in the early hours of December 16. Gunman: Man Haron Monis was killed during the siege's violent conclusion in the early hours of December 16 . Gunned down: A spray of police bullets killed Man Haron Monis on December 16 . In the foreword to its sixth edition, the IS magazine Dabiq described Monis as a brother and said he was fighting in the path of Allah. He did so 'by acting alone and striking the kuffar (non-believers) where it would hurt them most - in their own lands and on the very streets that they presumptively walk in safety,' the magazine states. 'It didn't take much; he got hold of a gun and stormed a cafe taking everyone inside hostage. 'Yet in doing so, he prompted mass panic, brought terror to the entire nation.The blessings in his efforts were apparent from the very outset.' At the start of the siege, a black flag with Arabic writing was visible in a window of the Lindt cafe in Sydney's Martin Place. Dabiq declared this was 'a testament to his sincerity'. Terror on the streets of Sydney: Lindt cafe worker Jieun Bae was the terrified face of the Sydney siege . It said Monis joined other Muslims who answered a jihadist call 'to strike those waging war against the Islamic State wherever they may be'. In October, Dabiq published calls for lone wolf attacks across western nations, especially US, UK, France, Australia and Germany. It repeated that statement in the latest edition. 'There will be others who follow the examples set by Man Haron Monis and Numan Haider in Australia,' it said. Melbourne teenager Haider was shot dead after stabbing two police officers. 'All that the West will be able to do is to anxiously await the next round of slaughter and then issue the same tired, cliche statements in condemnation of it when it occurs.' A week ago Prime Minister Tony Abbott warned there had been a heightened level of 'terrorist chatter' in the aftermath of the Sydney siege tragedy. However, Australia's terror threat level hasn't been lifted - remaining at high, which means an attack is likely.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"highlights\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"The Islamic State's Dabiq propaganda magazine praised Man Haron Monis for 'daring' raid . Two hostages, Tori Johnson and Katrina Dawson, were killed, as was the gunman . It also reissued a call for lone wolf attacks in Australia .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"111cdfb05f183b9d30231f50d3ad5931dd217d73\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "dup_rows"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-3aab3ecf-0cc4-4434-9d01-9703d0360ff2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article</th>\n",
              "      <th>highlights</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>265622</th>\n",
              "      <td>An Islamic State propaganda magazine has lavished Sydney cafe gunman Man Haron Monis with praise for the siege, labelling the hostage crisis a 'daring raid'. It's also reissued a call for lone wolf attacks in Australia and taunted Western helplessness in the face of the threat. Monis and two of his hostages, Tori Johnson and Katrina Dawson, were killed in the bloody conclusion to the siege in the early hours of December 16. Gunman: Man Haron Monis was killed during the siege's violent conclusion in the early hours of December 16 . Gunned down: A spray of police bullets killed Man Haron Monis on December 16 . In the foreword to its sixth edition, the IS magazine Dabiq described Monis as a brother and said he was fighting in the path of Allah. He did so 'by acting alone and striking the kuffar (non-believers) where it would hurt them most - in their own lands and on the very streets that they presumptively walk in safety,' the magazine states. 'It didn't take much; he got hold of a gun and stormed a cafe taking everyone inside hostage. 'Yet in doing so, he prompted mass panic, brought terror to the entire nation.The blessings in his efforts were apparent from the very outset.' At the start of the siege, a black flag with Arabic writing was visible in a window of the Lindt cafe in Sydney's Martin Place. Dabiq declared this was 'a testament to his sincerity'. Terror on the streets of Sydney: Lindt cafe worker Jieun Bae was the terrified face of the Sydney siege . It said Monis joined other Muslims who answered a jihadist call 'to strike those waging war against the Islamic State wherever they may be'. In October, Dabiq published calls for lone wolf attacks across western nations, especially US, UK, France, Australia and Germany. It repeated that statement in the latest edition. 'There will be others who follow the examples set by Man Haron Monis and Numan Haider in Australia,' it said. Melbourne teenager Haider was shot dead after stabbing two police officers. 'All that the West will be able to do is to anxiously await the next round of slaughter and then issue the same tired, cliche statements in condemnation of it when it occurs.' A week ago Prime Minister Tony Abbott warned there had been a heightened level of 'terrorist chatter' in the aftermath of the Sydney siege tragedy. However, Australia's terror threat level hasn't been lifted - remaining at high, which means an attack is likely.</td>\n",
              "      <td>The Islamic State's Dabiq propaganda magazine praised Man Haron Monis for 'daring' raid . Two hostages, Tori Johnson and Katrina Dawson, were killed, as was the gunman . It also reissued a call for lone wolf attacks in Australia .</td>\n",
              "      <td>77f30273d8ef588d26a1a28c7ed3158806290781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>265770</th>\n",
              "      <td>An Islamic State propaganda magazine has lavished Sydney cafe gunman Man Haron Monis with praise for the siege, labelling the hostage crisis a 'daring raid'. It's also reissued a call for lone wolf attacks in Australia and taunted Western helplessness in the face of the threat. Monis and two of his hostages, Tori Johnson and Katrina Dawson, were killed in the bloody conclusion to the siege in the early hours of December 16. Gunman: Man Haron Monis was killed during the siege's violent conclusion in the early hours of December 16 . Gunned down: A spray of police bullets killed Man Haron Monis on December 16 . In the foreword to its sixth edition, the IS magazine Dabiq described Monis as a brother and said he was fighting in the path of Allah. He did so 'by acting alone and striking the kuffar (non-believers) where it would hurt them most - in their own lands and on the very streets that they presumptively walk in safety,' the magazine states. 'It didn't take much; he got hold of a gun and stormed a cafe taking everyone inside hostage. 'Yet in doing so, he prompted mass panic, brought terror to the entire nation.The blessings in his efforts were apparent from the very outset.' At the start of the siege, a black flag with Arabic writing was visible in a window of the Lindt cafe in Sydney's Martin Place. Dabiq declared this was 'a testament to his sincerity'. Terror on the streets of Sydney: Lindt cafe worker Jieun Bae was the terrified face of the Sydney siege . It said Monis joined other Muslims who answered a jihadist call 'to strike those waging war against the Islamic State wherever they may be'. In October, Dabiq published calls for lone wolf attacks across western nations, especially US, UK, France, Australia and Germany. It repeated that statement in the latest edition. 'There will be others who follow the examples set by Man Haron Monis and Numan Haider in Australia,' it said. Melbourne teenager Haider was shot dead after stabbing two police officers. 'All that the West will be able to do is to anxiously await the next round of slaughter and then issue the same tired, cliche statements in condemnation of it when it occurs.' A week ago Prime Minister Tony Abbott warned there had been a heightened level of 'terrorist chatter' in the aftermath of the Sydney siege tragedy. However, Australia's terror threat level hasn't been lifted - remaining at high, which means an attack is likely.</td>\n",
              "      <td>The Islamic State's Dabiq propaganda magazine praised Man Haron Monis for 'daring' raid . Two hostages, Tori Johnson and Katrina Dawson, were killed, as was the gunman . It also reissued a call for lone wolf attacks in Australia .</td>\n",
              "      <td>111cdfb05f183b9d30231f50d3ad5931dd217d73</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3aab3ecf-0cc4-4434-9d01-9703d0360ff2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3aab3ecf-0cc4-4434-9d01-9703d0360ff2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3aab3ecf-0cc4-4434-9d01-9703d0360ff2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c059b28f-0ba0-488f-8e7c-f3a3d6a5e1d2\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c059b28f-0ba0-488f-8e7c-f3a3d6a5e1d2')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c059b28f-0ba0-488f-8e7c-f3a3d6a5e1d2 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_70c989ef-7de9-40f4-a447-7e081c341249\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('dup_rows')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_70c989ef-7de9-40f4-a447-7e081c341249 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('dup_rows');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             article  \\\n",
              "265622  An Islamic State propaganda magazine has lavished Sydney cafe gunman Man Haron Monis with praise for the siege, labelling the hostage crisis a 'daring raid'. It's also reissued a call for lone wolf attacks in Australia and taunted Western helplessness in the face of the threat. Monis and two of his hostages, Tori Johnson and Katrina Dawson, were killed in the bloody conclusion to the siege in the early hours of December 16. Gunman: Man Haron Monis was killed during the siege's violent conclusion in the early hours of December 16 . Gunned down: A spray of police bullets killed Man Haron Monis on December 16 . In the foreword to its sixth edition, the IS magazine Dabiq described Monis as a brother and said he was fighting in the path of Allah. He did so 'by acting alone and striking the kuffar (non-believers) where it would hurt them most - in their own lands and on the very streets that they presumptively walk in safety,' the magazine states. 'It didn't take much; he got hold of a gun and stormed a cafe taking everyone inside hostage. 'Yet in doing so, he prompted mass panic, brought terror to the entire nation.The blessings in his efforts were apparent from the very outset.' At the start of the siege, a black flag with Arabic writing was visible in a window of the Lindt cafe in Sydney's Martin Place. Dabiq declared this was 'a testament to his sincerity'. Terror on the streets of Sydney: Lindt cafe worker Jieun Bae was the terrified face of the Sydney siege . It said Monis joined other Muslims who answered a jihadist call 'to strike those waging war against the Islamic State wherever they may be'. In October, Dabiq published calls for lone wolf attacks across western nations, especially US, UK, France, Australia and Germany. It repeated that statement in the latest edition. 'There will be others who follow the examples set by Man Haron Monis and Numan Haider in Australia,' it said. Melbourne teenager Haider was shot dead after stabbing two police officers. 'All that the West will be able to do is to anxiously await the next round of slaughter and then issue the same tired, cliche statements in condemnation of it when it occurs.' A week ago Prime Minister Tony Abbott warned there had been a heightened level of 'terrorist chatter' in the aftermath of the Sydney siege tragedy. However, Australia's terror threat level hasn't been lifted - remaining at high, which means an attack is likely.   \n",
              "265770  An Islamic State propaganda magazine has lavished Sydney cafe gunman Man Haron Monis with praise for the siege, labelling the hostage crisis a 'daring raid'. It's also reissued a call for lone wolf attacks in Australia and taunted Western helplessness in the face of the threat. Monis and two of his hostages, Tori Johnson and Katrina Dawson, were killed in the bloody conclusion to the siege in the early hours of December 16. Gunman: Man Haron Monis was killed during the siege's violent conclusion in the early hours of December 16 . Gunned down: A spray of police bullets killed Man Haron Monis on December 16 . In the foreword to its sixth edition, the IS magazine Dabiq described Monis as a brother and said he was fighting in the path of Allah. He did so 'by acting alone and striking the kuffar (non-believers) where it would hurt them most - in their own lands and on the very streets that they presumptively walk in safety,' the magazine states. 'It didn't take much; he got hold of a gun and stormed a cafe taking everyone inside hostage. 'Yet in doing so, he prompted mass panic, brought terror to the entire nation.The blessings in his efforts were apparent from the very outset.' At the start of the siege, a black flag with Arabic writing was visible in a window of the Lindt cafe in Sydney's Martin Place. Dabiq declared this was 'a testament to his sincerity'. Terror on the streets of Sydney: Lindt cafe worker Jieun Bae was the terrified face of the Sydney siege . It said Monis joined other Muslims who answered a jihadist call 'to strike those waging war against the Islamic State wherever they may be'. In October, Dabiq published calls for lone wolf attacks across western nations, especially US, UK, France, Australia and Germany. It repeated that statement in the latest edition. 'There will be others who follow the examples set by Man Haron Monis and Numan Haider in Australia,' it said. Melbourne teenager Haider was shot dead after stabbing two police officers. 'All that the West will be able to do is to anxiously await the next round of slaughter and then issue the same tired, cliche statements in condemnation of it when it occurs.' A week ago Prime Minister Tony Abbott warned there had been a heightened level of 'terrorist chatter' in the aftermath of the Sydney siege tragedy. However, Australia's terror threat level hasn't been lifted - remaining at high, which means an attack is likely.   \n",
              "\n",
              "                                                                                                                                                                                                                                    highlights  \\\n",
              "265622  The Islamic State's Dabiq propaganda magazine praised Man Haron Monis for 'daring' raid . Two hostages, Tori Johnson and Katrina Dawson, were killed, as was the gunman . It also reissued a call for lone wolf attacks in Australia .   \n",
              "265770  The Islamic State's Dabiq propaganda magazine praised Man Haron Monis for 'daring' raid . Two hostages, Tori Johnson and Katrina Dawson, were killed, as was the gunman . It also reissued a call for lone wolf attacks in Australia .   \n",
              "\n",
              "                                              id  \n",
              "265622  77f30273d8ef588d26a1a28c7ed3158806290781  \n",
              "265770  111cdfb05f183b9d30231f50d3ad5931dd217d73  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the final boilerfree training file\n",
        "train_path = \"/content/cleaned_outputs/train_cleaned_safe_boilerfree.csv\"\n",
        "df_train_final = pd.read_csv(\n",
        "    train_path,\n",
        "    engine=\"python\",\n",
        "    quotechar='\"',\n",
        "    escapechar=\"\\\\\",\n",
        "    doublequote=False,\n",
        "    keep_default_na=False,\n",
        "    on_bad_lines=\"error\"\n",
        ")\n",
        "\n",
        "# Detect duplicate rows across article + highlight\n",
        "dup_mask = df_train_final.duplicated(subset=[\"article\", \"highlights\"], keep=False)\n",
        "dup_rows = df_train_final[dup_mask].copy()\n",
        "\n",
        "print(f\"Total duplicates: {len(dup_rows)}\")\n",
        "# Show grouped by content for clarity\n",
        "dup_rows_grouped = dup_rows.groupby([\"article\", \"highlights\"]).size().reset_index(name=\"count\")\n",
        "print(\"\\nGrouped duplicate pairs:\")\n",
        "display(dup_rows_grouped)\n",
        "\n",
        "# Optional: show full duplicate rows with line numbers\n",
        "print(\"\\nFull duplicate entries with index:\")\n",
        "display(dup_rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZtFEdQdtEfT",
        "outputId": "16be5403-4a9a-4f51-8efc-ca6afaca1027"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dropped 1 duplicate rows.\n",
            " Rewritten safely: /content/cleaned_outputs/train_cleaned_safe_boilerfree.csv\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Re-load the final cleaned training file\n",
        "path = \"/content/cleaned_outputs/train_cleaned_safe_boilerfree.csv\"\n",
        "df = pd.read_csv(\n",
        "    path,\n",
        "    engine=\"python\",\n",
        "    quotechar='\"',\n",
        "    escapechar=\"\\\\\",\n",
        "    doublequote=False,\n",
        "    keep_default_na=False,\n",
        "    on_bad_lines=\"error\"\n",
        ")\n",
        "\n",
        "# Drop second+ duplicate (keep first)\n",
        "before = len(df)\n",
        "df_dedup = df.drop_duplicates(subset=[\"article\", \"highlights\"], keep=\"first\").reset_index(drop=True)\n",
        "after = len(df_dedup)\n",
        "print(f\"Dropped {before - after} duplicate rows.\")\n",
        "\n",
        "# Overwrite same path safely\n",
        "from tempfile import mkstemp\n",
        "import os, hashlib, csv\n",
        "\n",
        "def _atomic_save_csv(df: pd.DataFrame, path: str):\n",
        "    d = os.path.dirname(path)\n",
        "    fd, tmp = mkstemp(prefix=\".tmp_\", dir=d, text=True)\n",
        "    os.close(fd)\n",
        "    try:\n",
        "        df.to_csv(\n",
        "            tmp,\n",
        "            index=False,\n",
        "            encoding=\"utf-8\",\n",
        "            quoting=csv.QUOTE_ALL,\n",
        "            quotechar='\"',\n",
        "            doublequote=False,\n",
        "            escapechar=\"\\\\\",\n",
        "            lineterminator=\"\\n\"  # ← compatible across all pandas\n",
        "        )\n",
        "        df2 = pd.read_csv(\n",
        "            tmp,\n",
        "            engine=\"python\",\n",
        "            quotechar='\"',\n",
        "            escapechar=\"\\\\\",\n",
        "            doublequote=False,\n",
        "            keep_default_na=False,\n",
        "            on_bad_lines=\"error\"\n",
        "        )\n",
        "        if len(df2) != len(df):\n",
        "            raise RuntimeError(f\"Mismatch after re-save: {len(df)} → {len(df2)}\")\n",
        "        os.replace(tmp, path)\n",
        "    except Exception:\n",
        "        os.remove(tmp)\n",
        "        raise\n",
        "\n",
        "\n",
        "# Save and confirm\n",
        "_atomic_save_csv(df_dedup, path)\n",
        "print(f\" Rewritten safely: {path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La_Sq0Zo3-NI"
      },
      "source": [
        "In this stage, repeated non-semantic boilerplate strings were identified and removed from the article field across the training, validation, and test splits.\n",
        "These strings consisted of navigational prompts, copyright notices, disclaimers, and editorial templates such as\n",
        "\"Scroll down for video\", \"All rights reserved.\", and \"Click here to access the transcript of today's...\".\n",
        "Such boilerplates do not contribute factual or narrative content and introduce noise in downstream learning.\n",
        "\n",
        "This decision aligns with prior findings that \"existing language modeling datasets contain many near-duplicate examples and long repetitive substrings\" and that these artifacts \"bias model selection towards models and hyperparameters that intentionally overfit their training datasets\"\n",
        "\n",
        ".\n",
        "While the referenced work focuses on large-scale deduplication, the same principle applies to template boilerplate: it is repeated, predictable, and semantically void, thus inflating token frequency distributions in ways that can harm generalization.\n",
        "\n",
        "Furthermore, See et al. (2017) note that abstractive models \"are liable to reproduce factual details inaccurately\" and may \"repeat themselves\"\n",
        "\n",
        ".\n",
        "Removing repetitive boilerplate reduces the risk that the model will overemphasize or copy irrelevant patterns.\n",
        "\n",
        "This cleaning also indirectly supports copying-mechanism architectures like CoCoNet, which \"enhance the standard copying mechanism by keeping track of the copying history\"\n",
        ".\n",
        "By eliminating irrelevant repetitive material, the copying history is dominated by contextually meaningful words, improving copying precision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqViuSbh-_Lw"
      },
      "source": [
        "# Removal of Source Tags that weren't detected in boilerplates\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3_O4sIjTZP1",
        "outputId": "5364ba2a-86e9-4328-ec6f-f5ac6423e329"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Processing: /content/cleaned_outputs/train_cleaned_safe_boilerfree.csv ===\n",
            "> Changed rows: 72700\n",
            "> Final shape: (284002, 3)\n",
            "\n",
            "--- Showing 10 example(s) ---\n",
            "\n",
            "[1] BEFORE: Editor's note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events. Here, Soledad O'Brien takes u\n",
            "    AFTER : Editor's note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events. Here, Soledad O'Brien takes u\n",
            "\n",
            "[2] BEFORE: MINNEAPOLIS, Minnesota (CNN) -- Drivers who were on the Minneapolis bridge when it collapsed told harrowing tales of survival. \"The whole bridge from one side of the Mississippi to\n",
            "    AFTER : MINNEAPOLIS, Minnesota Drivers who were on the Minneapolis bridge when it collapsed told harrowing tales of survival. \"The whole bridge from one side of the Mississippi to the othe\n",
            "\n",
            "[3] BEFORE: WASHINGTON (CNN) -- Doctors removed five small polyps from President Bush's colon on Saturday, and \"none appeared worrisome,\" a White House spokesman said. The polyps were removed\n",
            "    AFTER : WASHINGTON Doctors removed five small polyps from President Bush's colon on Saturday, and \"none appeared worrisome,\" a White House spokesman said. The polyps were removed and sent\n",
            "\n",
            "[4] BEFORE: (CNN) -- The National Football League has indefinitely suspended Atlanta Falcons quarterback Michael Vick without pay, officials with the league said Friday. NFL star Michael Vick\n",
            "    AFTER : The National Football League has indefinitely suspended Atlanta Falcons quarterback Michael Vick without pay, officials with the league said Friday. NFL star Michael Vick is set to\n",
            "\n",
            "[5] BEFORE: BAGHDAD, Iraq (CNN) -- Dressed in a Superman shirt, 5-year-old Youssif held his sister's hand Friday, seemingly unaware that millions of people across the world have been touched b\n",
            "    AFTER : BAGHDAD, Iraq Dressed in a Superman shirt, 5-year-old Youssif held his sister's hand Friday, seemingly unaware that millions of people across the world have been touched by his sto\n",
            "\n",
            "[6] BEFORE: BAGHDAD, Iraq (CNN) -- The women are too afraid and ashamed to show their faces or have their real names used. They have been driven to sell their bodies to put food on the table f\n",
            "    AFTER : BAGHDAD, Iraq The women are too afraid and ashamed to show their faces or have their real names used. They have been driven to sell their bodies to put food on the table for their\n",
            "\n",
            "[7] BEFORE: BOGOTA, Colombia (CNN) -- A key rebel commander and fugitive from a U.S. drug trafficking indictment was killed over the weekend in an air attack on a guerrilla encampment, the Col\n",
            "    AFTER : BOGOTA, Colombia A key rebel commander and fugitive from a U.S. drug trafficking indictment was killed over the weekend in an air attack on a guerrilla encampment, the Colombian mi\n",
            "\n",
            "[8] BEFORE: WASHINGTON (CNN) -- White House press secretary Tony Snow, who is undergoing treatment for cancer, will step down from his post September 14 and be replaced by deputy press secreta\n",
            "    AFTER : WASHINGTON White House press secretary Tony Snow, who is undergoing treatment for cancer, will step down from his post September 14 and be replaced by deputy press secretary Dana P\n",
            "\n",
            "[9] BEFORE: (CNN) -- Police and FBI agents are investigating the discovery of an empty rocket launcher tube on the front lawn of a Jersey City, New Jersey, home, FBI spokesman Sean Quinn said.\n",
            "    AFTER : Police and FBI agents are investigating the discovery of an empty rocket launcher tube on the front lawn of a Jersey City, New Jersey, home, FBI spokesman Sean Quinn said. Niranjan\n",
            "\n",
            "[10] BEFORE: WASHINGTON (CNN) -- As he awaits a crucial progress report on Iraq, President Bush will try to put a twist on comparisons of the war to Vietnam by invoking the historical lessons o\n",
            "    AFTER : WASHINGTON As he awaits a crucial progress report on Iraq, President Bush will try to put a twist on comparisons of the war to Vietnam by invoking the historical lessons of that co\n",
            "\n",
            " Saved: /content/cleaned_outputs/train_cleaned_safe_boilerfree_sourcetag.csv\n",
            "\n",
            "=== Processing: /content/cleaned_outputs/val_cleaned_safe_boilerfree.csv ===\n",
            "> Changed rows: 1189\n",
            "> Final shape: (13368, 3)\n",
            "\n",
            "--- Showing 10 example(s) ---\n",
            "\n",
            "[1] BEFORE: (CNN)Share, and your gift will be multiplied. That may sound like an esoteric adage, but when Zully Broussard selflessly decided to give one of her kidneys to a stranger, her gener\n",
            "    AFTER : Share, and your gift will be multiplied. That may sound like an esoteric adage, but when Zully Broussard selflessly decided to give one of her kidneys to a stranger, her generosity\n",
            "\n",
            "[2] BEFORE: (CNN)On the 6th of April 1996, San Jose Clash and DC United strode out in front of 31,683 expectant fans at the Spartan Stadium in San Jose, California. The historic occasion was t\n",
            "    AFTER : On the 6th of April 1996, San Jose Clash and DC United strode out in front of 31,683 expectant fans at the Spartan Stadium in San Jose, California. The historic occasion was the fi\n",
            "\n",
            "[3] BEFORE: (CNN)French striker Bafetimbi Gomis, who has a history of fainting, said he is now \"feeling well\" after collapsing during Swansea's 3-2 loss at Tottenham in the Premier League on W\n",
            "    AFTER : French striker Bafetimbi Gomis, who has a history of fainting, said he is now \"feeling well\" after collapsing during Swansea's 3-2 loss at Tottenham in the Premier League on Wednes\n",
            "\n",
            "[4] BEFORE: (CNN)It was an act of frustration perhaps more commonly associated with golf's fictional anti-hero Happy Gilmore than the world's reigning No 1. player. But when Rory McIlroy pulle\n",
            "    AFTER : It was an act of frustration perhaps more commonly associated with golf's fictional anti-hero Happy Gilmore than the world's reigning No 1. player. But when Rory McIlroy pulled his\n",
            "\n",
            "[5] BEFORE: (CNN)A Pennsylvania community is pulling together to search for an eighth-grade student who has been missing since Wednesday. The search has drawn hundreds of volunteers on foot an\n",
            "    AFTER : A Pennsylvania community is pulling together to search for an eighth-grade student who has been missing since Wednesday. The search has drawn hundreds of volunteers on foot and onl\n",
            "\n",
            "[6] BEFORE: (CNN)My vote for Father of the Year goes to Curt Schilling. The former Major League Baseball pitcher recently fired off a series of fastballs and mowed down a group of Twitter trol\n",
            "    AFTER : My vote for Father of the Year goes to Curt Schilling. The former Major League Baseball pitcher recently fired off a series of fastballs and mowed down a group of Twitter trolls wh\n",
            "\n",
            "[7] BEFORE: (CNN)Another one for the \"tourists behaving badly\" file. Two American women have reportedly been arrested for carving their initials into a wall with a coin inside Rome's Colosseum\n",
            "    AFTER : Another one for the \"tourists behaving badly\" file. Two American women have reportedly been arrested for carving their initials into a wall with a coin inside Rome's Colosseum. Dai\n",
            "\n",
            "[8] BEFORE: (CNN)Following last year's successful U.K. tour, Prince and 3rdEyeGirl are bringing the Hit & Run Tour to the U.S. for the first time. The first -- and so far only -- scheduled sho\n",
            "    AFTER : Following last year's successful U.K. tour, Prince and 3rdEyeGirl are bringing the Hit & Run Tour to the U.S. for the first time. The first -- and so far only -- scheduled show wil\n",
            "\n",
            "[9] BEFORE: (CNN)A shooting at a bar popular with expatriates in Mali on Saturday killed five people, including French and Belgian citizens, authorities said. One French citizen, one Belgian a\n",
            "    AFTER : A shooting at a bar popular with expatriates in Mali on Saturday killed five people, including French and Belgian citizens, authorities said. One French citizen, one Belgian and th\n",
            "\n",
            "[10] BEFORE: (CNN)Manchester United defender Jonny Evans and Newcastle United striker Papiss Cisse have been charged by the Football Association for allegedly spitting during an altercation in\n",
            "    AFTER : Manchester United defender Jonny Evans and Newcastle United striker Papiss Cisse have been charged by the Football Association for allegedly spitting during an altercation in Wedne\n",
            "\n",
            " Saved: /content/cleaned_outputs/val_cleaned_safe_boilerfree_sourcetag.csv\n",
            "\n",
            "=== Processing: /content/cleaned_outputs/test_cleaned_safe_boilerfree.csv ===\n",
            "> Changed rows: 1066\n",
            "> Final shape: (11488, 3)\n",
            "\n",
            "--- Showing 10 example(s) ---\n",
            "\n",
            "[1] BEFORE: (CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in\n",
            "    AFTER : The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Pales\n",
            "\n",
            "[2] BEFORE: (CNN)Never mind cats having nine lives. A stray pooch in Washington State has used up at least three of her own after being hit by a car, apparently whacked on the head with a hamm\n",
            "    AFTER : Never mind cats having nine lives. A stray pooch in Washington State has used up at least three of her own after being hit by a car, apparently whacked on the head with a hammer in\n",
            "\n",
            "[3] BEFORE: (CNN)If you've been following the news lately, there are certain things you doubtless know about Mohammad Javad Zarif. He is, of course, the Iranian foreign minister. He has been U\n",
            "    AFTER : If you've been following the news lately, there are certain things you doubtless know about Mohammad Javad Zarif. He is, of course, the Iranian foreign minister. He has been U.S. S\n",
            "\n",
            "[4] BEFORE: (CNN)Five Americans who were monitored for three weeks at an Omaha, Nebraska, hospital after being exposed to Ebola in West Africa have been released, a Nebraska Medicine spokesman\n",
            "    AFTER : Five Americans who were monitored for three weeks at an Omaha, Nebraska, hospital after being exposed to Ebola in West Africa have been released, a Nebraska Medicine spokesman said\n",
            "\n",
            "[5] BEFORE: (CNN)A Duke student has admitted to hanging a noose made of rope from a tree near a student union, university officials said Thursday. The prestigious private school didn't identif\n",
            "    AFTER : A Duke student has admitted to hanging a noose made of rope from a tree near a student union, university officials said Thursday. The prestigious private school didn't identify the\n",
            "\n",
            "[6] BEFORE: (CNN)He's a blue chip college basketball recruit. She's a high school freshman with Down syndrome. At first glance Trey Moses and Ellie Meredith couldn't be more different. But all\n",
            "    AFTER : He's a blue chip college basketball recruit. She's a high school freshman with Down syndrome. At first glance Trey Moses and Ellie Meredith couldn't be more different. But all that\n",
            "\n",
            "[7] BEFORE: (CNN)Governments around the world are using the threat of terrorism -- real or perceived -- to advance executions, Amnesty International alleges in its annual report on the death p\n",
            "    AFTER : Governments around the world are using the threat of terrorism -- real or perceived -- to advance executions, Amnesty International alleges in its annual report on the death penalt\n",
            "\n",
            "[8] BEFORE: (CNN)Andrew Getty, one of the heirs to billions of oil money, appears to have died of natural causes, a Los Angeles Police Department spokesman said. The coroner's preliminary asse\n",
            "    AFTER : Andrew Getty, one of the heirs to billions of oil money, appears to have died of natural causes, a Los Angeles Police Department spokesman said. The coroner's preliminary assessmen\n",
            "\n",
            "[9] BEFORE: (CNN)Filipinos are being warned to be on guard for flash floods and landslides as tropical storm Maysak approached the Asian island nation Saturday. Just a few days ago, Maysak gai\n",
            "    AFTER : Filipinos are being warned to be on guard for flash floods and landslides as tropical storm Maysak approached the Asian island nation Saturday. Just a few days ago, Maysak gained s\n",
            "\n",
            "[10] BEFORE: (CNN)For the first time in eight years, a TV legend returned to doing what he does best. Contestants told to \"come on down!\" on the April 1 edition of \"The Price Is Right\" encounte\n",
            "    AFTER : For the first time in eight years, a TV legend returned to doing what he does best. Contestants told to \"come on down!\" on the April 1 edition of \"The Price Is Right\" encountered n\n",
            "\n",
            " Saved: /content/cleaned_outputs/test_cleaned_safe_boilerfree_sourcetag.csv\n",
            "\n",
            "=== Summary ===\n",
            "TRAIN : 72700 lines changed | Final shape: (284002, 3)\n",
            "VAL   : 1189 lines changed | Final shape: (13368, 3)\n",
            "TEST  : 1066 lines changed | Final shape: (11488, 3)\n"
          ]
        }
      ],
      "source": [
        "import re, os, csv, hashlib, tempfile\n",
        "import pandas as pd\n",
        "\n",
        "# === Input: cleaned + boilerfree ===\n",
        "IN_PATHS = {\n",
        "    \"train\": \"/content/cleaned_outputs/train_cleaned_safe_boilerfree.csv\",\n",
        "    \"val\":   \"/content/cleaned_outputs/val_cleaned_safe_boilerfree.csv\",\n",
        "    \"test\":  \"/content/cleaned_outputs/test_cleaned_safe_boilerfree.csv\",\n",
        "}\n",
        "\n",
        "# === Output: source tag cleaned ===\n",
        "OUT_PATHS = {\n",
        "    \"train\": \"/content/cleaned_outputs/train_cleaned_safe_boilerfree_sourcetag.csv\",\n",
        "    \"val\":   \"/content/cleaned_outputs/val_cleaned_safe_boilerfree_sourcetag.csv\",\n",
        "    \"test\":  \"/content/cleaned_outputs/test_cleaned_safe_boilerfree_sourcetag.csv\",\n",
        "}\n",
        "\n",
        "TEXT_COL = \"article\"\n",
        "\n",
        "# === Regex rules ===\n",
        "LEDE_RE = re.compile(r'^\\s*\\(CNN\\)\\s*(?:(?=\")|[—–-]{1,2}|[,.:;])?\\s*', flags=re.IGNORECASE)\n",
        "INLINE_RE = re.compile(r'\\s*\\(CNN\\)\\s*(?:(?=\")|[—–-]{1,2}|[,.:;])?\\s*', flags=re.IGNORECASE)\n",
        "SPC_RE = re.compile(r'[ \\t]+')\n",
        "BYLINE_RE = re.compile(\n",
        "    r'\\bBy\\s+\\.\\s+[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)?\\s+\\.\\s+PUBLISHED:\\s+\\.\\s+[\\d:]+\\s+\\w+,\\s+\\d+\\s+\\w+\\s+\\d{4}\\s+\\.\\s+\\|\\s+\\.\\s+UPDATED:\\s+\\.\\s+[\\d:]+\\s+\\w+,\\s+\\d+\\s+\\w+\\s+\\d{4}\\s+\\.\\s*',\n",
        "    flags=re.IGNORECASE\n",
        ")\n",
        "\n",
        "def strip_source_tags(text: str) -> str:\n",
        "    if not isinstance(text, str) or not text:\n",
        "        return text\n",
        "    s = LEDE_RE.sub(\"\", text, count=1)\n",
        "    s = INLINE_RE.sub(\" \", s)\n",
        "    s = SPC_RE.sub(' ', s).strip()\n",
        "    s = BYLINE_RE.sub('', s)\n",
        "    return s\n",
        "\n",
        "# === Safe CSV writer ===\n",
        "def _atomic_save_csv(df: pd.DataFrame, path: str):\n",
        "    d = os.path.dirname(path)\n",
        "    fd, tmp = tempfile.mkstemp(prefix=\".tmp_\", dir=d, text=True)\n",
        "    os.close(fd)\n",
        "    try:\n",
        "        df.to_csv(\n",
        "            tmp,\n",
        "            index=False,\n",
        "            encoding=\"utf-8\",\n",
        "            quoting=csv.QUOTE_ALL,\n",
        "            quotechar='\"',\n",
        "            doublequote=False,\n",
        "            escapechar=\"\\\\\",\n",
        "            lineterminator=\"\\n\"\n",
        "        )\n",
        "        df2 = pd.read_csv(\n",
        "            tmp,\n",
        "            engine=\"python\",\n",
        "            quotechar='\"',\n",
        "            escapechar=\"\\\\\",\n",
        "            doublequote=False,\n",
        "            keep_default_na=False,\n",
        "            on_bad_lines=\"error\"\n",
        "        )\n",
        "        if len(df2) != len(df):\n",
        "            raise RuntimeError(f\"Row count mismatch after save: {len(df)} → {len(df2)}\")\n",
        "        os.replace(tmp, path)\n",
        "    except Exception:\n",
        "        os.remove(tmp)\n",
        "        raise\n",
        "\n",
        "# === Cleaner with logging ===\n",
        "def apply_and_save(in_path, out_path, col=TEXT_COL, n_examples=10):\n",
        "    print(f\"\\n=== Processing: {in_path} ===\")\n",
        "    df = pd.read_csv(\n",
        "        in_path,\n",
        "        engine=\"python\",\n",
        "        quotechar='\"',\n",
        "        escapechar=\"\\\\\",\n",
        "        doublequote=False,\n",
        "        keep_default_na=False,\n",
        "        on_bad_lines=\"error\"\n",
        "    )\n",
        "    df[col] = df[col].astype(str).fillna(\"\")\n",
        "    before = df[col].copy()\n",
        "\n",
        "    df[col] = df[col].map(strip_source_tags)\n",
        "    changed_mask = before != df[col]\n",
        "    n_changed = changed_mask.sum()\n",
        "    print(f\"> Changed rows: {n_changed}\")\n",
        "    print(f\"> Final shape: {df.shape}\")\n",
        "\n",
        "    if n_changed > 0:\n",
        "        changed_rows = df[changed_mask]\n",
        "        print(f\"\\n--- Showing {min(n_examples, n_changed)} example(s) ---\\n\")\n",
        "        for i, idx in enumerate(changed_rows.index[:n_examples]):\n",
        "            print(f\"[{i+1}] BEFORE: {before[idx][:180].strip()}\")\n",
        "            print(f\"    AFTER : {df.loc[idx, col][:180].strip()}\\n\")\n",
        "\n",
        "    _atomic_save_csv(df, out_path)\n",
        "    print(f\" Saved: {out_path}\")\n",
        "    return n_changed, df.shape\n",
        "\n",
        "# === Run all splits ===\n",
        "results = {\n",
        "    tag: apply_and_save(IN_PATHS[tag], OUT_PATHS[tag])\n",
        "    for tag in IN_PATHS\n",
        "}\n",
        "\n",
        "# === Summary ===\n",
        "print(\"\\n=== Summary ===\")\n",
        "for tag, (n, shape) in results.items():\n",
        "    print(f\"{tag.upper():<6}: {n} lines changed | Final shape: {shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVSPOHueF8IA"
      },
      "source": [
        "During cleaning, instances of the `(CNN)` tag were removed from the article text, as they represent source metadata rather than meaningful content. These tags often appeared at the start of articles or mid-sentence, sometimes directly attached to punctuation or the first word (e.g., `(CNN)The president...`, `(CNN)--Officials...`).\n",
        "\n",
        "A rule-based regex cleaning step was applied with the following constraints:\n",
        "\n",
        "* **Leading and inline `(CNN)`** tokens were stripped, along with any immediately attached punctuation.\n",
        "* **Exception:** If a double quote (`\"`) immediately followed `(CNN)`, the quote was preserved to maintain correct quotation formatting (e.g., `(CNN)\"It was...\" → `\"It was...\"\\`).\n",
        " They weren’t detected by the n-gram near-duplicate method because the `(CNN)` tags altered the token sequences at the start or mid-sentence.\n",
        "\n",
        "Since n-gram matching relies on exact overlap of consecutive tokens, even a single extra token like `(CNN)` (or `(CNN)--`) shifts the alignment and breaks the match, meaning two otherwise identical articles no longer meet the similarity threshold. Removing these tags restores the true content overlap so the near-duplication method can detect them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm7OWbn6VyBT"
      },
      "source": [
        "# Near-Dup removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaJMhNj6xd46",
        "outputId": "27177129-8157-48b2-c3aa-ef361bdacfa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/103.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/89.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q mmh3 datasketch rapidfuzz regex\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BD-8txOr1ldg",
        "outputId": "dbd464a7-1ed3-4f0a-ea1d-ce718f26943b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 1.3G\n",
            "-rw-r--r-- 1 root root  48M Aug 14 08:59 test_cleaned_safe_boilerfree_sourcetag.csv\n",
            "-rw-r--r-- 1 root root 1.2G Aug 14 09:14 train_cleaned_safe_boilerfree_sourcetag.csv\n",
            "-rw-r--r-- 1 root root  55M Aug 14 08:59 val_cleaned_safe_boilerfree_sourcetag.csv\n"
          ]
        }
      ],
      "source": [
        "!ls -lh /content/cleaned_outputs/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NUJ0qSpUl74",
        "outputId": "4ef997ca-a8f8-4992-d9c5-492b5324c9fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== TRAIN (streamed near-dedup) ===\n",
            "  processed 20,000 | kept 20,000 | exact 0 | near_found 36 | near_dropped 0\n",
            "  processed 40,000 | kept 40,000 | exact 0 | near_found 50 | near_dropped 0\n",
            "  processed 60,000 | kept 60,000 | exact 0 | near_found 60 | near_dropped 0\n",
            "  processed 80,000 | kept 79,994 | exact 6 | near_found 78 | near_dropped 0\n",
            "  processed 100,000 | kept 99,987 | exact 13 | near_found 84 | near_dropped 0\n",
            "  processed 120,000 | kept 119,987 | exact 13 | near_found 88 | near_dropped 0\n",
            "  processed 140,000 | kept 139,987 | exact 13 | near_found 91 | near_dropped 0\n",
            "  processed 160,000 | kept 159,987 | exact 13 | near_found 94 | near_dropped 0\n",
            "  processed 180,000 | kept 179,987 | exact 13 | near_found 97 | near_dropped 0\n",
            "  processed 200,000 | kept 199,987 | exact 13 | near_found 100 | near_dropped 0\n",
            "  processed 220,000 | kept 219,986 | exact 14 | near_found 104 | near_dropped 0\n",
            "  processed 240,000 | kept 239,985 | exact 15 | near_found 113 | near_dropped 0\n",
            "  processed 260,000 | kept 259,985 | exact 15 | near_found 121 | near_dropped 0\n",
            "  processed 280,000 | kept 279,985 | exact 15 | near_found 124 | near_dropped 0\n",
            "Done TRAIN: in=284,002 | kept=283,987 | dropped_exact=15 | near_found=126 | dropped_near=0\n",
            "Audit: /content/cleaned_outputs/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast_near_dedup_decisions.csv\n",
            "Samples: /content/cleaned_outputs/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast_near_dedup_samples.csv\n",
            "Summary: /content/cleaned_outputs/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast_near_dedup_summary.json\n",
            "\n",
            "=== EXACT DEDUP (val_cleaned_safe_boilerfree_sourcetag.csv) ===\n",
            "Done val_cleaned_safe_boilerfree_sourcetag.csv: in=13,368 | kept=13,368 | dropped=0\n",
            "\n",
            "=== EXACT DEDUP (test_cleaned_safe_boilerfree_sourcetag.csv) ===\n",
            "Done test_cleaned_safe_boilerfree_sourcetag.csv: in=11,488 | kept=11,488 | dropped=0\n",
            "OK (C-engine): /content/cleaned_outputs/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv | rows=283,987 | cols=3 | md5=04cee24e3616e64787177d6b520545c5\n",
            "OK (C-engine): /content/cleaned_outputs/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv | rows=13,368 | cols=3 | md5=5d44f9bd33bfed9fe8eb599e586d04fa\n",
            "OK (C-engine): /content/cleaned_outputs/test_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv | rows=11,488 | cols=3 | md5=dc8031d8b79fff25135598e5b8336d04\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "# Near-Dedup FAST MODE — LOW-RAM STREAMING (Colab CPU) + Audit\n",
        "# RFC-4180 safe CSV output (no escapechar, doublequote=True)\n",
        "# Your requested params are preserved (CHUNK_ROWS=50k, NGRAM=7, etc.)\n",
        "# ================================================================\n",
        "\n",
        "# --- Minimal installs  ---\n",
        "import sys, subprocess\n",
        "def _pip(pkg):\n",
        "    try:\n",
        "        __import__(pkg.split(\"==\")[0].split(\">=\")[0])\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
        "for pkg in [\"datasketch\", \"rapidfuzz\", \"regex\", \"mmh3\"]:\n",
        "    _pip(pkg)\n",
        "\n",
        "# --- Imports ---\n",
        "import os, csv, tempfile, hashlib, sqlite3, gc, json\n",
        "import pandas as pd\n",
        "import regex as re2\n",
        "import mmh3\n",
        "from datasketch import MinHash, MinHashLSH\n",
        "from rapidfuzz.fuzz import token_sort_ratio\n",
        "\n",
        "# ---- Paths ----\n",
        "TRAIN_PATH = \"/content/cleaned_outputs/train_cleaned_safe_boilerfree_sourcetag.csv\"\n",
        "VAL_PATH   = \"/content/cleaned_outputs/val_cleaned_safe_boilerfree_sourcetag.csv\"\n",
        "TEST_PATH  = \"/content/cleaned_outputs/test_cleaned_safe_boilerfree_sourcetag.csv\"\n",
        "OUT_DIR    = \"/content/near_dedup\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# ---- Columns ----\n",
        "TEXT_COL = \"article\"\n",
        "ID_COL   = \"id\"   # optional; falls back to chunk:index if absent\n",
        "\n",
        "# ---- Streaming + LSH params ----\n",
        "CHUNK_ROWS      = 50_000\n",
        "NGRAM_SIZE      = 7\n",
        "NUM_PERM        = 128\n",
        "LSH_THRESHOLD   = 0.95\n",
        "CONFIRM_FUZZ    = 96\n",
        "SHORT_TEXT_N    = NGRAM_SIZE\n",
        "PROGRESS_EVERY  = 20_000\n",
        "\n",
        "# ---- Audit / behavior toggles  ----\n",
        "DRY_RUN         = True\n",
        "WRITE_SAMPLES_N = 250\n",
        "\n",
        "# ---- Tokenizer (letters/digits; keep inner ' and -) ----\n",
        "WORD_RE = re2.compile(r\"\\p{L}+\\p{M}*(?:[\\u2019'\\-]\\p{L}+\\p{M}*)*|\\p{N}+\", re2.UNICODE)\n",
        "\n",
        "# ==============================\n",
        "# Helpers\n",
        "# ==============================\n",
        "def _md5(path: str) -> str:\n",
        "    h = hashlib.md5()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(1<<20), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def _tokens(text: str):\n",
        "    return WORD_RE.findall(text)\n",
        "\n",
        "def _ngram_hashes(tok, n=NGRAM_SIZE):\n",
        "    L = len(tok)\n",
        "    if L < n: return ()\n",
        "    for i in range(L - n + 1):\n",
        "        yield mmh3.hash64(\" \".join(tok[i:i+n]))[0]\n",
        "\n",
        "def _minhash_from_shingles(shingles, num_perm=NUM_PERM):\n",
        "    mh = MinHash(num_perm=num_perm, seed=0)\n",
        "    for h in shingles:\n",
        "        mh.update(str(h).encode(\"utf-8\"))\n",
        "    return mh\n",
        "\n",
        "def _normalize_text(s: str):\n",
        "    # light normalization for exact row dedup check\n",
        "    return \" \".join(str(s).strip().split())\n",
        "\n",
        "def _sanitize_field(x):\n",
        "    # Make every field safe for RFC-4180 writer + C-engine reader\n",
        "    s = \"\" if x is None else str(x)\n",
        "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\").replace(\"\\x00\", \"\")\n",
        "    return s\n",
        "\n",
        "# Tiny Bloom filter for exact-row dedup (cross-chunk)\n",
        "class Bloom:\n",
        "    def __init__(self, m_bits=1<<26, k=4):  # ~8MB\n",
        "        self.m = m_bits; self.k = k; self.bits = bytearray(self.m // 8)\n",
        "    def _set(self, idx): self.bits[idx>>3] |= (1 << (idx & 7))\n",
        "    def _get(self, idx): return (self.bits[idx>>3] >> (idx & 7)) & 1\n",
        "    def _hashes(self, b: bytes):\n",
        "        h1 = mmh3.hash(b, 17, signed=False)\n",
        "        h2 = mmh3.hash(b, 29, signed=False)\n",
        "        for i in range(self.k):\n",
        "            yield (h1 + i*h2) % self.m\n",
        "    def seen(self, s: str):\n",
        "        b = s.encode(\"utf-8\", \"ignore\")\n",
        "        flags = [self._get(idx) for idx in self._hashes(b)]\n",
        "        if all(flags): return True\n",
        "        for idx in self._hashes(b): self._set(idx)\n",
        "        return False\n",
        "\n",
        "# ==============================\n",
        "# RFC-4180 safe CSV writer factory\n",
        "# ==============================\n",
        "def _make_canonical_writer(out_path: str):\n",
        "    d = os.path.dirname(out_path) or \".\"\n",
        "    fd, tmp_csv = tempfile.mkstemp(prefix=\".tmp_\", dir=d, text=True)\n",
        "    os.close(fd)\n",
        "    fout = open(tmp_csv, \"w\", encoding=\"utf-8\", newline=\"\")\n",
        "    # RFC‑4180 canonical: QUOTE_ALL, default doublequote=True, NO escapechar\n",
        "    w = csv.writer(fout, quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n",
        "    return fout, w, tmp_csv\n",
        "\n",
        "# ==============================\n",
        "# Streaming near-dedup (train) with audit\n",
        "# ==============================\n",
        "def near_dedup_stream_to_csv(in_path: str, out_path: str, audit_prefix: str):\n",
        "    print(f\"\\n=== TRAIN (streamed near-dedup) ===\")\n",
        "\n",
        "    # Disk-backed store for kept texts (and ids)\n",
        "    db_path = out_path + \".sqlite\"\n",
        "    if os.path.exists(db_path): os.remove(db_path)\n",
        "    con = sqlite3.connect(db_path)\n",
        "    cur = con.cursor()\n",
        "    cur.execute(\"PRAGMA journal_mode=OFF;\")\n",
        "    cur.execute(\"CREATE TABLE keep (kid INTEGER PRIMARY KEY, rid TEXT, text TEXT NOT NULL)\")\n",
        "    con.commit()\n",
        "\n",
        "    # Outputs\n",
        "    fout, w, tmp_csv = _make_canonical_writer(out_path)\n",
        "\n",
        "    # Audit files (canonical dialect as well)\n",
        "    dec_path = f\"{audit_prefix}_near_dedup_decisions.csv\"\n",
        "    dec_f = open(dec_path, \"w\", encoding=\"utf-8\", newline=\"\")\n",
        "    dec_w = csv.writer(dec_f, quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n",
        "    dec_w.writerow([\"row_index\",\"rid\",\"decision\",\"reason\",\"candidate_kid\",\"candidate_rid\",\n",
        "                    \"lsh_threshold\",\"fuzzy_token_sort\",\"preview_source\",\"preview_candidate\"])\n",
        "\n",
        "    # Counters\n",
        "    total_in = kept = dropped_exact = dropped_near = 0\n",
        "    near_found = 0  # counts detections even in DRY_RUN\n",
        "    bloom = Bloom()\n",
        "    lsh = MinHashLSH(threshold=LSH_THRESHOLD, num_perm=NUM_PERM)\n",
        "    next_kid = 0\n",
        "    header_written = False\n",
        "    frozen_cols = None\n",
        "    last_progress = 0\n",
        "\n",
        "    reader = pd.read_csv(\n",
        "        in_path, chunksize=CHUNK_ROWS, encoding=\"utf-8\",\n",
        "        quotechar='\"', doublequote=False, escapechar=\"\\\\\",  # tolerant for *input* if backslash-escaped\n",
        "        keep_default_na=False, engine=\"python\"\n",
        "    )\n",
        "\n",
        "    samples = []\n",
        "\n",
        "    for chunk_idx, chunk in enumerate(reader):\n",
        "        if TEXT_COL not in chunk.columns:\n",
        "            raise KeyError(f\"Missing column '{TEXT_COL}' in {in_path}. Columns: {list(chunk.columns)}\")\n",
        "\n",
        "        # Freeze header/column order once\n",
        "        if not header_written:\n",
        "            frozen_cols = list(chunk.columns)\n",
        "            w.writerow(frozen_cols)\n",
        "            header_written = True\n",
        "\n",
        "        has_id = (ID_COL in frozen_cols)\n",
        "\n",
        "        for r_i, row in chunk.iterrows():\n",
        "            total_in += 1\n",
        "            txt = str(row[TEXT_COL])\n",
        "            norm = _normalize_text(txt)\n",
        "            if not norm:\n",
        "                continue\n",
        "\n",
        "            rid = str(row[ID_COL]) if has_id else f\"{chunk_idx}:{r_i}\"\n",
        "\n",
        "            # exact row dedup\n",
        "            if bloom.seen(norm):\n",
        "                dropped_exact += 1\n",
        "                dec_w.writerow([r_i, rid, \"drop_exact\", \"exact_row_seen_before\", \"\", \"\",\n",
        "                                LSH_THRESHOLD, \"\", norm[:200], \"\"])\n",
        "                continue\n",
        "\n",
        "            # near-dup check\n",
        "            tok = _tokens(norm)\n",
        "            is_dup = False\n",
        "            cand_kid = \"\"\n",
        "            cand_rid = \"\"\n",
        "            fuzz = \"\"\n",
        "\n",
        "            if len(tok) >= SHORT_TEXT_N:\n",
        "                shingles = tuple(_ngram_hashes(tok))\n",
        "                if shingles:\n",
        "                    mh = _minhash_from_shingles(shingles)\n",
        "                    cands = lsh.query(mh)\n",
        "                    if cands:\n",
        "                        for kid in cands:\n",
        "                            cur.execute(\"SELECT rid, text FROM keep WHERE kid=?\", (kid,))\n",
        "                            r = cur.fetchone()\n",
        "                            if r:\n",
        "                                rid2, t2 = r[0], r[1]\n",
        "                                fuzz_score = token_sort_ratio(norm, t2)\n",
        "                                if fuzz_score >= CONFIRM_FUZZ:\n",
        "                                    is_dup = True\n",
        "                                    cand_kid = kid\n",
        "                                    cand_rid = rid2\n",
        "                                    fuzz = fuzz_score\n",
        "                                    break\n",
        "                    if not is_dup:\n",
        "                        lsh.insert(next_kid, mh)\n",
        "\n",
        "            # record detection even in DRY_RUN\n",
        "            if is_dup:\n",
        "                near_found += 1\n",
        "\n",
        "            # Decision\n",
        "            if is_dup and not DRY_RUN:\n",
        "                dropped_near += 1\n",
        "                dec_w.writerow([r_i, rid, \"drop_near\", \"lsh+fuzzy\", cand_kid, cand_rid,\n",
        "                                LSH_THRESHOLD, fuzz, norm[:200], \"\"])\n",
        "                # stash a sample\n",
        "                if len(samples) < WRITE_SAMPLES_N and cand_kid != \"\":\n",
        "                    cur.execute(\"SELECT text FROM keep WHERE kid=?\", (cand_kid,))\n",
        "                    r = cur.fetchone()\n",
        "                    if r:\n",
        "                        samples.append({\n",
        "                            \"rid\": rid, \"cand_rid\": cand_rid,\n",
        "                            \"fuzzy\": fuzz, \"source\": norm[:500], \"candidate\": r[0][:500]\n",
        "                        })\n",
        "                continue\n",
        "\n",
        "            # keep row: sanitize every field before writing\n",
        "            row_out = [_sanitize_field(row.get(c, \"\")) for c in frozen_cols]\n",
        "            w.writerow(row_out)\n",
        "            cur.execute(\"INSERT INTO keep(kid, rid, text) VALUES(?,?,?)\", (next_kid, rid, norm))\n",
        "            if is_dup and DRY_RUN:\n",
        "                dec_w.writerow([r_i, rid, \"would_drop_near\", \"lsh+fuzzy\", cand_kid, cand_rid,\n",
        "                                LSH_THRESHOLD, fuzz, norm[:200], \"\"])\n",
        "            elif not is_dup:\n",
        "                dec_w.writerow([r_i, rid, \"keep\", \"\", \"\", \"\", LSH_THRESHOLD, \"\", norm[:200], \"\"])\n",
        "            next_kid += 1\n",
        "            kept += 1\n",
        "\n",
        "            # progress\n",
        "            if kept + dropped_exact + dropped_near - last_progress >= PROGRESS_EVERY:\n",
        "                last_progress = kept + dropped_exact + dropped_near\n",
        "                con.commit(); fout.flush(); dec_f.flush()\n",
        "                print(f\"  processed {total_in:,} | kept {kept:,} | exact {dropped_exact:,} | near_found {near_found:,} | near_dropped {dropped_near:,}\")\n",
        "\n",
        "        con.commit(); fout.flush(); dec_f.flush(); gc.collect()\n",
        "\n",
        "    # finalize atomic replace\n",
        "    fout.close(); dec_f.close()\n",
        "    os.replace(tmp_csv, out_path)\n",
        "    con.execute(\"VACUUM\"); con.close()\n",
        "\n",
        "    # samples file (canonical dialect)\n",
        "    samp_path = f\"{audit_prefix}_near_dedup_samples.csv\"\n",
        "    with open(samp_path, \"w\", encoding=\"utf-8\", newline=\"\") as sf:\n",
        "        sw = csv.writer(sf, quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n",
        "        sw.writerow([\"rid\",\"candidate_rid\",\"fuzzy_token_sort\",\"source_preview\",\"candidate_preview\"])\n",
        "        for s in samples:\n",
        "            sw.writerow([s[\"rid\"], s[\"cand_rid\"], s[\"fuzzy\"], s[\"source\"], s[\"candidate\"]])\n",
        "\n",
        "\n",
        "    summary = {\n",
        "        \"input_path\": in_path,\n",
        "        \"output_path\": out_path,\n",
        "        \"rows_in\": total_in,\n",
        "        \"kept\": kept,\n",
        "        \"dropped_exact\": dropped_exact,\n",
        "        \"near_found\": near_found,\n",
        "        \"dropped_near\": dropped_near,\n",
        "        \"dry_run\": DRY_RUN,\n",
        "        \"params\": {\n",
        "            \"CHUNK_ROWS\": CHUNK_ROWS,\n",
        "            \"NGRAM_SIZE\": NGRAM_SIZE,\n",
        "            \"NUM_PERM\": NUM_PERM,\n",
        "            \"LSH_THRESHOLD\": LSH_THRESHOLD,\n",
        "            \"CONFIRM_FUZZ\": CONFIRM_FUZZ,\n",
        "            \"SHORT_TEXT_N\": SHORT_TEXT_N\n",
        "        },\n",
        "        \"artifacts\": {\n",
        "            \"decisions_csv\": dec_path,\n",
        "            \"samples_csv\": samp_path,\n",
        "            \"sqlite_store\": db_path\n",
        "        }\n",
        "    }\n",
        "    with open(f\"{audit_prefix}_near_dedup_summary.json\", \"w\", encoding=\"utf-8\") as jf:\n",
        "        json.dump(summary, jf, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Done TRAIN: in={total_in:,} | kept={kept:,} | dropped_exact={dropped_exact:,} | near_found={near_found:,} | dropped_near={dropped_near:,}\")\n",
        "    print(f\"Audit: {dec_path}\")\n",
        "    print(f\"Samples: {samp_path}\")\n",
        "    print(f\"Summary: {audit_prefix}_near_dedup_summary.json\")\n",
        "\n",
        "# ==============================\n",
        "# Exact-row dedup (val/test) — canonical CSV\n",
        "# ==============================\n",
        "def exact_dedup_stream_to_csv(in_path: str, out_path: str):\n",
        "    print(f\"\\n=== EXACT DEDUP ({os.path.basename(in_path)}) ===\")\n",
        "    bloom = Bloom()\n",
        "    total, kept = 0, 0\n",
        "    fout, w, tmp_csv = _make_canonical_writer(out_path)\n",
        "\n",
        "    header_written = False\n",
        "    frozen_cols = None\n",
        "\n",
        "    for chunk in pd.read_csv(\n",
        "        in_path, chunksize=CHUNK_ROWS, encoding=\"utf-8\",\n",
        "        quotechar='\"', doublequote=False, escapechar=\"\\\\\",\n",
        "        keep_default_na=False, engine=\"python\"\n",
        "    ):\n",
        "        if TEXT_COL not in chunk.columns:\n",
        "            raise KeyError(f\"Missing column '{TEXT_COL}' in {in_path}. Columns: {list(chunk.columns)}\")\n",
        "\n",
        "        if not header_written:\n",
        "            frozen_cols = list(chunk.columns)\n",
        "            w.writerow(frozen_cols)\n",
        "            header_written = True\n",
        "\n",
        "        for _, row in chunk.iterrows():\n",
        "            total += 1\n",
        "            norm = _normalize_text(str(row[TEXT_COL]))\n",
        "            if not norm: continue\n",
        "            if bloom.seen(norm): continue\n",
        "            row_out = [_sanitize_field(row.get(c, \"\")) for c in frozen_cols]\n",
        "            w.writerow(row_out); kept += 1\n",
        "\n",
        "    fout.close(); os.replace(tmp_csv, out_path)\n",
        "    print(f\"Done {os.path.basename(in_path)}: in={total:,} | kept={kept:,} | dropped={total-kept:,}\")\n",
        "\n",
        "# ==============================\n",
        "# Run\n",
        "# ==============================\n",
        "train_out = TRAIN_PATH.replace(\".csv\", \"_near_deduped_fast.csv\")\n",
        "val_out   = VAL_PATH.replace(\".csv\",   \"_near_deduped_fast.csv\")\n",
        "test_out  = TEST_PATH.replace(\".csv\",  \"_near_deduped_fast.csv\")\n",
        "\n",
        "# TRAIN (streamed near-dedup) + audit\n",
        "audit_prefix = train_out.replace(\".csv\",\"\")\n",
        "near_dedup_stream_to_csv(TRAIN_PATH, train_out, audit_prefix)\n",
        "\n",
        "# VAL/TEST (exact only)\n",
        "exact_dedup_stream_to_csv(VAL_PATH,  val_out)\n",
        "exact_dedup_stream_to_csv(TEST_PATH, test_out)\n",
        "\n",
        "# Final strict verify with C engine (should succeed if outputs are canonical)\n",
        "for p in (train_out, val_out, test_out):\n",
        "    df2 = pd.read_csv(p, engine=\"c\")\n",
        "    print(f\"OK (C-engine): {p} | rows={len(df2):,} | cols={len(df2.columns)} | md5={_md5(p)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwrxdmaFfA_I",
        "outputId": "a50341c7-3032-4fb0-a64f-71ef796214e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train rows before: 283,987\n",
            "Near-dups to drop (from audit): 126\n",
            "Train rows after:  283,861  (dropped 126)\n",
            "OK (C-engine): /content/cleaned_outputs/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast_applied.csv | rows=283,861 | cols=3 | md5=593a8f77b5c8fe3908d4cedca73a0669\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Apply near-dup drops post hoc (no LSH re-run)\n",
        "# =========================\n",
        "import os, csv, hashlib\n",
        "import pandas as pd\n",
        "\n",
        "TEXT_COL = \"article\"\n",
        "ID_COL   = \"id\"\n",
        "\n",
        "train_out = \"/content/cleaned_outputs/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "audit_prefix = train_out.replace(\".csv\",\"\")\n",
        "decisions_csv = f\"{audit_prefix}_near_dedup_decisions.csv\"\n",
        "\n",
        "# 1) Load train (C-engine, canonical RFC-4180) and decisions\n",
        "df = pd.read_csv(train_out, engine=\"c\")\n",
        "dec = pd.read_csv(decisions_csv, engine=\"c\")\n",
        "\n",
        "# 2) Collect IDs to drop (near dup detections logged during DRY_RUN)\n",
        "to_drop_ids = set(dec.loc[dec[\"decision\"].eq(\"would_drop_near\"), \"rid\"].astype(str))\n",
        "\n",
        "print(f\"Train rows before: {len(df):,}\")\n",
        "print(f\"Near-dups to drop (from audit): {len(to_drop_ids):,}\")\n",
        "\n",
        "# Safety check\n",
        "assert ID_COL in df.columns, f\"'{ID_COL}' column missing in {train_out}; cannot map decisions to rows.\"\n",
        "\n",
        "# 3) Drop rows by id\n",
        "mask_keep = ~df[ID_COL].astype(str).isin(to_drop_ids)\n",
        "df2 = df.loc[mask_keep].reset_index(drop=True)\n",
        "\n",
        "print(f\"Train rows after:  {len(df2):,}  (dropped {len(df) - len(df2):,})\")\n",
        "\n",
        "# 4) Save canonically and verify with C-engine\n",
        "out_applied = train_out.replace(\".csv\", \"_applied.csv\")\n",
        "tmp = out_applied + \".tmp\"\n",
        "df2.to_csv(tmp, index=False, encoding=\"utf-8\", quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n",
        "probe = pd.read_csv(tmp, engine=\"c\")  # strict parse\n",
        "os.replace(tmp, out_applied)\n",
        "\n",
        "md5 = hashlib.md5(open(out_applied, \"rb\").read()).hexdigest()\n",
        "print(f\"OK (C-engine): {out_applied} | rows={len(probe):,} | cols={len(probe.columns)} | md5={md5}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moZ4B9-AgRUf",
        "outputId": "e3889567-df43-4c3f-fc8a-1c2a0fbf7649"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "orig rows: 283987\n",
            "applied rows: 283861\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "orig = \"/content/cleaned_outputs/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "appl = \"/content/cleaned_outputs/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast_applied.csv\"\n",
        "print(\"orig rows:\", len(pd.read_csv(orig, engine=\"c\")))\n",
        "print(\"applied rows:\", len(pd.read_csv(appl, engine=\"c\")))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLqr3tzzg7vL",
        "outputId": "a8024d57-c49e-4e4a-c94e-07381b85a06a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_orig     -> rows=283,987 | cols=3\n",
            "train_applied  -> rows=283,861 | cols=3\n",
            "val            -> rows=13,368 | cols=3\n",
            "test           -> rows=11,488 | cols=3\n"
          ]
        }
      ],
      "source": [
        "# === Shapes & sanity for train/val (and test) ===\n",
        "import os, pandas as pd\n",
        "\n",
        "paths = {\n",
        "    \"train_orig\":   \"/content/cleaned_outputs/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\",\n",
        "    \"train_applied\":\"/content/cleaned_outputs/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast_applied.csv\",\n",
        "    \"val\":          \"/content/cleaned_outputs/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\",\n",
        "    \"test\":         \"/content/cleaned_outputs/test_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\",\n",
        "}\n",
        "\n",
        "# Expected counts from the successful run\n",
        "expected_rows = {\n",
        "    \"train_orig\":    283_987,\n",
        "    \"train_applied\": 283_861,  # train_orig - 126 near-dups\n",
        "    \"val\":            13_368,\n",
        "    \"test\":           11_488,\n",
        "}\n",
        "\n",
        "for name, p in paths.items():\n",
        "    if not os.path.exists(p):\n",
        "        print(f\"{name:14s} -> MISSING ({p})\")\n",
        "        continue\n",
        "    df = pd.read_csv(p, engine=\"c\")  # RFC-4180 canonical; strict parser\n",
        "    r, c = df.shape\n",
        "    msg = f\"{name:14s} -> rows={r:,} | cols={c}\"\n",
        "    if name in expected_rows and r != expected_rows[name]:\n",
        "        msg += f\"  [WARNING: expected {expected_rows[name]:,}]\"\n",
        "    print(msg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "-IeAvvfbiYnq",
        "outputId": "5aa6265f-6370-4033-e444-9fddbd7294e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using decisions. Showing top 5 near-dup examples by fuzzy_token_sort:\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"    print(\\\"\\\\n[CANDIDATE text \\u2014 first 800 chars]\\\\n\\\", cand_text[:800])\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"rid\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"15a641335ac57225d84423173485f8985ae50ada\",\n          \"b0917fed0170e3fb6470b44ffbe55bcb34151450\",\n          \"d4864e86b818a25c6e841fcddd823ef05d3ef6ad\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"candidate_rid\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"0a3f2400ba4e5cdf4b3638ae6fb60fdfa12a2680\",\n          \"0929c47d2ac6ec9ee80067c881fc594d0ab74e40\",\n          \"f372b28e3d1f74d60aa8ee29d7bdd49a6e7f9779\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fuzzy_token_sort\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.008457103787770016,\n        \"min\": 99.9792960662526,\n        \"max\": 100.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          99.9873689528862,\n          99.9792960662526,\n          99.98482319016544\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source_preview\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"(CareerBuilder.com) -- 2009 has given employers and employees a run for their money -- literally. Budgets have been cut, layoffs made and furloughs instituted, and benefits and perks have evaporated. \",\n          \"(InStyle.com) -- The co-star of \\\"27 Dresses\\\" discusses his personal style. \\\"I just feel dirty,\\\" says James Marsden. Not to be alarmed: There's nothing indecent going on here. James Marsden talks about\",\n          \"(InStyle.com) -- A hit TV show. An Emmy. A summer blockbuster. A new company. A wedding! Katherine Heigl has every reason to smile for our camera. A star -- and a trio of gorgeous looks -- is born. Th\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"candidate_preview\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-290a351c-4462-491c-b2f2-5965d54c2ac4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rid</th>\n",
              "      <th>candidate_rid</th>\n",
              "      <th>fuzzy_token_sort</th>\n",
              "      <th>source_preview</th>\n",
              "      <th>candidate_preview</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11978</th>\n",
              "      <td>be19bb0b18f50ac8b11abbba154f4bbd578cf773</td>\n",
              "      <td>f18c39b8839384a67020f55fd02cfdef643cd2ad</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>Officials declared a state of emergency Saturd...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13094</th>\n",
              "      <td>15a641335ac57225d84423173485f8985ae50ada</td>\n",
              "      <td>0a3f2400ba4e5cdf4b3638ae6fb60fdfa12a2680</td>\n",
              "      <td>99.987369</td>\n",
              "      <td>(CareerBuilder.com) -- 2009 has given employer...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5330</th>\n",
              "      <td>d4864e86b818a25c6e841fcddd823ef05d3ef6ad</td>\n",
              "      <td>f372b28e3d1f74d60aa8ee29d7bdd49a6e7f9779</td>\n",
              "      <td>99.984823</td>\n",
              "      <td>(InStyle.com) -- A hit TV show. An Emmy. A sum...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73164</th>\n",
              "      <td>3aad69ac9173d0db3e5f477b85d4a2ae8b26d019</td>\n",
              "      <td>c6c488004dc55c3bfa7ce4bd3f86e44df4de51e0</td>\n",
              "      <td>99.979512</td>\n",
              "      <td>(OPRAH.com) -- Chris Rock is an Emmy-winning c...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3610</th>\n",
              "      <td>b0917fed0170e3fb6470b44ffbe55bcb34151450</td>\n",
              "      <td>0929c47d2ac6ec9ee80067c881fc594d0ab74e40</td>\n",
              "      <td>99.979296</td>\n",
              "      <td>(InStyle.com) -- The co-star of \"27 Dresses\" d...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-290a351c-4462-491c-b2f2-5965d54c2ac4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-290a351c-4462-491c-b2f2-5965d54c2ac4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-290a351c-4462-491c-b2f2-5965d54c2ac4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b3031563-9b62-4537-b0d0-00b487332230\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b3031563-9b62-4537-b0d0-00b487332230')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b3031563-9b62-4537-b0d0-00b487332230 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                            rid  \\\n",
              "11978  be19bb0b18f50ac8b11abbba154f4bbd578cf773   \n",
              "13094  15a641335ac57225d84423173485f8985ae50ada   \n",
              "5330   d4864e86b818a25c6e841fcddd823ef05d3ef6ad   \n",
              "73164  3aad69ac9173d0db3e5f477b85d4a2ae8b26d019   \n",
              "3610   b0917fed0170e3fb6470b44ffbe55bcb34151450   \n",
              "\n",
              "                                  candidate_rid  fuzzy_token_sort  \\\n",
              "11978  f18c39b8839384a67020f55fd02cfdef643cd2ad        100.000000   \n",
              "13094  0a3f2400ba4e5cdf4b3638ae6fb60fdfa12a2680         99.987369   \n",
              "5330   f372b28e3d1f74d60aa8ee29d7bdd49a6e7f9779         99.984823   \n",
              "73164  c6c488004dc55c3bfa7ce4bd3f86e44df4de51e0         99.979512   \n",
              "3610   0929c47d2ac6ec9ee80067c881fc594d0ab74e40         99.979296   \n",
              "\n",
              "                                          source_preview  candidate_preview  \n",
              "11978  Officials declared a state of emergency Saturd...                NaN  \n",
              "13094  (CareerBuilder.com) -- 2009 has given employer...                NaN  \n",
              "5330   (InStyle.com) -- A hit TV show. An Emmy. A sum...                NaN  \n",
              "73164  (OPRAH.com) -- Chris Rock is an Emmy-winning c...                NaN  \n",
              "3610   (InStyle.com) -- The co-star of \"27 Dresses\" d...                NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Top example (full texts truncated) ===\n",
            "rid: be19bb0b18f50ac8b11abbba154f4bbd578cf773  <->  candidate_rid: f18c39b8839384a67020f55fd02cfdef643cd2ad\n",
            "fuzzy_token_sort (logged):     100.0\n",
            "fuzzy_token_sort (recomputed): 100.0\n",
            "\n",
            "[SOURCE text — first 800 chars]\n",
            " Officials declared a state of emergency Saturday after a powerful predawn earthquake struck near Christchurch, New Zealand, sending people into the streets as windows exploded, water mains broke and buildings crumbled. No deaths were immediately reported. The Christchurch City Council declared a state of emergency in response to what it called \"significant damage,\" just hours after the 7.0-magnitude earthquake rattled residents. The order allows authorities to force evacuations and prohibit entry into areas believed unsafe. Officials in Selwyn, a rural district near where the quake hit, also declared a state of local emergency. A curfew from 7 p.m. to 7 a.m. was in place, and the army was assisting local police to ensure there is no looting. Roughly 100 people were being treated for minor \n",
            "\n",
            "[CANDIDATE text — first 800 chars]\n",
            " Officials declared a state of emergency Saturday after a powerful predawn earthquake struck near Christchurch, New Zealand, sending people into the streets as windows exploded, water mains broke and buildings crumbled. No deaths were immediately reported. The Christchurch City Council declared a state of emergency in response to what it called \"significant damage,\" just hours after the 7.0-magnitude earthquake rattled residents. The order allows authorities to force evacuations and prohibit entry into areas believed unsafe. Officials in Selwyn, a rural district near where the quake hit, also declared a state of local emergency. A curfew from 7 p.m. to 7 a.m. was in place, and the army was assisting local police to ensure there is no looting. Roughly 100 people were being treated for minor \n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Show near-dup examples from decisions (fallback if samples empty)\n",
        "# ============================================================\n",
        "import os, sqlite3, pandas as pd\n",
        "from rapidfuzz.fuzz import token_sort_ratio\n",
        "from IPython.display import display\n",
        "\n",
        "TRAIN_ORIG   = \"/content/cleaned_outputs/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "AUDIT_PREFIX = TRAIN_ORIG.replace(\".csv\",\"\")\n",
        "SAMPLES      = f\"{AUDIT_PREFIX}_near_dedup_samples.csv\"\n",
        "DECISIONS    = f\"{AUDIT_PREFIX}_near_dedup_decisions.csv\"\n",
        "SQLITE_DB    = TRAIN_ORIG + \".sqlite\"\n",
        "\n",
        "def load_examples():\n",
        "    # 1) try samples\n",
        "    if os.path.exists(SAMPLES) and os.path.getsize(SAMPLES) > 0:\n",
        "        df = pd.read_csv(SAMPLES, engine=\"c\")\n",
        "        if len(df) > 0:\n",
        "            df = df.rename(columns={\n",
        "                \"rid\":\"rid\",\n",
        "                \"candidate_rid\":\"candidate_rid\",\n",
        "                \"fuzzy_token_sort\":\"fuzzy_token_sort\",\n",
        "                \"source_preview\":\"source_preview\",\n",
        "                \"candidate_preview\":\"candidate_preview\",\n",
        "            })\n",
        "            df = df.sort_values(\"fuzzy_token_sort\", ascending=False)\n",
        "            return df, \"samples\"\n",
        "    # 2) fallback to decisions (DRY_RUN writes would_drop_near here)\n",
        "    df = pd.read_csv(DECISIONS, engine=\"c\")\n",
        "    keep = df[\"decision\"].isin([\"would_drop_near\",\"drop_near\"])\n",
        "    df = df.loc[keep, [\"rid\",\"candidate_rid\",\"fuzzy_token_sort\",\"preview_source\",\"preview_candidate\"]].copy()\n",
        "    df = df.rename(columns={\"preview_source\":\"source_preview\",\"preview_candidate\":\"candidate_preview\"})\n",
        "    # make sure scores are numeric for sorting\n",
        "    df[\"fuzzy_token_sort\"] = pd.to_numeric(df[\"fuzzy_token_sort\"], errors=\"coerce\").fillna(0)\n",
        "    df = df.sort_values(\"fuzzy_token_sort\", ascending=False)\n",
        "    return df, \"decisions\"\n",
        "\n",
        "examples, src_used = load_examples()\n",
        "\n",
        "if len(examples) == 0:\n",
        "    print(\"No examples found in samples/decisions (unexpected if near_found > 0).\")\n",
        "else:\n",
        "    top_n = min(5, len(examples))\n",
        "    print(f\"Using {src_used}. Showing top {top_n} near-dup examples by fuzzy_token_sort:\\n\")\n",
        "    display(examples[[\"rid\",\"candidate_rid\",\"fuzzy_token_sort\",\"source_preview\",\"candidate_preview\"]].head(top_n))\n",
        "\n",
        "    # Pull full texts + recompute score for the very top example\n",
        "    rid_src  = str(examples.iloc[0][\"rid\"])\n",
        "    rid_cand = str(examples.iloc[0][\"candidate_rid\"])\n",
        "    score_logged = float(examples.iloc[0][\"fuzzy_token_sort\"])\n",
        "\n",
        "    con = sqlite3.connect(SQLITE_DB)\n",
        "    cur = con.cursor()\n",
        "    cur.execute(\"SELECT text FROM keep WHERE rid=?\", (rid_src,))\n",
        "    r1 = cur.fetchone()\n",
        "    cur.execute(\"SELECT text FROM keep WHERE rid=?\", (rid_cand,))\n",
        "    r2 = cur.fetchone()\n",
        "    con.close()\n",
        "\n",
        "    src_text  = (r1[0] if r1 else \"\") or \"\"\n",
        "    cand_text = (r2[0] if r2 else \"\") or \"\"\n",
        "    score_recomputed = token_sort_ratio(src_text, cand_text)\n",
        "\n",
        "    print(\"\\n=== Top example (full texts truncated) ===\")\n",
        "    print(f\"rid: {rid_src}  <->  candidate_rid: {rid_cand}\")\n",
        "    print(f\"fuzzy_token_sort (logged):     {score_logged}\")\n",
        "    print(f\"fuzzy_token_sort (recomputed): {score_recomputed}\")\n",
        "    print(\"\\n[SOURCE text — first 800 chars]\\n\", src_text[:800])\n",
        "    print(\"\\n[CANDIDATE text — first 800 chars]\\n\", cand_text[:800])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "STYRKj46kNrC",
        "outputId": "1a3c7cc8-2265-4d12-c922-898eda2f4cd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DROPPED (from original) ===\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"display(df_appl[df_appl[\\\"id\\\"]\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"article\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Officials declared a state of emergency Saturday after a powerful predawn earthquake struck near Christchurch, New Zealand, sending people into the streets as windows exploded, water mains broke and buildings crumbled. No deaths were immediately reported. The Christchurch City Council declared a state of emergency in response to what it called \\\"significant damage,\\\" just hours after the 7.0-magnitude earthquake rattled residents. The order allows authorities to force evacuations and prohibit entry into areas believed unsafe. Officials in Selwyn, a rural district near where the quake hit, also declared a state of local emergency. A curfew from 7 p.m. to 7 a.m. was in place, and the army was assisting local police to ensure there is no looting. Roughly 100 people were being treated for minor bumps and cuts after the strong quake, hospital officials said. Two people suffered more serious injuries. \\\"The house felt like it was on wheels, like it was rolling around on marbles,\\\" resident Hadlee Wright told CNN's Rick Sanchez. Pictures that Wright took of the city before daybreak showed collapsed buildings and streets littered with bits of brick and rock. The facade of one structure was almost entirely torn off. Power was out in the northwest part of the city, while water and sewage services have been affected in several regions, the Christchurch Civil Defense Group said in a statement. Roads were also damaged. Images taken by Jimmy Le Comte, and sent to CNN's iReport, showed flooding in New Brighton, a Christchurch suburb. In one, a giant crack cuts across a road. The quake had a magnitude of 7.0, down from an initial assessment of 7.4, the U.S. Geological Survey said. It struck about 35 miles from Christchurch, a city with a population of some 386,000 people on the east coast of South Island. An aftershock with a magnitude of 5.7 struck not far from the epicenter about 20 minutes later, the survey said. A man in his 50s was hit by a falling chimney, while another suffered serious injuries after being cut by glass, said Michele Hider, a spokeswoman with Christchurch Hospital. Sebastian Koga, a hospital neurosurgeon, said roughly 100 people were being treated for minor injuries. He was not aware of any deaths. \\\"We've had a flood of lacerations and minor head injuries, but nothing that could not be handled,\\\" Koga said. Civil Defense Minister John Carter said the country's prime minister is headed to Christchurch to assess the earthquake damage. \\\"We don't know entirely what level of issues we're dealing with at the moment. We're still getting reports on it, but it has been extensive,\\\" Carter told CNN affiliate TVNZ. \\\"We were lucky that the impact on the people from a death point of view has not been what it could have been under normal circumstances.\\\" The earthquake struck at 4:35 a.m. Saturday (12:35 p.m. ET Friday), when few people would have been out and about. Police said there was some initial looting activity, but that it was quickly brought under control. A man who was at the international airport in Christchurch described the scene. \\\"The entire terminal started shaking,\\\" he said. \\\"I knew it was an earthquake. There was not much you could do at that point.\\\" Authorities evacuated the airport, he said, adding that he saw minor damage. Reinier Eulink, general manager of the Holiday Inn in Christchurch, said there is damage around the hotel corridors and \\\"big cracks in the walls.\\\" \\\"It was a big big long jolt, and the building moved a lot,\\\" he said. The 13-floor building, with about 150 rooms, was about 40 percent occupied, and he estimated that 80 or more people were staying at the hotel at the time. Power was knocked out, but emergency power came on, Eulink added. People were milling around in the hotel lobby, trying to get warm during the chilly Southern Hemisphere winter. The quake was 7.5 miles deep, according to the U.S. Geological Survey. The Pacific Tsunami Warning Center said the event is not likely to generate a tsunami. Prime Minister John Key told CNN affiliate TVNZ it would likely be some time before the full cost of the quake could be calculated. He sought to reassure residents. \\\"We're not going to let Christchurch suffer this great tragedy on its own,\\\" said Key. CNN's Nick Valencia, Mark Bixler, Joe Sterling and Katy Byron contributed to this report.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"highlights\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"NEW: Prime minister says government will not abandon Christchurch . Two people are being treated for serious injuries . A 7.0-magnitude earthquake hit the east coast of the South Island early Saturday . Roads and buildings were damaged .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"be19bb0b18f50ac8b11abbba154f4bbd578cf773\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-94f62884-4207-4c1a-a0c2-76fa7d76b3f0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article</th>\n",
              "      <th>highlights</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11978</th>\n",
              "      <td>Officials declared a state of emergency Saturd...</td>\n",
              "      <td>NEW: Prime minister says government will not a...</td>\n",
              "      <td>be19bb0b18f50ac8b11abbba154f4bbd578cf773</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-94f62884-4207-4c1a-a0c2-76fa7d76b3f0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-94f62884-4207-4c1a-a0c2-76fa7d76b3f0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-94f62884-4207-4c1a-a0c2-76fa7d76b3f0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                 article  \\\n",
              "11978  Officials declared a state of emergency Saturd...   \n",
              "\n",
              "                                              highlights  \\\n",
              "11978  NEW: Prime minister says government will not a...   \n",
              "\n",
              "                                             id  \n",
              "11978  be19bb0b18f50ac8b11abbba154f4bbd578cf773  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== KEPT (in applied) ===\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"display(df_appl[df_appl[\\\"id\\\"]\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"article\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Officials declared a state of emergency Saturday after a powerful predawn earthquake struck near Christchurch, New Zealand, sending people into the streets as windows exploded, water mains broke and buildings crumbled. No deaths were immediately reported. The Christchurch City Council declared a state of emergency in response to what it called \\\"significant damage,\\\" just hours after the 7.0-magnitude earthquake rattled residents. The order allows authorities to force evacuations and prohibit entry into areas believed unsafe. Officials in Selwyn, a rural district near where the quake hit, also declared a state of local emergency. A curfew from 7 p.m. to 7 a.m. was in place, and the army was assisting local police to ensure there is no looting. Roughly 100 people were being treated for minor bumps and cuts after the strong quake, hospital officials said. Two people suffered more serious injuries. \\\"The house felt like it was on wheels, like it was rolling around on marbles,\\\" resident Hadlee Wright told CNN's Rick Sanchez. Pictures that Wright took of the city before daybreak showed collapsed buildings and streets littered with bits of brick and rock. The facade of one structure was almost entirely torn off. Power was out in the northwest part of the city, while water and sewage services have been affected in several regions, the Christchurch Civil Defense Group said in a statement. Roads also were damaged. Images taken by Jimmy Le Comte, and sent to CNN's iReport, showed flooding in New Brighton, a Christchurch suburb. In one, a giant crack cuts across a road. The quake had a magnitude of 7.0, down from an initial assessment of 7.4, the U.S. Geological Survey said. It struck about 35 miles from Christchurch, a city with a population of some 386,000 people on the east coast of South Island. An aftershock with a magnitude of 5.7 struck not far from the epicenter about 20 minutes later, the survey said. A man in his 50s was hit by a falling chimney, while another suffered serious injuries after being cut by glass, said Michele Hider, a spokeswoman with Christchurch Hospital. Sebastian Koga, a hospital neurosurgeon, said roughly 100 people were being treated for minor injuries. He was not aware of any deaths. \\\"We've had a flood of lacerations and minor head injuries, but nothing that could not be handled,\\\" Koga said. Civil Defense Minister John Carter said the country's prime minister is headed to Christchurch to assess the earthquake damage. \\\"We don't know entirely what level of issues we're dealing with at the moment. We're still getting reports on it, but it has been extensive,\\\" Carter told CNN affiliate TVNZ. \\\"We were lucky that the impact on the people from a death point of view has not been what it could have been under normal circumstances.\\\" The earthquake struck at 4:35 a.m. Saturday (12:35 p.m. ET Friday), when few people would have been out and about. Police said there was some initial looting activity, but that it was quickly brought under control. A man who was at the international airport in Christchurch described the scene. \\\"The entire terminal started shaking,\\\" he said. \\\"I knew it was an earthquake. There was not much you could do at that point.\\\" Authorities evacuated the airport, he said, adding that he saw minor damage. Reinier Eulink, general manager of the Holiday Inn in Christchurch, said there is damage around the hotel corridors and \\\"big cracks in the walls.\\\" \\\"It was a big big long jolt, and the building moved a lot,\\\" he said. The 13-floor building, with about 150 rooms, was about 40 percent occupied, and he estimated that 80 or more people were staying at the hotel at the time. Power was knocked out, but emergency power came on, Eulink added. People were milling around in the hotel lobby, trying to get warm during the chilly Southern Hemisphere winter. The quake was 7.5 miles deep, according to the U.S. Geological Survey. The Pacific Tsunami Warning Center said the event is not likely to generate a tsunami. Prime Minister John Key told CNN affiliate TVNZ it would likely be some time before the full cost of the quake could be calculated. He sought to reassure residents. \\\"We're not going to let Christchurch suffer this great tragedy on its own,\\\" said Key. CNN's Nick Valencia, Mark Bixler, Joe Sterling and Katy Byron contributed to this report.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"highlights\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"NEW: Prime minister says government will not abandon Christchurch . Two people are being treated for serious injuries . A 7.0-magnitude earthquake hit the east coast of the South Island early Saturday . Roads and buildings were damaged .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"f18c39b8839384a67020f55fd02cfdef643cd2ad\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-e8c46820-306b-4056-8e72-f307c7672385\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article</th>\n",
              "      <th>highlights</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11948</th>\n",
              "      <td>Officials declared a state of emergency Saturd...</td>\n",
              "      <td>NEW: Prime minister says government will not a...</td>\n",
              "      <td>f18c39b8839384a67020f55fd02cfdef643cd2ad</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e8c46820-306b-4056-8e72-f307c7672385')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e8c46820-306b-4056-8e72-f307c7672385 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e8c46820-306b-4056-8e72-f307c7672385');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                 article  \\\n",
              "11948  Officials declared a state of emergency Saturd...   \n",
              "\n",
              "                                              highlights  \\\n",
              "11948  NEW: Prime minister says government will not a...   \n",
              "\n",
              "                                             id  \n",
              "11948  f18c39b8839384a67020f55fd02cfdef643cd2ad  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "TRAIN_ORIG = \"/content/cleaned_outputs/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "TRAIN_APPL = \"/content/cleaned_outputs/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast_applied.csv\"\n",
        "\n",
        "rid  = \"be19bb0b18f50ac8b11abbba154f4bbd578cf773\"   # dropped\n",
        "cand = \"f18c39b8839384a67020f55fd02cfdef643cd2ad\"   # kept\n",
        "\n",
        "df_orig = pd.read_csv(TRAIN_ORIG, engine=\"c\")\n",
        "df_appl = pd.read_csv(TRAIN_APPL, engine=\"c\")\n",
        "\n",
        "print(\"=== DROPPED (from original) ===\")\n",
        "display(df_orig[df_orig[\"id\"].astype(str).eq(rid)])\n",
        "\n",
        "print(\"=== KEPT (in applied) ===\")\n",
        "display(df_appl[df_appl[\"id\"].astype(str).eq(cand)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9V5pp98a2V_m",
        "outputId": "243fbba6-7841-4be9-e755-7790e7456917"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved: /content/drive/MyDrive/cnndm_near_dedup/20250814_135141/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv | 1185.30 MB | md5=04cee24e3616e64787177d6b520545c5\n",
            "Saved: /content/drive/MyDrive/cnndm_near_dedup/20250814_135141/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv | 54.76 MB | md5=5d44f9bd33bfed9fe8eb599e586d04fa\n",
            "Saved: /content/drive/MyDrive/cnndm_near_dedup/20250814_135141/test_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv | 47.35 MB | md5=dc8031d8b79fff25135598e5b8336d04\n",
            "Saved: /content/drive/MyDrive/cnndm_near_dedup/20250814_135141/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast_near_dedup_decisions.csv | 76.87 MB | md5=0fd7992395d8e17a85a2caa9f9f96c7e\n",
            "Saved: /content/drive/MyDrive/cnndm_near_dedup/20250814_135141/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast_near_dedup_samples.csv | 0.00 MB | md5=6ea436ce9bd2820150c9464e6b1633f1\n",
            "Saved: /content/drive/MyDrive/cnndm_near_dedup/20250814_135141/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast_near_dedup_summary.json | 0.00 MB | md5=adf2e4cc86037d1be427f3d6972b6d96\n",
            "Saved: /content/drive/MyDrive/cnndm_near_dedup/20250814_135141/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv.sqlite | 1316.91 MB | md5=faa0bc39b03747a2f9a531d008831216\n",
            "Saved: /content/drive/MyDrive/cnndm_near_dedup/20250814_135141/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast_applied.csv | 1184.65 MB | md5=593a8f77b5c8fe3908d4cedca73a0669\n",
            "Saved: /content/drive/MyDrive/cnndm_near_dedup/20250814_135141/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv | 54.76 MB | md5=5d44f9bd33bfed9fe8eb599e586d04fa\n",
            "Saved: /content/drive/MyDrive/cnndm_near_dedup/20250814_135141/test_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv | 47.35 MB | md5=dc8031d8b79fff25135598e5b8336d04\n",
            "\n",
            "All done. Files saved under: /content/drive/MyDrive/cnndm_near_dedup/20250814_135141\n"
          ]
        }
      ],
      "source": [
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, shutil, hashlib, time\n",
        "\n",
        "# --- Helper: md5 + size ---\n",
        "def _md5(path):\n",
        "    h = hashlib.md5()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(1<<20), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def _size_mb(path):\n",
        "    return os.path.getsize(path) / (1024*1024)\n",
        "\n",
        "# --- Destination folder on Drive (timestamped to avoid overwrites) ---\n",
        "stamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "DEST = f\"/content/drive/MyDrive/cnndm_near_dedup/{stamp}\"\n",
        "os.makedirs(DEST, exist_ok=True)\n",
        "\n",
        "# --- Files to copy ---\n",
        "maybe_files = [\n",
        "    # From your near-dedup run\n",
        "    train_out,\n",
        "    val_out,\n",
        "    test_out,\n",
        "    f\"{audit_prefix}_near_dedup_decisions.csv\",\n",
        "    f\"{audit_prefix}_near_dedup_samples.csv\",\n",
        "    f\"{audit_prefix}_near_dedup_summary.json\",\n",
        "    train_out + \".sqlite\",\n",
        "\n",
        "    # Extra paths you requested\n",
        "    \"/content/cleaned_outputs/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast_applied.csv\",\n",
        "    \"/content/cleaned_outputs/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\",\n",
        "    \"/content/cleaned_outputs/test_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "]\n",
        "\n",
        "# --- Copy with verification ---\n",
        "copied = []\n",
        "for src in maybe_files:\n",
        "    if os.path.exists(src):\n",
        "        dst = os.path.join(DEST, os.path.basename(src))\n",
        "        shutil.copy2(src, dst)\n",
        "        print(f\"Saved: {dst} | { _size_mb(dst):.2f} MB | md5={_md5(dst)}\")\n",
        "        copied.append(dst)\n",
        "    else:\n",
        "        print(f\"(skip) Not found: {src}\")\n",
        "\n",
        "print(f\"\\nAll done. Files saved under: {DEST}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcIYhZQcspjF"
      },
      "source": [
        "# EDA Before and After cleaning comparison Recap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVDTPQe9ufy7",
        "outputId": "8a6c4a69-ce2f-4fdf-b7dd-adcc8f18159e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Dataset Shapes (rows, columns) ===\n",
            "               Original      Cleaned\n",
            "Train       (287113, 3)  (283861, 3)\n",
            "Validation   (13368, 3)   (13368, 3)\n",
            "Test         (11490, 3)   (11488, 3)\n"
          ]
        }
      ],
      "source": [
        "# ====== Dataset Shape Comparison ======\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "# ---------------- Load Original Dataset ----------------\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "df_train = pd.DataFrame(dataset[\"train\"])\n",
        "df_val   = pd.DataFrame(dataset[\"validation\"])\n",
        "df_test  = pd.DataFrame(dataset[\"test\"])\n",
        "\n",
        "# ---------------- Load Cleaned CSVs ----------------\n",
        "train_csv = \"/content/drive/MyDrive/cleaned_outputs/naa/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast_applied.csv\"\n",
        "val_csv   = \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "test_csv  = \"/content/drive/MyDrive/cleaned_outputs/naa/test_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "\n",
        "df_train_clean = pd.read_csv(train_csv)\n",
        "df_val_clean   = pd.read_csv(val_csv)\n",
        "df_test_clean  = pd.read_csv(test_csv)\n",
        "\n",
        "# ---------------- Report Shapes ----------------\n",
        "shape_report = pd.DataFrame({\n",
        "    \"Original\": [df_train.shape, df_val.shape, df_test.shape],\n",
        "    \"Cleaned\":  [df_train_clean.shape, df_val_clean.shape, df_test_clean.shape],\n",
        "}, index=[\"Train\", \"Validation\", \"Test\"])\n",
        "\n",
        "print(\"=== Dataset Shapes (rows, columns) ===\")\n",
        "print(shape_report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "c5Z69hITso0L",
        "outputId": "feac92dd-8ddf-4394-9e74-f7b1f9d52e17"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ====== EDA: Token Length Distribution Before vs After Cleaning ======\n",
        "import os, pandas as pd, matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import load_dataset\n",
        "\n",
        "# ---------------- Load Original Dataset ----------------\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "df_train = pd.DataFrame(dataset[\"train\"])\n",
        "df_val   = pd.DataFrame(dataset[\"validation\"])\n",
        "df_test  = pd.DataFrame(dataset[\"test\"])\n",
        "\n",
        "# ---------------- Load Cleaned CSVs ----------------\n",
        "train_csv = \"/content/drive/MyDrive/cleaned_outputs/naa/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast_applied.csv\"\n",
        "val_csv   = \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "test_csv  = \"/content/drive/MyDrive/cleaned_outputs/naa/test_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "\n",
        "df_train_clean = pd.read_csv(train_csv)\n",
        "df_val_clean   = pd.read_csv(val_csv)\n",
        "df_test_clean  = pd.read_csv(test_csv)\n",
        "\n",
        "# ---------------- Function: compute token lengths ----------------\n",
        "def compute_lengths(df, col=\"article\"):\n",
        "    return df[col].astype(str).str.split().apply(len)\n",
        "\n",
        "# Example: focus on TRAIN first\n",
        "orig_lengths  = compute_lengths(df_train, \"article\")\n",
        "clean_lengths = compute_lengths(df_train_clean, \"article\")\n",
        "\n",
        "# ---------------- Plot Comparison ----------------\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.kdeplot(orig_lengths, label=\"Original\", color=\"red\", fill=True, alpha=0.3)\n",
        "sns.kdeplot(clean_lengths, label=\"Cleaned\", color=\"blue\", fill=True, alpha=0.3)\n",
        "\n",
        "plt.title(\"Token Length Distribution — Train Set (Original vs Cleaned)\", fontsize=14)\n",
        "plt.xlabel(\"Number of Tokens per Article\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.legend()\n",
        "\n",
        "# Add summary stats\n",
        "stats_text = (\n",
        "    f\"Original: mean={orig_lengths.mean():.1f}, median={orig_lengths.median()}, max={orig_lengths.max()}\\n\"\n",
        "    f\"Cleaned:  mean={clean_lengths.mean():.1f}, median={clean_lengths.median()}, max={clean_lengths.max()}\"\n",
        ")\n",
        "plt.gcf().text(0.65, 0.75, stats_text, fontsize=10, bbox=dict(facecolor='white', alpha=0.6))\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1732
        },
        "id": "sD0GigGntv1T",
        "outputId": "2517349d-38ea-4a39-bb3c-29a5e50e74b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m126.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ====== EDA: Entities and Rare Words Comparison ======\n",
        "import spacy, collections, seaborn as sns, matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "# ---------------- Load Data ----------------\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "df_train = pd.DataFrame(dataset[\"train\"])\n",
        "df_train_clean = pd.read_csv(\"/content/drive/MyDrive/cleaned_outputs/naa/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast_applied.csv\")\n",
        "\n",
        "# ---------------- Sample Subset (NER is expensive) ----------------\n",
        "SAMPLE_SIZE = 10_000\n",
        "df_orig_sample  = df_train.sample(n=SAMPLE_SIZE, random_state=0)\n",
        "df_clean_sample = df_train_clean.sample(n=SAMPLE_SIZE, random_state=0)\n",
        "\n",
        "# ---------------- Load spaCy model ----------------\n",
        "!python -m spacy download en_core_web_sm\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"tagger\"])\n",
        "\n",
        "def count_entities(texts):\n",
        "    ent_counts = []\n",
        "    ent_types = collections.Counter()\n",
        "    for doc in nlp.pipe(texts, batch_size=64):\n",
        "        ent_counts.append(len(doc.ents))\n",
        "        ent_types.update([ent.label_ for ent in doc.ents])\n",
        "    return ent_counts, ent_types\n",
        "\n",
        "# Compute entity stats\n",
        "orig_ent_counts, orig_ent_types   = count_entities(df_orig_sample[\"article\"].tolist())\n",
        "clean_ent_counts, clean_ent_types = count_entities(df_clean_sample[\"article\"].tolist())\n",
        "\n",
        "# ---------------- Rare Word Analysis ----------------\n",
        "def get_word_freqs(df, col=\"article\"):\n",
        "    counter = collections.Counter()\n",
        "    for text in df[col].astype(str):\n",
        "        counter.update(text.split())\n",
        "    return counter\n",
        "\n",
        "orig_freqs  = get_word_freqs(df_orig_sample)\n",
        "clean_freqs = get_word_freqs(df_clean_sample)\n",
        "\n",
        "def count_rare_words(texts, freqs, threshold=5):\n",
        "    rare = set([w for w,c in freqs.items() if c <= threshold])\n",
        "    return [sum(1 for w in t.split() if w in rare) for t in texts]\n",
        "\n",
        "orig_rare_counts  = count_rare_words(df_orig_sample[\"article\"], orig_freqs)\n",
        "clean_rare_counts = count_rare_words(df_clean_sample[\"article\"], clean_freqs)\n",
        "\n",
        "# ---------------- Plot Entity Counts ----------------\n",
        "plt.figure(figsize=(12,5))\n",
        "sns.kdeplot(orig_ent_counts, label=\"Original\", color=\"red\", fill=True, alpha=0.3)\n",
        "sns.kdeplot(clean_ent_counts, label=\"Cleaned\", color=\"blue\", fill=True, alpha=0.3)\n",
        "plt.title(\"Distribution of Entity Counts per Article \")\n",
        "plt.xlabel(\"Number of Entities per Article\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# ---------------- Plot Rare Word Counts ----------------\n",
        "plt.figure(figsize=(12,5))\n",
        "sns.kdeplot(orig_rare_counts, label=\"Original\", color=\"red\", fill=True, alpha=0.3)\n",
        "sns.kdeplot(clean_rare_counts, label=\"Cleaned\", color=\"blue\", fill=True, alpha=0.3)\n",
        "plt.title(\"Distribution of Rare Word Counts per Article (freq ≤5)\")\n",
        "plt.xlabel(\"Number of Rare Words per Article\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# ---------------- Entity Type Frequency Comparison ----------------\n",
        "ent_type_df = pd.DataFrame({\n",
        "    \"Entity Type\": list(set(orig_ent_types.keys()) | set(clean_ent_types.keys())),\n",
        "    \"Original\": [orig_ent_types.get(t,0) for t in set(orig_ent_types.keys()) | set(clean_ent_types.keys())],\n",
        "    \"Cleaned\": [clean_ent_types.get(t,0) for t in set(orig_ent_types.keys()) | set(clean_ent_types.keys())],\n",
        "})\n",
        "ent_type_df.set_index(\"Entity Type\").plot(kind=\"bar\", figsize=(12,5))\n",
        "plt.title(\"Entity Type Frequency Comparison\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        },
        "id": "sruBhsIUuRo1",
        "outputId": "c4e555b4-9262-41ca-d6ed-ac6b4e3b948c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'after': {'3-gram': {'train-test': 0.25962218030749834,\n",
            "                      'train-val': 0.2569626181239913,\n",
            "                      'val-test': 0.16006581056674898},\n",
            "           '4-gram': {'train-test': 0.1124789428513909,\n",
            "                      'train-val': 0.11314239241343023,\n",
            "                      'val-test': 0.07023170533102822}},\n",
            " 'before': {'3-gram': {'train-test': 0.2569367913847004,\n",
            "                       'train-val': 0.25404059929756534,\n",
            "                       'val-test': 0.15869404029518516},\n",
            "            '4-gram': {'train-test': 0.11101892327378571,\n",
            "                       'train-val': 0.11148236713818442,\n",
            "                       'val-test': 0.06961716773199154}}}\n"
          ]
        }
      ],
      "source": [
        "# ====== EDA: N-gram Overlap Analysis (3-grams & 4-grams) ======\n",
        "import pandas as pd, matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "import itertools\n",
        "\n",
        "# ---------------- Load Original Dataset ----------------\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "df_train = pd.DataFrame(dataset[\"train\"])\n",
        "df_val   = pd.DataFrame(dataset[\"validation\"])\n",
        "df_test  = pd.DataFrame(dataset[\"test\"])\n",
        "\n",
        "# ---------------- Load Cleaned CSVs ----------------\n",
        "train_csv = \"/content/drive/MyDrive/cleaned_outputs/naa/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast_applied.csv\"\n",
        "val_csv   = \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "test_csv  = \"/content/drive/MyDrive/cleaned_outputs/naa/test_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "\n",
        "df_train_clean = pd.read_csv(train_csv)\n",
        "df_val_clean   = pd.read_csv(val_csv)\n",
        "df_test_clean  = pd.read_csv(test_csv)\n",
        "\n",
        "# ---------------- N-gram Extraction ----------------\n",
        "def get_ngrams(texts, n=3, sample_size=50000, seed=0):\n",
        "    # sample subset for tractability\n",
        "    if len(texts) > sample_size:\n",
        "        texts = texts.sample(n=sample_size, random_state=seed)\n",
        "    ngrams = set()\n",
        "    for t in texts.astype(str):\n",
        "        words = t.split()\n",
        "        ngrams.update(tuple(words[i:i+n]) for i in range(len(words)-n+1))\n",
        "    return ngrams\n",
        "\n",
        "# ---------------- Compute Overlap ----------------\n",
        "def jaccard_overlap(setA, setB):\n",
        "    return len(setA & setB) / len(setB) if len(setB) > 0 else 0\n",
        "\n",
        "splits_before = {\n",
        "    \"train\": df_train[\"article\"],\n",
        "    \"val\":   df_val[\"article\"],\n",
        "    \"test\":  df_test[\"article\"],\n",
        "}\n",
        "splits_after = {\n",
        "    \"train\": df_train_clean[\"article\"],\n",
        "    \"val\":   df_val_clean[\"article\"],\n",
        "    \"test\":  df_test_clean[\"article\"],\n",
        "}\n",
        "\n",
        "results = {\"before\": {}, \"after\": {}}\n",
        "for label, splits in [(\"before\", splits_before), (\"after\", splits_after)]:\n",
        "    # Extract ngrams\n",
        "    ngrams3 = {s: get_ngrams(texts, n=3) for s, texts in splits.items()}\n",
        "    ngrams4 = {s: get_ngrams(texts, n=4) for s, texts in splits.items()}\n",
        "\n",
        "    # Compute overlaps\n",
        "    results[label][\"3-gram\"] = {\n",
        "        \"train-val\": jaccard_overlap(ngrams3[\"train\"], ngrams3[\"val\"]),\n",
        "        \"train-test\": jaccard_overlap(ngrams3[\"train\"], ngrams3[\"test\"]),\n",
        "        \"val-test\": jaccard_overlap(ngrams3[\"val\"], ngrams3[\"test\"]),\n",
        "    }\n",
        "    results[label][\"4-gram\"] = {\n",
        "        \"train-val\": jaccard_overlap(ngrams4[\"train\"], ngrams4[\"val\"]),\n",
        "        \"train-test\": jaccard_overlap(ngrams4[\"train\"], ngrams4[\"test\"]),\n",
        "        \"val-test\": jaccard_overlap(ngrams4[\"val\"], ngrams4[\"test\"]),\n",
        "    }\n",
        "\n",
        "# ---------------- Plot Results ----------------\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12,5))\n",
        "\n",
        "for i, ngram in enumerate([\"3-gram\", \"4-gram\"]):\n",
        "    before_vals = list(results[\"before\"][ngram].values())\n",
        "    after_vals  = list(results[\"after\"][ngram].values())\n",
        "    labels      = list(results[\"before\"][ngram].keys())\n",
        "\n",
        "    x = range(len(labels))\n",
        "    axes[i].bar([p-0.2 for p in x], before_vals, width=0.4, label=\"Before Cleaning\")\n",
        "    axes[i].bar([p+0.2 for p in x], after_vals, width=0.4, label=\"After Cleaning\")\n",
        "\n",
        "    axes[i].set_title(f\"{ngram} Overlap (Jaccard Index)\")\n",
        "    axes[i].set_xticks(x)\n",
        "    axes[i].set_xticklabels(labels)\n",
        "    axes[i].set_ylabel(\"Overlap\")\n",
        "    axes[i].legend()\n",
        "\n",
        "plt.suptitle(\"N-gram Overlap Across Splits (Before vs After Cleaning)\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print raw results\n",
        "import pprint\n",
        "pprint.pprint(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ga_0544twt0m",
        "outputId": "e585f918-6d70-48b9-b753-9b79fc127b3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes: (283987, 3) (283861, 3)\n",
            "Dropped rows: 126\n",
            "Unique articles removed: 126\n",
            "\n",
            "--- Example 1 ---\n",
            "By . Anthony Bond . PUBLISHED: . 07:45 EST, 23 April 2013 . UPDATED: . 02:14 EST, 24 April 2013 . It would not be out of place speeding around famous Formula One circuits like Silverstone or Monaco. But incredibly, this racing car is not the real thing. Rather than being built in a high-tech garage by a team of expert engineers, it was instead put together in a shed in Brighton by a complete novice. Kevin Thomas, who has no engineering experience, spent four years piecing together the BAR-Honda ...\n",
            "\n",
            "\n",
            "--- Example 2 ---\n",
            "(CareerBuilder.com) -- 2009 has given employers and employees a run for their money -- literally. Budgets have been cut, layoffs made and furloughs instituted, and benefits and perks have evaporated. At the beginning of the year, 38 percent of employers said the economy would force them to make administrative cuts sometime during 2009, according to a survey by CareerBuilder.com. Sixty-five percent of those employers indicated that they would cut back company social events, 61 percent anticipated...\n",
            "\n",
            "\n",
            "--- Example 3 ---\n",
            "Stretching out into the still blue lagoon waters, set against a backdrop of rugged mountains, these are the spectacular first images of how Rio de Janeiro's Olympic park will look in 2016. Designs for the complex in the Barra da Tijuca neighbourhood of the city have been revealed by the architects that won the contract to mastermind the project in London. The waterfront park will be built on the former Brazilian grand prix track in a striking triangular layout spread over 300 acres. Scroll down ...\n",
            "\n",
            "\n",
            "--- Example 4 ---\n",
            "By . Paul Collins . A group of Irish cyclists who travelled to Liverpool to raise money for the victims of the Hillsborough disaster had their bikes stolen - outside Anfield. The six intended to take part in a 96km charity ride across the city to mark the 25th anniversary of the tragedy that left 96 football fans dead. But after taking a tour of Liverpool FC's Anfield stadium, they returned to find their bikes, equipment and clothing worth more than £30,000 had been stolen from their van, which ...\n",
            "\n",
            "\n",
            "--- Example 5 ---\n",
            "She's become known for her controversial comments during her participation in debates on This Morning. But on Tuesday, Katie Hopkins finally met her match - after facing off against Peaches Geldof in a discussion about attachment parenting. Despite aiming several low blows at Peaches, including referencing a picture of Peaches' son Astala falling out of his pram while the star was on the phone, Katie's argument couldn't stand up to Peaches' comments about the principles of AP. 'The panto witch h...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ====== Inspect Near-Duplicates Dropped items ===\n",
        "import pandas as pd\n",
        "\n",
        "TRAIN_ORIG = \"/content/drive/MyDrive/cleaned_outputs/naa/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "TRAIN_APPL = \"/content/drive/MyDrive/cleaned_outputs/naa/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast_applied.csv\"\n",
        "\n",
        "# Load both versions\n",
        "df_orig  = pd.read_csv(TRAIN_ORIG)\n",
        "df_appl  = pd.read_csv(TRAIN_APPL)\n",
        "\n",
        "print(\"Shapes:\", df_orig.shape, df_appl.shape)\n",
        "print(\"Dropped rows:\", len(df_orig) - len(df_appl))\n",
        "\n",
        "# Find which rows were removed\n",
        "# (works if 'article' column was used for deduplication)\n",
        "orig_set  = set(df_orig[\"article\"])\n",
        "appl_set  = set(df_appl[\"article\"])\n",
        "\n",
        "removed_articles = list(orig_set - appl_set)\n",
        "\n",
        "print(f\"Unique articles removed: {len(removed_articles)}\")\n",
        "\n",
        "# Show a few examples\n",
        "for i, art in enumerate(removed_articles[:5], 1):\n",
        "    print(f\"\\n--- Example {i} ---\\n{art[:500]}...\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bfm6kju_5irH"
      },
      "source": [
        "# Tokenization using SentencePiece and Unigram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tl89pmA1_Tqv",
        "outputId": "34f53c2b-70b4-43c6-9412-8c601346c2f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.3 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "#Train  SentencePiece (Unigram)\n",
        "# ============================================================\n",
        "\n",
        "!pip -q install sentencepiece==0.2.0 pandas pyarrow\n",
        "\n",
        "import os, csv, json, time, gc, hashlib, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "from typing import List\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# --- Data paths (update if needed) ---\n",
        "DATA_DIR = \"/content/drive/MyDrive/cnndm_near_dedup/20250814_135141\"\n",
        "\n",
        "TRAIN_CSV = f\"{DATA_DIR}/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast_applied.csv\"\n",
        "VAL_CSV   = f\"{DATA_DIR}/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "TEST_CSV  = f\"{DATA_DIR}/test_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "\n",
        "for p in (TRAIN_CSV, VAL_CSV, TEST_CSV):\n",
        "    assert os.path.exists(p), f\"Missing: {p}\"\n",
        "\n",
        "# --- Output dirs ---\n",
        "STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "TOKENIZER_DIR = f\"/content/drive/MyDrive/tokenizers/sp_shared_{STAMP}\"\n",
        "OUT_DIR       = f\"/content/drive/MyDrive/tokenized_outputs/sp_shared_{STAMP}\"\n",
        "os.makedirs(TOKENIZER_DIR, exist_ok=True)\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "SP_PREFIX     = os.path.join(TOKENIZER_DIR, \"sp_unigram_shared\")\n",
        "SP_MODEL_PATH = SP_PREFIX + \".model\"\n",
        "\n",
        "ID_COL, SRC_COL, TGT_COL = \"id\", \"article\", \"highlights\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxgmzB8-_iR3",
        "outputId": "a560e54e-98c1-4ead-f102-edeba9d7b130"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corpus saved: /content/drive/MyDrive/tokenizers/sp_shared_20250819_110137/tokenizer_corpus.txt | lines=68,704\n",
            "Tokenizer trained. Vocab=32000 | UNK=1 | EOS=3\n"
          ]
        }
      ],
      "source": [
        "# --- Tokenizer training config ---\n",
        "VOCAB_SIZE     = 32000\n",
        "CHAR_COVERAGE  = 0.9995\n",
        "NORMALIZATION  = \"nmt_nfkc_cf\"\n",
        "NUM_THREADS    = os.cpu_count() or 4\n",
        "MAX_SENT_CHARS = 40000\n",
        "\n",
        "# Corpus sampling\n",
        "CORPUS_TXT       = os.path.join(TOKENIZER_DIR, \"tokenizer_corpus.txt\")\n",
        "CHUNK_ROWS       = 50_000\n",
        "KEEP_FRAC        = 1.0            # keep all rows (still capped below)\n",
        "MAX_CORPUS_LINES = 2_000_000\n",
        "INPUT_SAMPLE     = 1_500_000\n",
        "random.seed(0)\n",
        "\n",
        "# --- Safe CSV reader ---\n",
        "def strict_read_csv(path: str, usecols=None, chunksize=None, dtypes=None):\n",
        "    return pd.read_csv(\n",
        "        path,\n",
        "        engine=\"c\",\n",
        "        sep=\",\",\n",
        "        encoding=\"utf-8\",\n",
        "        keep_default_na=False,\n",
        "        on_bad_lines=\"error\",\n",
        "        usecols=usecols,\n",
        "        chunksize=chunksize,\n",
        "        quotechar='\"',\n",
        "        escapechar=\"\\\\\",\n",
        "        doublequote=False,\n",
        "        lineterminator=\"\\n\",\n",
        "        dtype=dtypes,\n",
        "    )\n",
        "\n",
        "# --- Build tokenizer corpus ---\n",
        "written = 0\n",
        "for df in strict_read_csv(TRAIN_CSV, usecols=[SRC_COL], chunksize=CHUNK_ROWS, dtypes={SRC_COL: str}):\n",
        "    with open(CORPUS_TXT, \"a\", encoding=\"utf-8\") as f:\n",
        "        for text in df[SRC_COL]:\n",
        "            if written >= MAX_CORPUS_LINES:\n",
        "                break\n",
        "            if random.random() > KEEP_FRAC:\n",
        "                continue\n",
        "            line = str(text).replace(\"\\r\",\" \").replace(\"\\n\",\" \").strip()\n",
        "            if line:\n",
        "                f.write(line + \"\\n\")\n",
        "                written += 1\n",
        "    del df; gc.collect()\n",
        "    if written >= MAX_CORPUS_LINES:\n",
        "        break\n",
        "\n",
        "print(f\"Corpus saved: {CORPUS_TXT} | lines={written:,}\")\n",
        "\n",
        "# --- Train SentencePiece ---\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    input=CORPUS_TXT,\n",
        "    model_prefix=SP_PREFIX,\n",
        "    model_type=\"unigram\",\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    character_coverage=CHAR_COVERAGE,\n",
        "    input_sentence_size=INPUT_SAMPLE,\n",
        "    shuffle_input_sentence=True,\n",
        "    num_threads=NUM_THREADS,\n",
        "    normalization_rule_name=NORMALIZATION,\n",
        "\n",
        "    pad_id=0,  pad_piece=\"[PAD]\",\n",
        "    unk_id=1,  unk_piece=\"[UNK]\",\n",
        "    bos_id=2,  bos_piece=\"[BOS]\",\n",
        "    eos_id=3,  eos_piece=\"[EOS]\",\n",
        "\n",
        "    hard_vocab_limit=False,\n",
        "    train_extremely_large_corpus=True,\n",
        "    byte_fallback=True,\n",
        "    max_sentence_length=MAX_SENT_CHARS,\n",
        ")\n",
        "\n",
        "sp = spm.SentencePieceProcessor(); sp.load(SP_MODEL_PATH)\n",
        "print(f\"Tokenizer trained. Vocab={sp.get_piece_size()} | UNK={sp.unk_id()} | EOS={sp.eos_id()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2sx4-LU_8B7"
      },
      "outputs": [],
      "source": [
        "# --- Length budgets ---\n",
        "MAX_SRC_LEN = 1024\n",
        "MAX_TGT_LEN = 256   # bumped from 192\n",
        "TAIL_FRAC   = 0.25\n",
        "\n",
        "def lead_tail_core(ids: List[int], max_len: int, tail_frac: float, eos_id: int) -> List[int]:\n",
        "    body_budget = max_len - 1\n",
        "    if body_budget <= 0: return [eos_id]\n",
        "    if len(ids) <= body_budget:\n",
        "        body = ids[:]\n",
        "    else:\n",
        "        t = max(1, int(round(body_budget * tail_frac)))\n",
        "        h = body_budget - t\n",
        "        body = ids[:h] + ids[-t:]\n",
        "    return body[:body_budget] + [eos_id]\n",
        "\n",
        "def encode_src(text: str):\n",
        "    raw = sp.encode(text, out_type=int)\n",
        "    truncated = len(raw) > (MAX_SRC_LEN - 1)\n",
        "    return lead_tail_core(raw, MAX_SRC_LEN, TAIL_FRAC, sp.eos_id()), truncated\n",
        "\n",
        "def encode_tgt(text: str):\n",
        "    raw = sp.encode(text, out_type=int)\n",
        "    body_budget = MAX_TGT_LEN - 1\n",
        "    truncated = len(raw) > body_budget\n",
        "    return raw[:body_budget] + [sp.eos_id()], truncated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHDd4-RDBEas",
        "outputId": "c3f9d1f7-6c9d-485d-88a2-edc268cb5679"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Example 0 ---\n",
            "SRC raw len: 18\n",
            "SRC enc len: 19 | truncated: False\n",
            "SRC tokens: manchester united striker danny welbeck has said he wants a central role for club and country.  ...\n",
            "TGT raw len: 11\n",
            "TGT enc len: 12 | truncated: False\n",
            "TGT tokens: this is the reference summary of the above article.\n",
            "\n",
            "--- Example 1 ---\n",
            "SRC raw len: 11\n",
            "SRC enc len: 12 | truncated: False\n",
            "SRC tokens: the government passed a controversial new law on climate policy.  ...\n",
            "TGT raw len: 11\n",
            "TGT enc len: 12 | truncated: False\n",
            "TGT tokens: this is the reference summary of the above article.\n",
            "\n",
            "--- Example 2 ---\n",
            "SRC raw len: 17\n",
            "SRC enc len: 18 | truncated: False\n",
            "SRC tokens: harry potter star daniel radcliffe gained access to his fortune as he turned 18 on monday.  ...\n",
            "TGT raw len: 11\n",
            "TGT enc len: 12 | truncated: False\n",
            "TGT tokens: this is the reference summary of the above article.\n"
          ]
        }
      ],
      "source": [
        "# --- Quick diagnostics: see how the new tokenizer encodes ---\n",
        "examples = [\n",
        "    \"Manchester United striker Danny Welbeck has said he wants a central role for club and country.\",\n",
        "    \"The government passed a controversial new law on climate policy.\",\n",
        "    \"Harry Potter star Daniel Radcliffe gained access to his fortune as he turned 18 on Monday.\",\n",
        "]\n",
        "\n",
        "for i, txt in enumerate(examples):\n",
        "    s_ids, s_trunc = encode_src(txt)\n",
        "    t_ids, t_trunc = encode_tgt(\"This is the reference summary of the above article.\")\n",
        "\n",
        "    print(f\"\\n--- Example {i} ---\")\n",
        "    print(\"SRC raw len:\", len(sp.encode(txt, out_type=int)))\n",
        "    print(\"SRC enc len:\", len(s_ids), \"| truncated:\", s_trunc)\n",
        "    print(\"SRC tokens:\", sp.decode_ids(s_ids[:40]), \" ...\")\n",
        "\n",
        "    print(\"TGT raw len:\", len(sp.encode(\"This is the reference summary of the above article.\", out_type=int)))\n",
        "    print(\"TGT enc len:\", len(t_ids), \"| truncated:\", t_trunc)\n",
        "    print(\"TGT tokens:\", sp.decode_ids(t_ids))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bano9BDdAhKB",
        "outputId": "9c734384-4fc0-48bc-a474-2f7aec6fa30a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== TRAIN (fixed IDs) ===\n",
            "Input: /content/drive/MyDrive/cnndm_near_dedup/20250814_135141/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast_applied.csv\n",
            "Saved /content/drive/MyDrive/tokenized_outputs/sp_shared_20250819_110137/train_sp_tokens.npz (283,859 rows)\n",
            "Manifest: /content/drive/MyDrive/tokenized_outputs/sp_shared_20250819_110137/train_sp_manifest.csv\n",
            "ID map: /content/drive/MyDrive/tokenized_outputs/sp_shared_20250819_110137/train_idmap.csv\n",
            "Dropped src=0 tgt=2\n",
            "Truncated src=50847 tgt=167\n",
            "Tgt mean=49.3 p95=108 max=256\n",
            "Time 432.4s\n",
            "\n",
            "=== VAL (fixed IDs) ===\n",
            "Input: /content/drive/MyDrive/cnndm_near_dedup/20250814_135141/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\n",
            "Saved /content/drive/MyDrive/tokenized_outputs/sp_shared_20250819_110137/val_sp_tokens.npz (13,366 rows)\n",
            "Manifest: /content/drive/MyDrive/tokenized_outputs/sp_shared_20250819_110137/val_sp_manifest.csv\n",
            "ID map: /content/drive/MyDrive/tokenized_outputs/sp_shared_20250819_110137/val_idmap.csv\n",
            "Dropped src=0 tgt=2\n",
            "Truncated src=3046 tgt=8\n",
            "Tgt mean=63.6 p95=117 max=256\n",
            "Time 21.3s\n",
            "\n",
            "=== TEST (fixed IDs) ===\n",
            "Input: /content/drive/MyDrive/cnndm_near_dedup/20250814_135141/test_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\n",
            "Saved /content/drive/MyDrive/tokenized_outputs/sp_shared_20250819_110137/test_sp_tokens.npz (11,488 rows)\n",
            "Manifest: /content/drive/MyDrive/tokenized_outputs/sp_shared_20250819_110137/test_sp_manifest.csv\n",
            "ID map: /content/drive/MyDrive/tokenized_outputs/sp_shared_20250819_110137/test_idmap.csv\n",
            "Dropped src=0 tgt=0\n",
            "Truncated src=2705 tgt=7\n",
            "Tgt mean=60.4 p95=112 max=256\n",
            "Time 18.0s\n",
            "\n",
            "=== Final stats with fixed IDs ===\n",
            "[\n",
            "  {\n",
            "    \"split\": \"train\",\n",
            "    \"rows\": 283859,\n",
            "    \"mean_tgt_len\": 49.27488295245175,\n",
            "    \"p95_tgt_len\": 108.0,\n",
            "    \"truncated_src\": 50847,\n",
            "    \"truncated_tgt\": 167\n",
            "  },\n",
            "  {\n",
            "    \"split\": \"val\",\n",
            "    \"rows\": 13366,\n",
            "    \"mean_tgt_len\": 63.61746221756696,\n",
            "    \"p95_tgt_len\": 117.0,\n",
            "    \"truncated_src\": 3046,\n",
            "    \"truncated_tgt\": 8\n",
            "  },\n",
            "  {\n",
            "    \"split\": \"test\",\n",
            "    \"rows\": 11488,\n",
            "    \"mean_tgt_len\": 60.40181058495822,\n",
            "    \"p95_tgt_len\": 112.0,\n",
            "    \"truncated_src\": 2705,\n",
            "    \"truncated_tgt\": 7\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Re-tokenize splits with FIXED ID logic\n",
        "# - Every row ID = sha1(src+tgt) (deterministic, stable)\n",
        "# - Writes NPZ + manifest + idmap (old_id → new_id)\n",
        "# ============================================================\n",
        "\n",
        "import hashlib, csv, gc, time, os, numpy as np\n",
        "\n",
        "def stable_hash_id(src: str, tgt: str) -> str:\n",
        "    return hashlib.sha1((src + tgt).encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "def process_split_fixed(csv_path: str, split_name: str):\n",
        "    print(f\"\\n=== {split_name.upper()} (fixed IDs) ===\\nInput: {csv_path}\")\n",
        "    start = time.time()\n",
        "\n",
        "    all_ids, all_src, all_tgt = [], [], []\n",
        "    src_lens, tgt_lens = [], []\n",
        "    trunc_src = trunc_tgt = 0\n",
        "    dropped_src = dropped_tgt = 0\n",
        "    idmap_rows = []\n",
        "\n",
        "    cols = [ID_COL, SRC_COL, TGT_COL]\n",
        "    dtypes = {ID_COL: object, SRC_COL: str, TGT_COL: str}\n",
        "\n",
        "    for df in strict_read_csv(csv_path, usecols=cols, chunksize=50_000, dtypes=dtypes):\n",
        "        for _, row in df.iterrows():\n",
        "            orig_id = str(row[ID_COL]).strip() if row[ID_COL] else \"\"\n",
        "            src = (row[SRC_COL] or \"\").strip()\n",
        "            tgt = (row[TGT_COL] or \"\").strip()\n",
        "            if not src: dropped_src += 1; continue\n",
        "            if not tgt: dropped_tgt += 1; continue\n",
        "\n",
        "            new_id = stable_hash_id(src, tgt)\n",
        "\n",
        "            s_ids, s_trunc = encode_src(src)\n",
        "            t_ids, t_trunc = encode_tgt(tgt)\n",
        "\n",
        "            all_ids.append(new_id)\n",
        "            all_src.append(np.asarray(s_ids, dtype=np.int32))\n",
        "            all_tgt.append(np.asarray(t_ids, dtype=np.int32))\n",
        "            src_lens.append(len(s_ids)); tgt_lens.append(len(t_ids))\n",
        "            trunc_src += int(s_trunc); trunc_tgt += int(t_trunc)\n",
        "\n",
        "            idmap_rows.append([orig_id, new_id])\n",
        "\n",
        "        del df; gc.collect()\n",
        "\n",
        "    # --- Save NPZ ---\n",
        "    out_npz = os.path.join(OUT_DIR, f\"{split_name}_sp_tokens.npz\")\n",
        "    np.savez_compressed(out_npz, id=np.array(all_ids, dtype=object),\n",
        "                        src=np.array(all_src, dtype=object),\n",
        "                        tgt=np.array(all_tgt, dtype=object))\n",
        "\n",
        "    # --- Save manifest ---\n",
        "    manifest_csv = os.path.join(OUT_DIR, f\"{split_name}_sp_manifest.csv\")\n",
        "    with open(manifest_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.writer(f, quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n",
        "        w.writerow([ID_COL, \"src_len\", \"tgt_len\", \"truncated_src\", \"truncated_tgt\"])\n",
        "        for rid, sl, tl in zip(all_ids, src_lens, tgt_lens):\n",
        "            w.writerow([rid, sl, tl, int(sl >= MAX_SRC_LEN), int(tl >= MAX_TGT_LEN)])\n",
        "\n",
        "    # --- Save ID map (orig → new) ---\n",
        "    idmap_csv = os.path.join(OUT_DIR, f\"{split_name}_idmap.csv\")\n",
        "    with open(idmap_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.writer(f, quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n",
        "        w.writerow([\"orig_id\", \"new_id\"])\n",
        "        w.writerows(idmap_rows)\n",
        "\n",
        "    print(f\"Saved {out_npz} ({len(all_ids):,} rows)\")\n",
        "    print(f\"Manifest: {manifest_csv}\")\n",
        "    print(f\"ID map: {idmap_csv}\")\n",
        "    print(f\"Dropped src={dropped_src} tgt={dropped_tgt}\")\n",
        "    print(f\"Truncated src={trunc_src} tgt={trunc_tgt}\")\n",
        "    print(f\"Tgt mean={np.mean(tgt_lens):.1f} p95={np.percentile(tgt_lens,95):.0f} max={max(tgt_lens)}\")\n",
        "    print(f\"Time {time.time()-start:.1f}s\")\n",
        "\n",
        "    return {\"split\": split_name, \"rows\": len(all_ids),\n",
        "            \"mean_tgt_len\": float(np.mean(tgt_lens)),\n",
        "            \"p95_tgt_len\": float(np.percentile(tgt_lens,95)),\n",
        "            \"truncated_src\": trunc_src, \"truncated_tgt\": trunc_tgt}\n",
        "\n",
        "# === Run all splits ===\n",
        "stats = []\n",
        "for path, name in [(TRAIN_CSV,\"train\"), (VAL_CSV,\"val\"), (TEST_CSV,\"test\")]:\n",
        "    stats.append(process_split_fixed(path, name))\n",
        "\n",
        "print(\"\\n=== Final stats with fixed IDs ===\")\n",
        "print(json.dumps(stats, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2427
        },
        "id": "k33BJgKxEhu3",
        "outputId": "480b9d7a-c817-4b30-94d3-6236d832e4ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== TRAIN ===\n",
            "Rows: 283,859\n",
            "Source lens → mean 560.0, p95 1024, max 1024\n",
            "Target lens → mean 49.3, p95 108, max 256\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Example 201979 ---\n",
            "ID: f5ddeefd6edd907ba36b4e23b99a7fe8e0f7ae12\n",
            "SRC len: 415 | TGT len: 51\n",
            "SRC: danny welbeck admits he is frustrated by his failure to hold down a first-team place up front for manchester united. the england forward wants a central role for club and country to prove he can score goals regularly at the highest level. welbeck has scored eight times in 21 appearances for england  ...\n",
            "TGT: welbeck was played out of position during david moyes' time at united . the england man says he is 'let off the leash' more under roy hodgson . it is unclear how new united boss louis van gaal will ut ...\n",
            "\n",
            "--- Example 220500 ---\n",
            "ID: d04bae1cb9aa27a09e0044a4b7da2a2cf3c9ef14\n",
            "SRC len: 1024 | TGT len: 82\n",
            "SRC: on september 12, 1989, liverpool humiliated crystal palace with a record 9-0 thumping at anfield in the old first division. but the teams met again later in the season and this time, remarkably, palace won a thrilling fa cup semi-final at villa park, 4-3 after extra-time. liverpool went on to win th ...\n",
            "TGT: liverpool humiliated crystal palace 9-0 at anfield in september 1989 . south london club then beat reds 4-3 in dramatic fa cup semi-final . palace welcome brendan rodgers' side to selhurst park on sat ...\n",
            "\n",
            "--- Example 21225 ---\n",
            "ID: 274863150cc624d7e82fd5010bb11506d61909d5\n",
            "SRC len: 76 | TGT len: 22\n",
            "SRC: your facebook password is none of your new boss' business. that's what the american civil liberties union is saying after reports that employers are increasingly asking for access to job applicants' social-media accounts. \"it's an invasion of privacy for private employers to insist on looking at peo ...\n",
            "TGT: attorney catherine crump said in a statement from the aclu. \"\"people are entitled to their private lives.\"\" recently ...\n",
            "\n",
            "=== VAL ===\n",
            "Rows: 13,366\n",
            "Source lens → mean 657.6, p95 1024, max 1024\n",
            "Target lens → mean 63.6, p95 117, max 256\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Example 6311 ---\n",
            "ID: 6baed05143b6a042904d311d8402d57ef1dbbb28\n",
            "SRC len: 387 | TGT len: 64\n",
            "SRC: a sheriff has credited facebook in assisting in the arrest of a domestic violence suspect on tuesday. andrew dale marcum, 21, was also wanted on charges of burglary (safe-cracking), abduction, and assault in ohio. the butler county sheriff's office posted three pictures of marcum on facebook on tues ...\n",
            "TGT: andrew dale marcum, 21, was also wanted on charges of burglary (safe-cracking), abduction and assault in ohio . the butler county sheriff's office posted marcum's wanted picture on facebook - to which ...\n",
            "\n",
            "--- Example 12418 ---\n",
            "ID: 4f4858a6354394e67dda559525457932b87853a7\n",
            "SRC len: 1024 | TGT len: 120\n",
            "SRC: an afghan woman who was savagely beaten to death in the streets of kabul was murdered because she spoke out against a controversial local cleric, witnesses have claimed. farkhunda, 27, was pushed from a roof, run over by a car and set on fire before her body was thrown in the kabul river - in full v ...\n",
            "TGT: farkhunda, 27, was savagely murdered by a rampaging mob last thursday . it was initially claimed she had burned a copy of the koran, inciting fury . but witnesses have now said she was actually killed ...\n",
            "\n",
            "--- Example 6890 ---\n",
            "ID: 915777a0978e9bf2fb3f3cadfafcf2cf38eda0d0\n",
            "SRC len: 1024 | TGT len: 85\n",
            "SRC: the boyfriend of a diet guru famous for eating 51 bananas a day claims he could 'retire in 100 countries' due to the $10,000 he makes a month 'pimping her out' across youtube. an e-book written by freelee the banana girl's partner, harley johnstone, has been tendered in south australia's supreme cou ...\n",
            "TGT: book written by boyfriend of freelee the banana girl tendered as evidence in sa supreme court as pair battle lawsuit . harley johnstone claims in e-book carb the f*** up to make $10k a month . rival f ...\n",
            "\n",
            "=== TEST ===\n",
            "Rows: 11,488\n",
            "Source lens → mean 663.1, p95 1024, max 1024\n",
            "Target lens → mean 60.4, p95 112, max 256\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Example 6311 ---\n",
            "ID: 4b782ace12f0edee2d7db47f307e8165074675cf\n",
            "SRC len: 1024 | TGT len: 83\n",
            "SRC: ted cruz is personally against the legalization of marijuana but the republican presidential candidate said this weekend that he believes states have the right to put decriminalization laws on the books if they want - even though they directly conflict with federal law. cruz implied during a convers ...\n",
            "TGT: cruz is personally against the legalization of marijuana but believes states have the right to put decriminalization laws on the books if they want . implied he wouldn't make his ag enforce federal po ...\n",
            "\n",
            "--- Example 6890 ---\n",
            "ID: 9963b49ada123a884d9740481229c0ba6639bc9d\n",
            "SRC len: 488 | TGT len: 30\n",
            "SRC: the state of oklahoma has removed protection for gay people who use ride-sharing services uber and lyft. initially, the oklahoma transportation network company services act included language that prohibited the companies from discriminating against customers based on sexual orientation or gender ide ...\n",
            "TGT: initially, the transport bill included language that protected lgbt users . but senator jason smalley rewrote the bill to allow drivers to discriminate . ...\n",
            "\n",
            "--- Example 663 ---\n",
            "ID: 4ad27f2eaeaf2b0737d8e42e29c1d13a22c1c193\n",
            "SRC len: 12 | TGT len: 36\n",
            "SRC: the seventh installment of the \"fast and furious\"\" franchise ...\n",
            "TGT: \"\"furious 7\"\" is sure to draw fans curious about how the film handles the real-life death of co-star paul walker. but minus the off-screen tragedy ...\n"
          ]
        }
      ],
      "source": [
        "# =======================================\n",
        "# Inspect freshly tokenized splits\n",
        "# =======================================\n",
        "import numpy as np, random, matplotlib.pyplot as plt\n",
        "import sentencepiece as spm\n",
        "\n",
        "# --- Paths ---\n",
        "SP_MODEL_PATH = \"/content/drive/MyDrive/tokenizers/sp_shared_20250819_110137/sp_unigram_shared.model\"\n",
        "DATA_DIR      = \"/content/drive/MyDrive/tokenized_outputs/sp_shared_20250819_110137\"\n",
        "\n",
        "TRAIN_NPZ = f\"{DATA_DIR}/train_sp_tokens.npz\"\n",
        "VAL_NPZ   = f\"{DATA_DIR}/val_sp_tokens.npz\"\n",
        "TEST_NPZ  = f\"{DATA_DIR}/test_sp_tokens.npz\"\n",
        "\n",
        "# --- Load tokenizer ---\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(SP_MODEL_PATH)\n",
        "\n",
        "def load_npz(path):\n",
        "    d = np.load(path, allow_pickle=True)\n",
        "    return d[\"id\"], d[\"src\"], d[\"tgt\"]\n",
        "\n",
        "# --- Helper: stats + histogram ---\n",
        "def inspect_split(name, path, show_examples=True, n_examples=3):\n",
        "    ids, src, tgt = load_npz(path)\n",
        "    src_lens = [len(x) for x in src]\n",
        "    tgt_lens = [len(x) for x in tgt]\n",
        "\n",
        "    print(f\"\\n=== {name.upper()} ===\")\n",
        "    print(f\"Rows: {len(ids):,}\")\n",
        "    print(f\"Source lens → mean {np.mean(src_lens):.1f}, p95 {np.percentile(src_lens,95):.0f}, max {np.max(src_lens)}\")\n",
        "    print(f\"Target lens → mean {np.mean(tgt_lens):.1f}, p95 {np.percentile(tgt_lens,95):.0f}, max {np.max(tgt_lens)}\")\n",
        "\n",
        "    # Histogram\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.hist(tgt_lens, bins=50, alpha=0.6, label=\"tgt\")\n",
        "    plt.hist(src_lens, bins=50, alpha=0.6, label=\"src\")\n",
        "    plt.legend(); plt.title(f\"{name} lengths\"); plt.xlabel(\"tokens\"); plt.ylabel(\"count\")\n",
        "    plt.show()\n",
        "\n",
        "    if show_examples:\n",
        "        random.seed(0)\n",
        "        for i in random.sample(range(len(ids)), min(n_examples, len(ids))):\n",
        "            print(f\"\\n--- Example {i} ---\")\n",
        "            print(\"ID:\", ids[i])\n",
        "            print(\"SRC len:\", len(src[i]), \"| TGT len:\", len(tgt[i]))\n",
        "            print(\"SRC:\", sp.decode(src[i].tolist())[:300], \"...\")\n",
        "            print(\"TGT:\", sp.decode(tgt[i].tolist())[:200], \"...\")\n",
        "\n",
        "# Run inspections\n",
        "inspect_split(\"train\", TRAIN_NPZ)\n",
        "inspect_split(\"val\", VAL_NPZ)\n",
        "inspect_split(\"test\", TEST_NPZ)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cz0r0oY7KUWq",
        "outputId": "5e93dd9c-3626-41f5-8304-6641068f544d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[info] Loaded 283,859 rows for split=train\n",
            "                                             orig_id  \\\n",
            "0  drink and celebrity parties. \"\"I don't plan to...   \n",
            "1  Florida The ninth floor of the Miami-Dade pret...   \n",
            "2  survivor Gary Babineau told CNN. \"\"I probably ...   \n",
            "3                                           Maryland   \n",
            "4                                            the NFL   \n",
            "\n",
            "                                     new_id  \n",
            "0  5009fd61c67f9da8323375caaacbbc7df95ecebb  \n",
            "1  27f4422903d0684532a13007e6aa68a841cca9ef  \n",
            "2  4bd93f3161d2ad213e9c57c07a2c0f4f3c89620f  \n",
            "3  63c59b11a8c4474c255aa40aa59ba7f3ee6f21d8  \n",
            "4  7c74bae7fe12f160b61487561a7cfef5ae1f3943  \n",
            "\n",
            "--- Example 201979 ---\n",
            "Original ID: 9d33de844ec1a8fcd84bff39fb0c4e941558a8cb\n",
            "Stable new_id: f5ddeefd6edd907ba36b4e23b99a7fe8e0f7ae12\n",
            "SRC len: 415 | TGT len: 51\n",
            "SRC text: danny welbeck admits he is frustrated by his failure to hold down a first-team place up front for manchester united. the england forward wants a central role for club and country to prove he can score goals regularly at the highest level. welbeck has scored eight times in 21 appearances for england  ...\n",
            "TGT text: welbeck was played out of position during david moyes' time at united . the england man says he is 'let off the leash' more under roy hodgson . it is unclear how new united boss louis van gaal will ut ...\n",
            "SRC tokens (first 15): [ 6576   309 24585  3057   276   273  5516   297   285  2479   262  1193\n",
            "   443   264   368]\n",
            "TGT tokens (first 15): [  309 24585   275  1080   332   266  1291   431   281   894  7117   267\n",
            "   347   289   444]\n",
            "\n",
            "--- Example 220500 ---\n",
            "Original ID: d4b6c15d9e3743d8ff5dc8f42635fa65453bee5c\n",
            "Stable new_id: d04bae1cb9aa27a09e0044a4b7da2a2cf3c9ef14\n",
            "SRC len: 1024 | TGT len: 82\n",
            "SRC text: on september 12, 1989, liverpool humiliated crystal palace with a record 9-0 thumping at anfield in the old first division. but the teams met again later in the season and this time, remarkably, palace won a thrilling fa cup semi-final at villa park, 4-3 after extra-time. liverpool went on to win th ...\n",
            "TGT text: liverpool humiliated crystal palace 9-0 at anfield in september 1989 . south london club then beat reds 4-3 in dramatic fa cup semi-final . palace welcome brendan rodgers' side to selhurst park on sat ...\n",
            "SRC tokens (first 15): [  278  1183   906   328  5519   328  2555 16886   283  6398  3909   282\n",
            "   264   868 13232]\n",
            "TGT tokens (first 15): [ 2555 16886   283  6398  3909 13232   408   289   303  2900   268  1183\n",
            "  5519   271   534]\n",
            "\n",
            "--- Example 21225 ---\n",
            "Original ID: multiple cases have come to light in which companies have either asked for passwords to Facebook or required that applicants \"\"friend\"\" people at those companies. Robert Collins of the Baltimore area has said that he was looking to be reinstated to his job as a correctional officer in 2010 when he was asked for his Facebook password. \"\"I did not want to do it\n",
            "Stable new_id: 274863150cc624d7e82fd5010bb11506d61909d5\n",
            "SRC len: 76 | TGT len: 22\n",
            "SRC text: your facebook password is none of your new boss' business. that's what the american civil liberties union is saying after reports that employers are increasingly asking for access to job applicants' social-media accounts. \"it's an invasion of privacy for private employers to insist on looking at peo ...\n",
            "TGT text: attorney catherine crump said in a statement from the aclu. \"\"people are entitled to their private lives.\"\" recently ...\n",
            "SRC tokens (first 15): [  427   804 14805   273  2863   266   427   337  2620   267   674   261\n",
            "   270   267   263]\n",
            "TGT tokens (first 15): [ 849 6373 8953  277  268  264  499  295  260 8464  261  269 2455  291\n",
            " 4976]\n"
          ]
        }
      ],
      "source": [
        "import os, random, numpy as np, pandas as pd\n",
        "import sentencepiece as spm\n",
        "\n",
        "# --- Config ---\n",
        "BASE = \"/content/drive/MyDrive/tokenized_outputs/sp_shared_20250819_110137\"\n",
        "SP_MODEL = \"/content/drive/MyDrive/tokenizers/sp_shared_20250819_110137/sp_unigram_shared.model\"\n",
        "\n",
        "SPLIT = \"train\"  # change to \"val\" or \"test\"\n",
        "IDMAP_PATH = f\"{BASE}/{SPLIT}_idmap.csv\"\n",
        "NPZ_PATH   = f\"{BASE}/{SPLIT}_sp_tokens.npz\"\n",
        "\n",
        "# --- Load tokenizer ---\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(SP_MODEL)\n",
        "\n",
        "# --- Load idmap + tokenized arrays ---\n",
        "df_map = pd.read_csv(IDMAP_PATH)\n",
        "npz = np.load(NPZ_PATH, allow_pickle=True)\n",
        "\n",
        "ids  = npz[\"id\"]   # these are the stable new_ids\n",
        "srcs = npz[\"src\"]\n",
        "tgts = npz[\"tgt\"]\n",
        "\n",
        "print(f\"[info] Loaded {len(ids):,} rows for split={SPLIT}\")\n",
        "print(df_map.head())\n",
        "\n",
        "def decode(ids_arr):\n",
        "    return sp.decode([int(x) for x in ids_arr if int(x) != 0])\n",
        "\n",
        "# --- Show random samples ---\n",
        "random.seed(0)\n",
        "for j in random.sample(range(len(ids)), 3):\n",
        "    new_id = ids[j]\n",
        "    row = df_map.loc[df_map[\"new_id\"] == new_id]\n",
        "    orig_id = row[\"orig_id\"].iloc[0] if len(row) > 0 else \"<none>\"\n",
        "\n",
        "    print(f\"\\n--- Example {j} ---\")\n",
        "    print(\"Original ID:\", orig_id)\n",
        "    print(\"Stable new_id:\", new_id)\n",
        "    print(\"SRC len:\", len(srcs[j]), \"| TGT len:\", len(tgts[j]))\n",
        "    print(\"SRC text:\", decode(srcs[j])[:300], \"...\")\n",
        "    print(\"TGT text:\", decode(tgts[j])[:200], \"...\")\n",
        "    print(\"SRC tokens (first 15):\", srcs[j][:15])\n",
        "    print(\"TGT tokens (first 15):\", tgts[j][:15])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIuZZw3QMIUj",
        "outputId": "6fc20b7e-a7c9-4021-c906-cd83630d5686"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Tokenizer] Vocab=32000 | PAD=0 UNK=1 BOS=2 EOS=3\n",
            "\n",
            "=== TRAIN ===\n",
            "[info] Loaded 283,859 rows\n",
            "Source lens → mean 560.0, p95 1024, max 1024\n",
            "Target lens → mean 49.3, p95 108, max 256\n",
            "EOS sanity → src bad=0, tgt bad=0\n",
            "[UNK] 0/172957746 = 0.0000%\n",
            "\n",
            "--- Random examples ---\n",
            "\n",
            "--- Random examples ---\n",
            "\n",
            "ID: 376962ec174d5342cbeb251d837b199278ab976f\n",
            "SRC len=1024 | TGT len=58\n",
            "SRC tokens (first 15): [297, 271, 4077, 5393, 4415, 272, 1626, 2924, 1335, 271, 8222, 2655, 18273, 283, 328]\n",
            "TGT tokens (first 15): [337, 940, 900, 2320, 3425, 273, 3409, 281, 328, 1151, 281, 284, 491, 284, 668]\n",
            "SRC decode: by . melissa hills for daily mail australia . calum shand, 26, found it very hard to get a job when he returned to austr...\n",
            "TGT decode: new research shows youth unemployment is climbing, reaching as high as 20 per cent in some areas of australia . data sho...\n",
            "\n",
            "ID: dff9c928543458248d79ed0c10d2c1cc7f519932\n",
            "SRC len=491 | TGT len=70\n",
            "SRC tokens (first 15): [297, 271, 2813, 2871, 271, 1530, 302, 271, 280, 1920, 302, 1933, 280, 992, 328]\n",
            "TGT tokens (first 15): [13041, 15474, 275, 4795, 292, 331, 909, 268, 264, 463, 340, 310, 264, 760, 1567]\n",
            "SRC decode: by . anthony bond . published: . 03:34 est, 21 august 2012 . updated: . 11:58 est, 21 august 2012 . a woman has been lef...\n",
            "TGT decode: erica richards was blinded after living in a house which had a large amount of pigeon feces in its attic . she developed...\n",
            "\n",
            "ID: 8a5c6f868f8942420fd46a7f4412c88f05425531\n",
            "SRC len=724 | TGT len=41\n",
            "SRC tokens (first 15): [300, 267, 377, 595, 260, 409, 4698, 9667, 266, 763, 1529, 328, 15662, 260, 380]\n",
            "TGT tokens (first 15): [260, 1058, 27600, 560, 296, 290, 22207, 267, 1034, 266, 260, 4698, 800, 8511, 263]\n",
            "SRC decode: they're among the most iconic artifacts of british culture, recognised the world over by their bright red exterior. but ...\n",
            "TGT decode: the red kiosk company has 'adopted' 100 of the iconic phone booths . they will be transformed into miniature businesses ...\n",
            "\n",
            "MD5 npz=7826df9ae60cc253847d90ef79d15e52 manifest=d387e17fc58b1707db06e21edba93dc0 idmap=f93e4d63544d9d2ff9edbba776a78a5e\n",
            "\n",
            "=== VAL ===\n",
            "[info] Loaded 13,366 rows\n",
            "Source lens → mean 657.6, p95 1024, max 1024\n",
            "Target lens → mean 63.6, p95 117, max 256\n",
            "EOS sanity → src bad=0, tgt bad=0\n",
            "[UNK] 0/9639262 = 0.0000%\n",
            "\n",
            "--- Random examples ---\n",
            "\n",
            "--- Random examples ---\n",
            "\n",
            "ID: 8e7c891f2ad36b0a92c79ea7f22caadd04a8f8d2\n",
            "SRC len=1024 | TGT len=91\n",
            "SRC tokens (first 15): [1238, 267, 493, 273, 271, 30501, 272, 3078, 917, 328, 315, 557, 267, 263, 280]\n",
            "TGT tokens (first 15): [11135, 267, 493, 272, 917, 267, 263, 2457, 11592, 296, 2094, 824, 4100, 1265, 439]\n",
            "SRC decode: democrats' support is . softening for hillary clinton, their party's presumed 2016 . presidential front-runner, with nea...\n",
            "TGT decode: dems' support for clinton's 2016 candidacy has dropped 15 percentage points since mid-february, a reuters poll found . 4...\n",
            "\n",
            "ID: cb379b4e55373da56a0c5a0ffbcba9b364d4f60f\n",
            "SRC len=280 | TGT len=57\n",
            "SRC tokens (first 15): [1685, 267, 263, 1391, 3134, 361, 9945, 5375, 22504, 268, 419, 3545, 262, 1151, 260]\n",
            "TGT tokens (first 15): [1391, 3134, 1687, 5375, 22504, 6995, 400, 328, 5374, 328, 6617, 12801, 408, 369, 278]\n",
            "SRC decode: britain's james ward overcame mitchell krueger in three sets to reach the second round of qualifying at the bnp paribas ...\n",
            "TGT decode: james ward beat mitchell krueger 2-6, 6-3, 7-6 (7-0) on tuesday evening . ward is one win away from the main draw of the...\n",
            "\n",
            "ID: 7b8ac22d3290b159eea281776893316b7b5e342d\n",
            "SRC len=782 | TGT len=98\n",
            "SRC tokens (first 15): [264, 14030, 467, 7021, 264, 379, 6855, 268, 1744, 875, 265, 6244, 292, 551, 328]\n",
            "TGT tokens (first 15): [4988, 11098, 1386, 9810, 266, 1399, 4784, 275, 850, 278, 4820, 266, 1640, 474, 260]\n",
            "SRC decode: a handcuffed man stole a police suv in southern california and sped away, forcing officers to commandeer a tow-truck and...\n",
            "TGT decode: aaron teruya of san diego was arrested on suspicion of driving under the influence, but jumped into the front seat of a ...\n",
            "\n",
            "MD5 npz=380a03aec0e1d3b0491bca0d101eea75 manifest=f7364de5f9b7df764761dad80cfd09d8 idmap=5b2061a6d844eaacfc1ec4d08c1652a0\n",
            "\n",
            "=== TEST ===\n",
            "[info] Loaded 11,488 rows\n",
            "Source lens → mean 663.1, p95 1024, max 1024\n",
            "Target lens → mean 60.4, p95 112, max 256\n",
            "EOS sanity → src bad=0, tgt bad=0\n",
            "[UNK] 0/8311362 = 0.0000%\n",
            "\n",
            "--- Random examples ---\n",
            "\n",
            "--- Random examples ---\n",
            "\n",
            "ID: e090d31d662c890ced66c82c202b4c881b75e302\n",
            "SRC len=313 | TGT len=96\n",
            "SRC tokens (first 15): [4875, 454, 1751, 2259, 6808, 21668, 781, 14332, 349, 276, 810, 263, 14641, 4540, 2164]\n",
            "TGT tokens (first 15): [21668, 781, 14332, 1711, 556, 262, 810, 2259, 289, 260, 526, 266, 260, 854, 271]\n",
            "SRC decode: arsenal should target chelsea goalkeeper petr cech if he leaves stamford bridge at the end of the season, insists carlo ...\n",
            "TGT decode: petr cech looks set to leave chelsea at the end of the season . the 32-year-old goalkeeper revealed last week he does no...\n",
            "\n",
            "ID: fd0572c8f8058419155357ddc0863aa10075f00f\n",
            "SRC len=556 | TGT len=8\n",
            "SRC tokens (first 15): [9276, 21483, 25678, 21397, 305, 296, 1615, 301, 296, 280, 342, 3798, 1204, 261, 268]\n",
            "TGT tokens (first 15): [265, 330, 309, 579, 262, 260, 3005, 3]\n",
            "SRC decode: avril lavigne has revealed she has lyme disease. in an interview with people magazine, the 30-year-old explained the deb...\n",
            "TGT decode: and when we went to the pool...\n",
            "\n",
            "ID: e602ae98d7f0be1e8a9df100b5b6034f9600fa01\n",
            "SRC len=1024 | TGT len=117\n",
            "SRC tokens (first 15): [279, 275, 298, 1561, 1426, 294, 265, 382, 1039, 267, 263, 9925, 583, 3119, 1865]\n",
            "TGT tokens (first 15): [17134, 2773, 4359, 7653, 482, 519, 260, 926, 268, 260, 368, 846, 2451, 262, 963]\n",
            "SRC decode: it was not particularly pretty but andy king's 86th minute winner has kept leicester city's hopes of premier league surv...\n",
            "TGT decode: relegation strugglers leicester city took the lead in the first half thanks to footballing legend esteban cambiasso . ni...\n",
            "\n",
            "MD5 npz=184c31f6eb450bf9097b893547cff26d manifest=4200738d77933ee38443188ccfa39b40 idmap=8b4f56435d5b19d3d4189524f94336bd\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Tokenization Sanity-Check Dashboard\n",
        "# ============================================================\n",
        "\n",
        "import os, random, hashlib, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "\n",
        "# ----------------\n",
        "# Config\n",
        "# ----------------\n",
        "SP_MODEL_PATH = \"/content/drive/MyDrive/tokenizers/sp_shared_20250819_110137/sp_unigram_shared.model\"\n",
        "OUT_DIR       = \"/content/drive/MyDrive/tokenized_outputs/sp_shared_20250819_110137\"\n",
        "CSV_DIR       = \"/content/drive/MyDrive/cnndm_near_dedup/20250814_135141\"\n",
        "\n",
        "SPLITS = {\n",
        "    \"train\": {\n",
        "        \"npz\": f\"{OUT_DIR}/train_sp_tokens.npz\",\n",
        "        \"manifest\": f\"{OUT_DIR}/train_sp_manifest.csv\",\n",
        "        \"idmap\": f\"{OUT_DIR}/train_idmap.csv\",\n",
        "        \"csv\": f\"{CSV_DIR}/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast_applied.csv\"\n",
        "    },\n",
        "    \"val\": {\n",
        "        \"npz\": f\"{OUT_DIR}/val_sp_tokens.npz\",\n",
        "        \"manifest\": f\"{OUT_DIR}/val_sp_manifest.csv\",\n",
        "        \"idmap\": f\"{OUT_DIR}/val_idmap.csv\",\n",
        "        \"csv\": f\"{CSV_DIR}/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "    },\n",
        "    \"test\": {\n",
        "        \"npz\": f\"{OUT_DIR}/test_sp_tokens.npz\",\n",
        "        \"manifest\": f\"{OUT_DIR}/test_sp_manifest.csv\",\n",
        "        \"idmap\": f\"{OUT_DIR}/test_idmap.csv\",\n",
        "        \"csv\": f\"{CSV_DIR}/test_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "    },\n",
        "}\n",
        "\n",
        "MAX_SHOW = 15  # number of tokens to show in sample previews\n",
        "\n",
        "# ----------------\n",
        "# Load tokenizer\n",
        "# ----------------\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(SP_MODEL_PATH)\n",
        "print(f\"[Tokenizer] Vocab={sp.get_piece_size()} | PAD={sp.pad_id()} UNK={sp.unk_id()} BOS={sp.bos_id()} EOS={sp.eos_id()}\")\n",
        "\n",
        "# ----------------\n",
        "# Helper: md5\n",
        "# ----------------\n",
        "def md5(path):\n",
        "    h = hashlib.md5()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(1<<20), b\"\"): h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "# ----------------\n",
        "# Run sanity checks per split\n",
        "# ----------------\n",
        "for split, paths in SPLITS.items():\n",
        "    print(f\"\\n=== {split.upper()} ===\")\n",
        "    # load npz\n",
        "    d = np.load(paths[\"npz\"], allow_pickle=True)\n",
        "    ids, srcs, tgts = d[\"id\"], d[\"src\"], d[\"tgt\"]\n",
        "    print(f\"[info] Loaded {len(ids):,} rows\")\n",
        "\n",
        "    # check manifest/idmap alignment\n",
        "    man = pd.read_csv(paths[\"manifest\"])\n",
        "    idmap = pd.read_csv(paths[\"idmap\"])\n",
        "    assert len(man) == len(ids) == len(idmap), \"Row count mismatch!\"\n",
        "    assert set(idmap[\"new_id\"]) == set(ids), \"IDs mismatch between npz and idmap\"\n",
        "\n",
        "    # length stats\n",
        "    src_lens = [len(x) for x in srcs]\n",
        "    tgt_lens = [len(x) for x in tgts]\n",
        "    print(f\"Source lens → mean {np.mean(src_lens):.1f}, p95 {np.percentile(src_lens,95):.0f}, max {np.max(src_lens)}\")\n",
        "    print(f\"Target lens → mean {np.mean(tgt_lens):.1f}, p95 {np.percentile(tgt_lens,95):.0f}, max {np.max(tgt_lens)}\")\n",
        "\n",
        "    # EOS check\n",
        "    bad_eos_src = sum([x[-1] != sp.eos_id() for x in srcs])\n",
        "    bad_eos_tgt = sum([x[-1] != sp.eos_id() for x in tgts])\n",
        "    print(f\"EOS sanity → src bad={bad_eos_src}, tgt bad={bad_eos_tgt}\")\n",
        "\n",
        "    # UNK check\n",
        "    unk_src = sum([(np.array(x)==sp.unk_id()).sum() for x in srcs])\n",
        "    unk_tgt = sum([(np.array(x)==sp.unk_id()).sum() for x in tgts])\n",
        "    unk_total = unk_src + unk_tgt\n",
        "    total_tokens = sum(src_lens) + sum(tgt_lens)\n",
        "    print(f\"[UNK] {unk_total}/{total_tokens} = {100*unk_total/total_tokens:.4f}%\")\n",
        "\n",
        "    # random spot checks\n",
        "    print(\"\\n--- Random examples ---\")\n",
        "    sample_idx = random.sample(range(len(ids)), 3)\n",
        "    orig_csv = pd.read_csv(paths[\"csv\"], usecols=[\"id\",\"article\",\"highlights\"], nrows=200_000) # partial load\n",
        "    csv_map = {str(r[\"id\"]): (r[\"article\"], r[\"highlights\"]) for _,r in orig_csv.iterrows()}\n",
        "\n",
        "        # random spot checks\n",
        "    print(\"\\n--- Random examples ---\")\n",
        "    sample_idx = random.sample(range(len(ids)), 3)\n",
        "    orig_csv = pd.read_csv(paths[\"csv\"], usecols=[\"id\",\"article\",\"highlights\"])  # no need to limit nrows\n",
        "    csv_map = {str(r[\"id\"]): (r[\"article\"], r[\"highlights\"]) for _,r in orig_csv.iterrows()}\n",
        "\n",
        "    for i in sample_idx:\n",
        "        nid = str(ids[i])\n",
        "        src_tokens, tgt_tokens = srcs[i].tolist(), tgts[i].tolist()  # <-- cast to list\n",
        "        src_dec, tgt_dec = sp.decode(src_tokens), sp.decode(tgt_tokens)\n",
        "\n",
        "        print(f\"\\nID: {nid}\")\n",
        "        if nid in csv_map:\n",
        "            art, summ = csv_map[nid]\n",
        "            print(f\"Orig article: {art[:100]}...\")\n",
        "            print(f\"Orig highlight: {summ[:100]}...\")\n",
        "        print(f\"SRC len={len(src_tokens)} | TGT len={len(tgt_tokens)}\")\n",
        "        print(f\"SRC tokens (first {MAX_SHOW}): {src_tokens[:MAX_SHOW]}\")\n",
        "        print(f\"TGT tokens (first {MAX_SHOW}): {tgt_tokens[:MAX_SHOW]}\")\n",
        "        print(f\"SRC decode: {src_dec[:120]}...\")\n",
        "        print(f\"TGT decode: {tgt_dec[:120]}...\")\n",
        "\n",
        "\n",
        "    # md5 integrity check\n",
        "    print(f\"\\nMD5 npz={md5(paths['npz'])} manifest={md5(paths['manifest'])} idmap={md5(paths['idmap'])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD2DNDKLuEHw"
      },
      "source": [
        "# Training for 2000 steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Iw1r9nh4_y-z",
        "outputId": "b7ae4ab0-e6a2-4f0b-ac95-7654c16db184"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Found existing installation: transformers 4.55.2\n",
            "Uninstalling transformers-4.55.2:\n",
            "  Successfully uninstalled transformers-4.55.2\n",
            "Found existing installation: rouge_score 0.1.2\n",
            "Uninstalling rouge_score-0.1.2:\n",
            "  Successfully uninstalled rouge_score-0.1.2\n",
            "Found existing installation: accelerate 1.10.0\n",
            "Uninstalling accelerate-1.10.0:\n",
            "  Successfully uninstalled accelerate-1.10.0\n",
            "Found existing installation: datasets 4.0.0\n",
            "Uninstalling datasets-4.0.0:\n",
            "  Successfully uninstalled datasets-4.0.0\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m126.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mdevice: cuda\n"
          ]
        }
      ],
      "source": [
        "!pip install -q rouge-score==0.1.2\n",
        "\n",
        "!pip uninstall -y transformers rouge-score accelerate datasets\n",
        "\n",
        "!pip install -q \\\n",
        "  transformers==4.43.4 \\\n",
        "  accelerate==0.34.2 \\\n",
        "  datasets==2.20.0 \\\n",
        "  rouge-score==0.1.2\n",
        "import os, csv, math, time, random, json, re, gc\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Optional\n",
        "from torch import amp\n",
        "from rouge_score import rouge_scorer\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM,\n",
        "    get_linear_schedule_with_warmup, set_seed\n",
        ")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8jdGL6P-jW0"
      },
      "outputs": [],
      "source": [
        "# --- Data paths (CSV with columns: id, article, highlights) ---\n",
        "TRAIN_CSV = \"/content/drive/MyDrive/cleaned_outputs/naa/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast_applied.csv\"\n",
        "VAL_CSV   = \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "\n",
        "# --- Output/checkpoints ---\n",
        "RUN_DIR   = \"/content/drive/MyDrive/student_pgc_bartbase\"\n",
        "os.makedirs(RUN_DIR, exist_ok=True)\n",
        "\n",
        "# --- Model/tokenizer ---\n",
        "MODEL_NAME = \"facebook/bart-base\"     # <<< switch to BASE\n",
        "\n",
        "# --- Lengths ---\n",
        "MAX_SRC_LEN = 400\n",
        "MAX_TGT_LEN = 100\n",
        "\n",
        "# --- Training ---\n",
        "SEED                 = 0\n",
        "BATCH_SIZE           = 48\n",
        "GRAD_ACCUM_STEPS     = 2\n",
        "NUM_WORKERS          = 4\n",
        "LR                   = 3e-5\n",
        "WEIGHT_DECAY         = 0.01\n",
        "WARMUP_RATIO         = 0.06\n",
        "EPOCHS               = 99            # we'll stop via MAX_STEPS\n",
        "MAX_STEPS            = 2000\n",
        "FP16                 = True\n",
        "FREEZE_ENCODER       = False         # keep trainable for pointer health\n",
        "LOG_EVERY            = 100\n",
        "SAVE_EVERY_STEPS     = 1000\n",
        "\n",
        "# --- Pointer/Coverage ---\n",
        "USE_POINTER          = True\n",
        "LAMBDA_COV           = 1.0\n",
        "EPS                  = 1e-8\n",
        "\n",
        "# --- Decoding (project-wide canonical) ---\n",
        "GEN_ARGS = dict(\n",
        "    num_beams=5,\n",
        "    min_new_tokens=55,\n",
        "    max_new_tokens=100,\n",
        "    min_length=55,\n",
        "    no_repeat_ngram_size=3,\n",
        "    length_penalty=2.0,\n",
        "    early_stopping=True,\n",
        ")\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzlu96Go_zX7"
      },
      "outputs": [],
      "source": [
        "def robust_read_csv(path: str) -> pd.DataFrame:\n",
        "    try:\n",
        "        df = pd.read_csv(path, engine=\"python\", quoting=csv.QUOTE_ALL, escapechar=\"\\\\\")\n",
        "        cols = {c.lower(): c for c in df.columns}\n",
        "        return df[[cols[\"id\"], cols[\"article\"], cols[\"highlights\"]]].rename(\n",
        "            columns={cols[\"id\"]:\"id\", cols[\"article\"]:\"article\", cols[\"highlights\"]:\"highlights\"}\n",
        "        )\n",
        "    except Exception:\n",
        "        rows = []\n",
        "        with open(path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "            reader = csv.reader(f, delimiter=\",\", quotechar='\"', escapechar=\"\\\\\")\n",
        "            header = next(reader)\n",
        "            header = [h.strip().lower() for h in header]\n",
        "            idx_id, idx_art, idx_sum = header.index(\"id\"), header.index(\"article\"), header.index(\"highlights\")\n",
        "            for row in reader:\n",
        "                rows.append([row[idx_id], row[idx_art], row[idx_sum]])\n",
        "        return pd.DataFrame(rows, columns=[\"id\",\"article\",\"highlights\"])\n",
        "\n",
        "# --- Dataset ---\n",
        "class CNNDMDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame):\n",
        "        self.ids  = df[\"id\"].astype(str).tolist()\n",
        "        self.srcs = df[\"article\"].astype(str).tolist()\n",
        "        self.tgts = df[\"highlights\"].astype(str).tolist()\n",
        "    def __len__(self): return len(self.ids)\n",
        "    def __getitem__(self, i):\n",
        "        return dict(id=self.ids[i], article=self.srcs[i], highlights=self.tgts[i])\n",
        "\n",
        "# --- Collator (no as_target_tokenizer; supports old/new HF) ---\n",
        "@dataclass\n",
        "class PGDataCollator:\n",
        "    tok: AutoTokenizer\n",
        "    max_src_len: int\n",
        "    max_tgt_len: int\n",
        "    def __call__(self, batch: List[Dict[str, str]]) -> Dict[str, torch.Tensor]:\n",
        "        src_texts = [b[\"article\"] for b in batch]\n",
        "        tgt_texts = [b[\"highlights\"] for b in batch]\n",
        "        try:\n",
        "            enc = self.tok(\n",
        "                src_texts,\n",
        "                text_target=tgt_texts,\n",
        "                padding=True, truncation=True,\n",
        "                max_length=self.max_src_len,\n",
        "                max_length_target=self.max_tgt_len,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "        except TypeError:\n",
        "            enc = self.tok(src_texts, padding=True, truncation=True,\n",
        "                           max_length=self.max_src_len, return_tensors=\"pt\")\n",
        "            tgt = self.tok(text_target=tgt_texts, padding=True, truncation=True,\n",
        "                           max_length=self.max_tgt_len, return_tensors=\"pt\")\n",
        "            enc[\"labels\"] = tgt[\"input_ids\"]\n",
        "        labels = enc[\"labels\"]\n",
        "        labels[labels == self.tok.pad_token_id] = -100\n",
        "        enc[\"labels\"] = labels\n",
        "        return enc\n",
        "\n",
        "# --- quick utils ---\n",
        "def _gpu_mem_mb():\n",
        "    if not torch.cuda.is_available(): return 0.0\n",
        "    return torch.cuda.max_memory_allocated() / (1024**2)\n",
        "\n",
        "class RunningMean:\n",
        "    def __init__(self, n=200):\n",
        "        self.buf, self.n = [], n\n",
        "    def add(self, x):\n",
        "        if x is None: return\n",
        "        self.buf.append(float(x))\n",
        "        if len(self.buf) > self.n: self.buf.pop(0)\n",
        "    @property\n",
        "    def mean(self):\n",
        "        return sum(self.buf)/len(self.buf) if self.buf else 0.0\n",
        "\n",
        "class StepLogger:\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "        if not os.path.exists(path):\n",
        "            with open(path, \"w\") as f:\n",
        "                f.write(\"epoch,step,loss,ce_loss,cov_loss,p_copy,p_gen,lr,grad_norm,toks_per_s,gpu_mem_mb\\n\")\n",
        "    def log(self, row: Dict):\n",
        "        with open(self.path, \"a\") as f:\n",
        "            f.write(\",\".join(str(row[k]) for k in [\"epoch\",\"step\",\"loss\",\"ce_loss\",\"cov_loss\",\"p_copy\",\"p_gen\",\"lr\",\"grad_norm\",\"toks_per_s\",\"gpu_mem_mb\"])+\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mZDMm-xAu06"
      },
      "outputs": [],
      "source": [
        "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
        "\n",
        "class CopyAwareBart(nn.Module):\n",
        "    \"\"\"\n",
        "    BART wrapper with pointer-generator + coverage.\n",
        "    - mixes vocab distribution with copy distribution built from cross-attn over source tokens\n",
        "    - computes CE over log(final_dist) + lambda_cov * coverage\n",
        "    \"\"\"\n",
        "    def __init__(self, base_lm: AutoModelForSeq2SeqLM, tokenizer: AutoTokenizer,\n",
        "                 lambda_cov: float = 1.0, eps: float = 1e-8, use_pointer: bool = True):\n",
        "        super().__init__()\n",
        "        self.base       = base_lm\n",
        "        self.tok        = tokenizer\n",
        "        self.lambda_cov = lambda_cov\n",
        "        self.eps        = eps\n",
        "        self.use_ptr    = use_pointer\n",
        "        hidden = self.base.config.d_model\n",
        "        self.p_gen_linear = nn.Linear(3*hidden, 1)\n",
        "        # caches for logging\n",
        "        self._last_ce_loss = None\n",
        "        self._last_cov_loss = None\n",
        "        self._last_p_copy_mean = None\n",
        "        self._last_p_gen_mean  = None\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        decoder_input_ids: Optional[torch.Tensor] = None,\n",
        "        decoder_attention_mask: Optional[torch.Tensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        past_key_values=None, use_cache: Optional[bool] = None, **kwargs\n",
        "    ) -> Seq2SeqLMOutput:\n",
        "\n",
        "        if labels is not None and decoder_input_ids is None:\n",
        "            decoder_input_ids = self.base.prepare_decoder_input_ids_from_labels(labels)\n",
        "        if decoder_attention_mask is None and decoder_input_ids is not None:\n",
        "            decoder_attention_mask = (decoder_input_ids != self.tok.pad_token_id).to(input_ids.dtype)\n",
        "\n",
        "        need_attn = self.use_ptr or bool(output_attentions)\n",
        "        need_hid  = self.use_ptr or bool(output_hidden_states)\n",
        "\n",
        "        base_out = self.base(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            output_attentions=need_attn,\n",
        "            output_hidden_states=need_hid,\n",
        "            labels=None,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        logits = base_out.logits  # [B,T,V]\n",
        "        final_logits = logits\n",
        "        cov_loss = None\n",
        "        copy_gate = None\n",
        "        gen_gate  = None\n",
        "        loss_ce   = None\n",
        "\n",
        "        if self.use_ptr:\n",
        "            # hidden states\n",
        "            dec_hid = base_out.decoder_hidden_states[-1]            # [B,T,H]\n",
        "            enc_out = base_out.encoder_last_hidden_state            # [B,S,H]\n",
        "\n",
        "            # cross-attn last layer: [B,heads,T,S] -> mean heads -> [B,T,S]\n",
        "            cross_atts = base_out.cross_attentions[-1].mean(dim=1)\n",
        "\n",
        "            # mask PAD in source attention\n",
        "            if attention_mask is not None:\n",
        "                src_pad_mask = (attention_mask == 0).unsqueeze(1)   # [B,1,S]\n",
        "                attn_clean   = cross_atts.masked_fill(src_pad_mask, 0.0)\n",
        "            else:\n",
        "                attn_clean   = cross_atts\n",
        "\n",
        "            # normalize per time step\n",
        "            denom     = attn_clean.sum(dim=-1, keepdim=True).clamp_min(self.eps)\n",
        "            attn_norm = attn_clean / denom                          # [B,T,S]\n",
        "\n",
        "            # context vectors\n",
        "            context = torch.bmm(attn_norm, enc_out)                 # [B,T,H]\n",
        "\n",
        "            # prev token embeddings\n",
        "            if decoder_input_ids is not None:\n",
        "                dec_emb = self.base.get_input_embeddings()(decoder_input_ids)  # [B,T,H]\n",
        "            else:\n",
        "                dec_emb = torch.zeros_like(dec_hid)\n",
        "\n",
        "            # generation/copy gates\n",
        "            p_gen  = torch.sigmoid(self.p_gen_linear(torch.cat([dec_hid, context, dec_emb], dim=-1)))  # [B,T,1]\n",
        "            p_copy = 1.0 - p_gen\n",
        "\n",
        "            # vocab distribution\n",
        "            vocab_dist = torch.softmax(logits, dim=-1)              # [B,T,V]\n",
        "\n",
        "            # copy distribution: scatter attention into vocab bins using source token ids\n",
        "            B, T, S = attn_norm.shape\n",
        "            V       = logits.size(-1)\n",
        "            copy_dist = torch.zeros(B, T, V, device=attn_norm.device, dtype=attn_norm.dtype)\n",
        "\n",
        "            batch_idx = torch.arange(B, device=input_ids.device)[:, None, None].expand(B, T, S)\n",
        "            time_idx  = torch.arange(T, device=input_ids.device)[None, :, None].expand(B, T, S)\n",
        "            vocab_idx = input_ids[:, None, :].expand(B, T, S)\n",
        "\n",
        "            copy_dist = copy_dist.index_put((batch_idx, time_idx, vocab_idx), attn_norm, accumulate=True)\n",
        "\n",
        "            # mix\n",
        "            final_dist   = p_gen * vocab_dist + p_copy * copy_dist   # [B,T,V]\n",
        "            final_logits = torch.log(final_dist + self.eps)\n",
        "\n",
        "            # gate means for logging\n",
        "            copy_gate = p_copy.mean().detach()\n",
        "            gen_gate  = p_gen.mean().detach()\n",
        "\n",
        "            # ---- coverage loss (fixed shape) ----\n",
        "            cov = torch.zeros_like(attn_norm[:, 0, :])               # [B,S]\n",
        "            step_losses = []\n",
        "            for t in range(T):\n",
        "                a_t = attn_norm[:, t, :]                             # [B,S]\n",
        "                step_losses.append(torch.min(a_t, cov).sum(dim=-1))  # [B]\n",
        "                cov = cov + a_t\n",
        "            cov_loss = torch.stack(step_losses, dim=1).mean()        # scalar\n",
        "\n",
        "        # loss over mixed distribution\n",
        "        if labels is not None:\n",
        "            V = final_logits.size(-1)\n",
        "            loss_fct = nn.NLLLoss(ignore_index=-100)\n",
        "            loss_ce  = loss_fct(final_logits.view(-1, V), labels.view(-1))\n",
        "            loss = loss_ce + (self.lambda_cov * cov_loss if cov_loss is not None else 0.0)\n",
        "        else:\n",
        "            loss = None\n",
        "\n",
        "        # expose scalars for logger\n",
        "        with torch.no_grad():\n",
        "            self._last_ce_loss     = loss_ce.detach() if loss_ce is not None else None\n",
        "            self._last_cov_loss    = cov_loss.detach() if cov_loss is not None else None\n",
        "            self._last_p_copy_mean = copy_gate if copy_gate is not None else None\n",
        "            self._last_p_gen_mean  = gen_gate  if gen_gate  is not None else None\n",
        "\n",
        "        return Seq2SeqLMOutput(\n",
        "            loss=loss,\n",
        "            logits=final_logits,\n",
        "            past_key_values=base_out.past_key_values,\n",
        "            decoder_hidden_states=base_out.decoder_hidden_states if need_hid else None,\n",
        "            decoder_attentions=base_out.decoder_attentions if need_attn else None,\n",
        "            cross_attentions=base_out.cross_attentions if need_attn else None,\n",
        "            encoder_last_hidden_state=base_out.encoder_last_hidden_state,\n",
        "            encoder_hidden_states=base_out.encoder_hidden_states if need_hid else None,\n",
        "            encoder_attentions=base_out.encoder_attentions if need_attn else None,\n",
        "        )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304,
          "referenced_widgets": [
            "0a3e5cbd26c44a36a8a05c40548968a2",
            "7bdccb7bda6e4694b554677bce884b44",
            "98d13d14665646eca085e9986e534e47",
            "185a28cab9df435692af285095cfa1fe",
            "bd98630dae604a49b4d2c6bcad8f56cd"
          ]
        },
        "id": "uMKpFb3PBQBT",
        "outputId": "440f42d4-2be5-431a-eeb7-bfa7ed2291a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a3e5cbd26c44a36a8a05c40548968a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7bdccb7bda6e4694b554677bce884b44",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98d13d14665646eca085e9986e534e47",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "185a28cab9df435692af285095cfa1fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd98630dae604a49b4d2c6bcad8f56cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ---- seed  ----\n",
        "import numpy as np\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# ---- tokenizer + base (force eager attention) ----\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "cfg = AutoConfig.from_pretrained(MODEL_NAME, attn_implementation=\"eager\")\n",
        "base = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, config=cfg)\n",
        "base.gradient_checkpointing_enable()\n",
        "\n",
        "# wrap with pointer/coverage\n",
        "model = CopyAwareBart(base, tok, lambda_cov=LAMBDA_COV, eps=EPS, use_pointer=USE_POINTER).to(DEVICE)\n",
        "\n",
        "# AMP scaler (new API)\n",
        "scaler = amp.GradScaler(\"cuda\", enabled=FP16)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt7K0_YyBQ0q"
      },
      "outputs": [],
      "source": [
        "# ---- read data ----\n",
        "train_df = robust_read_csv(TRAIN_CSV)\n",
        "val_df   = robust_read_csv(VAL_CSV)\n",
        "\n",
        "# ---- dataset + collate ----\n",
        "collate   = PGDataCollator(tok=tok, max_src_len=MAX_SRC_LEN, max_tgt_len=MAX_TGT_LEN)\n",
        "train_ds  = CNNDMDataset(train_df)\n",
        "val_ds    = CNNDMDataset(val_df)\n",
        "\n",
        "# ---- loaders ----\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True,\n",
        "    collate_fn=collate,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "    num_workers=NUM_WORKERS, pin_memory=True,\n",
        "    persistent_workers=(NUM_WORKERS>0), prefetch_factor=(2 if NUM_WORKERS>0 else None),\n",
        "    collate_fn=collate,\n",
        ")\n",
        "\n",
        "# ---- logger + checkpoint helpers ----\n",
        "LOG_CSV = os.path.join(RUN_DIR, \"train_log.csv\")\n",
        "logger  = StepLogger(LOG_CSV)\n",
        "\n",
        "def save_checkpoint(tag):\n",
        "    \"\"\"Save base model, tokenizer, plus pointer head state.\"\"\"\n",
        "    ck = os.path.join(RUN_DIR, f\"ckpt_step{tag}\" if isinstance(tag,int) else f\"ckpt_{tag}\")\n",
        "    os.makedirs(ck, exist_ok=True)\n",
        "    model.base.save_pretrained(ck)\n",
        "    tok.save_pretrained(ck)\n",
        "    torch.save({\n",
        "        \"p_gen_linear\": model.p_gen_linear.state_dict(),\n",
        "        \"lambda_cov\": model.lambda_cov,\n",
        "        \"use_pointer\": model.use_ptr,\n",
        "    }, os.path.join(ck, \"pointer_head.pt\"))\n",
        "    print(f\"[checkpoint] saved → {ck}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mt3yzoUyBUGa",
        "outputId": "6031d7e2-ea70-4636-fb42-583413a044bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[scheduler] total=2000, warmup=120\n"
          ]
        }
      ],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\"]\n",
        "params_decay, params_nodecay = [], []\n",
        "for n, p in model.named_parameters():\n",
        "    if not p.requires_grad:\n",
        "        continue\n",
        "    (params_nodecay if any(nd in n for nd in no_decay) else params_decay).append(p)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [{\"params\": params_decay, \"weight_decay\": WEIGHT_DECAY},\n",
        "     {\"params\": params_nodecay, \"weight_decay\": 0.0}],\n",
        "    lr=LR,\n",
        ")\n",
        "\n",
        "import math\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "num_update_steps_per_epoch = math.ceil(len(train_loader) / GRAD_ACCUM_STEPS)\n",
        "\n",
        "# >>> KEY FIX: respect MAX_STEPS if set\n",
        "t_total = int(MAX_STEPS) if MAX_STEPS else num_update_steps_per_epoch * EPOCHS\n",
        "warmup_steps = max(1, int(WARMUP_RATIO * t_total))  # e.g., 0.06 * 2000 = 120\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=t_total,\n",
        ")\n",
        "print(f\"[scheduler] total={t_total}, warmup={warmup_steps}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MoaWIq0BWCa",
        "outputId": "283485ff-152d-4aec-dd48-7de9b4e6a2b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "BartModel is using BartSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `layer_head_mask` not None. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ep 1] step   1100 | loss  2.9640 | ce  2.3699 | cov  0.5941 | p_copy 0.180 | p_gen 0.820 | lr 1.44e-05 | grad_norm 222559.80 | toks/s   9324.3 | gpu_mem 12996.4 MB\n",
            "[ep 1] step   1200 | loss  2.9586 | ce  2.3710 | cov  0.5876 | p_copy 0.182 | p_gen 0.818 | lr 1.28e-05 | grad_norm 212593.80 | toks/s   9493.6 | gpu_mem 12996.4 MB\n",
            "[ep 1] step   1300 | loss  2.9484 | ce  2.3731 | cov  0.5753 | p_copy 0.184 | p_gen 0.816 | lr 1.12e-05 | grad_norm 217491.92 | toks/s   9682.5 | gpu_mem 12996.4 MB\n",
            "[ep 1] step   1400 | loss  2.9294 | ce  2.3654 | cov  0.5640 | p_copy 0.187 | p_gen 0.813 | lr 9.57e-06 | grad_norm 215148.19 | toks/s   9710.0 | gpu_mem 12996.4 MB\n",
            "[ep 1] step   1500 | loss  2.9147 | ce  2.3613 | cov  0.5534 | p_copy 0.190 | p_gen 0.810 | lr 7.98e-06 | grad_norm 97175.05 | toks/s   9722.0 | gpu_mem 12996.4 MB\n",
            "[ep 1] step   1600 | loss  2.9046 | ce  2.3608 | cov  0.5438 | p_copy 0.193 | p_gen 0.807 | lr 6.38e-06 | grad_norm 99697.49 | toks/s   9728.3 | gpu_mem 12996.4 MB\n",
            "[ep 1] step   1700 | loss  2.8830 | ce  2.3490 | cov  0.5340 | p_copy 0.196 | p_gen 0.804 | lr 4.79e-06 | grad_norm 54555.22 | toks/s   9731.1 | gpu_mem 12996.4 MB\n",
            "[ep 1] step   1800 | loss  2.8673 | ce  2.3429 | cov  0.5244 | p_copy 0.198 | p_gen 0.802 | lr 3.19e-06 | grad_norm 47707.64 | toks/s   9733.8 | gpu_mem 12996.4 MB\n",
            "[ep 1] step   1900 | loss  2.8621 | ce  2.3448 | cov  0.5173 | p_copy 0.201 | p_gen 0.799 | lr 1.60e-06 | grad_norm 50475.98 | toks/s   9736.6 | gpu_mem 12996.4 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ep 1] step   2000 | loss  2.8625 | ce  2.3490 | cov  0.5135 | p_copy 0.204 | p_gen 0.796 | lr 0.00e+00 | grad_norm 49341.95 | toks/s   9737.1 | gpu_mem 12996.4 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[checkpoint] saved → /content/drive/MyDrive/student_pgc_bartbase/ckpt_step2000\n",
            "[train] Reached MAX_STEPS=2000. Stopping early.\n",
            "[checkpoint] saved → /content/drive/MyDrive/student_pgc_bartbase/ckpt_step2000\n"
          ]
        }
      ],
      "source": [
        "#global_step = 0\n",
        "best_val = float(\"inf\")\n",
        "\n",
        "# running averages\n",
        "m_loss = RunningMean(200); m_ce = RunningMean(200); m_cov = RunningMean(200)\n",
        "m_pcopy = RunningMean(200); m_pgen = RunningMean(200); m_toks = RunningMean(200)\n",
        "\n",
        "model.train()\n",
        "optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "start_time = time.time()\n",
        "tokens_seen = 0\n",
        "\n",
        "\n",
        "# running averages\n",
        "m_loss = RunningMean(200); m_ce = RunningMean(200); m_cov = RunningMean(200)\n",
        "m_pcopy = RunningMean(200); m_pgen = RunningMean(200); m_toks = RunningMean(200)\n",
        "\n",
        "model.train()\n",
        "optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "start_time = time.time()\n",
        "tokens_seen = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    for it, batch in enumerate(train_loader):\n",
        "        for k in batch:\n",
        "            batch[k] = batch[k].to(DEVICE)\n",
        "\n",
        "        # token count for throughput\n",
        "        with torch.no_grad():\n",
        "            toks_this_batch = int(batch[\"attention_mask\"].sum().item())\n",
        "            if \"labels\" in batch:\n",
        "                toks_this_batch += int((batch[\"labels\"] != -100).sum().item())\n",
        "\n",
        "        # forward pass with autocast\n",
        "        with amp.autocast(\"cuda\", enabled=FP16):\n",
        "            out = model(**batch)\n",
        "            loss = out.loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        took_step = False\n",
        "\n",
        "        if (it + 1) % GRAD_ACCUM_STEPS == 0:\n",
        "            # unscale once, then clip & log grad norm\n",
        "            # grad norm (scaled grads are fine for logging in AMP)\n",
        "            # grad norm (scaled grads are fine for logging in AMP)\n",
        "            total_norm = torch.norm(\n",
        "                torch.stack([p.grad.detach().norm(2) for p in model.parameters() if p.grad is not None])\n",
        "            ).item()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            scheduler.step()\n",
        "            global_step += 1\n",
        "            took_step = True\n",
        "\n",
        "            # update running stats\n",
        "            tokens_seen += toks_this_batch\n",
        "            m_loss.add(loss.item())\n",
        "            m_ce.add(getattr(model, \"_last_ce_loss\", None))\n",
        "            m_cov.add(getattr(model, \"_last_cov_loss\", None))\n",
        "            m_pcopy.add(getattr(model, \"_last_p_copy_mean\", None))\n",
        "            m_pgen.add(getattr(model, \"_last_p_gen_mean\", None))\n",
        "            elapsed = time.time() - start_time\n",
        "            m_toks.add(tokens_seen / max(elapsed, 1e-6))\n",
        "\n",
        "            if global_step % LOG_EVERY == 0:\n",
        "                lr = scheduler.get_last_lr()[0]\n",
        "                mem = _gpu_mem_mb()\n",
        "                print(\n",
        "                    f\"[ep {epoch+1}] step {global_step:>6} | \"\n",
        "                    f\"loss {m_loss.mean:7.4f} | ce {m_ce.mean:7.4f} | cov {m_cov.mean:7.4f} | \"\n",
        "                    f\"p_copy {m_pcopy.mean:5.3f} | p_gen {m_pgen.mean:5.3f} | \"\n",
        "                    f\"lr {lr:.2e} | grad_norm {total_norm:6.2f} | \"\n",
        "                    f\"toks/s {m_toks.mean:8.1f} | gpu_mem {mem:7.1f} MB\"\n",
        "                )\n",
        "                logger.log({\n",
        "                    \"epoch\": epoch+1,\n",
        "                    \"step\": global_step,\n",
        "                    \"loss\": round(m_loss.mean, 6),\n",
        "                    \"ce_loss\": round(m_ce.mean, 6),\n",
        "                    \"cov_loss\": round(m_cov.mean, 6),\n",
        "                    \"p_copy\": round(m_pcopy.mean, 6),\n",
        "                    \"p_gen\": round(m_pgen.mean, 6),\n",
        "                    \"lr\": lr,\n",
        "                    \"grad_norm\": total_norm,\n",
        "                    \"toks_per_s\": m_toks.mean,\n",
        "                    \"gpu_mem_mb\": mem,\n",
        "                })\n",
        "\n",
        "            if global_step % SAVE_EVERY_STEPS == 0:\n",
        "                save_checkpoint(global_step)\n",
        "\n",
        "            if global_step >= MAX_STEPS:\n",
        "                print(f\"[train] Reached MAX_STEPS={MAX_STEPS}. Stopping early.\")\n",
        "                save_checkpoint(f\"step{global_step}\")\n",
        "                break\n",
        "\n",
        "        if torch.cuda.is_available(): torch.cuda.synchronize()\n",
        "        del out\n",
        "        if took_step: torch.cuda.empty_cache()\n",
        "\n",
        "    if global_step >= MAX_STEPS:\n",
        "        break\n",
        "\n",
        "    # ---- quick validation ----\n",
        "    model.eval()\n",
        "    val_loss, val_seen = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for vb in val_loader:\n",
        "            for k in vb: vb[k] = vb[k].to(DEVICE)\n",
        "            with amp.autocast(\"cuda\", enabled=FP16):\n",
        "                vo = model(**vb)\n",
        "                if vo.loss is not None:\n",
        "                    val_loss += vo.loss.item()\n",
        "                    val_seen += 1\n",
        "    if val_seen > 0:\n",
        "        val_loss /= val_seen\n",
        "        print(f\"[val] epoch {epoch+1} | loss {val_loss:.4f}\")\n",
        "        if val_loss < best_val:\n",
        "            best_val = val_loss\n",
        "            save_checkpoint(f\"best_ep{epoch+1}\")\n",
        "    model.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsUDEPhaeXeK"
      },
      "source": [
        "Resuming Training from step 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YmUIx0DdmOa",
        "outputId": "b944ce9b-bb6e-4724-9850-f7f37f6e18a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[resume] step=1000, batch=56x2 (eff=112), warmup=120, lr_now=1.596e-05, attn_impl=eager, out_attn=True\n"
          ]
        }
      ],
      "source": [
        "# ===== RESUME + MORE CAPACITY (fixed) =====\n",
        "import os, math, torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import amp\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "\n",
        "# --- where to resume ---\n",
        "RESUME_CKPT = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step1000\"\n",
        "RESUME_STEP = 1000\n",
        "\n",
        "# --- capacity (you had VRAM headroom) ---\n",
        "BATCH_SIZE         = 56\n",
        "GRAD_ACCUM_STEPS   = 2\n",
        "NUM_WORKERS        = 4\n",
        "PERSISTENT_WORKERS = True\n",
        "PREFETCH_FACTOR    = 2\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- reload tokenizer + base; FORCE eager attention & return attentions by default ---\n",
        "tok = AutoTokenizer.from_pretrained(RESUME_CKPT, use_fast=True)\n",
        "cfg = AutoConfig.from_pretrained(RESUME_CKPT)\n",
        "cfg.attn_implementation = \"eager\"     # critical: sdpa will NOT return cross_attentions\n",
        "base = AutoModelForSeq2SeqLM.from_pretrained(RESUME_CKPT, config=cfg)\n",
        "\n",
        "# memory/compat\n",
        "base.gradient_checkpointing_enable()   # lets you push batch size\n",
        "base.config.use_cache = False          # avoid warning with gradient checkpointing\n",
        "\n",
        "# ensure attentions/hidden states are returned even if forward() doesn't pass flags\n",
        "base.config.output_attentions = True\n",
        "base.config.output_hidden_states = True\n",
        "\n",
        "# --- wrap with pointer/coverage and load pointer head (safe) ---\n",
        "model = CopyAwareBart(base, tok, lambda_cov=LAMBDA_COV, eps=EPS, use_pointer=USE_POINTER).to(DEVICE)\n",
        "ptr_path = os.path.join(RESUME_CKPT, \"pointer_head.pt\")\n",
        "if os.path.exists(ptr_path):\n",
        "    state = torch.load(ptr_path, map_location=\"cpu\")\n",
        "    if \"p_gen_linear\" in state:\n",
        "        model.p_gen_linear.load_state_dict(state[\"p_gen_linear\"])\n",
        "    if \"lambda_cov\" in state:\n",
        "        model.lambda_cov = float(state[\"lambda_cov\"])\n",
        "    if \"use_pointer\" in state:\n",
        "        model.use_ptr = bool(state[\"use_pointer\"])\n",
        "else:\n",
        "    print(\"[warn] pointer_head.pt not found — continuing with current pointer params\")\n",
        "\n",
        "# --- rebuild loaders with larger batch ---\n",
        "train_df = robust_read_csv(TRAIN_CSV)\n",
        "val_df   = robust_read_csv(VAL_CSV)\n",
        "collate  = PGDataCollator(tok=tok, max_src_len=MAX_SRC_LEN, max_tgt_len=MAX_TGT_LEN)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    CNNDMDataset(train_df),\n",
        "    batch_size=BATCH_SIZE, shuffle=True,\n",
        "    num_workers=NUM_WORKERS, pin_memory=True,\n",
        "    persistent_workers=PERSISTENT_WORKERS, prefetch_factor=PREFETCH_FACTOR,\n",
        "    collate_fn=collate,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    CNNDMDataset(val_df),\n",
        "    batch_size=BATCH_SIZE, shuffle=False,\n",
        "    num_workers=NUM_WORKERS, pin_memory=True,\n",
        "    persistent_workers=PERSISTENT_WORKERS, prefetch_factor=PREFETCH_FACTOR,\n",
        "    collate_fn=collate,\n",
        ")\n",
        "\n",
        "# --- optimizer + MAX_STEPS-aware scheduler positioned at RESUME_STEP ---\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\"]\n",
        "params_decay, params_nodecay = [], []\n",
        "for n, p in model.named_parameters():\n",
        "    if not p.requires_grad:\n",
        "        continue\n",
        "    (params_nodecay if any(nd in n for nd in no_decay) else params_decay).append(p)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [\n",
        "        {\"params\": params_decay,   \"weight_decay\": WEIGHT_DECAY, \"lr\": LR, \"initial_lr\": LR},\n",
        "        {\"params\": params_nodecay, \"weight_decay\": 0.0,          \"lr\": LR, \"initial_lr\": LR},\n",
        "    ],\n",
        "    lr=LR,\n",
        ")\n",
        "\n",
        "t_total      = int(MAX_STEPS)                        # e.g., 2000\n",
        "warmup_steps = max(1, int(WARMUP_RATIO * t_total))   # e.g., 120 if 0.06\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=t_total,\n",
        "    last_epoch=RESUME_STEP - 1,                      # continue from step 1000\n",
        ")\n",
        "\n",
        "scaler = amp.GradScaler(\"cuda\", enabled=FP16)\n",
        "\n",
        "# hand the loop your current step\n",
        "global_step = RESUME_STEP\n",
        "print(f\"[resume] step={global_step}, batch={BATCH_SIZE}x{GRAD_ACCUM_STEPS} (eff={BATCH_SIZE*GRAD_ACCUM_STEPS}), warmup={warmup_steps}, lr_now={scheduler.get_last_lr()[0]:.3e}, attn_impl={base.config.attn_implementation}, out_attn={base.config.output_attentions}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlDIwcCT3Lvp"
      },
      "source": [
        "\n",
        "\n",
        "> At step 1000:\n",
        "Resume — What changed and why\n",
        "\n",
        "Goal: use more VRAM and continue training from step 1000\n",
        "\n",
        "Capacity bump: BATCH_SIZE=56, GRAD_ACCUM_STEPS=2 (eff=112), gradient checkpointing enabled\n",
        "\n",
        "Critical resume fixes:\n",
        "\n",
        "Loaded with attn_implementation=\"eager\" to guarantee cross_attentions (pointer needs this)\n",
        "\n",
        "Ensured output_attentions=True & output_hidden_states=True via config\n",
        "\n",
        "Did not reset global_step; scheduler rebuilt with last_epoch=999\n",
        "\n",
        "Confirmed pointer head reloaded from pointer_head.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tktGgX7YxwMe"
      },
      "source": [
        "decoding without copy awareness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCf04qOgBYJy",
        "outputId": "337ff6df-e5b8-4f7f-fac5-bc9f51b168d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ROUGE] evaluating 1000 examples with batch_size=8...\n",
            "{'rouge1': 0.2975, 'rouge2': 0.11, 'rougeL': 0.2035, 'rougeLsum': 0.2035}\n"
          ]
        }
      ],
      "source": [
        "# ===== ROUGE-1/2/L/Lsum evaluation =====\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "try:\n",
        "    from rouge_score import rouge_scorer\n",
        "except Exception:\n",
        "    %pip -q install rouge-score\n",
        "    from rouge_score import rouge_scorer\n",
        "\n",
        "\n",
        "from transformers import GenerationConfig\n",
        "gen_cfg = GenerationConfig(\n",
        "    num_beams=5,\n",
        "    min_new_tokens=55,\n",
        "    max_new_tokens=100,\n",
        "    no_repeat_ngram_size=3,\n",
        "    length_penalty=2.0,\n",
        "    early_stopping=True,\n",
        ")\n",
        "\n",
        "# 3) Configure eval size\n",
        "N_EVAL  = 1000            # set to None for FULL validation\n",
        "EVAL_BS = 8               # generation batch size\n",
        "MAX_N   = len(val_df) if N_EVAL is None else min(N_EVAL, len(val_df))\n",
        "DEVICE  = next(model.parameters()).device\n",
        "\n",
        "print(f\"[ROUGE] evaluating {MAX_N} examples with batch_size={EVAL_BS}...\")\n",
        "\n",
        "# 4) Generate predictions\n",
        "preds, refs = [], []\n",
        "model.eval()\n",
        "\n",
        "for start in range(0, MAX_N, EVAL_BS):\n",
        "    batch = val_df.iloc[start:start+EVAL_BS]\n",
        "    srcs  = batch[\"article\"].astype(str).tolist()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ins  = tok(srcs, return_tensors=\"pt\", truncation=True, max_length=MAX_SRC_LEN, padding=True).to(DEVICE)\n",
        "        # Use the base model's generate (wrapper has no .generate)\n",
        "        outs = model.base.generate(**ins, generation_config=gen_cfg, output_attentions=True)\n",
        "    preds.extend(tok.batch_decode(outs, skip_special_tokens=True))\n",
        "    refs.extend(batch[\"highlights\"].astype(str).tolist())\n",
        "\n",
        "# 5) Compute ROUGE (average F1)\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL','rougeLsum'], use_stemmer=True)\n",
        "agg = {k: 0.0 for k in ['rouge1','rouge2','rougeL','rougeLsum']}\n",
        "for p, r in zip(preds, refs):\n",
        "    s = scorer.score(r, p)  # (reference, prediction)\n",
        "    for k in agg: agg[k] += s[k].fmeasure\n",
        "for k in agg: agg[k] /= max(1, len(preds))\n",
        "\n",
        "print({k: round(v, 4) for k, v in agg.items()})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8M53v156nWH",
        "outputId": "97c7de69-be3c-4b9e-f8c3-cf49c64ff026"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== TOKEN LENGTHS (tokenizer) ===\n",
            "Ref : {'mean': 43.3, 'median': 42.0, 'p25': 34.0, 'p50': 42.0, 'p75': 52.0}\n",
            "Pred: {'mean': 77.8, 'median': 74.0, 'p25': 66.0, 'p50': 74.0, 'p75': 89.0}\n",
            "\n",
            "=== WORD LENGTHS (whitespace) ===\n",
            "Ref : {'mean': 34.4, 'median': 33.0, 'p25': 27.0, 'p50': 33.0, 'p75': 41.0}\n",
            "Pred: {'mean': 60.9, 'median': 58.0, 'p25': 50.0, 'p50': 58.0, 'p75': 70.0}\n",
            "\n",
            "=== SUGGESTED min_new_tokens (in tokens) ===\n",
            "Try one of: 34  |  37  |  40\n",
            "(Current min_new_tokens=55)\n"
          ]
        }
      ],
      "source": [
        "# ===== Length stats & suggested min_new_tokens =====\n",
        "import numpy as np, torch, pandas as pd\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "assert 'model' in globals() and 'tok' in globals() and 'val_df' in globals(), \"Need model, tok, val_df in memory\"\n",
        "DEVICE = next(model.parameters()).device\n",
        "MAX_SRC_LEN = int(globals().get('MAX_SRC_LEN', 400))\n",
        "\n",
        "# Eval size & batch\n",
        "N_EVAL  = 1000\n",
        "EVAL_BS = 8\n",
        "MAX_N   = len(val_df) if N_EVAL is None else min(N_EVAL, len(val_df))\n",
        "\n",
        "\n",
        "gen_cfg = GenerationConfig(\n",
        "    num_beams=5,\n",
        "    min_new_tokens=55,\n",
        "    max_new_tokens=100,\n",
        "    no_repeat_ngram_size=3,\n",
        "    length_penalty=2.0,\n",
        "    early_stopping=True,\n",
        ")\n",
        "\n",
        "def _word_len(s): return len(str(s).split())\n",
        "\n",
        "preds, refs = [], []\n",
        "model.eval()\n",
        "for start in range(0, MAX_N, EVAL_BS):\n",
        "    batch = val_df.iloc[start:start+EVAL_BS]\n",
        "    srcs  = batch[\"article\"].astype(str).tolist()\n",
        "    with torch.no_grad():\n",
        "        ins  = tok(srcs, return_tensors=\"pt\", truncation=True, max_length=MAX_SRC_LEN, padding=True).to(DEVICE)\n",
        "        outs = model.base.generate(**ins, generation_config=gen_cfg)\n",
        "    preds.extend(tok.batch_decode(outs, skip_special_tokens=True))\n",
        "    refs.extend(batch[\"highlights\"].astype(str).tolist())\n",
        "\n",
        "# Token lengths via tokenizer (target side)\n",
        "ref_tok_lens  = [len(tok(r, return_tensors=None, truncation=True).input_ids) for r in refs]\n",
        "pred_tok_lens = [len(tok(p, return_tensors=None, truncation=True).input_ids) for p in preds]\n",
        "\n",
        "# Word lengths\n",
        "ref_w_lens  = [_word_len(r) for r in refs]\n",
        "pred_w_lens = [_word_len(p) for p in preds]\n",
        "\n",
        "def stats(arr):\n",
        "    a = np.array(arr)\n",
        "    return dict(\n",
        "        mean=float(a.mean()),\n",
        "        median=float(np.median(a)),\n",
        "        p25=float(np.percentile(a,25)),\n",
        "        p50=float(np.percentile(a,50)),\n",
        "        p75=float(np.percentile(a,75))\n",
        "    )\n",
        "\n",
        "ref_tok_stats  = stats(ref_tok_lens)\n",
        "pred_tok_stats = stats(pred_tok_lens)\n",
        "ref_w_stats    = stats(ref_w_lens)\n",
        "pred_w_stats   = stats(pred_w_lens)\n",
        "\n",
        "print(\"=== TOKEN LENGTHS (tokenizer) ===\")\n",
        "print(\"Ref :\", {k: round(v,1) for k,v in ref_tok_stats.items()})\n",
        "print(\"Pred:\", {k: round(v,1) for k,v in pred_tok_stats.items()})\n",
        "\n",
        "print(\"\\n=== WORD LENGTHS (whitespace) ===\")\n",
        "print(\"Ref :\", {k: round(v,1) for k,v in ref_w_stats.items()})\n",
        "print(\"Pred:\", {k: round(v,1) for k,v in pred_w_stats.items()})\n",
        "\n",
        "# Suggest min_new_tokens: aim near lower-to-middle of ref distribution so we don't over-constrain length\n",
        "p25 = int(round(ref_tok_stats[\"p25\"]))\n",
        "p35 = int(round(np.percentile(ref_tok_lens, 35)))\n",
        "p45 = int(round(np.percentile(ref_tok_lens, 45)))\n",
        "\n",
        "suggest_low, suggest_mid, suggest_high = max(10, p25), max(10, p35), max(10, p45)\n",
        "print(\"\\n=== SUGGESTED min_new_tokens (in tokens) ===\")\n",
        "print(f\"Try one of: {suggest_low}  |  {suggest_mid}  |  {suggest_high}\")\n",
        "print(f\"(Current min_new_tokens={gen_cfg.min_new_tokens})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGvqq9yx-Yu2",
        "outputId": "07c9563f-3efc-4e50-b686-4b7777319a55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Repetition (3-gram) ===\n",
            "mean: 0.0012  | median: 0.0\n",
            "p75 : 0.0\n",
            "\n",
            "=== Entity coverage (macro over examples) ===\n",
            "Precision: 0.2325\n",
            "Recall   : 0.3572\n",
            "F1       : 0.2619\n"
          ]
        }
      ],
      "source": [
        "# ===== Repetition (3-gram) & Entity coverage =====\n",
        "import re, numpy as np, torch, pandas as pd\n",
        "from collections import Counter\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "assert 'model' in globals() and 'tok' in globals() and 'val_df' in globals(), \"Need model, tok, val_df in memory\"\n",
        "DEVICE = next(model.parameters()).device\n",
        "MAX_SRC_LEN = int(globals().get('MAX_SRC_LEN', 400))\n",
        "\n",
        "# Eval size & batch\n",
        "N_EVAL  = 1000\n",
        "EVAL_BS = 8\n",
        "MAX_N   = len(val_df) if N_EVAL is None else min(N_EVAL, len(val_df))\n",
        "\n",
        "gen_cfg = GenerationConfig(\n",
        "    num_beams=5,\n",
        "    min_new_tokens=55,\n",
        "    max_new_tokens=100,\n",
        "    no_repeat_ngram_size=3,\n",
        "    length_penalty=2.0,\n",
        "    early_stopping=True,\n",
        ")\n",
        "\n",
        "# Generate preds/refs\n",
        "preds, refs = [], []\n",
        "model.eval()\n",
        "for start in range(0, MAX_N, EVAL_BS):\n",
        "    batch = val_df.iloc[start:start+EVAL_BS]\n",
        "    srcs  = batch[\"article\"].astype(str).tolist()\n",
        "    with torch.no_grad():\n",
        "        ins  = tok(srcs, return_tensors=\"pt\", truncation=True, max_length=MAX_SRC_LEN, padding=True).to(DEVICE)\n",
        "        outs = model.base.generate(**ins, generation_config=gen_cfg)\n",
        "    preds.extend(tok.batch_decode(outs, skip_special_tokens=True))\n",
        "    refs.extend(batch[\"highlights\"].astype(str).tolist())\n",
        "\n",
        "# --- Repetition: tri-gram repeat ratio ---\n",
        "def trigram_repeat_ratio(text):\n",
        "    toks = re.findall(r\"\\w+|\\S\", text)  # simple tokenization\n",
        "    if len(toks) < 3: return 0.0\n",
        "    trigs = [tuple(toks[i:i+3]) for i in range(len(toks)-2)]\n",
        "    total = len(trigs)\n",
        "    uniq  = len(set(trigs))\n",
        "    return 0.0 if total == 0 else 1.0 - (uniq / total)\n",
        "\n",
        "tri_rep = [trigram_repeat_ratio(p) for p in preds]\n",
        "print(\"=== Repetition (3-gram) ===\")\n",
        "print(\"mean:\", round(float(np.mean(tri_rep)), 4), \" | median:\", round(float(np.median(tri_rep)), 4))\n",
        "print(\"p75 :\", round(float(np.percentile(tri_rep, 75)), 4))\n",
        "\n",
        "# --- Entity coverage using spaCy NER ---\n",
        "try:\n",
        "    import spacy\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except Exception:\n",
        "    %pip -q install spacy\n",
        "    !python -m spacy download en_core_web_sm\n",
        "    import spacy\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def ents(text):\n",
        "    doc = nlp(text)\n",
        "    # use (surface, label) pairs to reduce false matches\n",
        "    return {(e.text.strip(), e.label_) for e in doc.ents}\n",
        "\n",
        "precisions, recalls, f1s = [], [], []\n",
        "for p, r in zip(preds, refs):\n",
        "    E_r, E_p = ents(r), ents(p)\n",
        "    if not E_r and not E_p:\n",
        "        continue\n",
        "    tp   = len(E_r & E_p)\n",
        "    prec = tp / max(1, len(E_p))\n",
        "    rec  = tp / max(1, len(E_r))\n",
        "    f1   = 0.0 if (prec+rec)==0 else 2*prec*rec/(prec+rec)\n",
        "    precisions.append(prec); recalls.append(rec); f1s.append(f1)\n",
        "\n",
        "if precisions:\n",
        "    print(\"\\n=== Entity coverage (macro over examples) ===\")\n",
        "    print(\"Precision:\", round(float(np.mean(precisions)), 4))\n",
        "    print(\"Recall   :\", round(float(np.mean(recalls)), 4))\n",
        "    print(\"F1       :\", round(float(np.mean(f1s)), 4))\n",
        "else:\n",
        "    print(\"\\n=== Entity coverage ===\")\n",
        "    print(\"No entities detected in this slice.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9suK_3rlBnjG",
        "outputId": "e7e7be7f-0065-4309-a46a-42a39721db89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==========================================================================================\n",
            "EXAMPLE 0\n",
            "- SOURCE (first 600 chars) -\n",
            "Share, and your gift will be multiplied. That may sound like an esoteric adage, but when Zully Broussard selflessly decided to give one of her kidneys to a stranger, her generosity paired up with big data. It resulted in six patients receiving transplants. That surprised and wowed her. \"I thought I was going to help this one person who I don't know, but the fact that so many people can have a life extension, that's pretty big,\" Broussard told CNN affiliate KGO. She may feel guided in her generosity by a higher power. \"Thanks for all the support and prayers,\" a comment on a Facebook page in ...\n",
            "\n",
            "- REFERENCE -\n",
            "Zully Broussard decided to give a kidney to a stranger . A new computer program helped her donation spur transplants for six kidney patients .\n",
            "\n",
            "- PREDICTION -\n",
            "Zully Broussard selflessly decided to give one of her kidneys to a stranger. It resulted in six patients receiving transplants. She may feel guided in her generosity by a higher power. The medical center is taking five surgeons, a covey of physician assistants, nurses and anesthesiologists, and more than 40 support staff to perform surgeries on 12 people.\n",
            "\n",
            "- POINTER (first 40 tokens) -\n",
            "00  Z                p_copy=0.001 \n",
            "01  ully             p_copy=0.006 \n",
            "02  ĠB               p_copy=0.028 \n",
            "03  rou              p_copy=0.295 \n",
            "04  ss               p_copy=0.026 \n",
            "05  ard              p_copy=0.026 \n",
            "06  Ġself            p_copy=0.035 \n",
            "07  lessly           p_copy=0.309 \n",
            "08  Ġdecided         p_copy=0.031 \n",
            "09  Ġto              p_copy=0.164 \n",
            "10  Ġgive            p_copy=0.069 \n",
            "11  Ġone             p_copy=0.158 \n",
            "12  Ġof              p_copy=0.483 \n",
            "13  Ġher             p_copy=0.254 \n",
            "14  Ġkidneys         p_copy=0.238 \n",
            "15  Ġto              p_copy=0.402 \n",
            "16  Ġa               p_copy=0.179 \n",
            "17  Ġstranger        p_copy=0.400 \n",
            "18  Ġ.               p_copy=0.447 \n",
            "19  ĠIt              p_copy=0.047 \n",
            "20  Ġresulted        p_copy=0.188 \n",
            "21  Ġin              p_copy=0.213 \n",
            "22  Ġsix             p_copy=0.055 \n",
            "23  Ġpatients        p_copy=0.765 COPY\n",
            "24  Ġreceiving       p_copy=0.368 \n",
            "25  Ġtranspl         p_copy=0.257 \n",
            "26  ants             p_copy=0.574 COPY\n",
            "27  Ġ.               p_copy=0.019 \n",
            "28  ĠShe             p_copy=0.020 \n",
            "29  Ġmay             p_copy=0.180 \n",
            "30  Ġfeel            p_copy=0.100 \n",
            "31  Ġguided          p_copy=0.141 \n",
            "32  Ġin              p_copy=0.211 \n",
            "33  Ġher             p_copy=0.082 \n",
            "34  Ġgenerosity      p_copy=0.205 \n",
            "35  Ġby              p_copy=0.232 \n",
            "36  Ġa               p_copy=0.112 \n",
            "37  Ġhigher          p_copy=0.508 COPY\n",
            "38  Ġpower           p_copy=0.478 \n",
            "39  Ġ.               p_copy=0.239 \n",
            "\n",
            "Pointer summary: mean p_copy=0.238 | fraction tokens tagged COPY=0.147\n",
            "\n",
            "- ENTITIES -\n",
            "Ref: [('Zully Broussard', 'PERSON'), ('six', 'CARDINAL')]\n",
            "Prd: [('12', 'CARDINAL'), ('Zully Broussard', 'PERSON'), ('five', 'CARDINAL'), ('more than 40', 'CARDINAL'), ('six', 'CARDINAL')]\n",
            "∩   : [('Zully Broussard', 'PERSON'), ('six', 'CARDINAL')]\n",
            "\n",
            "==========================================================================================\n",
            "EXAMPLE 1\n",
            "- SOURCE (first 600 chars) -\n",
            "On the 6th of April 1996, San Jose Clash and DC United strode out in front of 31,683 expectant fans at the Spartan Stadium in San Jose, California. The historic occasion was the first ever Major League Soccer match -- a brave new dawn for the world's favorite sport in a land its charms had yet to conquer. Summarizing the action for ESPN, commentator Ty Keough eagerly described the momentous \"birth of a new era for American soccer.\" Looking back at footage from that balmy evening now it's hard not to feel a certain nostalgia. Baggy shirts, questionable hairstyles and strange rule ...\n",
            "\n",
            "- REFERENCE -\n",
            "The 20th MLS season begins this weekend . League has changed dramatically since its inception in 1996 . Some question whether rules regarding salary caps and transfers need to change .\n",
            "\n",
            "- PREDICTION -\n",
            "MLS prepares to mark the beginning of its 20th season. Attendances are higher than ever before while the number of teams involved has doubled from 10 in the 1996 campaign to 20 in 2015. The new season is the first of a new domestic TV and media rights deal with FOX, ESPN and Univision worth $700 million over eight years.\n",
            "\n",
            "- POINTER (first 40 tokens) -\n",
            "00  ML               p_copy=0.001 \n",
            "01  S                p_copy=0.010 \n",
            "02  Ġprepares        p_copy=0.010 \n",
            "03  Ġto              p_copy=0.175 \n",
            "04  Ġmark            p_copy=0.152 \n",
            "05  Ġthe             p_copy=0.157 \n",
            "06  Ġbeginning       p_copy=0.587 COPY\n",
            "07  Ġof              p_copy=0.478 \n",
            "08  Ġits             p_copy=0.105 \n",
            "09  Ġ20              p_copy=0.547 COPY\n",
            "10  th               p_copy=0.338 \n",
            "11  Ġseason          p_copy=0.041 \n",
            "12  Ġ.               p_copy=0.102 \n",
            "13  ĠAttend          p_copy=0.045 \n",
            "14  ances            p_copy=0.424 \n",
            "15  Ġare             p_copy=0.041 \n",
            "16  Ġhigher          p_copy=0.175 \n",
            "17  Ġthan            p_copy=0.276 \n",
            "18  Ġever            p_copy=0.138 \n",
            "19  Ġbefore          p_copy=0.286 \n",
            "20  Ġwhile           p_copy=0.047 \n",
            "21  Ġthe             p_copy=0.160 \n",
            "22  Ġnumber          p_copy=0.716 COPY\n",
            "23  Ġof              p_copy=0.700 COPY\n",
            "24  Ġteams           p_copy=0.061 \n",
            "25  Ġinvolved        p_copy=0.445 \n",
            "26  Ġhas             p_copy=0.158 \n",
            "27  Ġdoubled         p_copy=0.263 \n",
            "28  Ġfrom            p_copy=0.227 \n",
            "29  Ġ10              p_copy=0.143 \n",
            "30  Ġin              p_copy=0.495 \n",
            "31  Ġthe             p_copy=0.250 \n",
            "32  Ġ1996            p_copy=0.383 \n",
            "33  Ġcampaign        p_copy=0.461 \n",
            "34  Ġto              p_copy=0.225 \n",
            "35  Ġ20              p_copy=0.106 \n",
            "36  Ġin              p_copy=0.366 \n",
            "37  Ġ2015            p_copy=0.205 \n",
            "38  Ġ.               p_copy=0.248 \n",
            "39  ĠThe             p_copy=0.013 \n",
            "\n",
            "Pointer summary: mean p_copy=0.288 | fraction tokens tagged COPY=0.159\n",
            "\n",
            "- ENTITIES -\n",
            "Ref: [('1996', 'DATE'), ('20th', 'ORDINAL'), ('MLS', 'ORG'), ('this weekend', 'DATE')]\n",
            "Prd: [('$700 million', 'MONEY'), ('10', 'CARDINAL'), ('1996', 'DATE'), ('20', 'CARDINAL'), ('2015', 'DATE'), ('20th season', 'DATE'), ('ESPN', 'ORG'), ('FOX', 'ORG'), ('MLS', 'ORG'), ('Univision', 'ORG'), ('eight years', 'DATE'), ('first', 'ORDINAL')]\n",
            "∩   : [('1996', 'DATE'), ('MLS', 'ORG')]\n",
            "\n",
            "==========================================================================================\n",
            "EXAMPLE 2\n",
            "- SOURCE (first 600 chars) -\n",
            "French striker Bafetimbi Gomis, who has a history of fainting, said he is now \"feeling well\" after collapsing during Swansea's 3-2 loss at Tottenham in the Premier League on Wednesday. The worrying incident occurred in the first half at White Hart Lane -- after Tottenham scored in the seventh minute -- but the 29-year-old left the pitch conscious following about five minutes of treatment. The Guardian added that he was wearing an oxygen mask. Play was temporarily stopped before resuming. As the match progressed, Swansea tweeted that Gomis was \"fine,\" with manager Garry Monk using the same ...\n",
            "\n",
            "- REFERENCE -\n",
            "Bafetimbi Gomis collapses within 10 minutes of kickoff at Tottenham . But he reportedly left the pitch conscious and wearing an oxygen mask . Gomis later said that he was \"feeling well\" The incident came three years after Fabrice Muamba collapsed at White Hart Lane .\n",
            "\n",
            "- PREDICTION -\n",
            "Bafetimbi Gomis 'feeling well' after collapsing during Swansea's 3-2 loss at Tottenham. The 29-year-old left the pitch conscious following about five minutes of treatment. The match was temporarily stopped before resuming. The striker has a history of fainting.\n",
            "\n",
            "- POINTER (first 40 tokens) -\n",
            "00  B                p_copy=0.001 \n",
            "01  af               p_copy=0.015 \n",
            "02  et               p_copy=0.042 \n",
            "03  im               p_copy=0.028 \n",
            "04  bi               p_copy=0.046 \n",
            "05  ĠG               p_copy=0.053 \n",
            "06  om               p_copy=0.445 \n",
            "07  is               p_copy=0.069 \n",
            "08  Ġ'               p_copy=0.051 \n",
            "09  fe               p_copy=0.354 \n",
            "10  eling            p_copy=0.083 \n",
            "11  Ġwell            p_copy=0.059 \n",
            "12  '                p_copy=0.397 \n",
            "13  Ġafter           p_copy=0.061 \n",
            "14  Ġcollapsing      p_copy=0.254 \n",
            "15  Ġduring          p_copy=0.530 COPY\n",
            "16  ĠSwansea         p_copy=0.139 \n",
            "17  's               p_copy=0.550 COPY\n",
            "18  Ġ3               p_copy=0.123 \n",
            "19  -                p_copy=0.496 \n",
            "20  2                p_copy=0.024 \n",
            "21  Ġloss            p_copy=0.012 \n",
            "22  Ġat              p_copy=0.178 \n",
            "23  ĠTottenham       p_copy=0.100 \n",
            "24  Ġ.               p_copy=0.573 COPY\n",
            "25  ĠThe             p_copy=0.054 \n",
            "26  Ġ29              p_copy=0.545 \n",
            "27  -                p_copy=0.806 COPY\n",
            "28  year             p_copy=0.020 \n",
            "29  -                p_copy=0.013 \n",
            "30  old              p_copy=0.010 \n",
            "31  Ġleft            p_copy=0.003 \n",
            "32  Ġthe             p_copy=0.546 COPY\n",
            "33  Ġpitch           p_copy=0.377 \n",
            "34  Ġconscious       p_copy=0.336 \n",
            "35  Ġfollowing       p_copy=0.368 \n",
            "36  Ġabout           p_copy=0.226 \n",
            "37  Ġfive            p_copy=0.589 COPY\n",
            "38  Ġminutes         p_copy=0.424 \n",
            "39  Ġof              p_copy=0.062 \n",
            "\n",
            "Pointer summary: mean p_copy=0.233 | fraction tokens tagged COPY=0.148\n",
            "\n",
            "- ENTITIES -\n",
            "Ref: [('10 minutes', 'TIME'), ('Bafetimbi Gomis', 'PERSON'), ('Fabrice Muamba', 'PERSON'), ('Tottenham', 'GPE'), ('White Hart Lane', 'FAC'), ('three years', 'DATE')]\n",
            "Prd: [('29-year-old', 'DATE'), ('3', 'CARDINAL'), (\"Bafetimbi Gomis '\", 'PERSON'), ('Swansea', 'ORG'), ('Tottenham', 'GPE'), ('about five minutes', 'TIME')]\n",
            "∩   : [('Tottenham', 'GPE')]\n"
          ]
        }
      ],
      "source": [
        "# ===== Show qualitative examples with pointer usage =====\n",
        "import torch, textwrap, numpy as np\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "assert 'model' in globals() and 'tok' in globals() and 'val_df' in globals(), \"Need model, tok, val_df in memory\"\n",
        "DEVICE = next(model.parameters()).device\n",
        "MAX_SRC_LEN = int(globals().get('MAX_SRC_LEN', 400))\n",
        "\n",
        "# --- knobs ---\n",
        "K        = 3                # how many examples to show\n",
        "IDX      = None             # or e.g. [5, 42, 123]; if None, picks first K\n",
        "MIN_NEW  = None             # e.g. 38 to match ref length; if None, uses your current default\n",
        "COPY_THRESH = 0.5           # p_copy threshold to tag a token as 'copied'\n",
        "\n",
        "# --- generation config (no min_length) ---\n",
        "gen_cfg = GenerationConfig(\n",
        "    num_beams=5,\n",
        "    min_new_tokens=MIN_NEW if MIN_NEW is not None else 55,\n",
        "    max_new_tokens=100,\n",
        "    no_repeat_ngram_size=3,\n",
        "    length_penalty=2.0,\n",
        "    early_stopping=True,\n",
        ")\n",
        "\n",
        "# --- helper: pointer analysis mirroring your forward ---\n",
        "@torch.no_grad()\n",
        "def analyze_pointer_usage(model, tok, src_texts, gen_ids, max_src_len, eps=1e-8):\n",
        "    enc = tok(src_texts, return_tensors=\"pt\", truncation=True, max_length=max_src_len, padding=True).to(DEVICE)\n",
        "    out = model.base(\n",
        "        input_ids=enc[\"input_ids\"],\n",
        "        attention_mask=enc[\"attention_mask\"],\n",
        "        decoder_input_ids=gen_ids,\n",
        "        output_attentions=True,\n",
        "        output_hidden_states=True,\n",
        "        use_cache=False,\n",
        "    )\n",
        "    dec_hid = out.decoder_hidden_states[-1]                 # [B,T,H]\n",
        "    enc_out = out.encoder_last_hidden_state                 # [B,S,H]\n",
        "    cross   = out.cross_attentions[-1].mean(dim=1)          # [B,T,S]\n",
        "    if enc[\"attention_mask\"] is not None:\n",
        "        cross = cross.masked_fill((enc[\"attention_mask\"]==0).unsqueeze(1), 0.0)\n",
        "    denom = cross.sum(dim=-1, keepdim=True).clamp_min(eps)\n",
        "    attn  = cross / denom                                   # [B,T,S]\n",
        "\n",
        "    dec_emb = model.base.get_input_embeddings()(gen_ids)    # [B,T,H]\n",
        "    p_gen   = torch.sigmoid(model.p_gen_linear(torch.cat([dec_hid, torch.bmm(attn, enc_out), dec_emb], dim=-1))).squeeze(-1)\n",
        "    p_copy  = 1.0 - p_gen                                   # [B,T]\n",
        "    return p_copy, attn, enc\n",
        "\n",
        "# --- pick examples ---\n",
        "if IDX is None:\n",
        "    rows = val_df.iloc[:K]\n",
        "else:\n",
        "    rows = val_df.iloc[IDX]\n",
        "srcs = rows[\"article\"].astype(str).tolist()\n",
        "refs = rows[\"highlights\"].astype(str).tolist()\n",
        "\n",
        "# --- generate summaries ---\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    ins  = tok(srcs, return_tensors=\"pt\", truncation=True, max_length=MAX_SRC_LEN, padding=True).to(DEVICE)\n",
        "    outs = model.base.generate(**ins, generation_config=gen_cfg)\n",
        "\n",
        "preds = tok.batch_decode(outs, skip_special_tokens=True)\n",
        "\n",
        "# --- pointer analysis on the generated sequences ---\n",
        "p_copy_seq, attn_seq, enc_pack = analyze_pointer_usage(model, tok, srcs, outs, MAX_SRC_LEN)\n",
        "src_ids = enc_pack[\"input_ids\"]\n",
        "\n",
        "# --- optional: entity overlap (skip if spacy not installed) ---\n",
        "def _ents_or_none(texts):\n",
        "    try:\n",
        "        import spacy\n",
        "        try:\n",
        "            nlp = spacy.load(\"en_core_web_sm\")\n",
        "        except Exception:\n",
        "            nlp = None\n",
        "    except Exception:\n",
        "        nlp = None\n",
        "    if nlp is None:\n",
        "        return [set() for _ in texts], False\n",
        "    E = []\n",
        "    for t in texts:\n",
        "        doc = nlp(t)\n",
        "        E.append({(e.text.strip(), e.label_) for e in doc.ents})\n",
        "    return E, True\n",
        "\n",
        "E_ref, has_ner = _ents_or_none(refs)\n",
        "E_pred, _      = _ents_or_none(preds) if has_ner else ( [set() for _ in preds], False )\n",
        "\n",
        "# --- pretty print ---\n",
        "for i, (src, ref, hyp) in enumerate(zip(srcs, refs, preds)):\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(f\"EXAMPLE {i}\")\n",
        "    print(\"- SOURCE (first 600 chars) -\")\n",
        "    print(textwrap.shorten(src.replace(\"\\n\",\" \"), width=600, placeholder=\" ...\"))\n",
        "    print(\"\\n- REFERENCE -\")\n",
        "    print(ref)\n",
        "    print(\"\\n- PREDICTION -\")\n",
        "    print(hyp)\n",
        "\n",
        "    # token-level pointer view (first ~40 tokens)\n",
        "    toks = tok.convert_ids_to_tokens(outs[i].tolist(), skip_special_tokens=True)\n",
        "    pcs  = p_copy_seq[i].tolist()[:len(toks)]\n",
        "    # mark tokens likely copied (p_copy > thresh AND id appears in source ids)\n",
        "    src_vocab = set(src_ids[i].tolist())\n",
        "    marks = []\n",
        "    copied_flags = []\n",
        "    for tid, (tk, pc) in enumerate(zip(tok.convert_ids_to_ids(toks) if hasattr(tok, \"convert_ids_to_ids\") else outs[i].tolist(), pcs)):\n",
        "        is_copied = (tk in src_vocab) and (pc > COPY_THRESH)\n",
        "        copied_flags.append(is_copied)\n",
        "    # print a compact table for first 40 tokens\n",
        "    print(\"\\n- POINTER (first 40 tokens) -\")\n",
        "    for t, (tk, pc, is_c) in enumerate(zip(toks[:40], pcs[:40], copied_flags[:40])):\n",
        "        tag = \"COPY\" if is_c else \"\"\n",
        "        print(f\"{t:02d}  {tk:<15}  p_copy={pc:0.3f} {tag}\")\n",
        "\n",
        "    pc_mean = float(np.mean(pcs)) if len(pcs)>0 else 0.0\n",
        "    frac_copied = float(np.mean(copied_flags)) if len(copied_flags)>0 else 0.0\n",
        "    print(f\"\\nPointer summary: mean p_copy={pc_mean:.3f} | fraction tokens tagged COPY={frac_copied:.3f}\")\n",
        "\n",
        "    if has_ner:\n",
        "        inter = E_ref[i] & E_pred[i]\n",
        "        print(\"\\n- ENTITIES -\")\n",
        "        print(\"Ref:\", sorted(list(E_ref[i]))[:15])\n",
        "        print(\"Prd:\", sorted(list(E_pred[i]))[:15])\n",
        "        print(\"∩   :\", sorted(list(inter))[:15])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekSeXuw-uNqO"
      },
      "source": [
        "# Training for 2k - 3k steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9lUZQiUEHRc"
      },
      "source": [
        "\n",
        "\n",
        "> **Resuming training from step 2000: unfreezes the encoder (for better grounding), keeps the pointer/coverage as-is, and runs +1000 steps with a fresh schedule at LR = 1e-5, warmup 3%. It does not depend on any optimizer state saved earlier.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKOzSdPNGSDF",
        "outputId": "c67c9d8f-b473-4426-b1cd-441458c7222d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Default generation config set:\n",
            " GenerationConfig {\n",
            "  \"early_stopping\": true,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_new_tokens\": 100,\n",
            "  \"min_new_tokens\": 38,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 5\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ===== Set default decoding config (applies to all future .generate calls) =====\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "gen_cfg = GenerationConfig(\n",
        "    num_beams=5,\n",
        "    min_new_tokens=38,   # <-- use 35/38/40 based on the length stats A/B\n",
        "    max_new_tokens=100,\n",
        "    no_repeat_ngram_size=3,\n",
        "    length_penalty=2.0,\n",
        "    early_stopping=True,\n",
        ")\n",
        "\n",
        "model.base.generation_config = gen_cfg\n",
        "print(\"Default generation config set:\\n\", model.base.generation_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gygDIUGkDl0N",
        "outputId": "83f13767-9063-4e19-9c76-2963673e7d42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[resume] starting at step 2000 with batch=56 x accum=2 (eff=112)\n",
            "[resume] will train until step 3000 (top-up 1000 updates), warmup=30, lr=1e-05\n",
            "[ep ?] step   2100 | loss  2.8558 | ce  2.3520 | cov  0.5037 | p_copy 0.205 | p_gen 0.795 | lr 9.28e-06 | toks/s   9568.3 | gpu_mem 17328.5 MB\n",
            "[ep ?] step   2200 | loss  2.8461 | ce  2.3566 | cov  0.4895 | p_copy 0.210 | p_gen 0.790 | lr 8.25e-06 | toks/s   9668.5 | gpu_mem 17328.5 MB\n",
            "[ep ?] step   2300 | loss  2.8074 | ce  2.3434 | cov  0.4640 | p_copy 0.218 | p_gen 0.782 | lr 7.22e-06 | toks/s   9773.1 | gpu_mem 17328.5 MB\n",
            "[ep ?] step   2400 | loss  2.7712 | ce  2.3248 | cov  0.4464 | p_copy 0.223 | p_gen 0.777 | lr 6.19e-06 | toks/s   9782.6 | gpu_mem 17328.5 MB\n",
            "[ep ?] step   2500 | loss  2.7644 | ce  2.3295 | cov  0.4349 | p_copy 0.227 | p_gen 0.773 | lr 5.15e-06 | toks/s   9790.0 | gpu_mem 17328.5 MB\n",
            "[ep ?] step   2600 | loss  2.7489 | ce  2.3248 | cov  0.4241 | p_copy 0.231 | p_gen 0.769 | lr 4.12e-06 | toks/s   9793.6 | gpu_mem 17328.5 MB\n",
            "[ep ?] step   2700 | loss  2.7331 | ce  2.3170 | cov  0.4161 | p_copy 0.235 | p_gen 0.765 | lr 3.09e-06 | toks/s   9795.2 | gpu_mem 17328.5 MB\n"
          ]
        }
      ],
      "source": [
        "# ===== Resume training for +1000 steps from ckpt_step2000 =====\n",
        "import os, math, time, torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import amp\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "\n",
        "# --- paths & resume point ---\n",
        "RESUME_CKPT = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step2000\"\n",
        "RESUME_STEP = 2000\n",
        "TARGET_STEPS = 3000\n",
        "\n",
        "# --- training knobs  ---\n",
        "BATCH_SIZE         = 56\n",
        "GRAD_ACCUM_STEPS   = 2\n",
        "NUM_WORKERS        = 4\n",
        "PERSISTENT_WORKERS = True\n",
        "PREFETCH_FACTOR    = 2\n",
        "FP16               = True\n",
        "LR                 = 1e-5\n",
        "WARMUP_RATIO       = 0.03\n",
        "WEIGHT_DECAY       = 0.01\n",
        "LOG_EVERY          = 100\n",
        "SAVE_EVERY_STEPS   = 1000\n",
        "\n",
        "# --- pointer/coverage (keep same; repetition already low) ---\n",
        "LAMBDA_COV = globals().get(\"LAMBDA_COV\", 1.0)\n",
        "EPS        = globals().get(\"EPS\", 1e-8)\n",
        "USE_POINTER= globals().get(\"USE_POINTER\", True)\n",
        "\n",
        "# --- lengths (your standard 400/100) ---\n",
        "MAX_SRC_LEN = int(globals().get(\"MAX_SRC_LEN\", 400))\n",
        "MAX_TGT_LEN = int(globals().get(\"MAX_TGT_LEN\", 100))\n",
        "\n",
        "# --- data paths---\n",
        "TRAIN_CSV = globals().get(\"TRAIN_CSV\", \"/content/drive/MyDrive/cleaned_outputs/naa/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast_applied.csv\")\n",
        "VAL_CSV   = globals().get(\"VAL_CSV\",   \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---- reload tokenizer + base from checkpoint (force eager attention for cross_attentions) ----\n",
        "tok = AutoTokenizer.from_pretrained(RESUME_CKPT, use_fast=True)\n",
        "cfg = AutoConfig.from_pretrained(RESUME_CKPT)\n",
        "cfg.attn_implementation = \"eager\"\n",
        "base = AutoModelForSeq2SeqLM.from_pretrained(RESUME_CKPT, config=cfg)\n",
        "\n",
        "# IMPORTANT for gradient checkpointing + reliable training\n",
        "base.config.use_cache = False\n",
        "base.gradient_checkpointing_enable()\n",
        "\n",
        "\n",
        "\n",
        "model = CopyAwareBart(base, tok, lambda_cov=LAMBDA_COV, eps=EPS, use_pointer=USE_POINTER).to(DEVICE)\n",
        "\n",
        "ptr_path = os.path.join(RESUME_CKPT, \"pointer_head.pt\")\n",
        "if os.path.exists(ptr_path):\n",
        "    state = torch.load(ptr_path, map_location=\"cpu\")\n",
        "    model.p_gen_linear.load_state_dict(state[\"p_gen_linear\"])\n",
        "    model.lambda_cov = float(state.get(\"lambda_cov\", model.lambda_cov))\n",
        "    model.use_ptr    = bool(state.get(\"use_pointer\", model.use_ptr))\n",
        "else:\n",
        "    print(\"[warn] pointer_head.pt not found — continuing with current pointer params\")\n",
        "\n",
        "# ----  encoder is UNFROZEN for this phase ----\n",
        "for p in model.base.model.encoder.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "# ---- data ----\n",
        "\n",
        "train_df = robust_read_csv(TRAIN_CSV)\n",
        "val_df   = robust_read_csv(VAL_CSV)\n",
        "collate  = PGDataCollator(tok=tok, max_src_len=MAX_SRC_LEN, max_tgt_len=MAX_TGT_LEN)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    CNNDMDataset(train_df),\n",
        "    batch_size=BATCH_SIZE, shuffle=True,\n",
        "    num_workers=NUM_WORKERS, pin_memory=True,\n",
        "    persistent_workers=PERSISTENT_WORKERS, prefetch_factor=PREFETCH_FACTOR,\n",
        "    collate_fn=collate,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    CNNDMDataset(val_df),\n",
        "    batch_size=BATCH_SIZE, shuffle=False,\n",
        "    num_workers=NUM_WORKERS, pin_memory=True,\n",
        "    persistent_workers=PERSISTENT_WORKERS, prefetch_factor=PREFETCH_FACTOR,\n",
        "    collate_fn=collate,\n",
        ")\n",
        "\n",
        "# ---- optimizer (fresh) ----\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\"]\n",
        "params_decay, params_nodecay = [], []\n",
        "for n, p in model.named_parameters():\n",
        "    if not p.requires_grad:\n",
        "        continue\n",
        "    (params_nodecay if any(nd in n for nd in no_decay) else params_decay).append(p)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [\n",
        "        {\"params\": params_decay,   \"weight_decay\": WEIGHT_DECAY, \"lr\": LR, \"initial_lr\": LR},\n",
        "        {\"params\": params_nodecay, \"weight_decay\": 0.0,          \"lr\": LR, \"initial_lr\": LR},\n",
        "    ],\n",
        "    lr=LR,\n",
        ")\n",
        "\n",
        "# ---- scheduler for the TOP-UP only (fresh warmup) ----\n",
        "steps_remaining = max(1, TARGET_STEPS - RESUME_STEP)\n",
        "warmup_steps    = max(1, int(WARMUP_RATIO * steps_remaining))\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=steps_remaining,\n",
        ")\n",
        "\n",
        "scaler = amp.GradScaler(\"cuda\", enabled=FP16)\n",
        "\n",
        "# ---- tiny running mean helper (no external deps) ----\n",
        "class _RM:\n",
        "    def __init__(self, w=200): self.w=w; self.buf=[]\n",
        "    def add(self, x):\n",
        "        if x is None: return\n",
        "        self.buf.append(float(x))\n",
        "        if len(self.buf)>self.w: self.buf.pop(0)\n",
        "    @property\n",
        "    def mean(self):\n",
        "        return float(sum(self.buf)/len(self.buf)) if self.buf else 0.0\n",
        "\n",
        "m_loss=_RM(); m_ce=_RM(); m_cov=_RM(); m_pcopy=_RM(); m_pgen=_RM(); m_toks=_RM()\n",
        "\n",
        "# ---- checkpoint saver  ----\n",
        "def save_checkpoint(tag):\n",
        "    path = os.path.join(RESUME_CKPT.rsplit(\"/\",1)[0], f\"ckpt_step{tag}\")\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    model.base.save_pretrained(path)\n",
        "    tok.save_pretrained(path)\n",
        "    torch.save({\n",
        "        \"p_gen_linear\": model.p_gen_linear.state_dict(),\n",
        "        \"lambda_cov\": float(model.lambda_cov),\n",
        "        \"use_pointer\": bool(model.use_ptr),\n",
        "    }, os.path.join(path, \"pointer_head.pt\"))\n",
        "    print(f\"[checkpoint] saved → {path}\")\n",
        "\n",
        "# ---- training loop  ----\n",
        "global_step = RESUME_STEP\n",
        "print(f\"[resume] starting at step {global_step} with batch={BATCH_SIZE} x accum={GRAD_ACCUM_STEPS} (eff={BATCH_SIZE*GRAD_ACCUM_STEPS})\")\n",
        "print(f\"[resume] will train until step {TARGET_STEPS} (top-up {steps_remaining} updates), warmup={warmup_steps}, lr={LR:g}\")\n",
        "\n",
        "model.train()\n",
        "optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "start_time = time.time()\n",
        "tokens_seen = 0\n",
        "updates_done = 0\n",
        "\n",
        "for epoch in range(10**9):  # just loop until TARGET_STEPS\n",
        "    for it, batch in enumerate(train_loader):\n",
        "        for k in batch: batch[k] = batch[k].to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            toks = int(batch[\"attention_mask\"].sum().item())\n",
        "            if \"labels\" in batch:\n",
        "                toks += int((batch[\"labels\"] != -100).sum().item())\n",
        "\n",
        "        with amp.autocast(\"cuda\", enabled=FP16):\n",
        "            out = model(**batch)\n",
        "            loss = out.loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        took_step = False\n",
        "        if (it + 1) % GRAD_ACCUM_STEPS == 0:\n",
        "            # clip after unscale (only once per update)\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            scheduler.step()\n",
        "\n",
        "            global_step += 1\n",
        "            updates_done += 1\n",
        "            took_step = True\n",
        "\n",
        "            tokens_seen += toks\n",
        "            m_loss.add(loss.item())\n",
        "            m_ce.add(getattr(model, \"_last_ce_loss\", None))\n",
        "            m_cov.add(getattr(model, \"_last_cov_loss\", None))\n",
        "            m_pcopy.add(getattr(model, \"_last_p_copy_mean\", None))\n",
        "            m_pgen.add(getattr(model, \"_last_p_gen_mean\", None))\n",
        "            m_toks.add(tokens_seen / max(1e-6, time.time()-start_time))\n",
        "\n",
        "            if global_step % LOG_EVERY == 0:\n",
        "                lr_now = scheduler.get_last_lr()[0]\n",
        "                mem_mb = torch.cuda.max_memory_allocated() / (1024**2) if torch.cuda.is_available() else 0.0\n",
        "                print(\n",
        "                    f\"[ep ?] step {global_step:>6} | \"\n",
        "                    f\"loss {m_loss.mean:7.4f} | ce {m_ce.mean:7.4f} | cov {m_cov.mean:7.4f} | \"\n",
        "                    f\"p_copy {m_pcopy.mean:5.3f} | p_gen {m_pgen.mean:5.3f} | \"\n",
        "                    f\"lr {lr_now:.2e} | toks/s {m_toks.mean:8.1f} | gpu_mem {mem_mb:7.1f} MB\"\n",
        "                )\n",
        "\n",
        "            if global_step % SAVE_EVERY_STEPS == 0:\n",
        "                save_checkpoint(global_step)\n",
        "\n",
        "            if updates_done >= steps_remaining or global_step >= TARGET_STEPS:\n",
        "                print(f\"[train] Reached TARGET_STEPS={TARGET_STEPS}. Stopping.\")\n",
        "                save_checkpoint(global_step)\n",
        "                break\n",
        "\n",
        "        if torch.cuda.is_available(): torch.cuda.synchronize()\n",
        "        del out\n",
        "        if took_step: torch.cuda.empty_cache()\n",
        "\n",
        "    if updates_done >= steps_remaining or global_step >= TARGET_STEPS:\n",
        "        break\n",
        "\n",
        "# quick val loss at the end\n",
        "model.eval()\n",
        "val_loss, val_seen = 0.0, 0\n",
        "with torch.no_grad():\n",
        "    for vb in val_loader:\n",
        "        for k in vb: vb[k] = vb[k].to(DEVICE)\n",
        "        with amp.autocast(\"cuda\", enabled=FP16):\n",
        "            vo = model(**vb)\n",
        "            if vo.loss is not None:\n",
        "                val_loss += vo.loss.item(); val_seen += 1\n",
        "if val_seen:\n",
        "    print(f\"[val] loss {val_loss/val_seen:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owKYloLNMdhV"
      },
      "source": [
        "\n",
        "\n",
        "> Quick Evaluation check after 3000 steps\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uaWCHLpLTr8",
        "outputId": "9aeeb2f5-1bfc-4be2-b35b-66b4787e8faa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Tokenizer IDs ===\n",
            "PAD: 1 <pad>\n",
            "BOS: 0 <s>\n",
            "EOS: 2 </s>\n",
            "UNK: 3 <unk>\n",
            "\n",
            "=== Model Config IDs ===\n",
            "decoder_start_token_id: 2\n",
            "bos_token_id: 0\n",
            "pad_token_id: 1\n",
            "eos_token_id: 2\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "STUDENT_DIR = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step3000\"\n",
        "\n",
        "tok   = AutoTokenizer.from_pretrained(STUDENT_DIR)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(STUDENT_DIR)\n",
        "\n",
        "print(\"=== Tokenizer IDs ===\")\n",
        "print(\"PAD:\", tok.pad_token_id, tok.pad_token)\n",
        "print(\"BOS:\", tok.bos_token_id, tok.bos_token)\n",
        "print(\"EOS:\", tok.eos_token_id, tok.eos_token)\n",
        "print(\"UNK:\", tok.unk_token_id, tok.unk_token)\n",
        "\n",
        "print(\"\\n=== Model Config IDs ===\")\n",
        "print(\"decoder_start_token_id:\", model.config.decoder_start_token_id)\n",
        "print(\"bos_token_id:\", model.config.bos_token_id)\n",
        "print(\"pad_token_id:\", model.config.pad_token_id)\n",
        "print(\"eos_token_id:\", model.config.eos_token_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXPz5TemG4Ex",
        "outputId": "b098c283-b93c-456b-fa09-23f790312ecd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "is_encdec: True\n",
            "cfg(model): 2 0 1 2\n",
            "cfg(gen):   2 0 1 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/500 [00:00<?, ?it/s]`sdpa` attention does not support `output_attentions=True` or `head_mask`. Please set your attention to `eager` if you want any of these features.\n",
            "100%|██████████| 500/500 [05:33<00:00,  1.50it/s]\n"
          ]
        }
      ],
      "source": [
        "# --- Eval with explicit token IDs from your checkpoint ---\n",
        "from transformers import GenerationConfig, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch, pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === Load ===\n",
        "STUDENT_DIR = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step3000\"\n",
        "tok   = AutoTokenizer.from_pretrained(STUDENT_DIR)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(STUDENT_DIR)\n",
        "\n",
        "# Use the true HF model that has .generate()\n",
        "base = model.base if hasattr(model, \"base\") else model\n",
        "base = base.to(\"cuda\").eval()\n",
        "\n",
        "# === IDs from the checkpoint/tokenizer ===\n",
        "bos_id = tok.bos_token_id   # 0\n",
        "eos_id = tok.eos_token_id   # 2\n",
        "pad_id = tok.pad_token_id   # 1\n",
        "dec_start_id = base.config.decoder_start_token_id  # 2 (as printed)\n",
        "\n",
        "# Mirror onto model.config\n",
        "base.config.bos_token_id = bos_id\n",
        "base.config.eos_token_id = eos_id\n",
        "base.config.pad_token_id = pad_id\n",
        "base.config.decoder_start_token_id = dec_start_id\n",
        "\n",
        "# === Decoding config (now WITH token IDs) ===\n",
        "gen_cfg = GenerationConfig(\n",
        "    num_beams=5,\n",
        "    min_new_tokens=38,\n",
        "    max_new_tokens=100,\n",
        "    no_repeat_ngram_size=3,\n",
        "    length_penalty=2.0,\n",
        "    early_stopping=True,\n",
        "    bos_token_id=bos_id,\n",
        "    eos_token_id=eos_id,\n",
        "    pad_token_id=pad_id,\n",
        "    decoder_start_token_id=dec_start_id,\n",
        ")\n",
        "\n",
        "\n",
        "base.generation_config = gen_cfg\n",
        "\n",
        "print(\"is_encdec:\", getattr(base.config, \"is_encoder_decoder\", None))\n",
        "print(\"cfg(model):\", base.config.decoder_start_token_id, base.config.bos_token_id, base.config.pad_token_id, base.config.eos_token_id)\n",
        "print(\"cfg(gen):  \", gen_cfg.decoder_start_token_id, gen_cfg.bos_token_id, gen_cfg.pad_token_id, gen_cfg.eos_token_id)\n",
        "\n",
        "# === Data ===\n",
        "VAL_CSV = \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "val_df = pd.read_csv(VAL_CSV)\n",
        "articles = val_df[\"article\"].astype(str).tolist()\n",
        "refs     = val_df[\"highlights\"].astype(str).tolist()\n",
        "\n",
        "# === Generate (pass only allowed kwargs) ===\n",
        "summaries = []\n",
        "for art in tqdm(articles[:500]):\n",
        "    inputs = tok(art, return_tensors=\"pt\", truncation=True, padding=True, max_length=400).to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        out = base.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs.get(\"attention_mask\"),\n",
        "            generation_config=gen_cfg,  # <- has all four IDs\n",
        "        )\n",
        "    summaries.append(tok.decode(out[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jI-1rSVEOkZz",
        "outputId": "92f8ad5e-0e49-4283-d86e-061a2f2ec90e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ROUGE ===\n",
            "{'rouge1': 0.3092, 'rouge2': 0.121, 'rougeL': 0.2177, 'rougeLsum': 0.2177}\n",
            "\n",
            "=== Repetition (3-gram) ===\n",
            "mean: 0.0014 | median: 0.0 | p75: 0.0\n",
            "\n",
            "=== Entity Coverage ===\n",
            "Precision: 0.2364 Recall: 0.3144 F1: 0.2527\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Student PG-Cov evaluation on validation summaries (500 examples)\n",
        "\n",
        "# ============================================================\n",
        "import numpy as np, re\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "preds = summaries           # already generated summaries\n",
        "refs  = refs[:len(preds)]   # gold highlights (aligned)\n",
        "\n",
        "# --- ROUGE (F1) ---\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL','rougeLsum'], use_stemmer=True)\n",
        "agg = {k: 0.0 for k in scorer.score(\"\", \"\").keys()}\n",
        "for p, r in zip(preds, refs):\n",
        "    s = scorer.score(r, p)\n",
        "    for k in agg: agg[k] += s[k].fmeasure\n",
        "for k in agg: agg[k] /= max(1, len(preds))\n",
        "print(\"=== ROUGE ===\")\n",
        "print({k: round(v, 4) for k,v in agg.items()})\n",
        "\n",
        "# --- Repetition: tri-gram repeat ratio ---\n",
        "def trigram_repeat_ratio(text):\n",
        "    toks = re.findall(r\"\\w+|\\S\", text)\n",
        "    if len(toks) < 3: return 0.0\n",
        "    trigs = [tuple(toks[i:i+3]) for i in range(len(toks)-2)]\n",
        "    return 1.0 - (len(set(trigs)) / len(trigs))\n",
        "\n",
        "tri_rep = [trigram_repeat_ratio(p) for p in preds]\n",
        "print(\"\\n=== Repetition (3-gram) ===\")\n",
        "print(\"mean:\", round(float(np.mean(tri_rep)), 4),\n",
        "      \"| median:\", round(float(np.median(tri_rep)), 4),\n",
        "      \"| p75:\", round(float(np.percentile(tri_rep, 75)), 4))\n",
        "\n",
        "# --- Entity coverage using spaCy NER ---\n",
        "import spacy\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    !python -m spacy download en_core_web_sm\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def ents(text): return {(e.text.strip(), e.label_) for e in nlp(text).ents}\n",
        "\n",
        "precisions, recalls, f1s = [], [], []\n",
        "for p, r in zip(preds, refs):\n",
        "    E_r, E_p = ents(r), ents(p)\n",
        "    if not E_r and not E_p: continue\n",
        "    tp   = len(E_r & E_p)\n",
        "    prec = tp / max(1, len(E_p))\n",
        "    rec  = tp / max(1, len(E_r))\n",
        "    f1   = 0.0 if (prec+rec)==0 else 2*prec*rec/(prec+rec)\n",
        "    precisions.append(prec); recalls.append(rec); f1s.append(f1)\n",
        "\n",
        "if precisions:\n",
        "    print(\"\\n=== Entity Coverage ===\")\n",
        "    print(\"Precision:\", round(np.mean(precisions),4),\n",
        "          \"Recall:\", round(np.mean(recalls),4),\n",
        "          \"F1:\", round(np.mean(f1s),4))\n",
        "else:\n",
        "    print(\"\\n=== Entity Coverage ===\\n(no entities found in this slice)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739,
          "referenced_widgets": [
            "bec8ad3d404841f4a067edee6a6d000c",
            "fb503770b05346debc8e09913f5f5b8a",
            "d85079c22b854cb28742350ac2d69cf1",
            "f678fcf8fe144ad8acd846a372f00533",
            "32e471eabfda47fd9c27e4ffdb832f7c",
            "5ec36d3ba3b645559a7639b2f24e1f2a",
            "f802b86551e84bfbbe9bb69681805b4a",
            "269c13807d554908953660091455ad15"
          ]
        },
        "id": "BKDBmR1zJPNV",
        "outputId": "b22fd5cc-b292-4045-fa9f-e08c2ce6fc86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ROUGE ===\n",
            "{'rouge1': 0.3092, 'rouge2': 0.121, 'rougeL': 0.2177, 'rougeLsum': 0.2177}\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bec8ad3d404841f4a067edee6a6d000c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb503770b05346debc8e09913f5f5b8a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d85079c22b854cb28742350ac2d69cf1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f678fcf8fe144ad8acd846a372f00533",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "32e471eabfda47fd9c27e4ffdb832f7c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ec36d3ba3b645559a7639b2f24e1f2a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "calculating scores...\n",
            "computing bert embedding.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f802b86551e84bfbbe9bb69681805b4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "computing greedy matching.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "269c13807d554908953660091455ad15",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done in 2.87 seconds, 174.50 sentences/sec\n",
            "\n",
            "=== BERTScore ===\n",
            "Precision: 0.8595 Recall: 0.8703 F1: 0.8648\n",
            "\n",
            "=== Repetition (3-gram) ===\n",
            "mean: 0.0014 | median: 0.0 | p75: 0.0\n",
            "\n",
            "=== Entity Coverage ===\n",
            "Precision: 0.2364 Recall: 0.3144 F1: 0.2527\n",
            "\n",
            "=== Copy vs Novelty ===\n",
            "Copy rate mean=0.9494 | Novelty mean=0.0506\n",
            "\n",
            "=== Length stats (words) ===\n",
            "Ref mean: 34.0 Ref median: 33.0\n",
            "Pred mean: 51.7 Pred median: 49.0\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Student PG-Cov evaluation on validation summaries (500 ex.)\n",
        "\n",
        "# ============================================================\n",
        "import numpy as np, re\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Align references\n",
        "preds = summaries\n",
        "refs  = refs[:len(preds)]\n",
        "\n",
        "# --- ROUGE (F1) ---\n",
        "scorer = rouge_scorer.RougeScorer(\n",
        "    ['rouge1','rouge2','rougeL','rougeLsum'], use_stemmer=True\n",
        ")\n",
        "agg = {k: 0.0 for k in scorer.score(\"\", \"\").keys()}\n",
        "for p, r in zip(preds, refs):\n",
        "    s = scorer.score(r, p)\n",
        "    for k in agg: agg[k] += s[k].fmeasure\n",
        "for k in agg: agg[k] /= max(1, len(preds))\n",
        "print(\"=== ROUGE ===\")\n",
        "print({k: round(v,4) for k,v in agg.items()})\n",
        "\n",
        "# --- BERTScore ---\n",
        "!pip install -q bert-score\n",
        "from bert_score import score as bert_score\n",
        "P, R, F1 = bert_score(preds, refs, lang=\"en\", verbose=True)\n",
        "print(\"\\n=== BERTScore ===\")\n",
        "print(\"Precision:\", round(float(P.mean()), 4),\n",
        "      \"Recall:\", round(float(R.mean()), 4),\n",
        "      \"F1:\", round(float(F1.mean()), 4))\n",
        "\n",
        "# --- Repetition: tri-gram repeat ratio ---\n",
        "def trigram_repeat_ratio(text):\n",
        "    toks = re.findall(r\"\\w+|\\S\", text)\n",
        "    if len(toks) < 3: return 0.0\n",
        "    trigs = [tuple(toks[i:i+3]) for i in range(len(toks)-2)]\n",
        "    return 1.0 - (len(set(trigs)) / len(trigs))\n",
        "\n",
        "tri_rep = [trigram_repeat_ratio(p) for p in preds]\n",
        "print(\"\\n=== Repetition (3-gram) ===\")\n",
        "print(\"mean:\", round(float(np.mean(tri_rep)), 4),\n",
        "      \"| median:\", round(float(np.median(tri_rep)), 4),\n",
        "      \"| p75:\", round(float(np.percentile(tri_rep, 75)), 4))\n",
        "\n",
        "# --- Entity coverage (spaCy NER) ---\n",
        "import spacy\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    !python -m spacy download en_core_web_sm\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def ents(text): return {(e.text.strip(), e.label_) for e in nlp(text).ents}\n",
        "\n",
        "precisions, recalls, f1s = [], [], []\n",
        "for p, r in zip(preds, refs):\n",
        "    E_r, E_p = ents(r), ents(p)\n",
        "    if not E_r and not E_p: continue\n",
        "    tp   = len(E_r & E_p)\n",
        "    prec = tp / max(1, len(E_p))\n",
        "    rec  = tp / max(1, len(E_r))\n",
        "    f1   = 0.0 if (prec+rec)==0 else 2*prec*rec/(prec+rec)\n",
        "    precisions.append(prec); recalls.append(rec); f1s.append(f1)\n",
        "\n",
        "if precisions:\n",
        "    print(\"\\n=== Entity Coverage ===\")\n",
        "    print(\"Precision:\", round(np.mean(precisions),4),\n",
        "          \"Recall:\", round(np.mean(recalls),4),\n",
        "          \"F1:\", round(np.mean(f1s),4))\n",
        "\n",
        "# --- Copy vs Novelty (word-level overlap) ---\n",
        "def copy_novel(src, pred):\n",
        "    s=set(src.lower().split()); ps=pred.lower().split()\n",
        "    if not ps: return 0.0, 0.0\n",
        "    copied=sum(1 for w in ps if w in s)\n",
        "    return copied/len(ps), 1-(copied/len(ps))\n",
        "\n",
        "copy_rates = [copy_novel(s,p)[0] for s,p in zip(articles[:len(preds)], preds)]\n",
        "print(\"\\n=== Copy vs Novelty ===\")\n",
        "print(f\"Copy rate mean={np.mean(copy_rates):.4f} | Novelty mean={1-np.mean(copy_rates):.4f}\")\n",
        "\n",
        "# --- Length stats ---\n",
        "def wlen(s): return len(str(s).split())\n",
        "pred_w = [wlen(p) for p in preds]\n",
        "ref_w  = [wlen(r) for r in refs]\n",
        "print(\"\\n=== Length stats (words) ===\")\n",
        "print(\"Ref mean:\", round(np.mean(ref_w),1), \"Ref median:\", np.median(ref_w))\n",
        "print(\"Pred mean:\", round(np.mean(pred_w),1), \"Pred median:\", np.median(pred_w))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJuo3bQpevPd",
        "outputId": "f171027c-6ac5-4168-a910-113c85b2a266"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[AMP] device=cuda:0 dtype=torch.bfloat16 bf16_supported=True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Choose fastest safe dtype for your GPU\n",
        "AMP_DTYPE = torch.bfloat16 if (DEVICE.type == \"cuda\" and torch.cuda.is_bf16_supported()) else torch.float16\n",
        "\n",
        "# Move model to GPU in low precision and enable KV cache\n",
        "model.eval().to(DEVICE, dtype=AMP_DTYPE)\n",
        "if hasattr(model, \"config\"): model.config.use_cache = True\n",
        "\n",
        "# Let TF32 speed up FP32 matmuls on Ampere+ (has no effect on fp16/bf16)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "# Quick sanity print: check that weights are actually low-precision on GPU\n",
        "p = next(model.parameters())\n",
        "print(f\"[AMP] device={p.device} dtype={p.dtype} bf16_supported={torch.cuda.is_bf16_supported()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1hjcM-efGxY",
        "outputId": "77e6f37b-cd55-4541-f5d3-d2330a551a2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[gen] device=cuda:0 dtype=torch.bfloat16 gpu=NVIDIA L4\n",
            "[gen] 1000 summaries in 198.2s (~5.0/s)  batch=6 beams=5\n",
            "[cuda] max_alloc=1.95 GB | max_reserved=2.22 GB\n",
            "[Phase A] preds_phaseA: 1000 summaries ready.\n"
          ]
        }
      ],
      "source": [
        "# ============== CELL B — FAST, BATCHED GENERATION (GPU-FORCED + ASSERTS) ==============\n",
        "import time, contextlib, torch\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "assert 'model' in globals() and 'tok' in globals() and 'val_df' in globals(), \"Need model, tok, val_df.\"\n",
        "\n",
        "# ---- force CUDA + AMP dtype ----\n",
        "assert torch.cuda.is_available(), \"CUDA not available. Check your runtime.\"\n",
        "DEVICE    = torch.device(\"cuda:0\")\n",
        "AMP_DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "# Use the SAME object for generation; do NOT swap to .base/.model (may be on CPU)\n",
        "GENERATOR = model\n",
        "GENERATOR.to(DEVICE, dtype=AMP_DTYPE).eval()\n",
        "\n",
        "# sanity: confirm params live on GPU and low-precision\n",
        "p = next(GENERATOR.parameters())\n",
        "print(f\"[gen] device={p.device} dtype={p.dtype} gpu={torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# autocast context for matmuls\n",
        "amp_ctx = lambda: torch.autocast(device_type=\"cuda\", dtype=AMP_DTYPE)\n",
        "\n",
        "# ---- knobs ----\n",
        "MAX_SRC_LEN    = int(globals().get('MAX_SRC_LEN', 400))\n",
        "N_EVAL         = int(globals().get('N_EVAL', 1000))\n",
        "BATCH_GEN      = int(globals().get('BATCH_GEN', 6))\n",
        "NUM_BEAMS      = int(globals().get('NUM_BEAMS', 5))\n",
        "MIN_NEW        = int(globals().get('MIN_NEW', 34))\n",
        "MAX_NEW        = int(globals().get('MAX_NEW', 85))\n",
        "NO_REPEAT      = int(globals().get('NO_REPEAT', 4))\n",
        "LENGTH_PENALTY = float(globals().get('LENGTH_PENALTY', 2.2))\n",
        "\n",
        "if hasattr(GENERATOR, \"config\"):\n",
        "    GENERATOR.config.use_cache = True\n",
        "\n",
        "gen_cfg = GenerationConfig(\n",
        "    num_beams=NUM_BEAMS,\n",
        "    min_new_tokens=MIN_NEW,\n",
        "    max_new_tokens=MAX_NEW,\n",
        "    no_repeat_ngram_size=NO_REPEAT,\n",
        "    length_penalty=LENGTH_PENALTY,\n",
        "    early_stopping=True,\n",
        ")\n",
        "\n",
        "# ---- slice data ----\n",
        "_rows = val_df.iloc[:N_EVAL]\n",
        "srcs_phaseA = _rows[\"article\"].astype(str).tolist()\n",
        "refs_phaseA = _rows[\"highlights\"].astype(str).tolist()\n",
        "\n",
        "# ---- fast batched generation ----\n",
        "def generate_batched(srcs):\n",
        "    preds, t0 = [], time.time()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    for i in range(0, len(srcs), BATCH_GEN):\n",
        "        batch = srcs[i:i+BATCH_GEN]\n",
        "        with torch.inference_mode():\n",
        "            ins = tok(\n",
        "                batch,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=MAX_SRC_LEN,\n",
        "                padding=True,\n",
        "                pad_to_multiple_of=16,\n",
        "            ).to(DEVICE)\n",
        "\n",
        "            # assert tensors are on GPU\n",
        "            assert ins[\"input_ids\"].is_cuda, \"inputs not on CUDA\"\n",
        "            with amp_ctx():\n",
        "                outs = GENERATOR.generate(**ins, generation_config=gen_cfg)\n",
        "\n",
        "            preds.extend(tok.batch_decode(outs, skip_special_tokens=True))\n",
        "            del ins, outs\n",
        "\n",
        "    dt = time.time() - t0\n",
        "    used = torch.cuda.max_memory_allocated() / (1024**3)\n",
        "    resv = torch.cuda.max_memory_reserved() / (1024**3)\n",
        "    print(f\"[gen] {len(srcs)} summaries in {dt:.1f}s (~{len(srcs)/max(dt,1):.1f}/s)  batch={BATCH_GEN} beams={NUM_BEAMS}\")\n",
        "    print(f\"[cuda] max_alloc={used:.2f} GB | max_reserved={resv:.2f} GB\")\n",
        "    return preds\n",
        "\n",
        "preds_phaseA = generate_batched(srcs_phaseA)\n",
        "print(f\"[Phase A] preds_phaseA: {len(preds_phaseA)} summaries ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqTUeyAFZbVK",
        "outputId": "6b46ec88-1a29-412e-b3f4-d024934efe33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Decoding only] length_ratio=1.62 | entities={'precision': 0.24902495364113011, 'recall': 0.3334033605283605, 'f1': 0.2657345883458759}\n"
          ]
        }
      ],
      "source": [
        "# ===============================EVALUATION ===============================\n",
        "import numpy as np\n",
        "\n",
        "assert 'srcs_phaseA' in globals() and 'refs_phaseA' in globals() and 'preds_phaseA_base' in globals(), \"Run the generation cell first.\"\n",
        "\n",
        "def length_ratio(preds, refs):\n",
        "    return float(np.mean([len(p.split())/max(1,len(r.split())) for p, r in zip(preds, refs)]))\n",
        "\n",
        "\n",
        "def eval_entities(preds, refs):\n",
        "    try:\n",
        "        import spacy\n",
        "        try:\n",
        "            nlp = spacy.load(\"en_core_web_sm\")\n",
        "        except Exception:\n",
        "            return {}\n",
        "    except Exception:\n",
        "        return {}\n",
        "    P, R, F = [], [], []\n",
        "    for pr, rf in zip(preds, refs):\n",
        "        Es = { (e.text.strip(), e.label_) for e in nlp(rf).ents }\n",
        "        Eh = { (e.text.strip(), e.label_) for e in nlp(pr).ents }\n",
        "        if not Eh:\n",
        "            P.append(0.0); R.append(0.0); F.append(0.0); continue\n",
        "        inter = Es & Eh\n",
        "        prec  = len(inter)/len(Eh) if Eh else 0.0\n",
        "        reca  = len(inter)/len(Es) if Es else 0.0\n",
        "        f1    = 2*prec*reca/(prec+reca) if (prec+reca)>0 else 0.0\n",
        "        P.append(prec); R.append(reca); F.append(f1)\n",
        "    return dict(precision=float(np.mean(P)), recall=float(np.mean(R)), f1=float(np.mean(F)))\n",
        "\n",
        "def report(name, preds, refs):\n",
        "    r = eval_rouge(preds, refs)\n",
        "    lr = length_ratio(preds, refs)\n",
        "    ent = eval_entities(preds, refs)\n",
        "    print(f\"[{name}] length_ratio={lr:.2f} | entities={ent}\")\n",
        "\n",
        "report(\"Decoding only\", preds_phaseA_base, refs_phaseA)\n",
        "if 'preds_phaseA_cb' in globals() and preds_phaseA_cb is not None:\n",
        "    report(\"Decoding + CopyBias\", preds_phaseA_cb, refs_phaseA)\n",
        "# ============================================================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "3976088fb2644f1d963330292d5ec3e0"
          ]
        },
        "id": "VV1W2IlXijFB",
        "outputId": "c589318b-a671-4393-f190-79947d492b29"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3976088fb2644f1d963330292d5ec3e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len_ratio=1.63\n",
            "{'rouge1': np.float64(0.3113), 'rouge2': np.float64(0.1179), 'rougeL': np.float64(0.2178), 'rougeLsum': np.float64(0.2178)}\n"
          ]
        }
      ],
      "source": [
        "# CELL — canonical ROUGE (strict)\n",
        "import numpy as np, evaluate\n",
        "\n",
        "# pick whichever predictions you have\n",
        "preds = (globals().get(\"preds_phaseA\")\n",
        "         or globals().get(\"preds_phaseA_base\")\n",
        "         or globals().get(\"preds_phaseA_cb\"))\n",
        "refs  = globals()[\"refs_phaseA\"]\n",
        "\n",
        "assert preds is not None and len(preds)==len(refs)>0, \"Missing preds or refs.\"\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "scores = rouge.compute(predictions=[p.strip() for p in preds],\n",
        "                       references=[r.strip() for r in refs],\n",
        "                       use_stemmer=True)\n",
        "\n",
        "len_ratio = float(np.mean([len(p.split())/max(1,len(r.split())) for p,r in zip(preds,refs)]))\n",
        "print(f\"len_ratio={len_ratio:.2f}\")\n",
        "print({k: round(v,4) for k,v in scores.items()})  # rouge1/rouge2/rougeL/rougeLsum (F1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_nNK15hjpIV",
        "outputId": "9fb1c1c0-c2bc-4e9a-c29d-248155d76a12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WORDS — Ref:\n",
            "  count=1000 | mean=34.37 | median=33.00 | min=10.00 | p25=27.00 | p75=41.00 | max=76.00 | std=10.23\n",
            "WORDS — Pred:\n",
            "  count=1000 | mean=51.36 | median=50.00 | min=28.00 | p25=44.00 | p75=59.00 | max=78.00 | std=10.61\n",
            "WORDS — Length ratio (Pred/Ref):\n",
            "  count=1000 | mean=1.63 | median=1.52 | min=0.53 | p25=1.18 | p75=1.96 | max=5.42 | std=0.63\n",
            "TOKENS — Ref:\n",
            "  count=1000 | mean=41.32 | median=40.00 | min=12.00 | p25=32.00 | p75=50.00 | max=84.00 | std=12.27\n",
            "TOKENS — Pred:\n",
            "  count=1000 | mean=64.26 | median=63.00 | min=37.00 | p25=55.00 | p75=73.00 | max=84.00 | std=11.75\n",
            "TOKENS — Length ratio (Pred/Ref):\n",
            "  count=1000 | mean=1.71 | median=1.57 | min=0.61 | p25=1.24 | p75=2.03 | max=5.36 | std=0.65\n"
          ]
        }
      ],
      "source": [
        "# ============================ CELL — LENGTH STATS ============================\n",
        "import numpy as np\n",
        "\n",
        "# pick whichever predictions exist\n",
        "preds = (globals().get(\"preds_phaseA\")\n",
        "         or globals().get(\"preds_phaseA_base\")\n",
        "         or globals().get(\"preds_phaseA_cb\"))\n",
        "assert preds is not None and 'refs_phaseA' in globals(), \"Run generation first.\"\n",
        "\n",
        "def word_counts(texts):\n",
        "    return [len(t.split()) for t in texts]\n",
        "\n",
        "def token_counts(texts, tokenizer):\n",
        "    return [len(tokenizer(t, add_special_tokens=False).input_ids) for t in texts]\n",
        "\n",
        "def summarize(name, arr):\n",
        "    a = np.array(arr, dtype=float)\n",
        "    print(f\"{name}:\")\n",
        "    print(f\"  count={a.size} | mean={a.mean():.2f} | median={np.median(a):.2f} | min={a.min():.2f} | p25={np.percentile(a,25):.2f} | p75={np.percentile(a,75):.2f} | max={a.max():.2f} | std={a.std():.2f}\")\n",
        "\n",
        "# --- words ---\n",
        "ref_w  = word_counts(refs_phaseA)\n",
        "pred_w = word_counts(preds)\n",
        "ratio_w = [pw / max(rw, 1) for pw, rw in zip(pred_w, ref_w)]\n",
        "\n",
        "summarize(\"WORDS — Ref\", ref_w)\n",
        "summarize(\"WORDS — Pred\", pred_w)\n",
        "summarize(\"WORDS — Length ratio (Pred/Ref)\", ratio_w)\n",
        "\n",
        "# --- tokenizer tokens (BPE) ---\n",
        "ref_t  = token_counts(refs_phaseA, tok)\n",
        "pred_t = token_counts(preds, tok)\n",
        "ratio_t = [pt / max(rt, 1) for pt, rt in zip(pred_t, ref_t)]\n",
        "\n",
        "summarize(\"TOKENS — Ref\", ref_t)\n",
        "summarize(\"TOKENS — Pred\", pred_t)\n",
        "summarize(\"TOKENS — Length ratio (Pred/Ref)\", ratio_t)\n",
        "# ============================================================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnFOYW1VkrxI"
      },
      "source": [
        "\n",
        "\n",
        "> Investigating a tighter deciding strategy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcMNuP9GkHH9",
        "outputId": "a823b82c-9cf5-4844-b6f3-64b3f77a671c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[gen] device=cuda:0 dtype=torch.bfloat16 gpu=NVIDIA L4\n",
            "[gen] 1000 summaries in 197.4s (~5.1/s)  batch=6 beams=5\n",
            "[cuda] max_alloc=1.67 GB | max_reserved=2.22 GB\n",
            "[Phase A] preds_phaseA: 1000 summaries ready.\n"
          ]
        }
      ],
      "source": [
        "# ============== CELL B — FAST, BATCHED GENERATION (GPU-FORCED + ASSERTS) ==============\n",
        "import time, contextlib, torch\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "assert 'model' in globals() and 'tok' in globals() and 'val_df' in globals(), \"Need model, tok, val_df.\"\n",
        "\n",
        "# ---- force CUDA + AMP dtype ----\n",
        "assert torch.cuda.is_available(), \"CUDA not available. Check your runtime.\"\n",
        "DEVICE    = torch.device(\"cuda:0\")\n",
        "AMP_DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "# Use the SAME object for generation; do NOT swap to .base/.model (may be on CPU)\n",
        "GENERATOR = model\n",
        "GENERATOR.to(DEVICE, dtype=AMP_DTYPE).eval()\n",
        "\n",
        "# sanity: confirm params live on GPU and low-precision\n",
        "p = next(GENERATOR.parameters())\n",
        "print(f\"[gen] device={p.device} dtype={p.dtype} gpu={torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# autocast context for matmuls\n",
        "amp_ctx = lambda: torch.autocast(device_type=\"cuda\", dtype=AMP_DTYPE)\n",
        "\n",
        "# ---- knobs ----\n",
        "MAX_SRC_LEN    = int(globals().get('MAX_SRC_LEN', 400))\n",
        "N_EVAL         = int(globals().get('N_EVAL', 1000))\n",
        "BATCH_GEN      = int(globals().get('BATCH_GEN', 384))\n",
        "NUM_BEAMS      = int(globals().get('NUM_BEAMS', 5))\n",
        "MIN_NEW        = int(globals().get('MIN_NEW', 30))\n",
        "MAX_NEW        = int(globals().get('MAX_NEW', 72))\n",
        "NO_REPEAT      = int(globals().get('NO_REPEAT', 4))\n",
        "LENGTH_PENALTY = float(globals().get('LENGTH_PENALTY', 2.2))\n",
        "\n",
        "if hasattr(GENERATOR, \"config\"):\n",
        "    GENERATOR.config.use_cache = True\n",
        "\n",
        "gen_cfg = GenerationConfig(\n",
        "    num_beams=NUM_BEAMS,\n",
        "    min_new_tokens=MIN_NEW,\n",
        "    max_new_tokens=MAX_NEW,\n",
        "    no_repeat_ngram_size=NO_REPEAT,\n",
        "    length_penalty=LENGTH_PENALTY,\n",
        "    early_stopping=True,\n",
        ")\n",
        "\n",
        "# ---- slice data ----\n",
        "_rows = val_df.iloc[:N_EVAL]\n",
        "srcs_phaseA = _rows[\"article\"].astype(str).tolist()\n",
        "refs_phaseA = _rows[\"highlights\"].astype(str).tolist()\n",
        "\n",
        "# ---- fast batched generation ----\n",
        "def generate_batched(srcs):\n",
        "    preds, t0 = [], time.time()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    for i in range(0, len(srcs), BATCH_GEN):\n",
        "        batch = srcs[i:i+BATCH_GEN]\n",
        "        with torch.inference_mode():\n",
        "            ins = tok(\n",
        "                batch,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=MAX_SRC_LEN,\n",
        "                padding=True,\n",
        "                pad_to_multiple_of=16,\n",
        "            ).to(DEVICE)\n",
        "\n",
        "            # assert tensors are on GPU\n",
        "            assert ins[\"input_ids\"].is_cuda, \"inputs not on CUDA\"\n",
        "            with amp_ctx():\n",
        "                outs = GENERATOR.generate(**ins, generation_config=gen_cfg)\n",
        "\n",
        "            preds.extend(tok.batch_decode(outs, skip_special_tokens=True))\n",
        "            del ins, outs\n",
        "\n",
        "    dt = time.time() - t0\n",
        "    used = torch.cuda.max_memory_allocated() / (1024**3)\n",
        "    resv = torch.cuda.max_memory_reserved() / (1024**3)\n",
        "    print(f\"[gen] {len(srcs)} summaries in {dt:.1f}s (~{len(srcs)/max(dt,1):.1f}/s)  batch={BATCH_GEN} beams={NUM_BEAMS}\")\n",
        "    print(f\"[cuda] max_alloc={used:.2f} GB | max_reserved={resv:.2f} GB\")\n",
        "    return preds\n",
        "\n",
        "preds_phaseA = generate_batched(srcs_phaseA)\n",
        "print(f\"[Phase A] preds_phaseA: {len(preds_phaseA)} summaries ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTM6rPX2lCXl",
        "outputId": "c97950bb-f48d-47df-a600-50621a2bd781"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ROUGE (canonical) ===\n",
            "{'rouge1': 0.3112752426525638, 'rouge2': 0.11793703827003424, 'rougeL': 0.21780415489912652, 'rougeLsum': 0.21779259711225119}\n",
            "\n",
            "=== LENGTH STATS (words) ===\n",
            "Ref : {'count': 1000, 'mean': 34.373, 'median': 33.0, 'min': 10.0, 'p25': 27.0, 'p75': 41.0, 'max': 76.0, 'std': 10.234543028391643}\n",
            "Pred: {'count': 1000, 'mean': 51.356, 'median': 50.0, 'min': 28.0, 'p25': 44.0, 'p75': 59.0, 'max': 78.0, 'std': 10.612881983702636}\n",
            "Pred/Ref ratio: {'count': 1000, 'mean': 1.634367797110999, 'median': 1.521286231884058, 'min': 0.5303030303030303, 'p25': 1.1761029411764707, 'p75': 1.9550395256916997, 'max': 5.416666666666667, 'std': 0.625252445054452}\n"
          ]
        }
      ],
      "source": [
        "# ========================== CELL — ROUGE + LENGTH STATS ==========================\n",
        "import numpy as np\n",
        "\n",
        "# use outputs from your generation cell\n",
        "preds = globals().get(\"preds_phaseA\")\n",
        "refs  = globals().get(\"refs_phaseA\")\n",
        "assert preds is not None and refs is not None and len(preds) == len(refs) > 0, \"Run generation first.\"\n",
        "\n",
        "# canonical ROUGE (HF evaluate); will raise if not installed\n",
        "try:\n",
        "    import evaluate\n",
        "    rouge = evaluate.load(\"rouge\")\n",
        "    rouge_scores = rouge.compute(\n",
        "        predictions=[p.strip() for p in preds],\n",
        "        references=[r.strip() for r in refs],\n",
        "        use_stemmer=True,\n",
        "    )\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"`evaluate` not available. Install with: pip install evaluate rouge-score\") from e\n",
        "\n",
        "def _stats(arr):\n",
        "    a = np.array(arr, dtype=float)\n",
        "    return {\n",
        "        \"count\": int(a.size),\n",
        "        \"mean\": float(a.mean()),\n",
        "        \"median\": float(np.median(a)),\n",
        "        \"min\": float(a.min()),\n",
        "        \"p25\": float(np.percentile(a, 25)),\n",
        "        \"p75\": float(np.percentile(a, 75)),\n",
        "        \"max\": float(a.max()),\n",
        "        \"std\": float(a.std()),\n",
        "    }\n",
        "\n",
        "# word lengths\n",
        "ref_w   = [len(t.split()) for t in refs]\n",
        "pred_w  = [len(t.split()) for t in preds]\n",
        "ratio_w = [pw / max(rw, 1) for pw, rw in zip(pred_w, ref_w)]\n",
        "\n",
        "print(\"=== ROUGE (canonical) ===\")\n",
        "print({k: float(v) for k, v in rouge_scores.items()})\n",
        "\n",
        "print(\"\\n=== LENGTH STATS (words) ===\")\n",
        "print(\"Ref :\", _stats(ref_w))\n",
        "print(\"Pred:\", _stats(pred_w))\n",
        "print(\"Pred/Ref ratio:\", _stats(ratio_w))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7phmM2rzmm6p",
        "outputId": "6a538aeb-2bd4-4137-9329-7ce740d2508b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[gen] device=cuda:0 dtype=torch.bfloat16 gpu=NVIDIA L4\n",
            "[eos-bias] min_len=28 trigger_len=52 (ref_median≈40)\n",
            "[gen] 1000 summaries in 149.1s (~6.7/s)  batch=6 beams=5\n",
            "[cuda] max_alloc=1.92 GB | max_reserved=2.22 GB\n",
            "[Phase A] preds_phaseA: 1000 summaries ready.\n"
          ]
        }
      ],
      "source": [
        "# ============== CELL B — FAST, BATCHED GENERATION (GPU-FORCED + ASSERTS + TIGHT DECODE) ==============\n",
        "import time, torch\n",
        "from transformers import GenerationConfig, LogitsProcessor, LogitsProcessorList\n",
        "\n",
        "assert 'model' in globals() and 'tok' in globals() and 'val_df' in globals(), \"Need model, tok, val_df.\"\n",
        "\n",
        "# ---- force CUDA + AMP dtype ----\n",
        "assert torch.cuda.is_available(), \"CUDA not available. Check your runtime.\"\n",
        "DEVICE    = torch.device(\"cuda:0\")\n",
        "AMP_DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "GENERATOR = model\n",
        "GENERATOR.to(DEVICE, dtype=AMP_DTYPE).eval()\n",
        "\n",
        "\n",
        "p = next(GENERATOR.parameters())\n",
        "print(f\"[gen] device={p.device} dtype={p.dtype} gpu={torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# autocast context for matmuls\n",
        "amp_ctx = lambda: torch.autocast(device_type=\"cuda\", dtype=AMP_DTYPE)\n",
        "\n",
        "# ---- knobs ----\n",
        "MAX_SRC_LEN     = int(globals().get('MAX_SRC_LEN', 400))\n",
        "N_EVAL          = int(globals().get('N_EVAL', 1000))\n",
        "BATCH_GEN       = int(globals().get('BATCH_GEN', 384))\n",
        "NUM_BEAMS       = int(globals().get('NUM_BEAMS', 5))\n",
        "MIN_NEW         = int(globals().get('MIN_NEW', 30))\n",
        "MAX_NEW         = int(globals().get('MAX_NEW', 72))\n",
        "NO_REPEAT       = int(globals().get('NO_REPEAT', 4))\n",
        "LENGTH_PENALTY  = float(globals().get('LENGTH_PENALTY', 2.6))\n",
        "\n",
        "USE_EOS_BIAS    = bool(globals().get('USE_EOS_BIAS', True))\n",
        "EOS_TARGET_SCALE= float(globals().get('EOS_TARGET_SCALE', 1.30))  # ~1.3× ref median tokens\n",
        "EOS_MIN_SCALE   = float(globals().get('EOS_MIN_SCALE', 0.70))     # disfavor EOS before ~0.7×\n",
        "PRE_EOS_BIAS    = float(globals().get('PRE_EOS_BIAS', -3.0))\n",
        "POST_EOS_BIAS   = float(globals().get('POST_EOS_BIAS', 2.8))\n",
        "\n",
        "if hasattr(GENERATOR, \"config\"):\n",
        "    GENERATOR.config.use_cache = True\n",
        "\n",
        "gen_cfg = GenerationConfig(\n",
        "    num_beams=NUM_BEAMS,\n",
        "    min_new_tokens=MIN_NEW,\n",
        "    max_new_tokens=MAX_NEW,\n",
        "    no_repeat_ngram_size=NO_REPEAT,\n",
        "    length_penalty=LENGTH_PENALTY,\n",
        "    early_stopping=True,\n",
        ")\n",
        "\n",
        "# ---- slice data ----\n",
        "_rows = val_df.iloc[:N_EVAL]\n",
        "srcs_phaseA = _rows[\"article\"].astype(str).tolist()\n",
        "refs_phaseA = _rows[\"highlights\"].astype(str).tolist()\n",
        "\n",
        "# ---- EOS length-bias processor ----\n",
        "class EosAfterNTokens(LogitsProcessor):\n",
        "    def __init__(self, eos_id, min_len, trigger_len, pre_bias=-3.0, post_bias=2.8):\n",
        "        self.eos_id = int(eos_id)\n",
        "        self.min_len = int(min_len)\n",
        "        self.trigger_len = int(trigger_len)\n",
        "        self.pre_bias = float(pre_bias)\n",
        "        self.post_bias = float(post_bias)\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        # new tokens so far (exclude decoder_start_token)\n",
        "        new_len = input_ids.shape[1] - 1\n",
        "        if new_len < self.min_len:\n",
        "            scores[:, self.eos_id] += self.pre_bias\n",
        "        elif new_len >= self.trigger_len:\n",
        "            scores[:, self.eos_id] += self.post_bias\n",
        "        return scores\n",
        "\n",
        "logits_proc = None\n",
        "if USE_EOS_BIAS:\n",
        "    # estimate target token length from reference median\n",
        "    ref_tok = [len(tok(r, add_special_tokens=False).input_ids) for r in refs_phaseA]\n",
        "    tgt = int(max(1, int(torch.tensor(ref_tok).median().item())) * EOS_TARGET_SCALE)\n",
        "    mn  = int(max(1, int(torch.tensor(ref_tok).median().item())) * EOS_MIN_SCALE)\n",
        "    eos_id = GENERATOR.config.eos_token_id\n",
        "    logits_proc = LogitsProcessorList([EosAfterNTokens(eos_id, mn, tgt, PRE_EOS_BIAS, POST_EOS_BIAS)])\n",
        "    print(f\"[eos-bias] min_len={mn} trigger_len={tgt} (ref_median≈{int(torch.tensor(ref_tok).median().item())})\")\n",
        "\n",
        "# ---- fast batched generation ----\n",
        "def generate_batched(srcs):\n",
        "    preds, t0 = [], time.time()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    for i in range(0, len(srcs), BATCH_GEN):\n",
        "        batch = srcs[i:i+BATCH_GEN]\n",
        "        with torch.inference_mode():\n",
        "            ins = tok(\n",
        "                batch,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=MAX_SRC_LEN,\n",
        "                padding=True,\n",
        "                pad_to_multiple_of=16,\n",
        "            ).to(DEVICE)\n",
        "            assert ins[\"input_ids\"].is_cuda, \"inputs not on CUDA\"\n",
        "            with amp_ctx():\n",
        "                outs = GENERATOR.generate(\n",
        "                    **ins,\n",
        "                    generation_config=gen_cfg,\n",
        "                    logits_processor=logits_proc,\n",
        "                )\n",
        "            preds.extend(tok.batch_decode(outs, skip_special_tokens=True))\n",
        "            del ins, outs\n",
        "    dt = time.time() - t0\n",
        "    used = torch.cuda.max_memory_allocated() / (1024**3)\n",
        "    resv = torch.cuda.max_memory_reserved() / (1024**3)\n",
        "    print(f\"[gen] {len(srcs)} summaries in {dt:.1f}s (~{len(srcs)/max(dt,1):.1f}/s)  batch={BATCH_GEN} beams={NUM_BEAMS}\")\n",
        "    print(f\"[cuda] max_alloc={used:.2f} GB | max_reserved={resv:.2f} GB\")\n",
        "    return preds\n",
        "\n",
        "preds_phaseA = generate_batched(srcs_phaseA)\n",
        "print(f\"[Phase A] preds_phaseA: {len(preds_phaseA)} summaries ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5R1GL5ZnJiw",
        "outputId": "02ea0559-e620-4daa-d665-4f395ffef2dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ROUGE (canonical) ===\n",
            "{'rouge1': 0.3127043809135984, 'rouge2': 0.11668947457640899, 'rougeL': 0.2204193402667914, 'rougeLsum': 0.22043097170736703}\n",
            "\n",
            "=== LENGTH STATS (words) ===\n",
            "Ref : {'count': 1000, 'mean': 34.373, 'median': 33.0, 'min': 10.0, 'p25': 27.0, 'p75': 41.0, 'max': 76.0, 'std': 10.234543028391643}\n",
            "Pred: {'count': 1000, 'mean': 44.786, 'median': 45.0, 'min': 24.0, 'p25': 41.0, 'p75': 49.0, 'max': 58.0, 'std': 5.6595233014804345}\n",
            "Pred/Ref ratio: {'count': 1000, 'mean': 1.4270702334049465, 'median': 1.3333333333333333, 'min': 0.4745762711864407, 'p25': 1.0788183694530444, 'p75': 1.6824137931034482, 'max': 4.333333333333333, 'std': 0.49609308108215394}\n",
            "\n",
            "=== LENGTH STATS (tokens) ===\n",
            "Ref : {'count': 1000, 'mean': 41.316, 'median': 40.0, 'min': 12.0, 'p25': 32.0, 'p75': 50.0, 'max': 84.0, 'std': 12.271110137228824}\n",
            "Pred: {'count': 1000, 'mean': 56.149, 'median': 57.5, 'min': 34.0, 'p25': 53.0, 'p75': 61.0, 'max': 61.0, 'std': 5.0865311362459975}\n",
            "Pred/Ref ratio: {'count': 1000, 'mean': 1.4924372898569904, 'median': 1.3883037694013303, 'min': 0.5483870967741935, 'p25': 1.1296296296296295, 'p75': 1.7601419878296145, 'max': 4.833333333333333, 'std': 0.512301899349545}\n",
            "\n",
            "[len] mean word-length ratio = 1.43\n"
          ]
        }
      ],
      "source": [
        "# ====================== CELL — ROUGE + LENGTH STATS (after generation) ======================\n",
        "import numpy as np\n",
        "\n",
        "# use outputs from your generation cell\n",
        "assert 'preds_phaseA' in globals() and 'refs_phaseA' in globals(), \"Run generation first.\"\n",
        "preds, refs = preds_phaseA, refs_phaseA\n",
        "assert len(preds) == len(refs) and len(preds) > 0, \"Empty preds/refs.\"\n",
        "\n",
        "# ---- canonical ROUGE (HF evaluate) ----\n",
        "try:\n",
        "    import evaluate\n",
        "    rouge = evaluate.load(\"rouge\")\n",
        "    rouge_scores = rouge.compute(\n",
        "        predictions=[p.strip() for p in preds],\n",
        "        references=[r.strip() for r in refs],\n",
        "        use_stemmer=True,\n",
        "    )\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"Install metrics: pip install evaluate rouge-score\") from e\n",
        "\n",
        "def _stats(arr):\n",
        "    a = np.asarray(arr, dtype=float)\n",
        "    return {\n",
        "        \"count\": int(a.size),\n",
        "        \"mean\": float(a.mean()),\n",
        "        \"median\": float(np.median(a)),\n",
        "        \"min\": float(a.min()),\n",
        "        \"p25\": float(np.percentile(a, 25)),\n",
        "        \"p75\": float(np.percentile(a, 75)),\n",
        "        \"max\": float(a.max()),\n",
        "        \"std\": float(a.std()),\n",
        "    }\n",
        "\n",
        "# word lengths\n",
        "ref_w   = [len(t.split()) for t in refs]\n",
        "pred_w  = [len(t.split()) for t in preds]\n",
        "ratio_w = [pw / max(rw, 1) for pw, rw in zip(pred_w, ref_w)]\n",
        "\n",
        "# tokenizer-token lengths (BPE)\n",
        "ref_t   = [len(tok(r, add_special_tokens=False).input_ids) for r in refs]\n",
        "pred_t  = [len(tok(p, add_special_tokens=False).input_ids) for p in preds]\n",
        "ratio_t = [pt / max(rt, 1) for pt, rt in zip(pred_t, ref_t)]\n",
        "\n",
        "# ---- print summary ----\n",
        "print(\"=== ROUGE (canonical) ===\")\n",
        "print({k: float(v) for k, v in rouge_scores.items()})\n",
        "\n",
        "print(\"\\n=== LENGTH STATS (words) ===\")\n",
        "print(\"Ref :\", _stats(ref_w))\n",
        "print(\"Pred:\", _stats(pred_w))\n",
        "print(\"Pred/Ref ratio:\", _stats(ratio_w))\n",
        "\n",
        "print(\"\\n=== LENGTH STATS (tokens) ===\")\n",
        "print(\"Ref :\", _stats(ref_t))\n",
        "print(\"Pred:\", _stats(pred_t))\n",
        "print(\"Pred/Ref ratio:\", _stats(ratio_t))\n",
        "\n",
        "print(f\"\\n[len] mean word-length ratio = {np.mean(ratio_w):.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ouy4U4sfoo_K",
        "outputId": "b0465da7-4345-4df3-e25a-5e53c8f3d8fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[gen] device=cuda:0 dtype=torch.bfloat16\n",
            "[eos-bias] min_len=28 trigger_len=50 (ref_median≈40)\n",
            "[gen] 1000 summaries in 161.1s (~6.2/s) batch=6 beams=5\n",
            "[cuda] max_alloc=1.64 GB | max_reserved=2.22 GB\n",
            "[Phase A'] preds_phaseA: 1000 summaries ready.\n"
          ]
        }
      ],
      "source": [
        "# ============== CELL B' — GENERATION (tighter cap + stronger EOS bias) ==============\n",
        "import time, torch\n",
        "from transformers import GenerationConfig, LogitsProcessor, LogitsProcessorList\n",
        "\n",
        "assert 'model' in globals() and 'tok' in globals() and 'val_df' in globals(), \"Need model, tok, val_df.\"\n",
        "\n",
        "# ---- force CUDA + AMP dtype (reuse same generator) ----\n",
        "assert torch.cuda.is_available(), \"CUDA not available.\"\n",
        "DEVICE    = torch.device(\"cuda:0\")\n",
        "AMP_DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "GENERATOR = model\n",
        "GENERATOR.to(DEVICE, dtype=AMP_DTYPE).eval()\n",
        "print(f\"[gen] device={next(GENERATOR.parameters()).device} dtype={next(GENERATOR.parameters()).dtype}\")\n",
        "\n",
        "# ---- knobs (tighter) ----\n",
        "MAX_SRC_LEN     = int(globals().get('MAX_SRC_LEN', 400))\n",
        "N_EVAL          = int(globals().get('N_EVAL', 1000))\n",
        "BATCH_GEN       = int(globals().get('BATCH_GEN', 384))\n",
        "NUM_BEAMS       = 5\n",
        "MIN_NEW         = 28\n",
        "MAX_NEW         = 68       #  from 72\n",
        "NO_REPEAT       = 4\n",
        "LENGTH_PENALTY  = 2.8      #  from 2.6\n",
        "\n",
        "# EOS bias around ~1.25× ref median tokens\n",
        "USE_EOS_BIAS    = True\n",
        "EOS_TARGET_SCALE= 1.25     # ↓ from 1.30\n",
        "EOS_MIN_SCALE   = 0.70\n",
        "PRE_EOS_BIAS    = -3.0\n",
        "POST_EOS_BIAS   = 3.2      # ↑ for earlier stop once target reached\n",
        "\n",
        "if hasattr(GENERATOR, \"config\"):\n",
        "    GENERATOR.config.use_cache = True\n",
        "\n",
        "gen_cfg = GenerationConfig(\n",
        "    num_beams=NUM_BEAMS,\n",
        "    min_new_tokens=MIN_NEW,\n",
        "    max_new_tokens=MAX_NEW,\n",
        "    no_repeat_ngram_size=NO_REPEAT,\n",
        "    length_penalty=LENGTH_PENALTY,\n",
        "    early_stopping=True,\n",
        ")\n",
        "\n",
        "# ---- slice data ----\n",
        "_rows = val_df.iloc[:N_EVAL]\n",
        "srcs_phaseA = _rows[\"article\"].astype(str).tolist()\n",
        "refs_phaseA = _rows[\"highlights\"].astype(str).tolist()\n",
        "\n",
        "# ---- EOS length-bias processor ----\n",
        "class EosAfterNTokens(LogitsProcessor):\n",
        "    def __init__(self, eos_id, min_len, trigger_len, pre_bias=-3.0, post_bias=3.2):\n",
        "        self.eos_id = int(eos_id); self.min_len = int(min_len); self.trigger_len = int(trigger_len)\n",
        "        self.pre_bias = float(pre_bias); self.post_bias = float(post_bias)\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        new_len = input_ids.shape[1] - 1\n",
        "        if new_len < self.min_len:\n",
        "            scores[:, self.eos_id] += self.pre_bias\n",
        "        elif new_len >= self.trigger_len:\n",
        "            scores[:, self.eos_id] += self.post_bias\n",
        "        return scores\n",
        "\n",
        "logits_proc = None\n",
        "if USE_EOS_BIAS:\n",
        "    ref_tok = [len(tok(r, add_special_tokens=False).input_ids) for r in refs_phaseA]\n",
        "    ref_med = int(torch.tensor(ref_tok).median().item())\n",
        "    tgt_len = int(max(1, ref_med) * EOS_TARGET_SCALE)\n",
        "    min_len = int(max(1, ref_med) * EOS_MIN_SCALE)\n",
        "    logits_proc = LogitsProcessorList([EosAfterNTokens(GENERATOR.config.eos_token_id, min_len, tgt_len, PRE_EOS_BIAS, POST_EOS_BIAS)])\n",
        "    print(f\"[eos-bias] min_len={min_len} trigger_len={tgt_len} (ref_median≈{ref_med})\")\n",
        "\n",
        "# ---- fast batched generation ----\n",
        "def generate_batched(srcs):\n",
        "    preds, t0 = [], time.time()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    for i in range(0, len(srcs), BATCH_GEN):\n",
        "        batch = srcs[i:i+BATCH_GEN]\n",
        "        with torch.inference_mode(), torch.autocast(device_type=\"cuda\", dtype=AMP_DTYPE):\n",
        "            ins = tok(batch, return_tensors=\"pt\", truncation=True, max_length=MAX_SRC_LEN,\n",
        "                      padding=True, pad_to_multiple_of=16).to(DEVICE)\n",
        "            outs = GENERATOR.generate(**ins, generation_config=gen_cfg, logits_processor=logits_proc)\n",
        "        preds.extend(tok.batch_decode(outs, skip_special_tokens=True))\n",
        "        del ins, outs\n",
        "    dt = time.time() - t0\n",
        "    used = torch.cuda.max_memory_allocated() / (1024**3)\n",
        "    resv = torch.cuda.max_memory_reserved() / (1024**3)\n",
        "    print(f\"[gen] {len(srcs)} summaries in {dt:.1f}s (~{len(srcs)/max(dt,1):.1f}/s) batch={BATCH_GEN} beams={NUM_BEAMS}\")\n",
        "    print(f\"[cuda] max_alloc={used:.2f} GB | max_reserved={resv:.2f} GB\")\n",
        "    return preds\n",
        "\n",
        "preds_phaseA = generate_batched(srcs_phaseA)\n",
        "print(f\"[Phase A'] preds_phaseA: {len(preds_phaseA)} summaries ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTMLIHCfpWVa",
        "outputId": "9f8332c5-7229-48d7-efe8-7dc4c9fe1c45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ROUGE (canonical) ===\n",
            "{'rouge1': 0.31366340607403437, 'rouge2': 0.11735810225738871, 'rougeL': 0.22072410140735044, 'rougeLsum': 0.22084018115127318}\n",
            "\n",
            "=== LENGTH STATS (words) ===\n",
            "Ref : {'count': 1000, 'mean': 34.373, 'median': 33.0, 'min': 10.0, 'p25': 27.0, 'p75': 41.0, 'max': 76.0, 'std': 10.234543028391643}\n",
            "Pred: {'count': 1000, 'mean': 45.169, 'median': 45.0, 'min': 24.0, 'p25': 41.0, 'p75': 49.0, 'max': 62.0, 'std': 6.455884060297241}\n",
            "Pred/Ref ratio: {'count': 1000, 'mean': 1.4380233619999845, 'median': 1.3333333333333333, 'min': 0.4745762711864407, 'p25': 1.0879156010230178, 'p75': 1.7037037037037037, 'max': 4.833333333333333, 'std': 0.506025143437095}\n",
            "\n",
            "=== LENGTH STATS (tokens) ===\n",
            "Ref : {'count': 1000, 'mean': 41.316, 'median': 40.0, 'min': 12.0, 'p25': 32.0, 'p75': 50.0, 'max': 84.0, 'std': 12.271110137228824}\n",
            "Pred: {'count': 1000, 'mean': 56.789, 'median': 57.0, 'min': 34.0, 'p25': 52.0, 'p75': 62.0, 'max': 67.0, 'std': 6.439136510433678}\n",
            "Pred/Ref ratio: {'count': 1000, 'mean': 1.5085509898850382, 'median': 1.4023809523809523, 'min': 0.5483870967741935, 'p25': 1.1355932203389831, 'p75': 1.7857142857142858, 'max': 4.833333333333333, 'std': 0.5279548534173835}\n",
            "\n",
            "[len] mean word-length ratio = 1.44\n"
          ]
        }
      ],
      "source": [
        "# ====================== CELL — ROUGE + LENGTH STATS (after generation) ======================\n",
        "import numpy as np\n",
        "\n",
        "# use outputs from your generation cell\n",
        "assert 'preds_phaseA' in globals() and 'refs_phaseA' in globals(), \"Run generation first.\"\n",
        "preds, refs = preds_phaseA, refs_phaseA\n",
        "assert len(preds) == len(refs) and len(preds) > 0, \"Empty preds/refs.\"\n",
        "\n",
        "# ---- canonical ROUGE (HF evaluate) ----\n",
        "try:\n",
        "    import evaluate\n",
        "    rouge = evaluate.load(\"rouge\")\n",
        "    rouge_scores = rouge.compute(\n",
        "        predictions=[p.strip() for p in preds],\n",
        "        references=[r.strip() for r in refs],\n",
        "        use_stemmer=True,\n",
        "    )\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"Install metrics: pip install evaluate rouge-score\") from e\n",
        "\n",
        "def _stats(arr):\n",
        "    a = np.asarray(arr, dtype=float)\n",
        "    return {\n",
        "        \"count\": int(a.size),\n",
        "        \"mean\": float(a.mean()),\n",
        "        \"median\": float(np.median(a)),\n",
        "        \"min\": float(a.min()),\n",
        "        \"p25\": float(np.percentile(a, 25)),\n",
        "        \"p75\": float(np.percentile(a, 75)),\n",
        "        \"max\": float(a.max()),\n",
        "        \"std\": float(a.std()),\n",
        "    }\n",
        "\n",
        "# word lengths\n",
        "ref_w   = [len(t.split()) for t in refs]\n",
        "pred_w  = [len(t.split()) for t in preds]\n",
        "ratio_w = [pw / max(rw, 1) for pw, rw in zip(pred_w, ref_w)]\n",
        "\n",
        "# tokenizer-token lengths (BPE)\n",
        "ref_t   = [len(tok(r, add_special_tokens=False).input_ids) for r in refs]\n",
        "pred_t  = [len(tok(p, add_special_tokens=False).input_ids) for p in preds]\n",
        "ratio_t = [pt / max(rt, 1) for pt, rt in zip(pred_t, ref_t)]\n",
        "\n",
        "# ---- print summary ----\n",
        "print(\"=== ROUGE (canonical) ===\")\n",
        "print({k: float(v) for k, v in rouge_scores.items()})\n",
        "\n",
        "print(\"\\n=== LENGTH STATS (words) ===\")\n",
        "print(\"Ref :\", _stats(ref_w))\n",
        "print(\"Pred:\", _stats(pred_w))\n",
        "print(\"Pred/Ref ratio:\", _stats(ratio_w))\n",
        "\n",
        "print(\"\\n=== LENGTH STATS (tokens) ===\")\n",
        "print(\"Ref :\", _stats(ref_t))\n",
        "print(\"Pred:\", _stats(pred_t))\n",
        "print(\"Pred/Ref ratio:\", _stats(ratio_t))\n",
        "\n",
        "print(f\"\\n[len] mean word-length ratio = {np.mean(ratio_w):.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45n5hbZyqbOZ",
        "outputId": "e15ef925-8463-4cb7-cb25-1c989ba2e3f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[gen] device=cuda:0 dtype=torch.bfloat16\n",
            "[eos-bias] min_len=28 trigger_len=48 (ref_median≈40)\n",
            "[gen] 1000 summaries in 144.3s (~6.9/s)  batch=6 beams=5\n",
            "[cuda] max_alloc=1.37 GB | max_reserved=2.22 GB\n",
            "[Phase A''] preds_phaseA: 1000 summaries ready.\n"
          ]
        }
      ],
      "source": [
        "# ============== CELL B'' — FAST GEN (harder cap + stronger EOS bias) ==============\n",
        "import time, torch\n",
        "from transformers import GenerationConfig, LogitsProcessor, LogitsProcessorList\n",
        "\n",
        "assert 'model' in globals() and 'tok' in globals() and 'val_df' in globals()\n",
        "\n",
        "# ---- force CUDA + AMP dtype ----\n",
        "assert torch.cuda.is_available(), \"CUDA not available.\"\n",
        "DEVICE    = torch.device(\"cuda:0\")\n",
        "AMP_DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "GENERATOR = model\n",
        "GENERATOR.to(DEVICE, dtype=AMP_DTYPE).eval()\n",
        "print(f\"[gen] device={next(GENERATOR.parameters()).device} dtype={next(GENERATOR.parameters()).dtype}\")\n",
        "\n",
        "# ---- knobs (tighter) ----\n",
        "MAX_SRC_LEN     = int(globals().get('MAX_SRC_LEN', 400))\n",
        "N_EVAL          = int(globals().get('N_EVAL', 1000))\n",
        "BATCH_GEN       = int(globals().get('BATCH_GEN', 384))\n",
        "NUM_BEAMS       = 5\n",
        "MIN_NEW         = 26          # allow earlier EOS\n",
        "MAX_NEW         = 60          #  from 72→68→60\n",
        "NO_REPEAT       = 4\n",
        "LENGTH_PENALTY  = 3.0         # for shorter beams\n",
        "\n",
        "# EOS bias ~1.22× ref median tokens (keeps ~1.28–1.32× words)\n",
        "USE_EOS_BIAS    = True\n",
        "EOS_TARGET_SCALE= 1.22\n",
        "EOS_MIN_SCALE   = 0.72\n",
        "PRE_EOS_BIAS    = -3.2\n",
        "POST_EOS_BIAS   = 3.6\n",
        "\n",
        "if hasattr(GENERATOR, \"config\"):\n",
        "    GENERATOR.config.use_cache = True\n",
        "\n",
        "gen_cfg = GenerationConfig(\n",
        "    num_beams=NUM_BEAMS,\n",
        "    min_new_tokens=MIN_NEW,\n",
        "    max_new_tokens=MAX_NEW,\n",
        "    no_repeat_ngram_size=NO_REPEAT,\n",
        "    length_penalty=LENGTH_PENALTY,\n",
        "    early_stopping=True,\n",
        ")\n",
        "\n",
        "# ---- slice data ----\n",
        "_rows = val_df.iloc[:N_EVAL]\n",
        "srcs_phaseA = _rows[\"article\"].astype(str).tolist()\n",
        "refs_phaseA = _rows[\"highlights\"].astype(str).tolist()\n",
        "\n",
        "# ---- EOS length-bias processor ----\n",
        "class EosAfterNTokens(LogitsProcessor):\n",
        "    def __init__(self, eos_id, min_len, trigger_len, pre_bias=-3.2, post_bias=3.6):\n",
        "        self.eos_id = int(eos_id); self.min_len = int(min_len); self.trigger_len = int(trigger_len)\n",
        "        self.pre_bias = float(pre_bias); self.post_bias = float(post_bias)\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        new_len = input_ids.shape[1] - 1\n",
        "        if new_len < self.min_len:\n",
        "            scores[:, self.eos_id] += self.pre_bias\n",
        "        elif new_len >= self.trigger_len:\n",
        "            scores[:, self.eos_id] += self.post_bias\n",
        "        return scores\n",
        "\n",
        "logits_proc = None\n",
        "if USE_EOS_BIAS:\n",
        "    ref_tok = [len(tok(r, add_special_tokens=False).input_ids) for r in refs_phaseA]\n",
        "    ref_med = int(torch.tensor(ref_tok).median().item())\n",
        "    tgt_len = int(max(1, ref_med) * EOS_TARGET_SCALE)\n",
        "    min_len = int(max(1, ref_med) * EOS_MIN_SCALE)\n",
        "    logits_proc = LogitsProcessorList([EosAfterNTokens(GENERATOR.config.eos_token_id, min_len, tgt_len, PRE_EOS_BIAS, POST_EOS_BIAS)])\n",
        "    print(f\"[eos-bias] min_len={min_len} trigger_len={tgt_len} (ref_median≈{ref_med})\")\n",
        "\n",
        "# ---- fast batched generation ----\n",
        "def generate_batched(srcs):\n",
        "    preds, t0 = [], time.time()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    for i in range(0, len(srcs), BATCH_GEN):\n",
        "        batch = srcs[i:i+BATCH_GEN]\n",
        "        with torch.inference_mode(), torch.autocast(device_type=\"cuda\", dtype=AMP_DTYPE):\n",
        "            ins = tok(batch, return_tensors=\"pt\", truncation=True, max_length=MAX_SRC_LEN,\n",
        "                      padding=True, pad_to_multiple_of=16).to(DEVICE)\n",
        "            outs = GENERATOR.generate(**ins, generation_config=gen_cfg, logits_processor=logits_proc)\n",
        "        preds.extend(tok.batch_decode(outs, skip_special_tokens=True))\n",
        "        del ins, outs\n",
        "    dt = time.time() - t0\n",
        "    used = torch.cuda.max_memory_allocated() / (1024**3)\n",
        "    resv = torch.cuda.max_memory_reserved() / (1024**3)\n",
        "    print(f\"[gen] {len(srcs)} summaries in {dt:.1f}s (~{len(srcs)/max(dt,1):.1f}/s)  batch={BATCH_GEN} beams={NUM_BEAMS}\")\n",
        "    print(f\"[cuda] max_alloc={used:.2f} GB | max_reserved={resv:.2f} GB\")\n",
        "    return preds\n",
        "\n",
        "preds_phaseA = generate_batched(srcs_phaseA)\n",
        "print(f\"[Phase A''] preds_phaseA: {len(preds_phaseA)} summaries ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ma3qWu59qbzV",
        "outputId": "43ca81d6-429c-4176-83d8-27829c5cc6ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ROUGE (canonical) ===\n",
            "{'rouge1': 0.31404881099194615, 'rouge2': 0.11756833248633208, 'rougeL': 0.22215217639875512, 'rougeLsum': 0.2221598684580285}\n",
            "\n",
            "=== LENGTH STATS (words) ===\n",
            "Ref : {'count': 1000, 'mean': 34.373, 'median': 33.0, 'min': 10.0, 'p25': 27.0, 'p75': 41.0, 'max': 76.0, 'std': 10.234543028391643}\n",
            "Pred: {'count': 1000, 'mean': 43.075, 'median': 43.0, 'min': 24.0, 'p25': 40.0, 'p75': 47.0, 'max': 57.0, 'std': 5.245509984739329}\n",
            "Pred/Ref ratio: {'count': 1000, 'mean': 1.372197584798802, 'median': 1.281650641025641, 'min': 0.4745762711864407, 'p25': 1.0256410256410255, 'p75': 1.625, 'max': 4.5, 'std': 0.4712362705755517}\n",
            "\n",
            "=== LENGTH STATS (tokens) ===\n",
            "Ref : {'count': 1000, 'mean': 41.316, 'median': 40.0, 'min': 12.0, 'p25': 32.0, 'p75': 50.0, 'max': 84.0, 'std': 12.271110137228824}\n",
            "Pred: {'count': 1000, 'mean': 54.125, 'median': 55.0, 'min': 34.0, 'p25': 51.0, 'p75': 58.0, 'max': 59.0, 'std': 4.554928649276517}\n",
            "Pred/Ref ratio: {'count': 1000, 'mean': 1.4385255794084268, 'median': 1.3333333333333333, 'min': 0.5483870967741935, 'p25': 1.0925925925925926, 'p75': 1.7096774193548387, 'max': 4.916666666666667, 'std': 0.4911676720055785}\n",
            "\n",
            "[len] mean word-length ratio = 1.37\n"
          ]
        }
      ],
      "source": [
        "# ====================== CELL — ROUGE + LENGTH STATS (after generation) ======================\n",
        "import numpy as np\n",
        "\n",
        "# use outputs from your generation cell\n",
        "assert 'preds_phaseA' in globals() and 'refs_phaseA' in globals(), \"Run generation first.\"\n",
        "preds, refs = preds_phaseA, refs_phaseA\n",
        "assert len(preds) == len(refs) and len(preds) > 0, \"Empty preds/refs.\"\n",
        "\n",
        "# ---- canonical ROUGE (HF evaluate) ----\n",
        "try:\n",
        "    import evaluate\n",
        "    rouge = evaluate.load(\"rouge\")\n",
        "    rouge_scores = rouge.compute(\n",
        "        predictions=[p.strip() for p in preds],\n",
        "        references=[r.strip() for r in refs],\n",
        "        use_stemmer=True,\n",
        "    )\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"Install metrics: pip install evaluate rouge-score\") from e\n",
        "\n",
        "def _stats(arr):\n",
        "    a = np.asarray(arr, dtype=float)\n",
        "    return {\n",
        "        \"count\": int(a.size),\n",
        "        \"mean\": float(a.mean()),\n",
        "        \"median\": float(np.median(a)),\n",
        "        \"min\": float(a.min()),\n",
        "        \"p25\": float(np.percentile(a, 25)),\n",
        "        \"p75\": float(np.percentile(a, 75)),\n",
        "        \"max\": float(a.max()),\n",
        "        \"std\": float(a.std()),\n",
        "    }\n",
        "\n",
        "# word lengths\n",
        "ref_w   = [len(t.split()) for t in refs]\n",
        "pred_w  = [len(t.split()) for t in preds]\n",
        "ratio_w = [pw / max(rw, 1) for pw, rw in zip(pred_w, ref_w)]\n",
        "\n",
        "# tokenizer-token lengths (BPE)\n",
        "ref_t   = [len(tok(r, add_special_tokens=False).input_ids) for r in refs]\n",
        "pred_t  = [len(tok(p, add_special_tokens=False).input_ids) for p in preds]\n",
        "ratio_t = [pt / max(rt, 1) for pt, rt in zip(pred_t, ref_t)]\n",
        "\n",
        "# ---- print summary ----\n",
        "print(\"=== ROUGE (canonical) ===\")\n",
        "print({k: float(v) for k, v in rouge_scores.items()})\n",
        "\n",
        "print(\"\\n=== LENGTH STATS (words) ===\")\n",
        "print(\"Ref :\", _stats(ref_w))\n",
        "print(\"Pred:\", _stats(pred_w))\n",
        "print(\"Pred/Ref ratio:\", _stats(ratio_w))\n",
        "\n",
        "print(\"\\n=== LENGTH STATS (tokens) ===\")\n",
        "print(\"Ref :\", _stats(ref_t))\n",
        "print(\"Pred:\", _stats(pred_t))\n",
        "print(\"Pred/Ref ratio:\", _stats(ratio_t))\n",
        "\n",
        "print(f\"\\n[len] mean word-length ratio = {np.mean(ratio_w):.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0BjEvdnr8dK",
        "outputId": "503bd204-f028-4e78-bc1a-f7238f93dc4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================================================================\n",
            "EXAMPLE 0  |  pred_len=33  ref_len=25  len_ratio=1.32  rep3=0.0000\n",
            "- SOURCE (first 600 chars) -\n",
            "Share, and your gift will be multiplied. That may sound like an esoteric adage, but when Zully Broussard selflessly decided to give one of her kidneys to a stranger, her generosity paired up with big data. It resulted in six patients receiving transplants. That surprised and wowed her. \"I thought I was going to help this one person who I don't know, but the fact that so many people can have a life extension, that's pretty big,\" Broussard told CNN affiliate KGO. She may feel guided in her generosity by a higher power. \"Thanks for all the support and prayers,\" a comment on a Facebook page in ...\n",
            "\n",
            "- REFERENCE -\n",
            "Zully Broussard decided to give a kidney to a stranger . A new computer program helped her donation spur transplants for six kidney patients .\n",
            "\n",
            "- PREDICTION -\n",
            "Zully Broussard selflessly decided to give one of her kidneys to a stranger. It resulted in six patients receiving transplants. Doctors are extracting six kidneys from donors and implanting them into six recipients.\n",
            "\n",
            "- ENTITIES -\n",
            "Ref: [('Zully Broussard', 'PERSON'), ('six', 'CARDINAL')]\n",
            "Prd: [('Zully Broussard', 'PERSON'), ('six', 'CARDINAL')]\n",
            "∩   : [('Zully Broussard', 'PERSON'), ('six', 'CARDINAL')]\n",
            "\n",
            "====================================================================================================\n",
            "EXAMPLE 1  |  pred_len=44  ref_len=31  len_ratio=1.42  rep3=0.0000\n",
            "- SOURCE (first 600 chars) -\n",
            "On the 6th of April 1996, San Jose Clash and DC United strode out in front of 31,683 expectant fans at the Spartan Stadium in San Jose, California. The historic occasion was the first ever Major League Soccer match -- a brave new dawn for the world's favorite sport in a land its charms had yet to conquer. Summarizing the action for ESPN, commentator Ty Keough eagerly described the momentous \"birth of a new era for American soccer.\" Looking back at footage from that balmy evening now it's hard not to feel a certain nostalgia. Baggy shirts, questionable hairstyles and strange rule ...\n",
            "\n",
            "- REFERENCE -\n",
            "The 20th MLS season begins this weekend . League has changed dramatically since its inception in 1996 . Some question whether rules regarding salary caps and transfers need to change .\n",
            "\n",
            "- PREDICTION -\n",
            "MLS prepares to mark the beginning of its 20th season. Attendances are higher than ever before and the number of teams involved has doubled from 10 in the 1996 campaign to 20 in 2015. A further four are set to be added by 2020.\n",
            "\n",
            "- ENTITIES -\n",
            "Ref: [('1996', 'DATE'), ('20th', 'ORDINAL'), ('MLS', 'ORG'), ('this weekend', 'DATE')]\n",
            "Prd: [('10', 'CARDINAL'), ('1996', 'DATE'), ('20', 'CARDINAL'), ('2015', 'DATE'), ('2020', 'DATE'), ('20th season', 'DATE'), ('MLS', 'ORG'), ('four', 'CARDINAL')]\n",
            "∩   : [('1996', 'DATE'), ('MLS', 'ORG')]\n",
            "\n",
            "====================================================================================================\n",
            "EXAMPLE 2  |  pred_len=30  ref_len=46  len_ratio=0.65  rep3=0.0000\n",
            "- SOURCE (first 600 chars) -\n",
            "French striker Bafetimbi Gomis, who has a history of fainting, said he is now \"feeling well\" after collapsing during Swansea's 3-2 loss at Tottenham in the Premier League on Wednesday. The worrying incident occurred in the first half at White Hart Lane -- after Tottenham scored in the seventh minute -- but the 29-year-old left the pitch conscious following about five minutes of treatment. The Guardian added that he was wearing an oxygen mask. Play was temporarily stopped before resuming. As the match progressed, Swansea tweeted that Gomis was \"fine,\" with manager Garry Monk using the same ...\n",
            "\n",
            "- REFERENCE -\n",
            "Bafetimbi Gomis collapses within 10 minutes of kickoff at Tottenham . But he reportedly left the pitch conscious and wearing an oxygen mask . Gomis later said that he was \"feeling well\" The incident came three years after Fabrice Muamba collapsed at White Hart Lane .\n",
            "\n",
            "- PREDICTION -\n",
            "French striker Bafetimbi Gomis collapses during Swansea's 3-2 loss at Tottenham. The 29-year-old left the pitch conscious following about five minutes of treatment. The match was temporarily stopped before resuming.\n",
            "\n",
            "- ENTITIES -\n",
            "Ref: [('10 minutes', 'TIME'), ('Bafetimbi Gomis', 'PERSON'), ('Fabrice Muamba', 'PERSON'), ('Tottenham', 'GPE'), ('White Hart Lane', 'FAC'), ('three years', 'DATE')]\n",
            "Prd: [('29-year-old', 'DATE'), ('3', 'CARDINAL'), ('Bafetimbi Gomis', 'PERSON'), ('French', 'NORP'), ('Swansea', 'ORG'), ('Tottenham', 'GPE'), ('about five minutes', 'TIME')]\n",
            "∩   : [('Bafetimbi Gomis', 'PERSON'), ('Tottenham', 'GPE')]\n",
            "\n",
            "====================================================================================================\n",
            "EXAMPLE 3  |  pred_len=42  ref_len=21  len_ratio=2.00  rep3=0.0000\n",
            "- SOURCE (first 600 chars) -\n",
            "It was an act of frustration perhaps more commonly associated with golf's fictional anti-hero Happy Gilmore than the world's reigning No 1. player. But when Rory McIlroy pulled his second shot on the eighth hole of the WGC Cadillac Championship into a lake Friday, he might as well have been channeling the much loved Adam Sandler character. Before continuing his round with a dropped ball, the four-time major winner launched the 3-iron used to play the offending shot into the water as well. \"(It) felt good at the time,\" a rueful McIlroy later said of the incident in comments carried by the ...\n",
            "\n",
            "- REFERENCE -\n",
            "Rory McIlroy throws club into water at WGC Cadillac Championship . Northern Irishman frustrated after pulling shot into water hazard .\n",
            "\n",
            "- PREDICTION -\n",
            "Rory McIlroy pulled his second shot on the eighth hole of the WGC Cadillac Championship into a lake. Before continuing his round with a dropped ball, the four-time major winner launched the 3-iron used to play the offending shot into the water.\n",
            "\n",
            "- ENTITIES -\n",
            "Ref: [('Northern Irishman', 'NORP'), ('Rory McIlroy', 'PERSON'), ('WGC Cadillac Championship', 'ORG')]\n",
            "Prd: [('3', 'CARDINAL'), ('Rory McIlroy', 'PERSON'), ('eighth', 'ORDINAL'), ('four', 'CARDINAL'), ('second', 'ORDINAL'), ('the WGC Cadillac Championship', 'ORG')]\n",
            "∩   : [('Rory McIlroy', 'PERSON')]\n",
            "\n",
            "====================================================================================================\n",
            "EXAMPLE 4  |  pred_len=41  ref_len=17  len_ratio=2.41  rep3=0.0000\n",
            "- SOURCE (first 600 chars) -\n",
            "A Pennsylvania community is pulling together to search for an eighth-grade student who has been missing since Wednesday. The search has drawn hundreds of volunteers on foot and online. The parents of Cayman Naib, 13, have been communicating through the Facebook group \"Find Cayman\" since a day after his disappearance, according to close friend David Binswanger. Newtown Police say Cayman was last seen wearing a gray down winter jacket, black ski pants and hiking boots. He could be in the Radnor-Wayne area, roughly 20 miles from Philadelphia, or may have purchased a train ticket to ...\n",
            "\n",
            "- REFERENCE -\n",
            "Cayman Naib, 13, hasn't been heard from since Wednesday . Police, family, volunteers search for eighth-grader .\n",
            "\n",
            "- PREDICTION -\n",
            "Cayman Naib, 13, was last seen wearing a gray down winter jacket, black ski pants and hiking boots. He could be in the Radnor-Wayne area, roughly 20 miles from Philadelphia. The search has drawn hundreds of volunteers on foot and online.\n",
            "\n",
            "- ENTITIES -\n",
            "Ref: [('13', 'DATE'), ('Cayman Naib', 'PERSON'), ('Wednesday', 'DATE'), ('eighth', 'ORDINAL')]\n",
            "Prd: [('13', 'DATE'), ('Cayman Naib', 'PERSON'), ('Philadelphia', 'GPE'), ('Radnor-Wayne', 'ORG'), ('hundreds', 'CARDINAL'), ('roughly 20 miles', 'QUANTITY'), ('winter', 'DATE')]\n",
            "∩   : [('13', 'DATE'), ('Cayman Naib', 'PERSON')]\n"
          ]
        }
      ],
      "source": [
        "# ============================ CELL — QUALITATIVE EXAMPLES ============================\n",
        "import textwrap, re, numpy as np\n",
        "\n",
        "assert 'srcs_phaseA' in globals() and 'refs_phaseA' in globals() and 'preds_phaseA' in globals()\n",
        "\n",
        "K    = int(globals().get('K_EXAMPLES', 5))      # how many to show\n",
        "IDX  = globals().get('IDX_EXAMPLES', None)      # e.g., [3, 42, 77]; or None to pick first K\n",
        "SNIP = int(globals().get('SOURCE_SNIP', 600))   # source preview width\n",
        "\n",
        "def rep_3gram_rate(text: str) -> float:\n",
        "    toks = text.split()\n",
        "    if len(toks) < 3: return 0.0\n",
        "    seen, repeat = set(), 0\n",
        "    for i in range(len(toks)-2):\n",
        "        g = (toks[i], toks[i+1], toks[i+2])\n",
        "        if g in seen: repeat += 1\n",
        "        seen.add(g)\n",
        "    return repeat / max(1, len(toks)-2)\n",
        "\n",
        "def ents_or_empty(texts):\n",
        "    try:\n",
        "        import spacy\n",
        "        try:\n",
        "            nlp = spacy.load(\"en_core_web_sm\")\n",
        "        except Exception:\n",
        "            return [set() for _ in texts], False\n",
        "    except Exception:\n",
        "        return [set() for _ in texts], False\n",
        "    out = []\n",
        "    for t in texts:\n",
        "        doc = nlp(t)\n",
        "        out.append({(e.text.strip(), e.label_) for e in doc.ents})\n",
        "    return out, True\n",
        "\n",
        "# choose rows\n",
        "if IDX is None:\n",
        "    idxs = list(range(min(K, len(srcs_phaseA))))\n",
        "else:\n",
        "    idxs = list(IDX)[:K]\n",
        "\n",
        "E_ref, has_ner = ents_or_empty([refs_phaseA[i] for i in idxs])\n",
        "E_prd, _       = ents_or_empty([preds_phaseA[i] for i in idxs]) if has_ner else ([set()]*len(idxs), False)\n",
        "\n",
        "for j, i in enumerate(idxs):\n",
        "    src, ref, prd = srcs_phaseA[i], refs_phaseA[i], preds_phaseA[i]\n",
        "    ref_w, prd_w = len(ref.split()), len(prd.split())\n",
        "    ratio = prd_w / max(ref_w, 1)\n",
        "    rep3  = rep_3gram_rate(prd)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(f\"EXAMPLE {i}  |  pred_len={prd_w}  ref_len={ref_w}  len_ratio={ratio:.2f}  rep3={rep3:.4f}\")\n",
        "    print(\"- SOURCE (first {} chars) -\".format(SNIP))\n",
        "    print(textwrap.shorten(src.replace(\"\\n\",\" \"), width=SNIP, placeholder=\" ...\"))\n",
        "\n",
        "    print(\"\\n- REFERENCE -\")\n",
        "    print(ref)\n",
        "\n",
        "    print(\"\\n- PREDICTION -\")\n",
        "    print(prd)\n",
        "\n",
        "    if has_ner:\n",
        "        inter = E_ref[j] & E_prd[j]\n",
        "        print(\"\\n- ENTITIES -\")\n",
        "        print(\"Ref:\", sorted(list(E_ref[j]))[:12])\n",
        "        print(\"Prd:\", sorted(list(E_prd[j]))[:12])\n",
        "        print(\"∩   :\", sorted(list(inter))[:12])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYPAObtLtvbe"
      },
      "source": [
        "# Training 3k - 4k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AovInqlsw1PM"
      },
      "outputs": [],
      "source": [
        "# must be before loading the model\n",
        "import torch\n",
        "try:\n",
        "    from torch.backends.cuda import sdp_kernel\n",
        "    sdp_kernel.enable_flash(False); sdp_kernel.enable_mem_efficient(False); sdp_kernel.enable_math(True)\n",
        "except Exception:\n",
        "    pass\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "try: torch.set_float32_matmul_precision(\"high\")\n",
        "except Exception: pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training was successfully resumed from step 3000 and completed until the target step 4000, with updated weights saved to the checkpoint directory. The subsequent **OutOfMemoryError** occurred only during the evaluation forward pass, after the checkpoint had already been written, so it does not affect the integrity of the saved model. The error reflects GPU memory fragmentation at evaluation time rather than a failure of the training process, and the logs are kept intact to document this outcome.\n"
      ],
      "metadata": {
        "id": "xJqB7rWcGBAo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1133
        },
        "id": "uze7PbCHr9FY",
        "outputId": "fa67beb6-eb84-42f8-a8e8-4bbafe99c44e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[resume] starting at step 3000 with batch=56 x accum=2 (eff=112)\n",
            "[resume] will train until step 4000 (top-up 1000 updates), warmup=30, lr=1e-05\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ep ?] step   3100 | loss  2.7497 | ce  2.3153 | cov  0.3979 | gate 1.8272 | p_copy 0.241 | p_gen 0.759 | lr 9.28e-06 | toks/s   7125.1 | gpu_mem 19812.2 MB\n",
            "[ep ?] step   3200 | loss  2.7406 | ce  2.3112 | cov  0.3932 | gate 1.8112 | p_copy 0.243 | p_gen 0.757 | lr 8.25e-06 | toks/s   7193.8 | gpu_mem 19813.2 MB\n",
            "[ep ?] step   3300 | loss  2.7202 | ce  2.3013 | cov  0.3833 | gate 1.7738 | p_copy 0.249 | p_gen 0.751 | lr 7.22e-06 | toks/s   7270.7 | gpu_mem 19813.3 MB\n",
            "[ep ?] step   3400 | loss  2.7044 | ce  2.2946 | cov  0.3749 | gate 1.7441 | p_copy 0.253 | p_gen 0.747 | lr 6.19e-06 | toks/s   7282.8 | gpu_mem 19813.3 MB\n",
            "[ep ?] step   3500 | loss  2.6952 | ce  2.2913 | cov  0.3693 | gate 1.7284 | p_copy 0.255 | p_gen 0.745 | lr 5.15e-06 | toks/s   7288.8 | gpu_mem 19813.6 MB\n",
            "[ep ?] step   3600 | loss  2.6929 | ce  2.2941 | cov  0.3644 | gate 1.7186 | p_copy 0.257 | p_gen 0.743 | lr 4.12e-06 | toks/s   7291.6 | gpu_mem 19813.6 MB\n",
            "[ep ?] step   3700 | loss  2.6975 | ce  2.3026 | cov  0.3608 | gate 1.7098 | p_copy 0.258 | p_gen 0.743 | lr 3.09e-06 | toks/s   7291.9 | gpu_mem 19815.2 MB\n",
            "[ep ?] step   3800 | loss  2.6869 | ce  2.2954 | cov  0.3575 | gate 1.6983 | p_copy 0.259 | p_gen 0.741 | lr 2.06e-06 | toks/s   7292.8 | gpu_mem 19815.2 MB\n",
            "[ep ?] step   3900 | loss  2.6686 | ce  2.2796 | cov  0.3553 | gate 1.6851 | p_copy 0.263 | p_gen 0.737 | lr 1.03e-06 | toks/s   7293.9 | gpu_mem 19815.2 MB\n",
            "[ep ?] step   4000 | loss  2.6613 | ce  2.2741 | cov  0.3536 | gate 1.6780 | p_copy 0.263 | p_gen 0.737 | lr 0.00e+00 | toks/s   7294.0 | gpu_mem 19815.2 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3917: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  torch.set_default_dtype(dtype_orig)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[checkpoint] saved → /content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000\n",
            "[train] Reached TARGET_STEPS=4000. Stopping.\n",
            "[checkpoint] saved → /content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 796.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 773.38 MiB is free. Process 26818 has 21.40 GiB memory in use. Of the allocated memory 19.24 GiB is allocated by PyTorch, and 1.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2395815286.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFP16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0mvo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mvb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                 \u001b[0mval_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mval_seen\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3685150595.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, labels, decoder_input_ids, decoder_attention_mask, output_attentions, output_hidden_states, past_key_values, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mV\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mcopy_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mbatch_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 796.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 773.38 MiB is free. Process 26818 has 21.40 GiB memory in use. Of the allocated memory 19.24 GiB is allocated by PyTorch, and 1.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "# ===== Resume training for +1000 steps from ckpt_step3000 (Phase B: pointer nudge) =====\n",
        "import os, time, torch\n",
        "from torch import amp\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "\n",
        "# --- paths & resume point ---\n",
        "RESUME_CKPT = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step3000\"\n",
        "RESUME_STEP = 3000\n",
        "TARGET_STEPS = 4000\n",
        "\n",
        "# --- training knobs  ---\n",
        "BATCH_SIZE         = 56\n",
        "GRAD_ACCUM_STEPS   = 2\n",
        "NUM_WORKERS        = 4\n",
        "PERSISTENT_WORKERS = True\n",
        "PREFETCH_FACTOR    = 2\n",
        "FP16               = True\n",
        "LR                 = 1e-5\n",
        "WARMUP_RATIO       = 0.03\n",
        "WEIGHT_DECAY       = 0.01\n",
        "LOG_EVERY          = 100\n",
        "SAVE_EVERY_STEPS   = 1000\n",
        "\n",
        "# --- pointer/coverage (PHASE B NUDGE) ---\n",
        "LAMBDA_COV  = float(globals().get(\"LAMBDA_COV\", 1.2))   # was 1.0\n",
        "LAMBDA_GATE = float(globals().get(\"LAMBDA_GATE\", 0.02)) # tiny pointer-head nudge\n",
        "EPS         = float(globals().get(\"EPS\", 1e-8))\n",
        "USE_POINTER = bool(globals().get(\"USE_POINTER\", True))\n",
        "\n",
        "# --- lengths (your standard 400/100) ---\n",
        "MAX_SRC_LEN = int(globals().get(\"MAX_SRC_LEN\", 400))\n",
        "MAX_TGT_LEN = int(globals().get(\"MAX_TGT_LEN\", 100))\n",
        "\n",
        "# --- data paths---\n",
        "TRAIN_CSV = globals().get(\"TRAIN_CSV\", \"/content/drive/MyDrive/cleaned_outputs/naa/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast_applied.csv\")\n",
        "VAL_CSV   = globals().get(\"VAL_CSV\",   \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---- reload tokenizer + base from checkpoint (ensure attentions are returned) ----\n",
        "tok = AutoTokenizer.from_pretrained(RESUME_CKPT, use_fast=True)\n",
        "cfg = AutoConfig.from_pretrained(RESUME_CKPT)\n",
        "cfg.attn_implementation  = \"eager\"\n",
        "cfg.output_attentions    = True\n",
        "cfg.output_hidden_states = True\n",
        "\n",
        "base = AutoModelForSeq2SeqLM.from_pretrained(RESUME_CKPT, config=cfg)\n",
        "base.config.use_cache            = False\n",
        "base.config.output_attentions    = True\n",
        "base.config.output_hidden_states = True\n",
        "base.gradient_checkpointing_enable()\n",
        "\n",
        "model = CopyAwareBart(base, tok, lambda_cov=LAMBDA_COV, eps=EPS, use_pointer=USE_POINTER).to(DEVICE)\n",
        "\n",
        "# ---- pointer head restore  ----\n",
        "ptr_path = os.path.join(RESUME_CKPT, \"pointer_head.pt\")\n",
        "if os.path.exists(ptr_path):\n",
        "    state = torch.load(ptr_path, map_location=\"cpu\")\n",
        "    model.p_gen_linear.load_state_dict(state[\"p_gen_linear\"])\n",
        "    model.lambda_cov = float(state.get(\"lambda_cov\", model.lambda_cov))\n",
        "    model.use_ptr    = bool(state.get(\"use_pointer\", model.use_ptr))\n",
        "else:\n",
        "    print(\"[warn] pointer_head.pt not found — continuing with current pointer params\")\n",
        "\n",
        "# ----  encoder is UNFROZEN for this phase ----\n",
        "for p in model.base.model.encoder.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "# ---- data (assumes these utilities/classes exist in your notebook) ----\n",
        "train_df = robust_read_csv(TRAIN_CSV)\n",
        "val_df   = robust_read_csv(VAL_CSV)\n",
        "collate  = PGDataCollator(tok=tok, max_src_len=MAX_SRC_LEN, max_tgt_len=MAX_TGT_LEN)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    CNNDMDataset(train_df),\n",
        "    batch_size=BATCH_SIZE, shuffle=True,\n",
        "    num_workers=NUM_WORKERS, pin_memory=True,\n",
        "    persistent_workers=PERSISTENT_WORKERS, prefetch_factor=PREFETCH_FACTOR,\n",
        "    collate_fn=collate,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    CNNDMDataset(val_df),\n",
        "    batch_size=BATCH_SIZE, shuffle=False,\n",
        "    num_workers=NUM_WORKERS, pin_memory=True,\n",
        "    persistent_workers=PERSISTENT_WORKERS, prefetch_factor=PREFETCH_FACTOR,\n",
        "    collate_fn=collate,\n",
        ")\n",
        "\n",
        "# ---- sanity: verify cross-attentions exist once before training ----\n",
        "_sanity_batch = next(iter(val_loader))\n",
        "for k in _sanity_batch: _sanity_batch[k] = _sanity_batch[k].to(DEVICE)\n",
        "with torch.no_grad():\n",
        "    def _shift_right(labels, start_id, pad_id):\n",
        "        y_in = labels.clone()\n",
        "        y_in[y_in == -100] = pad_id\n",
        "        y_in = torch.roll(y_in, 1, dims=1)\n",
        "        y_in[:, 0] = start_id\n",
        "        return y_in\n",
        "    y_in = _shift_right(_sanity_batch[\"labels\"], base.config.decoder_start_token_id, tok.pad_token_id)\n",
        "    o = base(\n",
        "        input_ids=_sanity_batch[\"input_ids\"],\n",
        "        attention_mask=_sanity_batch[\"attention_mask\"],\n",
        "        decoder_input_ids=y_in,\n",
        "        output_attentions=True, output_hidden_states=True,\n",
        "        use_cache=False, return_dict=True\n",
        "    )\n",
        "    assert o.cross_attentions is not None and o.cross_attentions[-1] is not None, \\\n",
        "        \"cross_attentions is None — check SDPA/attn_implementation/output_attentions.\"\n",
        "del _sanity_batch, o; torch.cuda.empty_cache()\n",
        "\n",
        "# ---- optimizer (fresh) ----\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\"]\n",
        "params_decay, params_nodecay = [], []\n",
        "for n, p in model.named_parameters():\n",
        "    if not p.requires_grad:\n",
        "        continue\n",
        "    (params_nodecay if any(nd in n for nd in no_decay) else params_decay).append(p)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [\n",
        "        {\"params\": params_decay,   \"weight_decay\": WEIGHT_DECAY, \"lr\": LR, \"initial_lr\": LR},\n",
        "        {\"params\": params_nodecay, \"weight_decay\": 0.0,          \"lr\": LR, \"initial_lr\": LR},\n",
        "    ],\n",
        "    lr=LR,\n",
        ")\n",
        "\n",
        "# ---- scheduler for the TOP-UP only (fresh warmup) ----\n",
        "steps_remaining = max(1, TARGET_STEPS - RESUME_STEP)\n",
        "warmup_steps    = max(1, int(WARMUP_RATIO * steps_remaining))\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=steps_remaining,\n",
        ")\n",
        "\n",
        "scaler = amp.GradScaler(\"cuda\", enabled=FP16)\n",
        "\n",
        "# ---- tiny running mean helper (no external deps) ----\n",
        "class _RM:\n",
        "    def __init__(self, w=200): self.w=w; self.buf=[]\n",
        "    def add(self, x):\n",
        "        if x is None: return\n",
        "        self.buf.append(float(x))\n",
        "        if len(self.buf)>self.w: self.buf.pop(0)\n",
        "    @property\n",
        "    def mean(self):\n",
        "        return float(sum(self.buf)/len(self.buf)) if self.buf else 0.0\n",
        "\n",
        "m_loss=_RM(); m_ce=_RM(); m_cov=_RM(); m_gate=_RM(); m_pcopy=_RM(); m_pgen=_RM(); m_toks=_RM()\n",
        "\n",
        "# ---- checkpoint saver  ----\n",
        "def save_checkpoint(tag):\n",
        "    path = os.path.join(RESUME_CKPT.rsplit(\"/\",1)[0], f\"ckpt_step{tag}\")\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    model.base.save_pretrained(path)\n",
        "    tok.save_pretrained(path)\n",
        "    torch.save({\n",
        "        \"p_gen_linear\": model.p_gen_linear.state_dict(),\n",
        "        \"lambda_cov\": float(model.lambda_cov),\n",
        "        \"use_pointer\": bool(model.use_ptr),\n",
        "    }, os.path.join(path, \"pointer_head.pt\"))\n",
        "    print(f\"[checkpoint] saved → {path}\")\n",
        "\n",
        "# ---- helpers for gate reg (no backprop through base) ----\n",
        "def _gate_regularizer_only_ptr(model, dec_hid, enc_out, attn_n, dec_inp_ids, src_ids):\n",
        "    # context + emb (detached inputs), only p_gen_linear has gradients\n",
        "    with torch.no_grad():\n",
        "        ctx = torch.bmm(attn_n, enc_out)                                 # [B,T,H]\n",
        "        dec_emb = model.base.get_input_embeddings()(dec_inp_ids).detach() # [B,T,H]\n",
        "    cat   = torch.cat([dec_hid, ctx.detach(), dec_emb], dim=-1)           # [B,T,3H]\n",
        "    p_gen = torch.sigmoid(model.p_gen_linear(cat)).squeeze(-1)            # [B,T]\n",
        "    p_copy = (1.0 - p_gen).clamp_min(1e-6)\n",
        "\n",
        "    # mask: encourage copy where target id appears in source OR looks numeric\n",
        "    with torch.no_grad():\n",
        "        src_sets = [set(x.tolist()) for x in src_ids]\n",
        "        mask = torch.zeros_like(p_copy, dtype=torch.bool)\n",
        "        for b in range(dec_inp_ids.size(0)):\n",
        "            ids = dec_inp_ids[b]\n",
        "            keep = torch.tensor([(i.item() in src_sets[b]) for i in ids], device=ids.device, dtype=torch.bool)\n",
        "            toks = tok.convert_ids_to_tokens(ids.tolist())\n",
        "            has_digit = torch.tensor([any(ch.isdigit() for ch in t.replace(\"Ġ\",\"\")) for t in toks], device=ids.device, dtype=torch.bool)\n",
        "            m = keep | has_digit\n",
        "            m[0] = False  # ignore BOS\n",
        "            mask[b] = m\n",
        "    if mask.any():\n",
        "        gate_loss = -torch.log(p_copy)[mask].mean()\n",
        "    else:\n",
        "        gate_loss = torch.tensor(0.0, device=p_copy.device)\n",
        "    return gate_loss, p_copy.detach().mean()\n",
        "\n",
        "def _shift_right(labels, start_id, pad_id):\n",
        "    y_in = labels.clone()\n",
        "    y_in[y_in == -100] = pad_id\n",
        "    y_in = torch.roll(y_in, 1, dims=1)\n",
        "    y_in[:, 0] = start_id\n",
        "    return y_in\n",
        "\n",
        "@torch.no_grad()\n",
        "def _get_dec_feats_for_gate(base_model, batch):\n",
        "    out = base_model(\n",
        "        input_ids=batch[\"input_ids\"],\n",
        "        attention_mask=batch[\"attention_mask\"],\n",
        "        decoder_input_ids=_shift_right(batch[\"labels\"], base_model.config.decoder_start_token_id, tok.pad_token_id),\n",
        "        output_attentions=True, output_hidden_states=True,\n",
        "        use_cache=False, return_dict=True,\n",
        "    )\n",
        "    dec_hid = out.decoder_hidden_states[-1].detach()        # [B,T,H]\n",
        "    enc_out = out.encoder_last_hidden_state.detach()         # [B,S,H]\n",
        "    cross   = out.cross_attentions[-1].mean(dim=1).detach()  # [B,T,S]\n",
        "    if batch[\"attention_mask\"] is not None:\n",
        "        cross = cross.masked_fill(batch[\"attention_mask\"].unsqueeze(1)==0, 0.0)\n",
        "    denom = cross.sum(dim=-1, keepdim=True).clamp_min(1e-8)\n",
        "    attn_n = cross / denom\n",
        "    return dec_hid, enc_out, attn_n\n",
        "\n",
        "# ---- training loop  ----\n",
        "global_step = RESUME_STEP\n",
        "print(f\"[resume] starting at step {global_step} with batch={BATCH_SIZE} x accum={GRAD_ACCUM_STEPS} (eff={BATCH_SIZE*GRAD_ACCUM_STEPS})\")\n",
        "print(f\"[resume] will train until step {TARGET_STEPS} (top-up {TARGET_STEPS-RESUME_STEP} updates), warmup={warmup_steps}, lr={LR:g}\")\n",
        "\n",
        "model.train()\n",
        "optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "start_time = time.time()\n",
        "tokens_seen = 0\n",
        "updates_done = 0\n",
        "\n",
        "for epoch in range(10**9):  # loop until TARGET_STEPS\n",
        "    for it, batch in enumerate(train_loader):\n",
        "        for k in batch: batch[k] = batch[k].to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            toks = int(batch[\"attention_mask\"].sum().item())\n",
        "            if \"labels\" in batch:\n",
        "                toks += int((batch[\"labels\"] != -100).sum().item())\n",
        "\n",
        "        with amp.autocast(\"cuda\", enabled=FP16):\n",
        "            out = model(**batch)                     # wrapper computes CE + coverage (λ = LAMBDA_COV)\n",
        "            loss = out.loss\n",
        "\n",
        "            # ----- Phase B: tiny gate regularizer (updates pointer head only) -----\n",
        "            gate_loss  = torch.tensor(0.0, device=DEVICE)\n",
        "            p_copy_ave = torch.tensor(0.0, device=DEVICE)\n",
        "            if LAMBDA_GATE > 0.0 and hasattr(model, \"p_gen_linear\"):\n",
        "                dec_hid, enc_out, attn_n = _get_dec_feats_for_gate(model.base, batch)  # no grad through base\n",
        "                dec_inp_ids = _shift_right(batch[\"labels\"], model.base.config.decoder_start_token_id, tok.pad_token_id)\n",
        "                g_loss, p_copy_mean = _gate_regularizer_only_ptr(model, dec_hid, enc_out, attn_n, dec_inp_ids, batch[\"input_ids\"])\n",
        "                gate_loss  = g_loss\n",
        "                p_copy_ave = p_copy_mean\n",
        "                loss = loss + LAMBDA_GATE * gate_loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        took_step = False\n",
        "        if (it + 1) % GRAD_ACCUM_STEPS == 0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(optimizer); scaler.update()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            scheduler.step()\n",
        "\n",
        "            global_step += 1\n",
        "            updates_done += 1\n",
        "            took_step = True\n",
        "\n",
        "            tokens_seen += toks\n",
        "            m_loss.add(loss.item())\n",
        "            m_ce.add(getattr(model, \"_last_ce_loss\", None))\n",
        "            m_cov.add(getattr(model, \"_last_cov_loss\", None))\n",
        "            m_gate.add(float(gate_loss.detach().cpu()) if torch.is_tensor(gate_loss) else None)\n",
        "            _last_pcopy = getattr(model, \"_last_p_copy_mean\", None)\n",
        "            m_pcopy.add(_last_pcopy if _last_pcopy is not None else float(p_copy_ave.detach().cpu()))\n",
        "            m_pgen.add(getattr(model, \"_last_p_gen_mean\", None))\n",
        "            m_toks.add(tokens_seen / max(1e-6, time.time()-start_time))\n",
        "\n",
        "            if global_step % LOG_EVERY == 0:\n",
        "                lr_now = scheduler.get_last_lr()[0]\n",
        "                mem_mb = torch.cuda.max_memory_allocated() / (1024**2) if torch.cuda.is_available() else 0.0\n",
        "                print(\n",
        "                    f\"[ep ?] step {global_step:>6} | \"\n",
        "                    f\"loss {m_loss.mean:7.4f} | ce {m_ce.mean:7.4f} | cov {m_cov.mean:7.4f} | gate {m_gate.mean:6.4f} | \"\n",
        "                    f\"p_copy {m_pcopy.mean:5.3f} | p_gen {m_pgen.mean:5.3f} | \"\n",
        "                    f\"lr {lr_now:.2e} | toks/s {m_toks.mean:8.1f} | gpu_mem {mem_mb:7.1f} MB\"\n",
        "                )\n",
        "\n",
        "            if global_step % SAVE_EVERY_STEPS == 0:\n",
        "                save_checkpoint(global_step)\n",
        "\n",
        "            if updates_done >= (TARGET_STEPS - RESUME_STEP) or global_step >= TARGET_STEPS:\n",
        "                print(f\"[train] Reached TARGET_STEPS={TARGET_STEPS}. Stopping.\")\n",
        "                save_checkpoint(global_step)\n",
        "                break\n",
        "\n",
        "        if torch.cuda.is_available(): torch.cuda.synchronize()\n",
        "        del out\n",
        "        if took_step: torch.cuda.empty_cache()\n",
        "\n",
        "    if updates_done >= (TARGET_STEPS - RESUME_STEP) or global_step >= TARGET_STEPS:\n",
        "        break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06e3n0BowBfw",
        "outputId": "ebc156ae-412c-4705-804c-2a9daf7bd5d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[val] loss 2.3038 | ppl 10.01 (pointer active, AMP)\n"
          ]
        }
      ],
      "source": [
        "# ---- QUICK VAL LOSS (pointer active + AMP) ----\n",
        "model.eval()\n",
        "VAL_BATCH =16\n",
        "val_loss, val_seen = 0.0, 0\n",
        "\n",
        "AMP_DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "with torch.no_grad():\n",
        "    val_loader_small = DataLoader(\n",
        "        CNNDMDataset(val_df),\n",
        "        batch_size=VAL_BATCH, shuffle=False,\n",
        "        num_workers=2, pin_memory=True,\n",
        "        collate_fn=collate\n",
        "    )\n",
        "\n",
        "    for vb in val_loader_small:\n",
        "        for k in vb: vb[k] = vb[k].to(DEVICE)\n",
        "        with torch.autocast(device_type=\"cuda\", dtype=AMP_DTYPE):\n",
        "            vo = model(**vb)   # pointer ON\n",
        "            if vo.loss is not None:\n",
        "                val_loss += vo.loss.item()\n",
        "                val_seen += 1\n",
        "        # free memory between batches\n",
        "        del vb, vo\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "if val_seen:\n",
        "    avg_loss = val_loss / val_seen\n",
        "    ppl = torch.exp(torch.tensor(avg_loss))\n",
        "    print(f\"[val] loss {avg_loss:.4f} | ppl {ppl:.2f} (pointer active, AMP)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fnUd0DjICJn",
        "outputId": "9495ad25-be82-4e3b-fef7-4cb92b3ff83e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[patch] CopyAwareBart now returns pointer-aware logits during generation.\n",
            "[patch] CopyAwareBart now has .generate() method.\n"
          ]
        }
      ],
      "source": [
        "# ===== PATCH CopyAwareBart TO SUPPORT GENERATE =====\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def copyaware_forward_for_generate(\n",
        "    self, input_ids=None, attention_mask=None,\n",
        "    decoder_input_ids=None, decoder_attention_mask=None,\n",
        "    output_attentions=None, output_hidden_states=None,\n",
        "    past_key_values=None, use_cache=None, labels=None, **kwargs\n",
        "):\n",
        "    \"\"\"\n",
        "    Pointer-aware forward pass.\n",
        "    - Training (labels != None): your normal loss logic is used (already defined earlier).\n",
        "    - Generation (labels=None): returns pointer-fused logits so .generate() uses copy mechanism.\n",
        "    \"\"\"\n",
        "    # run base model without labels\n",
        "    out = self.base(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        decoder_input_ids=decoder_input_ids,\n",
        "        decoder_attention_mask=decoder_attention_mask,\n",
        "        output_attentions=True,\n",
        "        output_hidden_states=True,\n",
        "        past_key_values=past_key_values,\n",
        "        use_cache=use_cache,\n",
        "        return_dict=True,\n",
        "    )\n",
        "\n",
        "    logits = out.logits  # [B,T,V]\n",
        "    if not getattr(self, \"use_ptr\", True):\n",
        "        return out  # plain logits if pointer disabled\n",
        "\n",
        "    B,T,V = logits.shape\n",
        "    attn = out.cross_attentions[-1].mean(dim=1)  # [B,T,S]\n",
        "    if attention_mask is not None:\n",
        "        attn = attn.masked_fill(attention_mask[:,None,:]==0, 0.0)\n",
        "    attn = attn / attn.sum(-1, keepdim=True).clamp_min(1e-8)\n",
        "\n",
        "    dec_hid = out.decoder_hidden_states[-1]  # [B,T,H]\n",
        "    ctx     = torch.bmm(attn, out.encoder_last_hidden_state)\n",
        "    dec_inp = self.base.get_input_embeddings()(decoder_input_ids)\n",
        "    cat     = torch.cat([dec_hid, ctx, dec_inp], dim=-1)\n",
        "    p_gen   = torch.sigmoid(self.p_gen_linear(cat)).squeeze(-1)  # [B,T]\n",
        "    p_copy  = 1.0 - p_gen\n",
        "\n",
        "    # build copy distribution in vocab space\n",
        "    copy_logits = torch.full_like(logits, -1e9)\n",
        "    copy_logits.scatter_add_(\n",
        "        2,\n",
        "        input_ids[:,None,:].expand(B,T,-1),\n",
        "        attn.unsqueeze(1).expand(B,T,-1)\n",
        "    )\n",
        "    copy_probs = copy_logits.log_softmax(dim=-1)\n",
        "\n",
        "    logp_vocab = F.log_softmax(logits, dim=-1)\n",
        "    logp_final = torch.logaddexp(\n",
        "        (p_gen.unsqueeze(-1).log() + logp_vocab),\n",
        "        (p_copy.unsqueeze(-1).log() + copy_probs)\n",
        "    )\n",
        "\n",
        "    out.logits = logp_final\n",
        "    return out\n",
        "\n",
        "# Patch the forward\n",
        "CopyAwareBart.forward = copyaware_forward_for_generate\n",
        "print(\"[patch] CopyAwareBart now returns pointer-aware logits during generation.\")\n",
        "# --- add generate delegation to CopyAwareBart ---\n",
        "def copyaware_generate(self, *args, **kwargs):\n",
        "    \"\"\"Delegate to base.generate() but still use our patched forward for logits.\"\"\"\n",
        "    return self.base.generate(*args, **kwargs)\n",
        "\n",
        "CopyAwareBart.generate = copyaware_generate\n",
        "print(\"[patch] CopyAwareBart now has .generate() method.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pROLvYs-GSwL",
        "outputId": "3ba24c52-4b99-4881-abb5-4c026e3f1662"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[gen] device=cuda:0 dtype=torch.bfloat16\n",
            "[eos-bias] min_len=28 trigger_len=48 (ref_median≈40)\n",
            "[gen] 1000 summaries in 158.0s (~6.3/s)  batch=6 beams=5\n",
            "[cuda] max_alloc=7.57 GB | max_reserved=12.75 GB\n",
            "[Phase A''] preds_phaseA: 1000 summaries ready.\n"
          ]
        }
      ],
      "source": [
        "# ============== CELL B'' — FAST GEN (pointer-aware, EOS bias) ==============\n",
        "import time, torch\n",
        "from transformers import GenerationConfig, LogitsProcessor, LogitsProcessorList\n",
        "\n",
        "assert 'model' in globals() and 'tok' in globals() and 'val_df' in globals()\n",
        "\n",
        "# ---- force CUDA + AMP dtype ----\n",
        "DEVICE    = torch.device(\"cuda:0\")\n",
        "AMP_DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "GENERATOR = model\n",
        "GENERATOR.to(DEVICE, dtype=AMP_DTYPE).eval()\n",
        "print(f\"[gen] device={next(GENERATOR.parameters()).device} dtype={next(GENERATOR.parameters()).dtype}\")\n",
        "\n",
        "# ---- knobs ----\n",
        "MAX_SRC_LEN     = int(globals().get('MAX_SRC_LEN', 400))\n",
        "N_EVAL          = int(globals().get('N_EVAL', 1000))\n",
        "BATCH_GEN       = int(globals().get('BATCH_GEN', 128))  # reduce if OOM\n",
        "NUM_BEAMS       = 5\n",
        "MIN_NEW         = 26\n",
        "MAX_NEW         = 60\n",
        "NO_REPEAT       = 4\n",
        "LENGTH_PENALTY  = 3.0\n",
        "\n",
        "# EOS bias\n",
        "USE_EOS_BIAS    = True\n",
        "EOS_TARGET_SCALE= 1.22\n",
        "EOS_MIN_SCALE   = 0.72\n",
        "PRE_EOS_BIAS    = -3.2\n",
        "POST_EOS_BIAS   = 3.6\n",
        "\n",
        "if hasattr(GENERATOR, \"base\") and hasattr(GENERATOR.base, \"config\"):\n",
        "    GENERATOR.base.config.use_cache = True\n",
        "\n",
        "gen_cfg = GenerationConfig(\n",
        "    num_beams=NUM_BEAMS,\n",
        "    min_new_tokens=MIN_NEW,\n",
        "    max_new_tokens=MAX_NEW,\n",
        "    no_repeat_ngram_size=NO_REPEAT,\n",
        "    length_penalty=LENGTH_PENALTY,\n",
        "    early_stopping=True,\n",
        ")\n",
        "\n",
        "# ---- slice data ----\n",
        "_rows = val_df.iloc[:N_EVAL]\n",
        "srcs_phaseA = _rows[\"article\"].astype(str).tolist()\n",
        "refs_phaseA = _rows[\"highlights\"].astype(str).tolist()\n",
        "\n",
        "# ---- EOS length-bias processor ----\n",
        "class EosAfterNTokens(LogitsProcessor):\n",
        "    def __init__(self, eos_id, min_len, trigger_len, pre_bias=-3.2, post_bias=3.6):\n",
        "        self.eos_id = int(eos_id); self.min_len = int(min_len); self.trigger_len = int(trigger_len)\n",
        "        self.pre_bias = float(pre_bias); self.post_bias = float(post_bias)\n",
        "    def __call__(self, input_ids, scores):\n",
        "        new_len = input_ids.shape[1] - 1\n",
        "        if new_len < self.min_len:\n",
        "            scores[:, self.eos_id] += self.pre_bias\n",
        "        elif new_len >= self.trigger_len:\n",
        "            scores[:, self.eos_id] += self.post_bias\n",
        "        return scores\n",
        "\n",
        "logits_proc = None\n",
        "if USE_EOS_BIAS:\n",
        "    ref_tok = [len(tok(r, add_special_tokens=False).input_ids) for r in refs_phaseA]\n",
        "    ref_med = int(torch.tensor(ref_tok).median().item())\n",
        "    tgt_len = int(max(1, ref_med) * EOS_TARGET_SCALE)\n",
        "    min_len = int(max(1, ref_med) * EOS_MIN_SCALE)\n",
        "    logits_proc = LogitsProcessorList([\n",
        "        EosAfterNTokens(GENERATOR.base.config.eos_token_id, min_len, tgt_len, PRE_EOS_BIAS, POST_EOS_BIAS)\n",
        "    ])\n",
        "    print(f\"[eos-bias] min_len={min_len} trigger_len={tgt_len} (ref_median≈{ref_med})\")\n",
        "\n",
        "# ---- fast batched generation ----\n",
        "def generate_batched(srcs):\n",
        "    preds, t0 = [], time.time()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    for i in range(0, len(srcs), BATCH_GEN):\n",
        "        batch = srcs[i:i+BATCH_GEN]\n",
        "        with torch.inference_mode(), torch.autocast(device_type=\"cuda\", dtype=AMP_DTYPE):\n",
        "            ins = tok(batch, return_tensors=\"pt\", truncation=True, max_length=MAX_SRC_LEN,\n",
        "                      padding=True, pad_to_multiple_of=16).to(DEVICE)\n",
        "            outs = GENERATOR.generate(**ins, generation_config=gen_cfg, logits_processor=logits_proc)\n",
        "        preds.extend(tok.batch_decode(outs, skip_special_tokens=True))\n",
        "        del ins, outs\n",
        "        torch.cuda.empty_cache()\n",
        "    dt = time.time() - t0\n",
        "    used = torch.cuda.max_memory_allocated() / (1024**3)\n",
        "    resv = torch.cuda.max_memory_reserved() / (1024**3)\n",
        "    print(f\"[gen] {len(srcs)} summaries in {dt:.1f}s (~{len(srcs)/max(dt,1):.1f}/s)  batch={BATCH_GEN} beams={NUM_BEAMS}\")\n",
        "    print(f\"[cuda] max_alloc={used:.2f} GB | max_reserved={resv:.2f} GB\")\n",
        "    return preds\n",
        "\n",
        "preds_phaseA = generate_batched(srcs_phaseA)\n",
        "print(f\"[Phase A''] preds_phaseA: {len(preds_phaseA)} summaries ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_G3lhPIpIMS9",
        "outputId": "5ede41d2-0861-4b52-b359-6af99042ef26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ROUGE SCORES ===\n",
            "rouge1: 0.3184\n",
            "rouge2: 0.1194\n",
            "rougeL: 0.2242\n",
            "rougeLsum: 0.2243\n",
            "\n",
            "=== TOKEN LENGTHS (tokenizer) ===\n",
            "Ref : {'mean': np.float64(41.316), 'median': np.float64(40.0), 'p25': np.float64(32.0), 'p50': np.float64(40.0), 'p75': np.float64(50.0)}\n",
            "Pred: {'mean': np.float64(53.742), 'median': np.float64(55.0), 'p25': np.float64(51.0), 'p50': np.float64(55.0), 'p75': np.float64(58.0)}\n",
            "\n",
            "=== WORD LENGTHS (whitespace) ===\n",
            "Ref : {'mean': np.float64(34.373), 'median': np.float64(33.0), 'p25': np.float64(27.0), 'p50': np.float64(33.0), 'p75': np.float64(41.0)}\n",
            "Pred: {'mean': np.float64(42.773), 'median': np.float64(43.0), 'p25': np.float64(39.0), 'p50': np.float64(43.0), 'p75': np.float64(47.0)}\n"
          ]
        }
      ],
      "source": [
        "# ===== Evaluate ROUGE + Length stats =====\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "\n",
        "refs = refs_phaseA\n",
        "preds = preds_phaseA\n",
        "\n",
        "# --- ROUGE ---\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "rouge_res = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
        "print(\"=== ROUGE SCORES ===\")\n",
        "for k,v in rouge_res.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "# --- Length stats (tokens + words) ---\n",
        "tok_lens_ref  = [len(tok(r, add_special_tokens=False).input_ids) for r in refs]\n",
        "tok_lens_pred = [len(tok(p, add_special_tokens=False).input_ids) for p in preds]\n",
        "\n",
        "print(\"\\n=== TOKEN LENGTHS (tokenizer) ===\")\n",
        "print(\"Ref :\", dict(mean=np.mean(tok_lens_ref), median=np.median(tok_lens_ref),\n",
        "                  p25=np.percentile(tok_lens_ref,25), p50=np.percentile(tok_lens_ref,50),\n",
        "                  p75=np.percentile(tok_lens_ref,75)))\n",
        "print(\"Pred:\", dict(mean=np.mean(tok_lens_pred), median=np.median(tok_lens_pred),\n",
        "                  p25=np.percentile(tok_lens_pred,25), p50=np.percentile(tok_lens_pred,50),\n",
        "                  p75=np.percentile(tok_lens_pred,75)))\n",
        "\n",
        "word_lens_ref  = [len(r.split()) for r in refs]\n",
        "word_lens_pred = [len(p.split()) for p in preds]\n",
        "\n",
        "print(\"\\n=== WORD LENGTHS (whitespace) ===\")\n",
        "print(\"Ref :\", dict(mean=np.mean(word_lens_ref), median=np.median(word_lens_ref),\n",
        "                  p25=np.percentile(word_lens_ref,25), p50=np.percentile(word_lens_ref,50),\n",
        "                  p75=np.percentile(word_lens_ref,75)))\n",
        "print(\"Pred:\", dict(mean=np.mean(word_lens_pred), median=np.median(word_lens_pred),\n",
        "                  p25=np.percentile(word_lens_pred,25), p50=np.percentile(word_lens_pred,50),\n",
        "                  p75=np.percentile(word_lens_pred,75)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kv2HhPcHKHdo",
        "outputId": "961eb886-4528-4ee7-beaf-dd328a7c1e93"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entity overlap: 1380/4302 = 32.08%\n",
            "Number overlap: 202/655 = 30.84%\n",
            "Copy-rate: mean=0.933, median=0.943\n"
          ]
        }
      ],
      "source": [
        "# ===== Entity + Copy Evaluation =====\n",
        "import spacy\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "refs = refs_phaseA\n",
        "preds = preds_phaseA\n",
        "srcs  = srcs_phaseA   # from earlier slice\n",
        "\n",
        "# --- Named entity overlap (SpaCy) ---\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\",\"tagger\"])\n",
        "\n",
        "def extract_ents(texts):\n",
        "    return [set(ent.text.lower() for ent in nlp(t).ents) for t in texts]\n",
        "\n",
        "ref_ents  = extract_ents(refs)\n",
        "pred_ents = extract_ents(preds)\n",
        "\n",
        "ent_correct = sum(len(re & pe) for re, pe in zip(ref_ents, pred_ents))\n",
        "ent_total   = sum(len(re) for re in ref_ents)\n",
        "ent_acc = ent_correct / max(1, ent_total)\n",
        "\n",
        "print(f\"Entity overlap: {ent_correct}/{ent_total} = {ent_acc:.2%}\")\n",
        "\n",
        "# --- Digit / number overlap ---\n",
        "def extract_numbers(text):\n",
        "    return set(re.findall(r\"\\d+\", text))\n",
        "\n",
        "ref_nums  = [extract_numbers(r) for r in refs]\n",
        "pred_nums = [extract_numbers(p) for p in preds]\n",
        "\n",
        "num_correct = sum(len(rn & pn) for rn, pn in zip(ref_nums, pred_nums))\n",
        "num_total   = sum(len(rn) for rn in ref_nums)\n",
        "num_acc = num_correct / max(1, num_total)\n",
        "\n",
        "print(f\"Number overlap: {num_correct}/{num_total} = {num_acc:.2%}\")\n",
        "\n",
        "# --- Copy-rate (fraction of predicted tokens found in source) ---\n",
        "tok_preds = [p.split() for p in preds]\n",
        "tok_srcs  = [s.split() for s in srcs]\n",
        "\n",
        "copy_rates = []\n",
        "for pred_tokens, src_tokens in zip(tok_preds, tok_srcs):\n",
        "    src_set = set(src_tokens)\n",
        "    copied = sum(1 for t in pred_tokens if t in src_set)\n",
        "    copy_rates.append(copied / max(1, len(pred_tokens)))\n",
        "\n",
        "print(f\"Copy-rate: mean={np.mean(copy_rates):.3f}, median={np.median(copy_rates):.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6VpiOsqKjoc",
        "outputId": "c930883e-3482-44e9-bca2-a37299a8eebd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean BARTScore: -3.3670\n"
          ]
        }
      ],
      "source": [
        "# ===== Fixed HuggingFace-based BARTScore =====\n",
        "import torch, numpy as np\n",
        "from torch.nn import functional as F\n",
        "\n",
        "def compute_bartscore(srcs, tgts, batch_size=4):\n",
        "    scores = []\n",
        "    bart_model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(srcs), batch_size):\n",
        "            src_batch = srcs[i:i+batch_size]\n",
        "            tgt_batch = tgts[i:i+batch_size]\n",
        "\n",
        "            enc = bart_tok(src_batch, return_tensors=\"pt\", padding=True,\n",
        "                           truncation=True, max_length=512).to(DEVICE)\n",
        "            labels = bart_tok(tgt_batch, return_tensors=\"pt\", padding=True,\n",
        "                              truncation=True, max_length=128).input_ids.to(DEVICE)\n",
        "\n",
        "            # Shift labels to create decoder inputs\n",
        "            decoder_input_ids = torch.roll(labels, 1, dims=1)\n",
        "            decoder_input_ids[:,0] = bart_model.config.decoder_start_token_id\n",
        "\n",
        "            outputs = bart_model(**enc, decoder_input_ids=decoder_input_ids)\n",
        "            logits = outputs.logits   # [B, T, V]\n",
        "\n",
        "            # log softmax\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "            # gather log probs of gold tokens\n",
        "            gold_logp = log_probs.gather(-1, labels.unsqueeze(-1)).squeeze(-1)  # [B, T]\n",
        "\n",
        "            # mask out padding\n",
        "            mask = labels.ne(bart_tok.pad_token_id).float()\n",
        "            seq_scores = (gold_logp * mask).sum(dim=1) / mask.sum(dim=1)\n",
        "\n",
        "            scores.extend(seq_scores.cpu().tolist())\n",
        "\n",
        "    return float(np.mean(scores)), scores\n",
        "\n",
        "mean_bartscore, bart_scores_list = compute_bartscore(preds_phaseA, refs_phaseA, batch_size=2)\n",
        "print(f\"Mean BARTScore: {mean_bartscore:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwXHMEwDNIpF",
        "outputId": "ef2f9c59-3d6b-4586-f57d-b4785e73a554"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexity-style: 28.990060450268697\n",
            "Scaled BARTScore (like paper): 336.6953028142452\n"
          ]
        }
      ],
      "source": [
        "# convert mean log prob (≈ -3) into perplexity-like scale\n",
        "ppl = np.exp(-mean_bartscore)   # exp(3.36) ≈ 29\n",
        "print(\"Perplexity-style:\", ppl)\n",
        "\n",
        "# or rescale to 100-point scale\n",
        "scaled = -100 * mean_bartscore\n",
        "print(\"Scaled BARTScore (like paper):\", scaled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hJHyBuxK2DV",
        "outputId": "37e8aed2-97aa-42ae-d398-78110108543c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "ID: 864\n",
            "Article:    The maker of a controversial documentary about rape in India, Leslee Udwin, told CNN she is shocked by the Indian authorities' move to ban the film without even having seen it. The storm over \"India's Daughter\" blew up out of nowhere and is \"based on nothing,\" Udwin said, in an interview for CNN's \"The World Right Now with Hala Gorani.\" \"The tragedy here is it's a missed opportunity for India to a...\n",
            "Reference:  Documentary maker Leslee Udwin says she's shocked by Indian authorities' ban on showing her film . She says the ban is a \"missed opportunity\" for India to show it cares about equality for women . Udwin says the root of the problem of sexual violence in India and elsewhere is gender inequality .\n",
            "Prediction: Leslee Udwin says she is shocked by the Indian authorities' move to ban the film. She says it's a missed opportunity for India to actually show by embracing the film. The film featured an interview with one of the men convicted in an infamous 2012 gang rape case.\n",
            "BARTScore:  -2.517\n",
            "\n",
            "================================================================================\n",
            "ID: 394\n",
            "Article:    When photographer Richard Ross wants to talk to a child at a juvenile detention center, he knocks on their cell door. He asks them if he can come inside. The 67-year-old Californian is used to taking off his shoes when he enters homes, so he does the same in a cell. \"Most of the kids, they've never had that kind of respect,\" he said. \"But I give it to them, I give them the power. I sit on the floo...\n",
            "Reference:  Photographer Richard Ross shows what life is like for girls in a juvenile detention center . He wants to \"wake people up\" about the system and bring about \"immediate change\"\n",
            "Prediction: Richard Ross wants to talk to a child at a juvenile detention center. He asks them if he can come inside. The 67-year-old Californian is used to taking off his shoes when he enters homes. He does the same in a cell.\n",
            "BARTScore:  -3.276\n",
            "\n",
            "================================================================================\n",
            "ID: 776\n",
            "Article:    After postponing its first execution of a woman in 70 years because of \"cloudy\" lethal injection drugs, Georgia has indefinitely postponed at least one other execution until it can analyze the cocktail it uses for the procedures, the state said Tuesday. Kelly Renee Gissendaner was scheduled to die at 7 p.m. ET Monday, but for the second time in less than a week, it was called off. The state postpo...\n",
            "Reference:  Brian Keith Terrell execution halted while Georgia analyzes drugs used for executions . Kelly Gissendaner's execution postponed because drugs \"appeared cloudy,\" Georgia says .\n",
            "Prediction: Kelly Renee Gissendaner was scheduled to die at 7 p.m. ET Monday. But for the second time in less than a week, it was called off. The state postponed the first planned execution because of \"weather and associated scheduling issues,\" department spokeswoman says.\n",
            "BARTScore:  -4.551\n",
            "\n",
            "================================================================================\n",
            "ID: 911\n",
            "Article:    They were not-so-affectionately dubbed the \"bed-wetters\" by then-Obama senior adviser David Plouffe. The definition: Democrats who run to the hills every time there's a bump in the road. The Chicken Littles. The woe-is-us crowd. Well, they're at it again. Not surprisingly, it's because of the Hillary Clinton email brouhaha, an unforced error that is now ricocheting around the political world. And ...\n",
            "Reference:  Gloria Borger: Democratic Party has no real alternative for 2016 and its bench is painfully thin . She says it may not be fun, but Democrats have no choice but to defend Clinton on issues like the email question .\n",
            "Prediction: Democrats don't have the luxury of standing by and seeing how this all plays out, says one senior Democratic strategist. He says there's been a sharp structural decline of elected Democrats everywhere below the presidential level. He says the Clinton behemoth is just too big - too well funded, too well supported\n",
            "BARTScore:  -3.632\n",
            "\n",
            "================================================================================\n",
            "ID: 430\n",
            "Article:    Things have been messy between singer Chris Brown and his now ex-girlfriend Karrueche Tran, and it looks to be about to get messier. Oprah Winfrey's network, OWN, posted a teaser Tuesday for Tran's interview with Iyanla Vanzant. The inspirational speaker and host of \"Iyanla: Fix My Life\" fires off at Tran: \"He betrayed you. He lied to you. He did it all publicly,\" before asking, \"How did you find ...\n",
            "Reference:  Tran and Brown have had an on-again, off again relationship . They broke it off recently amid reports that he fathered child with another woman . He once publicly proclaimed being in love with both her and Rihanna .\n",
            "Prediction: Oprah Winfrey's network, OWN, posted a teaser for Tran's interview. The inspirational speaker and host of \"Iyanla: Fix My Life\" fires off at Tran: \"He betrayed you. He lied to you. He did it all publicly\"\n",
            "BARTScore:  -3.614\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# pick a few random examples\n",
        "idxs = random.sample(range(len(preds_phaseA)), 5)\n",
        "\n",
        "for i in idxs:\n",
        "    print(\"=\"*80)\n",
        "    print(f\"ID: {i}\")\n",
        "    print(f\"Article:    {srcs_phaseA[i][:400]}...\")  # truncate for readability\n",
        "    print(f\"Reference:  {refs_phaseA[i]}\")\n",
        "    print(f\"Prediction: {preds_phaseA[i]}\")\n",
        "    print(f\"BARTScore:  {bart_scores_list[i]:.3f}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90nA81WvM0oY",
        "outputId": "9ad4a989-e2e0-4f27-824e-0ea254be91aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== REPETITION RATES ===\n",
            "3-gram repetition: mean=0.0011, max=0.0476\n",
            "4-gram repetition: mean=0.0000, max=0.0000\n",
            "\n",
            "=== LEAD-3 BASELINE ROUGE ===\n",
            "rouge1: 0.2883\n",
            "rouge2: 0.1070\n",
            "rougeL: 0.1928\n",
            "rougeLsum: 0.1927\n",
            "\n",
            "=== LEAD-3 LENGTHS (tokens) ===\n",
            "{'mean': np.float64(83.578), 'median': np.float64(82.0)}\n"
          ]
        }
      ],
      "source": [
        "# ===== Repetition rate + Lead-3 baseline =====\n",
        "import re\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from nltk.util import ngrams\n",
        "from statistics import mean\n",
        "\n",
        "# --- 1. Repetition rate (n-gram self overlap) ---\n",
        "def repetition_rate(texts, n=4):\n",
        "    rates = []\n",
        "    for t in texts:\n",
        "        toks = t.split()\n",
        "        if len(toks) < n:\n",
        "            continue\n",
        "        ngrams_list = list(ngrams(toks, n))\n",
        "        uniq = set(ngrams_list)\n",
        "        rep = 1 - (len(uniq) / len(ngrams_list))\n",
        "        rates.append(rep)\n",
        "    return np.mean(rates), np.max(rates)\n",
        "\n",
        "rep3_mean, rep3_max = repetition_rate(preds_phaseA, n=3)\n",
        "rep4_mean, rep4_max = repetition_rate(preds_phaseA, n=4)\n",
        "\n",
        "print(\"=== REPETITION RATES ===\")\n",
        "print(f\"3-gram repetition: mean={rep3_mean:.4f}, max={rep3_max:.4f}\")\n",
        "print(f\"4-gram repetition: mean={rep4_mean:.4f}, max={rep4_max:.4f}\")\n",
        "\n",
        "# --- 2. Lead-3 baseline summaries ---\n",
        "def lead3(text):\n",
        "    sents = re.split(r'(?<=[.!?]) +', text)\n",
        "    return \" \".join(sents[:3])\n",
        "\n",
        "lead3_preds = [lead3(src) for src in srcs_phaseA]\n",
        "refs = refs_phaseA\n",
        "\n",
        "# Evaluate Lead-3 with ROUGE\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "lead3_rouge = rouge.compute(predictions=lead3_preds, references=refs, use_stemmer=True)\n",
        "\n",
        "print(\"\\n=== LEAD-3 BASELINE ROUGE ===\")\n",
        "for k,v in lead3_rouge.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "# Length comparison\n",
        "lead3_lens = [len(tok(l, add_special_tokens=False).input_ids) for l in lead3_preds]\n",
        "print(\"\\n=== LEAD-3 LENGTHS (tokens) ===\")\n",
        "print(dict(mean=np.mean(lead3_lens), median=np.median(lead3_lens)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nVrs-geNl-5",
        "outputId": "af0b76ad-0c5e-496d-dd03-2f24c227b292"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================================================================\n",
            "EXAMPLE 0  |  pred_len=46  ref_len=25  len_ratio=1.84  rep3=0.0000\n",
            "- SOURCE (first 600 chars) -\n",
            "Share, and your gift will be multiplied. That may sound like an esoteric adage, but when Zully Broussard selflessly decided to give one of her kidneys to a stranger, her generosity paired up with big data. It resulted in six patients receiving transplants. That surprised and wowed her. \"I thought I was going to help this one person who I don't know, but the fact that so many people can have a life extension, that's pretty big,\" Broussard told CNN affiliate KGO. She may feel guided in her generosity by a higher power. \"Thanks for all the support and prayers,\" a comment on a Facebook page in ...\n",
            "\n",
            "- REFERENCE -\n",
            "Zully Broussard decided to give a kidney to a stranger . A new computer program helped her donation spur transplants for six kidney patients .\n",
            "\n",
            "- PREDICTION -\n",
            "Zully Broussard selflessly decided to give one of her kidneys to a stranger. It resulted in six patients receiving transplants. She may feel guided by her generosity by a higher power. The medical center is taking five surgeons, a covey of physician assistants, nurses and anesthes\n",
            "\n",
            "- ENTITIES -\n",
            "Ref: [('Zully Broussard', 'PERSON'), ('six', 'CARDINAL')]\n",
            "Prd: [('Zully Broussard', 'PERSON'), ('five', 'CARDINAL'), ('six', 'CARDINAL')]\n",
            "∩   : [('Zully Broussard', 'PERSON'), ('six', 'CARDINAL')]\n",
            "\n",
            "====================================================================================================\n",
            "EXAMPLE 1  |  pred_len=44  ref_len=31  len_ratio=1.42  rep3=0.0000\n",
            "- SOURCE (first 600 chars) -\n",
            "On the 6th of April 1996, San Jose Clash and DC United strode out in front of 31,683 expectant fans at the Spartan Stadium in San Jose, California. The historic occasion was the first ever Major League Soccer match -- a brave new dawn for the world's favorite sport in a land its charms had yet to conquer. Summarizing the action for ESPN, commentator Ty Keough eagerly described the momentous \"birth of a new era for American soccer.\" Looking back at footage from that balmy evening now it's hard not to feel a certain nostalgia. Baggy shirts, questionable hairstyles and strange rule ...\n",
            "\n",
            "- REFERENCE -\n",
            "The 20th MLS season begins this weekend . League has changed dramatically since its inception in 1996 . Some question whether rules regarding salary caps and transfers need to change .\n",
            "\n",
            "- PREDICTION -\n",
            "MLS prepares to mark the beginning of its 20th season. Attendances are higher than ever before and the number of teams involved has doubled from 10 in the 1996 campaign to 20 in 2015. A further four are set to be added by 2020.\n",
            "\n",
            "- ENTITIES -\n",
            "Ref: [('1996', 'DATE'), ('20th', 'ORDINAL'), ('MLS', 'ORG'), ('this weekend', 'DATE')]\n",
            "Prd: [('10', 'CARDINAL'), ('1996', 'DATE'), ('20', 'CARDINAL'), ('2015', 'DATE'), ('2020', 'DATE'), ('20th season', 'DATE'), ('MLS', 'ORG'), ('four', 'CARDINAL')]\n",
            "∩   : [('1996', 'DATE'), ('MLS', 'ORG')]\n",
            "\n",
            "====================================================================================================\n",
            "EXAMPLE 2  |  pred_len=30  ref_len=46  len_ratio=0.65  rep3=0.0000\n",
            "- SOURCE (first 600 chars) -\n",
            "French striker Bafetimbi Gomis, who has a history of fainting, said he is now \"feeling well\" after collapsing during Swansea's 3-2 loss at Tottenham in the Premier League on Wednesday. The worrying incident occurred in the first half at White Hart Lane -- after Tottenham scored in the seventh minute -- but the 29-year-old left the pitch conscious following about five minutes of treatment. The Guardian added that he was wearing an oxygen mask. Play was temporarily stopped before resuming. As the match progressed, Swansea tweeted that Gomis was \"fine,\" with manager Garry Monk using the same ...\n",
            "\n",
            "- REFERENCE -\n",
            "Bafetimbi Gomis collapses within 10 minutes of kickoff at Tottenham . But he reportedly left the pitch conscious and wearing an oxygen mask . Gomis later said that he was \"feeling well\" The incident came three years after Fabrice Muamba collapsed at White Hart Lane .\n",
            "\n",
            "- PREDICTION -\n",
            "French striker Bafetimbi Gomis collapses during Swansea's 3-2 loss at Tottenham. The 29-year-old left the pitch conscious following about five minutes of treatment. The match was temporarily stopped before resuming.\n",
            "\n",
            "- ENTITIES -\n",
            "Ref: [('10 minutes', 'TIME'), ('Bafetimbi Gomis', 'PERSON'), ('Fabrice Muamba', 'PERSON'), ('Tottenham', 'GPE'), ('White Hart Lane', 'FAC'), ('three years', 'DATE')]\n",
            "Prd: [('29-year-old', 'DATE'), ('3', 'CARDINAL'), ('Bafetimbi Gomis', 'PERSON'), ('French', 'NORP'), ('Swansea', 'ORG'), ('Tottenham', 'GPE'), ('about five minutes', 'TIME')]\n",
            "∩   : [('Bafetimbi Gomis', 'PERSON'), ('Tottenham', 'GPE')]\n",
            "\n",
            "====================================================================================================\n",
            "EXAMPLE 3  |  pred_len=42  ref_len=21  len_ratio=2.00  rep3=0.0000\n",
            "- SOURCE (first 600 chars) -\n",
            "It was an act of frustration perhaps more commonly associated with golf's fictional anti-hero Happy Gilmore than the world's reigning No 1. player. But when Rory McIlroy pulled his second shot on the eighth hole of the WGC Cadillac Championship into a lake Friday, he might as well have been channeling the much loved Adam Sandler character. Before continuing his round with a dropped ball, the four-time major winner launched the 3-iron used to play the offending shot into the water as well. \"(It) felt good at the time,\" a rueful McIlroy later said of the incident in comments carried by the ...\n",
            "\n",
            "- REFERENCE -\n",
            "Rory McIlroy throws club into water at WGC Cadillac Championship . Northern Irishman frustrated after pulling shot into water hazard .\n",
            "\n",
            "- PREDICTION -\n",
            "Rory McIlroy pulled his second shot on the eighth hole of the WGC Cadillac Championship into a lake. Before continuing his round with a dropped ball, the four-time major winner launched the 3-iron used to play the offending shot into the water.\n",
            "\n",
            "- ENTITIES -\n",
            "Ref: [('Northern Irishman', 'NORP'), ('Rory McIlroy', 'PERSON'), ('WGC Cadillac Championship', 'ORG')]\n",
            "Prd: [('3', 'CARDINAL'), ('Rory McIlroy', 'PERSON'), ('eighth', 'ORDINAL'), ('four', 'CARDINAL'), ('second', 'ORDINAL'), ('the WGC Cadillac Championship', 'ORG')]\n",
            "∩   : [('Rory McIlroy', 'PERSON')]\n",
            "\n",
            "====================================================================================================\n",
            "EXAMPLE 4  |  pred_len=41  ref_len=17  len_ratio=2.41  rep3=0.0000\n",
            "- SOURCE (first 600 chars) -\n",
            "A Pennsylvania community is pulling together to search for an eighth-grade student who has been missing since Wednesday. The search has drawn hundreds of volunteers on foot and online. The parents of Cayman Naib, 13, have been communicating through the Facebook group \"Find Cayman\" since a day after his disappearance, according to close friend David Binswanger. Newtown Police say Cayman was last seen wearing a gray down winter jacket, black ski pants and hiking boots. He could be in the Radnor-Wayne area, roughly 20 miles from Philadelphia, or may have purchased a train ticket to ...\n",
            "\n",
            "- REFERENCE -\n",
            "Cayman Naib, 13, hasn't been heard from since Wednesday . Police, family, volunteers search for eighth-grader .\n",
            "\n",
            "- PREDICTION -\n",
            "Cayman Naib, 13, was last seen wearing a gray down winter jacket, black ski pants and hiking boots. He could be in the Radnor-Wayne area, roughly 20 miles from Philadelphia. The search has drawn hundreds of volunteers on foot and online.\n",
            "\n",
            "- ENTITIES -\n",
            "Ref: [('13', 'DATE'), ('Cayman Naib', 'PERSON'), ('Wednesday', 'DATE'), ('eighth', 'ORDINAL')]\n",
            "Prd: [('13', 'DATE'), ('Cayman Naib', 'PERSON'), ('Philadelphia', 'GPE'), ('Radnor-Wayne', 'ORG'), ('hundreds', 'CARDINAL'), ('roughly 20 miles', 'QUANTITY'), ('winter', 'DATE')]\n",
            "∩   : [('13', 'DATE'), ('Cayman Naib', 'PERSON')]\n"
          ]
        }
      ],
      "source": [
        "# ============================ CELL — QUALITATIVE EXAMPLES ============================\n",
        "import textwrap, re, numpy as np\n",
        "\n",
        "assert 'srcs_phaseA' in globals() and 'refs_phaseA' in globals() and 'preds_phaseA' in globals()\n",
        "\n",
        "K    = int(globals().get('K_EXAMPLES', 5))\n",
        "IDX  = globals().get('IDX_EXAMPLES', None)\n",
        "SNIP = int(globals().get('SOURCE_SNIP', 600))\n",
        "\n",
        "def rep_3gram_rate(text: str) -> float:\n",
        "    toks = text.split()\n",
        "    if len(toks) < 3: return 0.0\n",
        "    seen, repeat = set(), 0\n",
        "    for i in range(len(toks)-2):\n",
        "        g = (toks[i], toks[i+1], toks[i+2])\n",
        "        if g in seen: repeat += 1\n",
        "        seen.add(g)\n",
        "    return repeat / max(1, len(toks)-2)\n",
        "\n",
        "def ents_or_empty(texts):\n",
        "    try:\n",
        "        import spacy\n",
        "        try:\n",
        "            nlp = spacy.load(\"en_core_web_sm\")\n",
        "        except Exception:\n",
        "            return [set() for _ in texts], False\n",
        "    except Exception:\n",
        "        return [set() for _ in texts], False\n",
        "    out = []\n",
        "    for t in texts:\n",
        "        doc = nlp(t)\n",
        "        out.append({(e.text.strip(), e.label_) for e in doc.ents})\n",
        "    return out, True\n",
        "\n",
        "# choose rows\n",
        "if IDX is None:\n",
        "    idxs = list(range(min(K, len(srcs_phaseA))))\n",
        "else:\n",
        "    idxs = list(IDX)[:K]\n",
        "\n",
        "E_ref, has_ner = ents_or_empty([refs_phaseA[i] for i in idxs])\n",
        "E_prd, _       = ents_or_empty([preds_phaseA[i] for i in idxs]) if has_ner else ([set()]*len(idxs), False)\n",
        "\n",
        "for j, i in enumerate(idxs):\n",
        "    src, ref, prd = srcs_phaseA[i], refs_phaseA[i], preds_phaseA[i]\n",
        "    ref_w, prd_w = len(ref.split()), len(prd.split())\n",
        "    ratio = prd_w / max(ref_w, 1)\n",
        "    rep3  = rep_3gram_rate(prd)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(f\"EXAMPLE {i}  |  pred_len={prd_w}  ref_len={ref_w}  len_ratio={ratio:.2f}  rep3={rep3:.4f}\")\n",
        "    print(\"- SOURCE (first {} chars) -\".format(SNIP))\n",
        "    print(textwrap.shorten(src.replace(\"\\n\",\" \"), width=SNIP, placeholder=\" ...\"))\n",
        "\n",
        "    print(\"\\n- REFERENCE -\")\n",
        "    print(ref)\n",
        "\n",
        "    print(\"\\n- PREDICTION -\")\n",
        "    print(prd)\n",
        "\n",
        "    if has_ner:\n",
        "        inter = E_ref[j] & E_prd[j]\n",
        "        print(\"\\n- ENTITIES -\")\n",
        "        print(\"Ref:\", sorted(list(E_ref[j]))[:12])\n",
        "        print(\"Prd:\", sorted(list(E_prd[j]))[:12])\n",
        "        print(\"∩   :\", sorted(list(inter))[:12])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMpzuGFWiCtn"
      },
      "source": [
        "\n",
        "\n",
        "> Further Decoding tweaks to Match human length and reduce over copy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZfrUpzJjXFQ",
        "outputId": "3f4e9a32-f128-4c77-a6d0-c7b5156420d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -q install evaluate rouge-score\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXUaq_dbkYvb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CopyAwareBart(nn.Module):\n",
        "    def __init__(self, base_model, tokenizer, lambda_cov=1.0, eps=1e-8, use_pointer=True):\n",
        "        super().__init__()\n",
        "        self.base   = base_model        # Hugging Face BART model\n",
        "        self.tok    = tokenizer\n",
        "        self.lambda_cov = lambda_cov\n",
        "        self.eps    = eps\n",
        "        self.use_ptr= use_pointer\n",
        "\n",
        "        hidden = self.base.config.d_model\n",
        "        self.p_gen_linear = nn.Linear(hidden*3, 1)   # gate: [dec_hid, ctx, dec_inp]\n",
        "\n",
        "        # for logging\n",
        "        self._last_ce_loss = None\n",
        "        self._last_cov_loss = None\n",
        "        self._last_p_copy_mean = None\n",
        "        self._last_p_gen_mean  = None\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        labels=None,\n",
        "        decoder_input_ids=None,\n",
        "        decoder_attention_mask=None,\n",
        "        output_attentions=True,\n",
        "        output_hidden_states=True,\n",
        "        past_key_values=None,\n",
        "        use_cache=False,\n",
        "        **kwargs\n",
        "    ):\n",
        "        \"\"\"\n",
        "        - Training: labels provided → compute pointer+coverage loss.\n",
        "        - Generation: labels=None → return pointer-aware logits for beam search.\n",
        "        \"\"\"\n",
        "\n",
        "        out = self.base(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=None,  # we’ll compute loss ourselves\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            output_attentions=True,\n",
        "            output_hidden_states=True,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            return_dict=True,\n",
        "        )\n",
        "\n",
        "        logits = out.logits   # [B,T,V]\n",
        "        B,T,V  = logits.shape\n",
        "\n",
        "        if not self.use_ptr:\n",
        "            # no pointer: behave like plain BART\n",
        "            if labels is not None:\n",
        "                loss = F.cross_entropy(\n",
        "                    logits.view(-1,V), labels.view(-1), ignore_index=-100\n",
        "                )\n",
        "                out.loss = loss\n",
        "            return out\n",
        "\n",
        "        # === pointer mechanism ===\n",
        "        # average last layer cross-attention over heads\n",
        "        attn = out.cross_attentions[-1].mean(dim=1)  # [B,T,S]\n",
        "        if attention_mask is not None:\n",
        "            attn = attn.masked_fill(attention_mask[:,None,:]==0, 0.0)\n",
        "        attn = attn / attn.sum(-1, keepdim=True).clamp_min(self.eps)\n",
        "\n",
        "        dec_hid = out.decoder_hidden_states[-1]          # [B,T,H]\n",
        "        ctx     = torch.bmm(attn, out.encoder_last_hidden_state) # [B,T,H]\n",
        "        dec_inp = self.base.get_input_embeddings()(decoder_input_ids) # [B,T,H]\n",
        "        cat     = torch.cat([dec_hid, ctx, dec_inp], dim=-1)\n",
        "        p_gen   = torch.sigmoid(self.p_gen_linear(cat)).squeeze(-1)   # [B,T]\n",
        "        p_copy  = 1.0 - p_gen\n",
        "\n",
        "        # build copy distribution in vocab space\n",
        "        copy_logits = torch.full_like(logits, -1e9)  # log(0)\n",
        "        copy_logits.scatter_add_(\n",
        "            2,\n",
        "            input_ids[:,None,:].expand(B,T,-1),\n",
        "            attn.unsqueeze(1).expand(B,T,-1)\n",
        "        )\n",
        "        copy_probs = copy_logits.log_softmax(dim=-1)\n",
        "\n",
        "        # final mixture logits\n",
        "        logp_vocab = F.log_softmax(logits, dim=-1)\n",
        "        logp_final = torch.logaddexp(\n",
        "            (p_gen.unsqueeze(-1).log() + logp_vocab),\n",
        "            (p_copy.unsqueeze(-1).log() + copy_probs)\n",
        "        )\n",
        "        out.logits = logp_final\n",
        "\n",
        "        if labels is not None:\n",
        "            # === training loss ===\n",
        "            gold = labels\n",
        "            mask = gold.ne(-100).float()\n",
        "\n",
        "            logp_gold = logp_final.gather(-1, gold.unsqueeze(-1)).squeeze(-1)\n",
        "            nll = -(logp_gold * mask).sum() / mask.sum().clamp_min(1.0)\n",
        "\n",
        "            # coverage loss (See et al. 2017 style)\n",
        "            cov = torch.min(attn, attn.cumsum(dim=-2).detach())\n",
        "            cov_loss = cov.sum(-1).mean() * self.lambda_cov\n",
        "\n",
        "            out.loss = nll + cov_loss\n",
        "            self._last_ce_loss  = float(nll.detach().cpu())\n",
        "            self._last_cov_loss = float(cov_loss.detach().cpu())\n",
        "            self._last_p_copy_mean = float(p_copy.detach().mean().cpu())\n",
        "            self._last_p_gen_mean  = float(p_gen.detach().mean().cpu())\n",
        "\n",
        "        return out\n",
        "\n",
        "# --- add generate delegate so model.generate() works ---\n",
        "def copyaware_generate(self, *args, **kwargs):\n",
        "    return self.base.generate(*args, **kwargs)\n",
        "CopyAwareBart.generate = copyaware_generate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D09q4TBcjpkm",
        "outputId": "bf7771b5-f7c3-4f4f-872a-754256df2228"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM\n",
        "import pandas as pd\n",
        "\n",
        "RESUME_CKPT = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000\"\n",
        "VAL_CSV     = \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "\n",
        "# reload tokenizer + base\n",
        "tok = AutoTokenizer.from_pretrained(RESUME_CKPT, use_fast=True)\n",
        "cfg = AutoConfig.from_pretrained(RESUME_CKPT)\n",
        "base = AutoModelForSeq2SeqLM.from_pretrained(RESUME_CKPT, config=cfg)\n",
        "\n",
        "# wrap back into your CopyAwareBart class\n",
        "model = CopyAwareBart(base, tok, lambda_cov=1.2, use_pointer=True)\n",
        "\n",
        "# load validation set\n",
        "val_df = pd.read_csv(VAL_CSV)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBP3mV8RN2HN",
        "outputId": "75ec2a9f-5759-49ae-caa8-47fef4d641d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[gen] device=cuda:0 dtype=torch.bfloat16\n",
            "[eos-bias] min_len=28 trigger_len=43 (ref_median≈40)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`generation_config` default values have been modified to match model-specific defaults: {'forced_bos_token_id': 0, 'forced_eos_token_id': 2, 'pad_token_id': 1, 'bos_token_id': 0, 'eos_token_id': 2, 'decoder_start_token_id': 2}. If this is not desired, please set these values explicitly.\n",
            "`sdpa` attention does not support `output_attentions=True` or `head_mask`. Please set your attention to `eager` if you want any of these features.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[gen] 1000 summaries in 67.1s (~14.9/s)  batch=64 beams=5\n",
            "[cuda] max_alloc=3.43 GB | max_reserved=4.30 GB\n",
            "[done] decoded: 1000\n"
          ]
        }
      ],
      "source": [
        "# ===== Fast decode (shorter outputs) + full eval for ckpt_step4000 =====\n",
        "import time, re, numpy as np, torch, spacy, evaluate\n",
        "from transformers import GenerationConfig, LogitsProcessor, LogitsProcessorList\n",
        "\n",
        "assert 'model' in globals() and 'tok' in globals() and 'val_df' in globals()\n",
        "\n",
        "# --- CUDA + dtype ---\n",
        "DEVICE    = torch.device(\"cuda:0\")\n",
        "AMP_DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "GENERATOR = model\n",
        "GENERATOR.to(DEVICE, dtype=AMP_DTYPE).eval()\n",
        "if hasattr(GENERATOR, \"base\") and hasattr(GENERATOR.base, \"config\"):\n",
        "    GENERATOR.base.config.use_cache = True\n",
        "\n",
        "print(f\"[gen] device={next(GENERATOR.parameters()).device} dtype={next(GENERATOR.parameters()).dtype}\")\n",
        "\n",
        "# --- decode knobs (tuned shorter) ---\n",
        "MAX_SRC_LEN     = int(globals().get('MAX_SRC_LEN', 400))\n",
        "N_EVAL          = int(globals().get('N_EVAL', 1000))\n",
        "BATCH_GEN       = int(globals().get('BATCH_GEN', 64))\n",
        "NUM_BEAMS       = 5\n",
        "MIN_NEW         = 26\n",
        "MAX_NEW         = 50          #  from 60\n",
        "NO_REPEAT       = 4\n",
        "LENGTH_PENALTY  = 1.2         #  from 3.0 Because stop controls were also tifghtened\n",
        "\n",
        "# EOS bias (stronger stop, closer to gold median)\n",
        "USE_EOS_BIAS    = True\n",
        "EOS_TARGET_SCALE= 1.08        #  from 1.22\n",
        "EOS_MIN_SCALE   = 0.72\n",
        "PRE_EOS_BIAS    = -3.2\n",
        "POST_EOS_BIAS   = 4.5         #  from 3.6\n",
        "\n",
        "# --- slice data ---\n",
        "_rows = val_df.iloc[:N_EVAL]\n",
        "srcs = _rows[\"article\"].astype(str).tolist()\n",
        "refs = _rows[\"highlights\"].astype(str).tolist()\n",
        "\n",
        "# --- eos bias processor ---\n",
        "class EosAfterNTokens(LogitsProcessor):\n",
        "    def __init__(self, eos_id, min_len, trigger_len, pre_bias=-3.2, post_bias=3.6):\n",
        "        self.eos_id = int(eos_id); self.min_len = int(min_len); self.trigger_len = int(trigger_len)\n",
        "        self.pre_bias = float(pre_bias); self.post_bias = float(post_bias)\n",
        "    def __call__(self, input_ids, scores):\n",
        "        new_len = input_ids.shape[1] - 1\n",
        "        if new_len < self.min_len:        scores[:, self.eos_id] += self.pre_bias\n",
        "        elif new_len >= self.trigger_len: scores[:, self.eos_id] += self.post_bias\n",
        "        return scores\n",
        "\n",
        "logits_proc = None\n",
        "if USE_EOS_BIAS:\n",
        "    ref_tok = [len(tok(r, add_special_tokens=False).input_ids) for r in refs]\n",
        "    ref_med = int(torch.tensor(ref_tok).median().item())\n",
        "    tgt_len = int(max(1, ref_med) * EOS_TARGET_SCALE)\n",
        "    min_len = int(max(1, ref_med) * EOS_MIN_SCALE)\n",
        "    eos_id  = GENERATOR.base.config.eos_token_id\n",
        "    logits_proc = LogitsProcessorList([EosAfterNTokens(eos_id, min_len, tgt_len, PRE_EOS_BIAS, POST_EOS_BIAS)])\n",
        "    print(f\"[eos-bias] min_len={min_len} trigger_len={tgt_len} (ref_median≈{ref_med})\")\n",
        "\n",
        "gen_cfg = GenerationConfig(\n",
        "    num_beams=NUM_BEAMS,\n",
        "    min_new_tokens=MIN_NEW,\n",
        "    max_new_tokens=MAX_NEW,\n",
        "    no_repeat_ngram_size=NO_REPEAT,\n",
        "    length_penalty=LENGTH_PENALTY,\n",
        "    early_stopping=True,\n",
        ")\n",
        "\n",
        "# --- generate ---\n",
        "def generate_batched(srcs):\n",
        "    preds, t0 = [], time.time()\n",
        "    torch.cuda.empty_cache(); torch.cuda.reset_peak_memory_stats()\n",
        "    for i in range(0, len(srcs), BATCH_GEN):\n",
        "        batch = srcs[i:i+BATCH_GEN]\n",
        "        with torch.inference_mode(), torch.autocast(device_type=\"cuda\", dtype=AMP_DTYPE):\n",
        "            ins = tok(batch, return_tensors=\"pt\", truncation=True, max_length=MAX_SRC_LEN,\n",
        "                      padding=True, pad_to_multiple_of=16).to(DEVICE)\n",
        "            outs = GENERATOR.generate(**ins, generation_config=gen_cfg, logits_processor=logits_proc)\n",
        "        preds.extend(tok.batch_decode(outs, skip_special_tokens=True))\n",
        "        del ins, outs\n",
        "        torch.cuda.empty_cache()\n",
        "    dt = time.time() - t0\n",
        "    used = torch.cuda.max_memory_allocated() / (1024**3)\n",
        "    resv = torch.cuda.max_memory_reserved() / (1024**3)\n",
        "    print(f\"[gen] {len(srcs)} summaries in {dt:.1f}s (~{len(srcs)/max(dt,1):.1f}/s)  batch={BATCH_GEN} beams={NUM_BEAMS}\")\n",
        "    print(f\"[cuda] max_alloc={used:.2f} GB | max_reserved={resv:.2f} GB\")\n",
        "    return preds\n",
        "\n",
        "preds = generate_batched(srcs)\n",
        "print(f\"[done] decoded: {len(preds)}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497,
          "referenced_widgets": [
            "7a05a8cf34c74ac2b68a30ee4d0abc3d"
          ]
        },
        "id": "mPmCP7wEjJzC",
        "outputId": "1c9d2338-7c17-4410-902c-2a642498be4c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a05a8cf34c74ac2b68a30ee4d0abc3d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== ROUGE ===\n",
            "rouge1: 0.3182\n",
            "rouge2: 0.1189\n",
            "rougeL: 0.2265\n",
            "rougeLsum: 0.2262\n",
            "\n",
            "=== TOKEN LENGTHS ===\n",
            "Ref : {'mean': 41.316, 'median': 40.0, 'p25': 32.0, 'p50': 40.0, 'p75': 50.0}\n",
            "Pred: {'mean': 46.931, 'median': 48.0, 'p25': 47.0, 'p50': 48.0, 'p75': 48.0}\n",
            "\n",
            "=== WORD LENGTHS ===\n",
            "Ref : {'mean': 34.373, 'median': 33.0, 'p25': 27.0, 'p50': 33.0, 'p75': 41.0}\n",
            "Pred: {'mean': 37.407, 'median': 38.0, 'p25': 35.0, 'p50': 38.0, 'p75': 40.0}\n",
            "\n",
            "Copy-rate: mean=0.935, median=0.946\n",
            "Entity overlap: 1304/4302 = 30.31%\n",
            "Number overlap: 189/655 = 28.85%\n"
          ]
        }
      ],
      "source": [
        "# --- ROUGE ---\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "rouge_res = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
        "print(\"\\n=== ROUGE ===\")\n",
        "for k,v in rouge_res.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "# --- Length stats (tokens + words) ---\n",
        "tok_lens_ref  = [len(tok(r, add_special_tokens=False).input_ids) for r in refs]\n",
        "tok_lens_pred = [len(tok(p, add_special_tokens=False).input_ids) for p in preds]\n",
        "word_lens_ref  = [len(r.split()) for r in refs]\n",
        "word_lens_pred = [len(p.split()) for p in preds]\n",
        "\n",
        "import statistics as stats\n",
        "def stats4(xs): return dict(mean=float(np.mean(xs)), median=float(np.median(xs)),\n",
        "                            p25=float(np.percentile(xs,25)), p50=float(np.percentile(xs,50)),\n",
        "                            p75=float(np.percentile(xs,75)))\n",
        "\n",
        "print(\"\\n=== TOKEN LENGTHS ===\")\n",
        "print(\"Ref :\", stats4(tok_lens_ref))\n",
        "print(\"Pred:\", stats4(tok_lens_pred))\n",
        "print(\"\\n=== WORD LENGTHS ===\")\n",
        "print(\"Ref :\", stats4(word_lens_ref))\n",
        "print(\"Pred:\", stats4(word_lens_pred))\n",
        "\n",
        "# --- Copy-rate  ---\n",
        "tok_preds = [p.split() for p in preds]\n",
        "tok_srcs  = [s.split() for s in srcs]\n",
        "copy_rates = []\n",
        "for pred_tokens, src_tokens in zip(tok_preds, tok_srcs):\n",
        "    src_set = set(src_tokens)\n",
        "    copied = sum(1 for t in pred_tokens if t in src_set)\n",
        "    copy_rates.append(copied / max(1, len(pred_tokens)))\n",
        "print(f\"\\nCopy-rate: mean={np.mean(copy_rates):.3f}, median={np.median(copy_rates):.3f}\")\n",
        "\n",
        "# --- Entity & number overlap ---\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\",\"tagger\",\"lemmatizer\"])\n",
        "def ents(texts):\n",
        "    return [set(ent.text.lower() for ent in nlp(t).ents) for t in texts]\n",
        "ref_ents  = ents(refs)\n",
        "pred_ents = ents(preds)\n",
        "ent_correct = sum(len(re & pe) for re, pe in zip(ref_ents, pred_ents))\n",
        "ent_total   = sum(len(re) for re in ref_ents)\n",
        "ent_acc = ent_correct / max(1, ent_total)\n",
        "print(f\"Entity overlap: {ent_correct}/{ent_total} = {ent_acc:.2%}\")\n",
        "\n",
        "import re\n",
        "def nums(s): return set(re.findall(r\"\\d+\", s))\n",
        "ref_nums  = [nums(r) for r in refs]\n",
        "pred_nums = [nums(p) for p in preds]\n",
        "num_correct = sum(len(rn & pn) for rn, pn in zip(ref_nums, pred_nums))\n",
        "num_total   = sum(len(rn) for rn in ref_nums)\n",
        "num_acc = num_correct / max(1, num_total)\n",
        "print(f\"Number overlap: {num_correct}/{num_total} = {num_acc:.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXC2BglRlZ8r"
      },
      "source": [
        "Summaries were shortened without hurting the ROUGE score but copying is still too aggressive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Cpe0jGfj-0I",
        "outputId": "cf48c7a5-b53d-43d2-8bb4-8c814e96d0f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[decode] ckpt=/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000\n",
            "[data]  val=/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\n",
            "[data] columns -> source='article'  reference='highlights'\n",
            "[data] total rows: 13368\n",
            "[data] n=1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[gen] device=cuda dtype=torch.float32\n",
            "[mode] POINTER_OFF (plain generate)\n",
            "[gen] 160/1000\n",
            "[gen] 320/1000\n",
            "[gen] 480/1000\n",
            "[gen] 640/1000\n",
            "[gen] 800/1000\n",
            "[gen] 960/1000\n",
            "[gen] done 1000 in 95.4s  (10.5 ex/s)\n",
            "[save] /content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000/val_POINTEROFF_beam5_min28_max48.csv\n",
            "\n",
            "=== ROUGE (POINTER_OFF) ===\n",
            "rouge1    F1=0.3819  (P=0.4755 R=0.3371)\n",
            "rouge2    F1=0.1717  (P=0.2154 R=0.1510)\n",
            "rougeL    F1=0.2693  (P=0.3338 R=0.2383)\n",
            "rougeLsum  F1=0.2694  (P=0.3342 R=0.2385)\n",
            "\n",
            "[len stats]\n",
            " gold: {'count': 1000, 'mean': 58.368, 'median': 55.0, 'p25': 43.0, 'p75': 71.0, 'min': 14, 'max': 155}\n",
            " pred: {'count': 1000, 'mean': 36.35, 'median': 37.0, 'p25': 34.0, 'p75': 39.0, 'min': 23, 'max': 44}\n",
            "\n",
            "--- idx 0 ---\n",
            "REF: Kieran Carroll, of Luton, came forward today following a three-day search . Police previously released CCTV footage of father in their bid to locate him . When he saw his image online he sent a Facebook message to the police . He told them: 'I'm fine, so is my child, go look for some killers' He was\n",
            "PRED: Kieran Carroll, 22, of Luton, Bedfordshire, was reported missing at around midday on Tuesday. He was arrested for assaulting a woman and damaging her mobile phone. Police released CCTV footage of him pushing a pram\n",
            "\n",
            "--- idx 500 ---\n",
            "REF: Stevenage's Ronnie Henry accused Joss Labadie of biting him . Incident took place shortly before the end of their League Two clash . Stevenage manager Graham Westley says it 'has become a police matter' Hertfordshire police continuing to make inquiries into alleged bite . Labadie served a 10-match b\n",
            "PRED: Joss Labadie, 24, was accused of sinking his teeth into the hand of Stevenage defender Ronnie Henry. The pair clashed near the touchline when Henry tried to wrestle the ball out of Labadie's arms.\n",
            "\n",
            "--- idx 999 ---\n",
            "REF: Up to seven boys locked the helpless 10-year-old victim in a wooden cage . They poured petrol over the young boy's body before setting him alight . Luckily residents heard the child's cries and were able to save his life . Act is believed to be in imitation of ISIS' murder of Mu'ath al-Kasasbeh . At\n",
            "PRED: A group of children in Yemen have attempted to burn a 10-year-old boy to death. The attack is understood to have been a sickening imitation of the brutal murder of Jordanian pilot Mu'ath al-Kasas\n"
          ]
        }
      ],
      "source": [
        "# ===== DECODE & EVAL (POINTER OFF — plain beam search) =====\n",
        "import os, time, math, json, gc\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from rouge_score import rouge_scorer, scoring\n",
        "\n",
        "# ---------------- paths ----------------\n",
        "CKPT_DIR = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000\"\n",
        "VAL_CSV  = \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "OUT_CSV  = os.path.join(CKPT_DIR, \"val_POINTEROFF_beam5_min28_max48.csv\")\n",
        "\n",
        "SRC_COL, REF_COL = \"article\", \"highlights\"\n",
        "\n",
        "# ---------------- knobs ----------------\n",
        "MAX_SRC_LEN     = 400\n",
        "N_EVAL          = 1000            # set to None to run full val\n",
        "BATCH_SIZE      = 32              # conservative to avoid OOM\n",
        "NUM_BEAMS       = 5\n",
        "MIN_NEW         = 28\n",
        "MAX_NEW         = 48\n",
        "NO_REPEAT       = 4\n",
        "LENGTH_PENALTY  = 1.2\n",
        "\n",
        "print(f\"[decode] ckpt={CKPT_DIR}\")\n",
        "print(f\"[data]  val={VAL_CSV}\")\n",
        "\n",
        "# ---------------- load data ----------------\n",
        "def safe_read_csv(path, required):\n",
        "    last = None\n",
        "    for enc in (\"utf-8\", \"utf-8-sig\", \"latin1\"):\n",
        "        for eng in (\"c\",\"python\"):\n",
        "            try:\n",
        "                df = pd.read_csv(path, encoding=enc, engine=eng)\n",
        "                for c in required:\n",
        "                    if c not in df.columns:\n",
        "                        raise ValueError(f\"Missing column '{c}' in {path}\")\n",
        "                return df\n",
        "            except Exception as e:\n",
        "                last = e\n",
        "    raise last\n",
        "\n",
        "val_df = safe_read_csv(VAL_CSV, [SRC_COL, REF_COL])\n",
        "n_total = len(val_df)\n",
        "if N_EVAL is None or N_EVAL >= n_total:\n",
        "    work_df = val_df.reset_index(drop=True)\n",
        "else:\n",
        "    work_df = val_df.sample(N_EVAL, random_state=13).reset_index(drop=True)\n",
        "\n",
        "print(f\"[data] columns -> source='{SRC_COL}'  reference='{REF_COL}'\")\n",
        "print(f\"[data] total rows: {n_total}\")\n",
        "print(f\"[data] n={len(work_df)}\")\n",
        "\n",
        "# ---------------- load model/tokenizer ----------------\n",
        "tok = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "print(f\"[gen] device={device} dtype={next(model.parameters()).dtype}\")\n",
        "print(\"[mode] POINTER_OFF (plain generate)\")\n",
        "\n",
        "# ---------------- generation ----------------\n",
        "def generate_batch(texts):\n",
        "    enc = tok(\n",
        "        texts,\n",
        "        max_length=MAX_SRC_LEN,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "\n",
        "    out = model.generate(\n",
        "        input_ids=enc.input_ids,\n",
        "        attention_mask=enc.attention_mask,\n",
        "        do_sample=False,\n",
        "        num_beams=NUM_BEAMS,\n",
        "        length_penalty=LENGTH_PENALTY,\n",
        "        no_repeat_ngram_size=NO_REPEAT,\n",
        "        min_new_tokens=MIN_NEW,\n",
        "        max_new_tokens=MAX_NEW,\n",
        "        early_stopping=True,\n",
        "        return_dict_in_generate=False,   # IMPORTANT: return plain tensor\n",
        "    )\n",
        "    # out: Tensor [B, T]\n",
        "    return tok.batch_decode(out, skip_special_tokens=True)\n",
        "\n",
        "preds = []\n",
        "i, n = 0, len(work_df)\n",
        "t0 = time.time()\n",
        "bs = BATCH_SIZE\n",
        "while i < n:\n",
        "    batch = work_df[SRC_COL].iloc[i:i+bs].astype(str).tolist()\n",
        "    try:\n",
        "        preds.extend(generate_batch(batch))\n",
        "        i += bs\n",
        "        if i % (bs*5) == 0 or i == n:\n",
        "            print(f\"[gen] {i}/{n}\")\n",
        "    except RuntimeError as e:\n",
        "        if \"CUDA out of memory\" in str(e) and bs > 4:\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            bs = max(4, bs // 2)\n",
        "            print(f\"[gen][OOM] reducing batch to {bs} and retrying…\")\n",
        "            continue\n",
        "        raise\n",
        "\n",
        "elapsed = time.time() - t0\n",
        "print(f\"[gen] done {len(preds)} in {elapsed:.1f}s  ({len(preds)/max(elapsed,1):.1f} ex/s)\")\n",
        "\n",
        "# ---------------- save ----------------\n",
        "out_df = work_df[[SRC_COL, REF_COL]].copy()\n",
        "out_df[\"prediction\"] = preds[:len(out_df)]\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "out_df.to_csv(OUT_CSV, index=False)\n",
        "print(f\"[save] {OUT_CSV}\")\n",
        "\n",
        "# ---------------- ROUGE ----------------\n",
        "def compute_rouge(refs, hyps):\n",
        "    scorer = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\",\"rougeLsum\"], use_stemmer=True)\n",
        "    agg = scoring.BootstrapAggregator()\n",
        "    for r, h in zip(refs, hyps):\n",
        "        agg.add_scores(scorer.score(r, h))\n",
        "    res = agg.aggregate()\n",
        "    return {\n",
        "        k: {\n",
        "            \"f\":  res[k].mid.fmeasure,\n",
        "            \"p\":  res[k].mid.precision,\n",
        "            \"r\":  res[k].mid.recall,\n",
        "        } for k in res\n",
        "    }\n",
        "\n",
        "scores = compute_rouge(out_df[REF_COL].astype(str).tolist(),\n",
        "                       out_df[\"prediction\"].astype(str).tolist())\n",
        "\n",
        "print(\"\\n=== ROUGE (POINTER_OFF) ===\")\n",
        "for k in (\"rouge1\",\"rouge2\",\"rougeL\",\"rougeLsum\"):\n",
        "    s = scores[k]\n",
        "    print(f\"{k:8s}  F1={s['f']:.4f}  (P={s['p']:.4f} R={s['r']:.4f})\")\n",
        "\n",
        "# ---------------- quick sanity print ----------------\n",
        "def lens(series):\n",
        "    L = series.astype(str).str.split().apply(len)\n",
        "    return {\n",
        "        \"count\": int(L.shape[0]),\n",
        "        \"mean\": float(L.mean()),\n",
        "        \"median\": float(L.median()),\n",
        "        \"p25\": float(L.quantile(0.25)),\n",
        "        \"p75\": float(L.quantile(0.75)),\n",
        "        \"min\": int(L.min()),\n",
        "        \"max\": int(L.max()),\n",
        "    }\n",
        "\n",
        "print(\"\\n[len stats]\")\n",
        "print(\" gold:\", lens(out_df[REF_COL]))\n",
        "print(\" pred:\", lens(out_df[\"prediction\"]))\n",
        "\n",
        "for idx in [0, len(out_df)//2, len(out_df)-1]:\n",
        "    print(f\"\\n--- idx {idx} ---\")\n",
        "    print(\"REF:\", out_df.iloc[idx][REF_COL][:300])\n",
        "    print(\"PRED:\", out_df.iloc[idx][\"prediction\"][:300])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l05UZZkQtDFA"
      },
      "source": [
        "Earlier summaries were too long, hurting precision. Summaries are too short, which hurts recall but boosts precision.\n",
        "ROUGE-F1 often rises if predictions hover around reference medians, even if recall is sacrificed. That’s why there's a “jump” in scores. cutting off sentences is an artifact of a harsh max cap, not model ability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmWKvlbhkpg7",
        "outputId": "35027e06-7c4e-40c2-b623-cd49c46d9323"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[decode] ckpt=/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000\n",
            "[data]  val=/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\n",
            "[data] columns -> source='article'  reference='highlights'\n",
            "[data] total rows: 13368\n",
            "[data] n=1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[gen] device=cuda dtype=torch.float32\n",
            "[mode] POINTER_OFF (plain generate)\n",
            "[gen] 160/1000\n",
            "[gen] 320/1000\n",
            "[gen] 480/1000\n",
            "[gen] 640/1000\n",
            "[gen] 800/1000\n",
            "[gen] 960/1000\n",
            "[gen] done 1000 in 146.3s  (6.8 ex/s)\n",
            "[save] /content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000/val_POINTEROFF_beam5_min28_max48.csv\n",
            "\n",
            "=== ROUGE (POINTER_OFF) ===\n",
            "rouge1    F1=0.4092  (P=0.4307 R=0.4138)\n",
            "rouge2    F1=0.1850  (P=0.1952 R=0.1865)\n",
            "rougeL    F1=0.2802  (P=0.2932 R=0.2850)\n",
            "rougeLsum  F1=0.2799  (P=0.2932 R=0.2845)\n",
            "\n",
            "[len stats]\n",
            " gold: {'count': 1000, 'mean': 58.368, 'median': 55.0, 'p25': 43.0, 'p75': 71.0, 'min': 14, 'max': 155}\n",
            " pred: {'count': 1000, 'mean': 50.168, 'median': 51.0, 'p25': 46.0, 'p75': 55.0, 'min': 28, 'max': 64}\n",
            "\n",
            "--- idx 0 ---\n",
            "REF: Kieran Carroll, of Luton, came forward today following a three-day search . Police previously released CCTV footage of father in their bid to locate him . When he saw his image online he sent a Facebook message to the police . He told them: 'I'm fine, so is my child, go look for some killers' He was\n",
            "PRED: Kieran Carroll, 22, of Luton, Bedfordshire, was reported missing at around midday on Tuesday. He was arrested for assaulting a woman and damaging her mobile phone. Police released CCTV footage of him pushing a pram as he left a hotel. He was later released on bail and is due to report back to Luton \n",
            "\n",
            "--- idx 500 ---\n",
            "REF: Stevenage's Ronnie Henry accused Joss Labadie of biting him . Incident took place shortly before the end of their League Two clash . Stevenage manager Graham Westley says it 'has become a police matter' Hertfordshire police continuing to make inquiries into alleged bite . Labadie served a 10-match b\n",
            "PRED: Joss Labadie was accused of sinking his teeth into the hand of Stevenage defender Ronnie Henry. The pair clashed near the touchline when Henry tried to wrestle the ball out of Labadie's arms. Henry immediately appeared to signal to the nearby assistant referee that he had been bitten. The 24-year-ol\n",
            "\n",
            "--- idx 999 ---\n",
            "REF: Up to seven boys locked the helpless 10-year-old victim in a wooden cage . They poured petrol over the young boy's body before setting him alight . Luckily residents heard the child's cries and were able to save his life . Act is believed to be in imitation of ISIS' murder of Mu'ath al-Kasasbeh . At\n",
            "PRED: A group of children in Yemen have attempted to burn a 10-year-old boy to death. The attack is understood to have been a sickening imitation of the brutal murder of Jordanian pilot Mu'ath al-Kasasbeh. Local journalist Mohammad Mouzahem was told about the attack and posted details of it on his Faceboo\n"
          ]
        }
      ],
      "source": [
        "# ===== DECODE & EVAL (POINTER OFF — plain beam search) =====\n",
        "import os, time, math, json, gc\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from rouge_score import rouge_scorer, scoring\n",
        "\n",
        "# ---------------- paths ----------------\n",
        "CKPT_DIR = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000\"\n",
        "VAL_CSV  = \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "OUT_CSV  = os.path.join(CKPT_DIR, \"val_POINTEROFF_beam5_min28_max48.csv\")\n",
        "\n",
        "SRC_COL, REF_COL = \"article\", \"highlights\"\n",
        "\n",
        "# ---------------- knobs ----------------\n",
        "MAX_SRC_LEN     = 400\n",
        "N_EVAL          = 1000\n",
        "BATCH_SIZE      = 32\n",
        "NUM_BEAMS       = 5\n",
        "MIN_NEW         = 40\n",
        "MAX_NEW         = 70\n",
        "NO_REPEAT       = 4\n",
        "LENGTH_PENALTY  = 1.2\n",
        "\n",
        "print(f\"[decode] ckpt={CKPT_DIR}\")\n",
        "print(f\"[data]  val={VAL_CSV}\")\n",
        "\n",
        "# ---------------- load data ----------------\n",
        "def safe_read_csv(path, required):\n",
        "    last = None\n",
        "    for enc in (\"utf-8\", \"utf-8-sig\", \"latin1\"):\n",
        "        for eng in (\"c\",\"python\"):\n",
        "            try:\n",
        "                df = pd.read_csv(path, encoding=enc, engine=eng)\n",
        "                for c in required:\n",
        "                    if c not in df.columns:\n",
        "                        raise ValueError(f\"Missing column '{c}' in {path}\")\n",
        "                return df\n",
        "            except Exception as e:\n",
        "                last = e\n",
        "    raise last\n",
        "\n",
        "val_df = safe_read_csv(VAL_CSV, [SRC_COL, REF_COL])\n",
        "n_total = len(val_df)\n",
        "if N_EVAL is None or N_EVAL >= n_total:\n",
        "    work_df = val_df.reset_index(drop=True)\n",
        "else:\n",
        "    work_df = val_df.sample(N_EVAL, random_state=13).reset_index(drop=True)\n",
        "\n",
        "print(f\"[data] columns -> source='{SRC_COL}'  reference='{REF_COL}'\")\n",
        "print(f\"[data] total rows: {n_total}\")\n",
        "print(f\"[data] n={len(work_df)}\")\n",
        "\n",
        "# ---------------- load model/tokenizer ----------------\n",
        "tok = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "print(f\"[gen] device={device} dtype={next(model.parameters()).dtype}\")\n",
        "print(\"[mode] POINTER_OFF (plain generate)\")\n",
        "\n",
        "# ---------------- generation ----------------\n",
        "def generate_batch(texts):\n",
        "    enc = tok(\n",
        "        texts,\n",
        "        max_length=MAX_SRC_LEN,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "\n",
        "    out = model.generate(\n",
        "        input_ids=enc.input_ids,\n",
        "        attention_mask=enc.attention_mask,\n",
        "        do_sample=False,\n",
        "        num_beams=NUM_BEAMS,\n",
        "        length_penalty=LENGTH_PENALTY,\n",
        "        no_repeat_ngram_size=NO_REPEAT,\n",
        "        min_new_tokens=MIN_NEW,\n",
        "        max_new_tokens=MAX_NEW,\n",
        "        early_stopping=True,\n",
        "        return_dict_in_generate=False,   # IMPORTANT: return plain tensor\n",
        "    )\n",
        "    # out: Tensor [B, T]\n",
        "    return tok.batch_decode(out, skip_special_tokens=True)\n",
        "\n",
        "preds = []\n",
        "i, n = 0, len(work_df)\n",
        "t0 = time.time()\n",
        "bs = BATCH_SIZE\n",
        "while i < n:\n",
        "    batch = work_df[SRC_COL].iloc[i:i+bs].astype(str).tolist()\n",
        "    try:\n",
        "        preds.extend(generate_batch(batch))\n",
        "        i += bs\n",
        "        if i % (bs*5) == 0 or i == n:\n",
        "            print(f\"[gen] {i}/{n}\")\n",
        "    except RuntimeError as e:\n",
        "        if \"CUDA out of memory\" in str(e) and bs > 4:\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            bs = max(4, bs // 2)\n",
        "            print(f\"[gen][OOM] reducing batch to {bs} and retrying…\")\n",
        "            continue\n",
        "        raise\n",
        "\n",
        "elapsed = time.time() - t0\n",
        "print(f\"[gen] done {len(preds)} in {elapsed:.1f}s  ({len(preds)/max(elapsed,1):.1f} ex/s)\")\n",
        "\n",
        "# ---------------- save ----------------\n",
        "out_df = work_df[[SRC_COL, REF_COL]].copy()\n",
        "out_df[\"prediction\"] = preds[:len(out_df)]\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "out_df.to_csv(OUT_CSV, index=False)\n",
        "print(f\"[save] {OUT_CSV}\")\n",
        "\n",
        "# ---------------- ROUGE ----------------\n",
        "def compute_rouge(refs, hyps):\n",
        "    scorer = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\",\"rougeLsum\"], use_stemmer=True)\n",
        "    agg = scoring.BootstrapAggregator()\n",
        "    for r, h in zip(refs, hyps):\n",
        "        agg.add_scores(scorer.score(r, h))\n",
        "    res = agg.aggregate()\n",
        "    return {\n",
        "        k: {\n",
        "            \"f\":  res[k].mid.fmeasure,\n",
        "            \"p\":  res[k].mid.precision,\n",
        "            \"r\":  res[k].mid.recall,\n",
        "        } for k in res\n",
        "    }\n",
        "\n",
        "scores = compute_rouge(out_df[REF_COL].astype(str).tolist(),\n",
        "                       out_df[\"prediction\"].astype(str).tolist())\n",
        "\n",
        "print(\"\\n=== ROUGE (POINTER_OFF) ===\")\n",
        "for k in (\"rouge1\",\"rouge2\",\"rougeL\",\"rougeLsum\"):\n",
        "    s = scores[k]\n",
        "    print(f\"{k:8s}  F1={s['f']:.4f}  (P={s['p']:.4f} R={s['r']:.4f})\")\n",
        "\n",
        "# ---------------- quick sanity print ----------------\n",
        "def lens(series):\n",
        "    L = series.astype(str).str.split().apply(len)\n",
        "    return {\n",
        "        \"count\": int(L.shape[0]),\n",
        "        \"mean\": float(L.mean()),\n",
        "        \"median\": float(L.median()),\n",
        "        \"p25\": float(L.quantile(0.25)),\n",
        "        \"p75\": float(L.quantile(0.75)),\n",
        "        \"min\": int(L.min()),\n",
        "        \"max\": int(L.max()),\n",
        "    }\n",
        "\n",
        "print(\"\\n[len stats]\")\n",
        "print(\" gold:\", lens(out_df[REF_COL]))\n",
        "print(\" pred:\", lens(out_df[\"prediction\"]))\n",
        "\n",
        "for idx in [0, len(out_df)//2, len(out_df)-1]:\n",
        "    print(f\"\\n--- idx {idx} ---\")\n",
        "    print(\"REF:\", out_df.iloc[idx][REF_COL][:300])\n",
        "    print(\"PRED:\", out_df.iloc[idx][\"prediction\"][:300])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRTtRU9K3uUs"
      },
      "source": [
        "> Found the best configs (pointer off)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VU2rTIK1JFM"
      },
      "outputs": [],
      "source": [
        "from transformers import LogitsProcessor\n",
        "\n",
        "class EosAfterThreshold(LogitsProcessor):\n",
        "    def __init__(self, eos_token_id, start_step=70, boost=5.0):\n",
        "        self.eos_token_id = eos_token_id\n",
        "        self.start_step = start_step\n",
        "        self.boost = boost\n",
        "    def __call__(self, input_ids, scores):\n",
        "        if input_ids.shape[1] >= self.start_step:\n",
        "            scores[:, self.eos_token_id] += self.boost\n",
        "        return scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Mc-u6LXtwZU",
        "outputId": "06677411-d8b4-482e-ad7b-4c627102656c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[decode] ckpt=/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000\n",
            "[data]  val=/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\n",
            "[data] columns -> source='article'  reference='highlights'\n",
            "[data] total rows: 13368\n",
            "[data] n=1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[gen] device=cuda dtype=torch.float32\n",
            "[mode] POINTER_OFF (plain generate)\n",
            "[gen] 160/1000\n",
            "[gen] 320/1000\n",
            "[gen] 480/1000\n",
            "[gen] 640/1000\n",
            "[gen] 800/1000\n",
            "[gen] 960/1000\n",
            "[gen] done 1000 in 255.1s  (3.9 ex/s)\n",
            "[save] /content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000/val_POINTEROFF_beam5_min28_max48.csv\n",
            "\n",
            "=== ROUGE (POINTER_OFF) ===\n",
            "rouge1    F1=0.4148  (P=0.4044 R=0.4552)\n",
            "rouge2    F1=0.1873  (P=0.1829 R=0.2051)\n",
            "rougeL    F1=0.2809  (P=0.2729 R=0.3091)\n",
            "rougeLsum  F1=0.2809  (P=0.2728 R=0.3091)\n",
            "\n",
            "[len stats]\n",
            " gold: {'count': 1000, 'mean': 58.368, 'median': 55.0, 'p25': 43.0, 'p75': 71.0, 'min': 14, 'max': 155}\n",
            " pred: {'count': 1000, 'mean': 59.663, 'median': 58.0, 'p25': 50.0, 'p75': 69.0, 'min': 30, 'max': 109}\n",
            "\n",
            "--- idx 0 ---\n",
            "REF: Kieran Carroll, of Luton, came forward today following a three-day search . Police previously released CCTV footage of father in their bid to locate him . When he saw his image online he sent a Facebook message to the police . He told them: 'I'm fine, so is my child, go look for some killers' He was arrested today for assaulting a woman on the day he went missing .\n",
            "PRED: Kieran Carroll, 22, of Luton, Bedfordshire, was reported missing at around midday on Tuesday. He was arrested today for assaulting a woman and damaging her mobile phone. Police released CCTV footage of him pushing a pram as he left a hotel. He was later released on bail and is due to report back to Luton police station on 31 March.\n",
            "\n",
            "--- idx 500 ---\n",
            "REF: Stevenage's Ronnie Henry accused Joss Labadie of biting him . Incident took place shortly before the end of their League Two clash . Stevenage manager Graham Westley says it 'has become a police matter' Hertfordshire police continuing to make inquiries into alleged bite . Labadie served a 10-match ban and was fined £2,000 in 2014 for biting .\n",
            "PRED: Joss Labadie, 24, was accused of sinking his teeth into the hand of Stevenage defender Ronnie Henry. The pair clashed near the touchline when Henry tried to wrestle the ball out of Labadie's arms. Henry immediately appeared to signal to the nearby assistant referee that he had been bitten. Police are continuing to make inquiries into the incident.\n",
            "\n",
            "--- idx 999 ---\n",
            "REF: Up to seven boys locked the helpless 10-year-old victim in a wooden cage . They poured petrol over the young boy's body before setting him alight . Luckily residents heard the child's cries and were able to save his life . Act is believed to be in imitation of ISIS' murder of Mu'ath al-Kasasbeh . Attack came after video emerged of another group of Yemeni boys reenacting the gruesome murder of 21 Egyptian Coptic Christians .\n",
            "PRED: As many as seven boys are believed to have attacked the as-yet unnamed victim in Al Dahthath village in Yemen's northern Ibb province. The defenceless child was then locked in a wooden cage and had petrol poured over his body. The attack is understood to have been a sickening imitation of the brutal murder of Jordanian pilot Mu'ath al-Kasasbeh. Local journalist Mohammad Mouzahem posted details of the attack on his Facebook page.\n"
          ]
        }
      ],
      "source": [
        "# ===== DECODE & EVAL (POINTER OFF — plain beam search) =====\n",
        "import os, time, math, json, gc\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from rouge_score import rouge_scorer, scoring\n",
        "\n",
        "# ---------------- paths ----------------\n",
        "CKPT_DIR = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000\"\n",
        "VAL_CSV  = \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "OUT_CSV  = os.path.join(CKPT_DIR, \"val_POINTEROFF_beam5_min28_max48.csv\") #change thisssss and save\n",
        "\n",
        "SRC_COL, REF_COL = \"article\", \"highlights\"\n",
        "\n",
        "# ---------------- knobs ----------------\n",
        "MAX_SRC_LEN     = 400\n",
        "N_EVAL          = 1000\n",
        "BATCH_SIZE      = 32\n",
        "NUM_BEAMS       = 5\n",
        "MIN_NEW         = 40\n",
        "MAX_NEW         = 120\n",
        "NO_REPEAT       = 4\n",
        "LENGTH_PENALTY  = 1.8\n",
        "forced_eos_token_id=tokenizer.eos_token_id\n",
        "print(f\"[decode] ckpt={CKPT_DIR}\")\n",
        "print(f\"[data]  val={VAL_CSV}\")\n",
        "\n",
        "# ---------------- load data ----------------\n",
        "def safe_read_csv(path, required):\n",
        "    last = None\n",
        "    for enc in (\"utf-8\", \"utf-8-sig\", \"latin1\"):\n",
        "        for eng in (\"c\",\"python\"):\n",
        "            try:\n",
        "                df = pd.read_csv(path, encoding=enc, engine=eng)\n",
        "                for c in required:\n",
        "                    if c not in df.columns:\n",
        "                        raise ValueError(f\"Missing column '{c}' in {path}\")\n",
        "                return df\n",
        "            except Exception as e:\n",
        "                last = e\n",
        "    raise last\n",
        "\n",
        "val_df = safe_read_csv(VAL_CSV, [SRC_COL, REF_COL])\n",
        "n_total = len(val_df)\n",
        "if N_EVAL is None or N_EVAL >= n_total:\n",
        "    work_df = val_df.reset_index(drop=True)\n",
        "else:\n",
        "    work_df = val_df.sample(N_EVAL, random_state=13).reset_index(drop=True)\n",
        "\n",
        "print(f\"[data] columns -> source='{SRC_COL}'  reference='{REF_COL}'\")\n",
        "print(f\"[data] total rows: {n_total}\")\n",
        "print(f\"[data] n={len(work_df)}\")\n",
        "\n",
        "# ---------------- load model/tokenizer ----------------\n",
        "tok = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "print(f\"[gen] device={device} dtype={next(model.parameters()).dtype}\")\n",
        "print(\"[mode] POINTER_OFF (plain generate)\")\n",
        "\n",
        "# ---------------- generation ----------------\n",
        "def generate_batch(texts):\n",
        "    enc = tok(\n",
        "        texts,\n",
        "        max_length=MAX_SRC_LEN,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "\n",
        "    out = model.generate(\n",
        "    input_ids=enc.input_ids,\n",
        "    attention_mask=enc.attention_mask,\n",
        "    num_beams=NUM_BEAMS,\n",
        "    length_penalty=LENGTH_PENALTY,\n",
        "    no_repeat_ngram_size=NO_REPEAT,\n",
        "    min_new_tokens=MIN_NEW,\n",
        "    max_new_tokens=MAX_NEW,\n",
        "    early_stopping=True,\n",
        "    forced_eos_token_id=tok.eos_token_id,\n",
        "    repetition_penalty=1.1,\n",
        "    logits_processor=[EosAfterThreshold(tok.eos_token_id, start_step=100)]\n",
        ")\n",
        "    # out: Tensor [B, T]\n",
        "    return tok.batch_decode(out, skip_special_tokens=True)\n",
        "\n",
        "preds = []\n",
        "i, n = 0, len(work_df)\n",
        "t0 = time.time()\n",
        "bs = BATCH_SIZE\n",
        "while i < n:\n",
        "    batch = work_df[SRC_COL].iloc[i:i+bs].astype(str).tolist()\n",
        "    try:\n",
        "        preds.extend(generate_batch(batch))\n",
        "        i += bs\n",
        "        if i % (bs*5) == 0 or i == n:\n",
        "            print(f\"[gen] {i}/{n}\")\n",
        "    except RuntimeError as e:\n",
        "        if \"CUDA out of memory\" in str(e) and bs > 4:\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            bs = max(4, bs // 2)\n",
        "            print(f\"[gen][OOM] reducing batch to {bs} and retrying…\")\n",
        "            continue\n",
        "        raise\n",
        "\n",
        "elapsed = time.time() - t0\n",
        "print(f\"[gen] done {len(preds)} in {elapsed:.1f}s  ({len(preds)/max(elapsed,1):.1f} ex/s)\")\n",
        "\n",
        "# ---------------- save ----------------\n",
        "out_df = work_df[[SRC_COL, REF_COL]].copy()\n",
        "out_df[\"prediction\"] = preds[:len(out_df)]\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "out_df.to_csv(OUT_CSV, index=False)\n",
        "print(f\"[save] {OUT_CSV}\")\n",
        "\n",
        "# ---------------- ROUGE ----------------\n",
        "def compute_rouge(refs, hyps):\n",
        "    scorer = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\",\"rougeLsum\"], use_stemmer=True)\n",
        "    agg = scoring.BootstrapAggregator()\n",
        "    for r, h in zip(refs, hyps):\n",
        "        agg.add_scores(scorer.score(r, h))\n",
        "    res = agg.aggregate()\n",
        "    return {\n",
        "        k: {\n",
        "            \"f\":  res[k].mid.fmeasure,\n",
        "            \"p\":  res[k].mid.precision,\n",
        "            \"r\":  res[k].mid.recall,\n",
        "        } for k in res\n",
        "    }\n",
        "\n",
        "scores = compute_rouge(out_df[REF_COL].astype(str).tolist(),\n",
        "                       out_df[\"prediction\"].astype(str).tolist())\n",
        "\n",
        "print(\"\\n=== ROUGE (POINTER_OFF) ===\")\n",
        "for k in (\"rouge1\",\"rouge2\",\"rougeL\",\"rougeLsum\"):\n",
        "    s = scores[k]\n",
        "    print(f\"{k:8s}  F1={s['f']:.4f}  (P={s['p']:.4f} R={s['r']:.4f})\")\n",
        "\n",
        "# ---------------- quick sanity print ----------------\n",
        "def lens(series):\n",
        "    L = series.astype(str).str.split().apply(len)\n",
        "    return {\n",
        "        \"count\": int(L.shape[0]),\n",
        "        \"mean\": float(L.mean()),\n",
        "        \"median\": float(L.median()),\n",
        "        \"p25\": float(L.quantile(0.25)),\n",
        "        \"p75\": float(L.quantile(0.75)),\n",
        "        \"min\": int(L.min()),\n",
        "        \"max\": int(L.max()),\n",
        "    }\n",
        "\n",
        "print(\"\\n[len stats]\")\n",
        "print(\" gold:\", lens(out_df[REF_COL]))\n",
        "print(\" pred:\", lens(out_df[\"prediction\"]))\n",
        "\n",
        "for idx in [0, len(out_df)//2, len(out_df)-1]:\n",
        "    print(f\"\\n--- idx {idx} ---\")\n",
        "    print(\"REF:\", out_df.iloc[idx][REF_COL][:600])\n",
        "    print(\"PRED:\", out_df.iloc[idx][\"prediction\"][:600])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFKR-lPgvumQ",
        "outputId": "0baf60e9-8955-4088-f664-8b42b884232b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entity P/R/F1: 0.3903240369613298 0.393491478703897 0.37091809253815416\n",
            "Number Accuracy: 0.29752958152958153\n",
            "Repetition Rate: 0.0\n",
            "Copy Rate: 0.9507226022141566\n"
          ]
        }
      ],
      "source": [
        "import spacy, re\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def entities(text):\n",
        "    return {ent.text for ent in nlp(text).ents}\n",
        "\n",
        "def numbers(text):\n",
        "    return set(re.findall(r\"\\d+\", text))\n",
        "\n",
        "def repetition_rate(text, n=3):\n",
        "    toks = text.split()\n",
        "    ngrams = [\" \".join(toks[i:i+n]) for i in range(len(toks)-n+1)]\n",
        "    return 1 - len(set(ngrams)) / max(1, len(ngrams))\n",
        "\n",
        "# loop over dataset\n",
        "entity_precisions, entity_recalls, entity_f1s = [], [], []\n",
        "num_accs, rep_rates, copy_rates = [], [], []\n",
        "\n",
        "for ref, pred, src in zip(out_df[REF_COL], out_df[\"prediction\"], out_df[SRC_COL]):\n",
        "    # Entities\n",
        "    ref_ents, pred_ents = entities(ref), entities(pred)\n",
        "    inter = ref_ents & pred_ents\n",
        "    if pred_ents:\n",
        "        entity_precisions.append(len(inter)/len(pred_ents))\n",
        "    if ref_ents:\n",
        "        entity_recalls.append(len(inter)/len(ref_ents))\n",
        "    if ref_ents and pred_ents:\n",
        "        f1 = 2*len(inter)/(len(ref_ents)+len(pred_ents))\n",
        "        entity_f1s.append(f1)\n",
        "\n",
        "    # Numbers\n",
        "    ref_nums, pred_nums = numbers(ref), numbers(pred)\n",
        "    num_accs.append(len(ref_nums & pred_nums)/max(1,len(ref_nums)))\n",
        "\n",
        "    # Repetition\n",
        "    rep_rates.append(repetition_rate(pred, n=4))\n",
        "\n",
        "    # Copy rate\n",
        "    src_tokens = set(src.split())\n",
        "    pred_tokens = pred.split()\n",
        "    copy_rates.append(sum(t in src_tokens for t in pred_tokens)/len(pred_tokens))\n",
        "\n",
        "print(\"Entity P/R/F1:\",\n",
        "      sum(entity_precisions)/len(entity_precisions),\n",
        "      sum(entity_recalls)/len(entity_recalls),\n",
        "      sum(entity_f1s)/len(entity_f1s))\n",
        "print(\"Number Accuracy:\", sum(num_accs)/len(num_accs))\n",
        "print(\"Repetition Rate:\", sum(rep_rates)/len(rep_rates))\n",
        "print(\"Copy Rate:\", sum(copy_rates)/len(copy_rates))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzVdisZk5J6z"
      },
      "source": [
        "Copy rate is computed as the proportion of generated tokens that also appear in the input article, regardless of whether the pointer module was active.\n",
        "\n",
        "When pointer_off: it’s simply a measure of extractiveness.\n",
        "\n",
        "When pointer_on: it reflects both the natural extractiveness of the dataset and the model’s explicit copy behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCtxAPPm4Zx2",
        "outputId": "1b4da872-f0e5-4da6-fcc6-3161479142c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ENTITY (overall) ===\n",
            "Precision: 0.3903240369613298\n",
            "Recall:    0.393491478703897\n",
            "F1:        0.37091809253815416\n",
            "\n",
            "=== ENTITY (by type) ===\n",
            "ORG       P=0.338 R=0.330 F1=0.429\n",
            "PERSON    P=0.540 R=0.494 F1=0.532\n",
            "GPE       P=0.427 R=0.445 F1=0.559\n",
            "TIME      P=0.202 R=0.209 F1=0.448\n",
            "DATE      P=0.285 R=0.294 F1=0.336\n",
            "NORP      P=0.279 R=0.301 F1=0.569\n",
            "CARDINAL  P=0.281 R=0.285 F1=0.406\n",
            "MONEY     P=0.340 R=0.282 F1=0.524\n",
            "EVENT     P=0.241 R=0.293 F1=0.536\n",
            "ORDINAL   P=0.258 R=0.335 F1=0.758\n",
            "FAC       P=0.299 R=0.350 F1=0.733\n",
            "WORK_OF_ART  P=0.163 R=0.200 F1=0.611\n",
            "LOC       P=0.318 R=0.429 F1=0.791\n",
            "QUANTITY  P=0.375 R=0.339 F1=0.641\n",
            "PRODUCT   P=0.260 R=0.333 F1=0.857\n",
            "PERCENT   P=0.260 R=0.250 F1=0.893\n",
            "LAW       P=0.444 R=0.444 F1=1.000\n",
            "LANGUAGE  P=0.250 R=0.286 F1=1.000\n",
            "\n",
            "=== OTHER METRICS ===\n",
            "Number Accuracy: 0.4494404554827516\n",
            "Repetition Rate (3-gram): 0.0012353419600380445\n",
            "Repetition Rate (4-gram): 0.0\n",
            "Copy Rate: 0.9507226022141566\n"
          ]
        }
      ],
      "source": [
        "import spacy, re\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def numbers(text):\n",
        "    return set(re.findall(r\"\\d+\", text))\n",
        "\n",
        "def repetition_rate(text, n=3):\n",
        "    toks = text.split()\n",
        "    ngrams = [\" \".join(toks[i:i+n]) for i in range(len(toks)-n+1)]\n",
        "    return 1 - len(set(ngrams)) / max(1, len(ngrams))\n",
        "\n",
        "# Storage\n",
        "entity_scores = {}   # per type\n",
        "entity_precisions, entity_recalls, entity_f1s = [], [], []\n",
        "num_accs, rep3_rates, rep4_rates, copy_rates = [], [], [], []\n",
        "\n",
        "for ref, pred, src in zip(out_df[\"highlights\"], out_df[\"prediction\"], out_df[\"article\"]):\n",
        "    # Entities (with types)\n",
        "    ref_doc, pred_doc = nlp(ref), nlp(pred)\n",
        "    ref_ents = {(ent.text, ent.label_) for ent in ref_doc.ents}\n",
        "    pred_ents = {(ent.text, ent.label_) for ent in pred_doc.ents}\n",
        "\n",
        "    # overall set for entity F1\n",
        "    ref_ent_texts = {ent.text for ent in ref_doc.ents}\n",
        "    pred_ent_texts = {ent.text for ent in pred_doc.ents}\n",
        "    inter = ref_ent_texts & pred_ent_texts\n",
        "    if pred_ent_texts:\n",
        "        entity_precisions.append(len(inter)/len(pred_ent_texts))\n",
        "    if ref_ent_texts:\n",
        "        entity_recalls.append(len(inter)/len(ref_ent_texts))\n",
        "    if ref_ent_texts and pred_ent_texts:\n",
        "        f1 = 2*len(inter)/(len(ref_ent_texts)+len(pred_ent_texts))\n",
        "        entity_f1s.append(f1)\n",
        "\n",
        "    # per-type stats\n",
        "    for etype in set([e.label_ for e in ref_doc.ents] + [e.label_ for e in pred_doc.ents]):\n",
        "        r = {e.text for e in ref_doc.ents if e.label_ == etype}\n",
        "        p = {e.text for e in pred_doc.ents if e.label_ == etype}\n",
        "        inter = r & p\n",
        "        if etype not in entity_scores:\n",
        "            entity_scores[etype] = {\"p\":[], \"r\":[], \"f\":[]}\n",
        "        if p:\n",
        "            entity_scores[etype][\"p\"].append(len(inter)/len(p))\n",
        "        if r:\n",
        "            entity_scores[etype][\"r\"].append(len(inter)/len(r))\n",
        "        if p and r:\n",
        "            f = 2*len(inter)/(len(r)+len(p))\n",
        "            entity_scores[etype][\"f\"].append(f)\n",
        "\n",
        "    # Numbers\n",
        "    ref_nums, pred_nums = numbers(ref), numbers(pred)\n",
        "    if ref_nums:\n",
        "        num_accs.append(len(ref_nums & pred_nums)/len(ref_nums))\n",
        "\n",
        "    # Repetition\n",
        "    rep3_rates.append(repetition_rate(pred, n=3))\n",
        "    rep4_rates.append(repetition_rate(pred, n=4))\n",
        "\n",
        "    # Copy rate\n",
        "    src_tokens = set(src.split())\n",
        "    pred_tokens = pred.split()\n",
        "    if pred_tokens:\n",
        "        copy_rates.append(sum(t in src_tokens for t in pred_tokens)/len(pred_tokens))\n",
        "\n",
        "# ---- Aggregation ----\n",
        "print(\"=== ENTITY (overall) ===\")\n",
        "print(\"Precision:\", sum(entity_precisions)/len(entity_precisions))\n",
        "print(\"Recall:   \", sum(entity_recalls)/len(entity_recalls))\n",
        "print(\"F1:       \", sum(entity_f1s)/len(entity_f1s))\n",
        "\n",
        "print(\"\\n=== ENTITY (by type) ===\")\n",
        "for etype, vals in entity_scores.items():\n",
        "    print(f\"{etype:8s}  P={sum(vals['p'])/len(vals['p']):.3f} \"\n",
        "          f\"R={sum(vals['r'])/len(vals['r']):.3f} \"\n",
        "          f\"F1={sum(vals['f'])/len(vals['f']):.3f}\")\n",
        "\n",
        "print(\"\\n=== OTHER METRICS ===\")\n",
        "print(\"Number Accuracy:\", sum(num_accs)/len(num_accs))\n",
        "print(\"Repetition Rate (3-gram):\", sum(rep3_rates)/len(rep3_rates))\n",
        "print(\"Repetition Rate (4-gram):\", sum(rep4_rates)/len(rep4_rates))\n",
        "print(\"Copy Rate:\", sum(copy_rates)/len(copy_rates))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-aE8gXSV_6x"
      },
      "source": [
        "# Sample Sanity check and fine tunning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1228,
          "referenced_widgets": [
            "f1754a5de2244aea8e684ff105a2ae94"
          ]
        },
        "id": "XZ8i5P-YUXOf",
        "outputId": "eedb4202-8d1d-4685-baae-012fff23ff18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[device] cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
            "  self.gen = func(*args, **kwds)\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[pointer_head] loaded=True\n",
            "[probe] p_copy_mean=0.173 | p_gen_mean=0.827\n",
            "[data] 10 samples ready\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1754a5de2244aea8e684ff105a2ae94",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Eval:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== SAMPLE OUTPUTS (10 rows) ===\n",
            "\n",
            "--- idx 0 ---\n",
            "REF       : Arsene Wenger has qualified for the Champions League for last 17 seasons . But despite his managerial pedigree he boasts no European trophies . Iconic boss has only reached on final and three semi-finals . Monaco game is hugely poignant after 3-1 defeat in first leg at the Emirates . Arsenal lost the 2006 final and could have gone on to win the cup in 2004 . The Champions League shows missing link in Wenger's tactical psyche . CLICK HERE for all the latest Arsenal news .\n",
            "BASE (b5): Arsene Wenger's Champions League record is as phenomenal as it is deeply disappointing. The club's record of qualifying for the tournament for 17 consecutive years is extraordinary. Yet having contested the Champions League and its predecessor the European Cup 18 times now (including twice at Monaco), he has never won it. Wenger has been in the final just once and the semi-finals three times, once with Monaco and twice with Arsenal.\n",
            "PTR-BEAM: Wenger's record of qualifying for the Champions League for 17 consecutive years is extraordinary. Yet having contested the Champions League and its predecessor the European Cup 18 times now (including twice at Monaco), he has never won it. Wenger is undoubtedly one of the managerial greats but his European record is a blot on his CV.\n",
            "\n",
            "--- idx 1 ---\n",
            "REF       : Cristiano Ronaldo gone nearly a year without scoring a free-kick in La Liga . Real Madrid fans asking whether Gareth Bale should take over set-pieces . Bale scores a free-kick every 9.5 attempts while Ronaldo is every 15.6 . Gareth Bale: 'It is surreal to be called a Galactico' CLICK HERE for all the latest Real Madrid news .\n",
            "BASE (b5): Cristiano Ronaldo has gone almost a year without scoring a free-kick in La Liga. The 30-year-old last hit the back of the net from a set-piece in the Spanish league on March 26 last year. He has now failed to score a single goal with any of his last 51 free-kicks.\n",
            "PTR-BEAM: The 30-year-old last hit the back of the net from a set-piece in the Spanish league on March 26 last year. His last goal from a free-kick in any competition came in the Champions League last April against Bayern Munich. He now has now failed to score a single goal with any of his last 51 free-kicks.\n",
            "\n",
            "--- idx 2 ---\n",
            "REF       : Nathan Thompson allegedly killed 10 bull terriers aged six to eight weeks . He pleaded guilty to nine extra animal cruelty charges on Thursday . Police say he took them to bushland where he hit their heads with rocks . He was released on bail but is banned from being around animals alone . RSPCA confirmed the only pup that survived is doing well in their care .\n",
            "BASE (b5): A man from the NSW Hunter region killed a litter of puppies by smashing them over the head with a rock. The only pup to survive the attack has been named 'Lucky' and is being cared for by the RSPCA. Police allege a witness saw the 25-year-old man begin to kill the puppies. He drove away from the area and found five dead pups and another two still clinging to life. He pleaded guilty to nine extra animal cruelty charges.\n",
            "PTR-BEAM: Man from the NSW Hunter region who killed a litter of puppies by smashing them over the head with a rock. The only pup to survive the attack, nicknamed 'Lucky', is'stable' Police allege a witness saw the 25-year-old man begin to kill a number of the puppies, aged six to eight weeks. The man drove away from the area and found five dead pups and another two still clinging to life.\n",
            "\n",
            "--- idx 3 ---\n",
            "REF       : Heather Barwick says she is against gay marriage because she missed out on having her father around when she was growing up . The 31-year-old mother-of-four admits her mother's partner 'treated me as if I was her own daughter' She also says that her biological father 'wasn't a great guy,' and 'didn't bother coming around anymore' Despite being an advocate for gay marriage in her 20s, Barwick now says she's had a change of heart . 'My father's absence created a huge hole, and I ached every day. I loved my mom's partner, but she could never have replaced the father I lost'\n",
            "BASE (b5): Heather Barwick, a 31-year-old mother-of-four from South Carolina, says her mother left her father when she was 2 or 3 so that she could move in with the woman she loved. She also admits that her biological father 'wasn't a great guy' and 'didn't bother coming around anymore' Despite being an advocate for gay marriage in her 20s, Barwick now says she's had a change of heart.\n",
            "PTR-BEAM:  Heather Barwick, 31, says her mother left her father when she was 2 or 3 so that she could move in with the woman she loved. She says her mom raised me with her same-sex partner back in the '80s and '90s,' writes Barwick for the conservative publication The Federalist. Barwick recalls growing up in a very liberal and open-minded suburb surrounded by a 'tight-knit community of gay and lesbian friends' and says her mother's partner 'treated me as if I was her own daughter'\n",
            "\n",
            "--- idx 4 ---\n",
            "REF       : Adelaide Archbishop Philip Wilson has been charged with failing to report child sex abuse in his diocese in the 1970s . He has been the archbishop of the South Australian capital since 2011 . In the 1970s, Wilson did not report abuse carried out by another priest . If convicted of the crime, Wilson faces face up to two years behind bars .\n",
            "BASE (b5): The Archbishop of Adelaide has been charged with covering up child sex abuse. It is alleged Philip Wilson tried to hide abuse carried out by another priest in the 1970s. He was appointed archbishop for the South Australian capital in 2011. The charge comes after it was revealed the senior figure in the Catholic Church did not report priest Jim Fletcher who was accused of paedophilia. He was convicted of raping a 13-year-old boy in 1989.\n",
            "PTR-BEAM: Philip Wilson was appointed archbishop for the South Australian capital in 2011. The charge comes after it was revealed the senior figure in the Catholic Church did not report priest Jim Fletcher. Fletcher was accused of paedophilia while they were working in the Maitland region, north of Sydney in New South Wales. Fletcher ended up serving prison time from 1989 to 1991 after he was convicted of raping a 13-year-old boy. Five years later, the convicted paedophile died. A Special Commission of Inquiry conducted last year handed down that Fletcher 'had an extensive history of perpetrating child sexual abuse\n",
            "\n",
            "--- idx 5 ---\n",
            "REF       : Diane Nash has said that she didn't participate in a march across the Edmund Pettus bridge on Saturday because of the former president . She's said she made the choice 'when it was apparent that he was going to be part of it . Nash was a leader in the Freedom Riders movement . She and her and her husband at the time James Bevel drafted the first plan for the Selma voting rights movement .\n",
            "BASE (b5): Civil rights activist Diane Nash has said that she didn't participate in a march across the Edmund Pettus bridge in Selma, Alabama on Saturday. She said she made the choice 'when it was apparent that he was going to be part of it' She continued, claiming 'And George Bush stands for just the opposite' for violence, and war, and stolen elections'\n",
            "PTR-BEAM: Civil rights activist Diane Nash has said that she didn't participate in a march across the Edmund Pettus bridge in Selma, Alabama on Saturday. Nash revealed her reason to opt out of the Saturday event in an interview with NewsOne Now. She said she made the choice 'when it was apparent that he was going to be part of it' She continued, claiming 'And George Bush stands for just the opposite, for violence, and war, and stolen elections' NewsOne Now reported that Nash 'is widely considered the architect of many of King's campaigns, and one of the only women in\n",
            "\n",
            "--- idx 6 ---\n",
            "REF       : A teenager has been attacked by an angry swarm of bees after rolling his car on a Northern Territory highway . He was transporting a trailer-full of bees towards Katherine when he drove off the road, flipping the vehicle . The young driver escaped unhurt, but suffered welts from stings to his face .\n",
            "BASE (b5): The 18-year-old was towing a trailer with crates full of bees when the trailer's wheel clipped the edge of the road. The driver lost control, the vehicle fish-tailed and flipped about 10 metres into scrub land. The driver managed to crawl out with minor swelling and pain, but was otherwise uninjured.\n",
            "PTR-BEAM: Police say the 18-year-old was towing a trailer with crates full of bees when the trailer's wheel clipped the edge of the road, 17 km north of Katherine on Thursday afternoon. The driver lost control, the vehicle fish-tailed and flipped about 10 metres into scrub land. The young driver managed to crawl out with minor swelling and pain, but was otherwise uninjured.\n",
            "\n",
            "--- idx 7 ---\n",
            "REF       : Maven probe spotted ultraviolet auroral glow in northern hemisphere . What was suprising was how deep in the atmosphere aurora occurred . Dust was also found up to 190 miles (300 km) above planet's surface . No known process on Mars can explain the appearance of dust clouds .\n",
            "BASE (b5): Mars has a very thin atmosphere, and the sun's energetic particles hit it directly and penetrate deeper creating incredibly bright and vast light shows. But their behaviour doesn't always follow existing rules. Nasa scientists were recently stunned to discover aurora that reaches deep into the Martian atmosphere. It is similar to Earth's \"Northern Lights\" but penetrates deep into the atmosphere.\n",
            "PTR-BEAM: NASA scientists were recently stunned to discover aurora that reaches deep into the Martian atmosphere. The aurora is similar to Earth's \"Northern Lights\" but penetrates deep into the atmosphere. Earth has a magnetic shield, known as a magnetosphere, that protects its atmosphere from radiation from the sun. Aurora take place on Earth because some of the sun's energetic particles have managed to break through this shield. Mars, however, only has a very thin atmosphere. Its magnetosphere was lost billions of years ago turning a once watery world into the barren planet we see today. When solar particles hit the\n",
            "\n",
            "--- idx 8 ---\n",
            "REF       : Manchester City face Barcelona on Wednesday night at the Nou Camp . David Silva feels City must stop Barca from scoring in the second leg . City are 2-1 down after the first leg of the Champions League tie .\n",
            "BASE (b5): Manchester City playmaker David Silva believes they have to stop Barcelona scoring if they are to have any chance of pulling off a miracle. Silva and his team-mates are 2-1 down after the first leg of their Champions League last-16 tie. It is a task easier said than done to stop Barcelona from scoring at home. Lionel Messi and co will have something to say about City's progress when they come to the Nou Camp.\n",
            "PTR-BEAM: Man City playmaker David Silva believes they have to stop Barcelona scoring if they are to have any chance of pulling off a miracle. Silva and his City team-mates are 2-1 down after the first leg of their Champions League last-16 tie. It is a task easier said than done to stop Barcelona, led by Lionel Messi, from scoring at home. Messi (centre) and co will have something to say about City's progress when the come to the Nou Camp.\n",
            "\n",
            "--- idx 9 ---\n",
            "REF       : Forensic experts painstakingly try to identify 600 body parts of victims and have isolated 78 distinct DNA strands . Remains will be photographed and scanned in 3D before being placed in morgue until identification has occurred . It is hoped 95% will be identified within next three weeks as guards continue to keep 24-hour watch at site in Alps . Leading forensic expert Michael Tsokos told of horrifying task, saying: 'These images will never go out of my head'\n",
            "BASE (b5): Remains of the 150 passengers and crew are being categorised after co-pilot Andreas Lubitz deliberately flew the Airbus A320 into the French Alps on Tuesday. Investigators at the crash site have so far retrieved about 600 body parts. They have managed to isolate 78 distinct DNA strands from the remains. But not one body has been found intact by rescue crews.\n",
            "PTR-BEAM: Germany's most prestigious forensic scientist Michael Tsokos spoke of the work ahead. Investigators at the Germanwings crash site have so far retrieved about 600 body parts and have managed to isolate 78 distinct DNA strands. But not one body has been found intact. Forensic experts are photographing and 3D scanning each and every body part before placing them in a morgue until ID is fully confirmed.\n",
            "\n",
            "[pointer probe] p_copy_mean=0.523 | p_gen_mean=0.477\n"
          ]
        }
      ],
      "source": [
        "# ===== Pointer sanity: fair comparison (pointer-aware BEAM vs baseline BEAM) =====\n",
        "import os, gc, math, pandas as pd, torch, torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ---------------- paths ----------------\n",
        "CKPT_DIR = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000\"\n",
        "VAL_CSV  = \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "SRC_COL, REF_COL = \"article\", \"highlights\"\n",
        "\n",
        "# ---------------- device & attention mode ----------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"[device]\", device)\n",
        "try:\n",
        "    torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ---------------- load tokenizer + base model ----------------\n",
        "tok  = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
        "base = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR, attn_implementation=\"eager\").to(device).eval()\n",
        "\n",
        "# ---------------- CopyAwareBart (last-layer attn; prob-space mix; optional gate bias) ----------------\n",
        "class CopyAwareBart(nn.Module):\n",
        "    def __init__(self, base_model, tokenizer, lambda_cov=1.0, eps=1e-8, use_pointer=True, gate_bias=0.0):\n",
        "        super().__init__()\n",
        "        self.base   = base_model\n",
        "        self.tok    = tokenizer\n",
        "        self.lambda_cov = float(lambda_cov)\n",
        "        self.eps    = float(eps)\n",
        "        self.use_ptr= bool(use_pointer)\n",
        "        self.gate_bias = float(gate_bias)  # inference-only bias added to gate pre-activation\n",
        "\n",
        "        H = self.base.config.d_model\n",
        "        self.p_gen_linear = nn.Linear(3*H, 1)  # gate over [dec_hid, context, decoder_input]\n",
        "\n",
        "        # logs\n",
        "        self._last_ce_loss = self._last_cov_loss = None\n",
        "        self._last_p_copy_mean = self._last_p_gen_mean = None\n",
        "\n",
        "    def _mix_pointer(self, out, input_ids, attention_mask, decoder_input_ids):\n",
        "        logits = out.logits  # (B,T,V)\n",
        "        B,T,V  = logits.shape\n",
        "\n",
        "        # use ONLY the last decoder layer cross-attention (mean over heads)\n",
        "        last_ca = out.cross_attentions[-1].mean(1)  # (B,T,S)\n",
        "        attn = last_ca\n",
        "        if attention_mask is not None:\n",
        "            attn = attn.masked_fill(attention_mask[:, None, :] == 0, 0.0)\n",
        "        attn = attn / attn.sum(-1, keepdim=True).clamp_min(self.eps)  # normalize per step\n",
        "\n",
        "        dec_hid = out.decoder_hidden_states[-1]                         # (B,T,H)\n",
        "        ctx     = torch.bmm(attn, out.encoder_last_hidden_state)        # (B,T,H)\n",
        "\n",
        "        # decoder inputs -> embeddings\n",
        "        di = decoder_input_ids\n",
        "        dec_inp = self.base.get_input_embeddings()(di)                  # (B,T,H)\n",
        "\n",
        "        # gate (with optional bias to nudge toward copying)\n",
        "        gate_in = torch.cat([dec_hid, ctx, dec_inp], dim=-1)            # (B,T,3H)\n",
        "        p_gen   = torch.sigmoid(self.p_gen_linear(gate_in) + self.gate_bias).squeeze(-1)  # (B,T)\n",
        "        p_copy  = 1.0 - p_gen\n",
        "\n",
        "        # pointer distribution over vocab (probabilities)\n",
        "        copy_probs = torch.zeros_like(logits)                            # (B,T,V)\n",
        "        idx = input_ids.unsqueeze(1).expand(B, T, input_ids.size(1))     # (B,T,S)\n",
        "        copy_probs.scatter_add_(2, idx, attn)\n",
        "\n",
        "        # generator distribution\n",
        "        vocab_logp = F.log_softmax(logits, dim=-1)                      # (B,T,V)\n",
        "\n",
        "        # final mixture in log-space: log( p_gen*Pvocab + p_copy*Pcopy )\n",
        "        logp_final = torch.logaddexp(\n",
        "            vocab_logp + (p_gen + self.eps).log().unsqueeze(-1),\n",
        "            (copy_probs + self.eps).log() + (p_copy + self.eps).log().unsqueeze(-1)\n",
        "        )\n",
        "\n",
        "        # logs\n",
        "        self._last_p_copy_mean = float(p_copy.detach().mean().cpu())\n",
        "        self._last_p_gen_mean  = float(p_gen.detach().mean().cpu())\n",
        "        return logp_final, attn\n",
        "\n",
        "    def forward(self,\n",
        "        input_ids=None, attention_mask=None, labels=None,\n",
        "        decoder_input_ids=None, decoder_attention_mask=None,\n",
        "        output_attentions=True, output_hidden_states=True,\n",
        "        past_key_values=None, use_cache=False, **kwargs\n",
        "    ):\n",
        "        # prepare teacher-forced decoder inputs if training\n",
        "        if decoder_input_ids is None and labels is not None:\n",
        "            di = labels.clone()\n",
        "            di[di == -100] = self.tok.pad_token_id\n",
        "            di = torch.roll(di, 1, dims=1)\n",
        "            start_id = getattr(self.base.config, \"decoder_start_token_id\", None)\n",
        "            if start_id is None:\n",
        "                start_id = self.tok.bos_token_id if self.tok.bos_token_id is not None else self.tok.eos_token_id\n",
        "            di[:, 0] = start_id\n",
        "        else:\n",
        "            di = decoder_input_ids\n",
        "\n",
        "        out = self.base(\n",
        "            input_ids=input_ids, attention_mask=attention_mask, labels=None,\n",
        "            decoder_input_ids=di, decoder_attention_mask=decoder_attention_mask,\n",
        "            output_attentions=True, output_hidden_states=True,\n",
        "            past_key_values=past_key_values, use_cache=use_cache, return_dict=True,\n",
        "        )\n",
        "\n",
        "        if not self.use_ptr:\n",
        "            if labels is not None:\n",
        "                out.loss = F.cross_entropy(out.logits.view(-1, out.logits.size(-1)),\n",
        "                                           labels.view(-1), ignore_index=-100)\n",
        "            return out\n",
        "\n",
        "        logp_final, attn = self._mix_pointer(out, input_ids, attention_mask, di)\n",
        "        out.logits = logp_final  # replace with pointer-mixed log-probs\n",
        "\n",
        "        if labels is not None:\n",
        "            gold = labels\n",
        "            mask = gold.ne(-100).float()\n",
        "            logp_gold = logp_final.gather(-1, gold.unsqueeze(-1)).squeeze(-1)\n",
        "            nll = -(logp_gold * mask).sum() / mask.sum().clamp_min(1.0)\n",
        "\n",
        "            # coverage loss: use PREVIOUS coverage only (exclusive cumsum)\n",
        "            cov_prev = (attn.cumsum(dim=1) - attn).detach()\n",
        "            cov_loss = torch.min(attn, cov_prev).sum(-1).mean() * self.lambda_cov\n",
        "\n",
        "            out.loss = nll + cov_loss\n",
        "            self._last_ce_loss  = float(nll.detach().cpu())\n",
        "            self._last_cov_loss = float(cov_loss.detach().cpu())\n",
        "        return out\n",
        "\n",
        "def copyaware_generate(self, *args, **kwargs):\n",
        "    return self.base.generate(*args, **kwargs)\n",
        "CopyAwareBart.generate = copyaware_generate\n",
        "\n",
        "# ---------------- attach pointer head from 4k ----------------\n",
        "# tip: set gate_bias to something like -0.3 to gently increase copying at inference\n",
        "model_ptr = CopyAwareBart(base, tok, lambda_cov=1.0, use_pointer=True, gate_bias=-0.3).to(device).eval()\n",
        "ph_path = os.path.join(CKPT_DIR, \"pointer_head.pt\")\n",
        "sd = torch.load(ph_path, map_location=\"cpu\")\n",
        "loaded = False\n",
        "if isinstance(sd, dict) and any(k.startswith(\"p_gen_linear.\") for k in sd.keys()):\n",
        "    model_ptr.load_state_dict(sd, strict=False); loaded = True\n",
        "elif isinstance(sd, dict) and all(k in {\"weight\", \"bias\"} for k in sd.keys()):\n",
        "    model_ptr.p_gen_linear.load_state_dict(sd); loaded = True\n",
        "elif isinstance(sd, dict) and \"p_gen_linear\" in sd and all(k in {\"weight\",\"bias\"} for k in sd[\"p_gen_linear\"]):\n",
        "    model_ptr.p_gen_linear.load_state_dict(sd[\"p_gen_linear\"]); loaded = True\n",
        "print(f\"[pointer_head] loaded={loaded}\")\n",
        "\n",
        "# quick probe\n",
        "with torch.no_grad():\n",
        "    enc = tok([\"short test article\"], return_tensors=\"pt\").to(device)\n",
        "    lab = tok([\"short ref summary\"], return_tensors=\"pt\").input_ids.to(device)\n",
        "    lab[lab==tok.pad_token_id] = -100\n",
        "    _ = model_ptr(**enc, labels=lab)\n",
        "print(f\"[probe] p_copy_mean={model_ptr._last_p_copy_mean:.3f} | p_gen_mean={model_ptr._last_p_gen_mean:.3f}\")\n",
        "\n",
        "# ---------------- data ----------------\n",
        "def safe_read_csv(path, required):\n",
        "    last=None\n",
        "    for enc in (\"utf-8\",\"utf-8-sig\",\"latin1\"):\n",
        "        for eng in (\"c\",\"python\"):\n",
        "            try:\n",
        "                df = pd.read_csv(path, encoding=enc, engine=eng)\n",
        "                for c in required:\n",
        "                    if c not in df.columns: raise ValueError(f\"Missing column '{c}'\")\n",
        "                return df\n",
        "            except Exception as e:\n",
        "                last=e\n",
        "    raise last\n",
        "\n",
        "df = safe_read_csv(VAL_CSV, [SRC_COL, REF_COL]).sample(10, random_state=123).reset_index(drop=True)\n",
        "print(\"[data] 10 samples ready\")\n",
        "\n",
        "# ---------------- decoding knobs ----------------\n",
        "NUM_BEAMS      = 5\n",
        "MIN_NEW        = 40\n",
        "MAX_NEW        = 120\n",
        "NO_REPEAT      = 4\n",
        "LENGTH_PENALTY = 1.8\n",
        "MAX_SRC_LEN    = 400\n",
        "\n",
        "# ---------------- helpers ----------------\n",
        "def generate_baseline_beam_batch(texts):\n",
        "    enc = tok(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_SRC_LEN).to(device)\n",
        "    with torch.no_grad():\n",
        "        out = base.generate(\n",
        "            input_ids=enc.input_ids,\n",
        "            attention_mask=enc.attention_mask,\n",
        "            num_beams=NUM_BEAMS,\n",
        "            length_penalty=LENGTH_PENALTY,\n",
        "            no_repeat_ngram_size=NO_REPEAT,\n",
        "            min_new_tokens=MIN_NEW,\n",
        "            max_new_tokens=MAX_NEW,\n",
        "            early_stopping=True,\n",
        "            forced_eos_token_id=tok.eos_token_id,\n",
        "            repetition_penalty=1.1,\n",
        "            return_dict_in_generate=False,\n",
        "        )\n",
        "    return tok.batch_decode(out, skip_special_tokens=True)\n",
        "\n",
        "def _ban_repeating_ngrams(logits: torch.Tensor, seq: torch.Tensor, ngram: int):\n",
        "    \"\"\"\n",
        "    logits: (V,) log-probs for the next token\n",
        "    seq: (1, L) or (L,) int64 tensor of generated token ids\n",
        "    ngram: size of no-repeat constraint\n",
        "    \"\"\"\n",
        "    if ngram <= 0:\n",
        "        return\n",
        "\n",
        "    # normalize to 1-D python list of tokens\n",
        "    if seq.dim() == 2:\n",
        "        toks = seq[0].tolist()\n",
        "        L = seq.size(1)\n",
        "    else:\n",
        "        toks = seq.tolist()\n",
        "        L = seq.size(0)\n",
        "\n",
        "    if L < max(1, ngram - 1):\n",
        "        return\n",
        "\n",
        "    if ngram == 1:\n",
        "        banned = set(toks)\n",
        "        if banned:\n",
        "            logits[list(banned)] = -1e9\n",
        "        return\n",
        "\n",
        "    prefix = tuple(toks[-(ngram - 1):])\n",
        "    banned = set()\n",
        "    for i in range(len(toks) - ngram + 1):\n",
        "        if tuple(toks[i:i + ngram - 1]) == prefix:\n",
        "            banned.add(toks[i + ngram - 1])\n",
        "    if banned:\n",
        "        logits[list(banned)] = -1e9\n",
        "\n",
        "def generate_pointer_beam_batch(texts):\n",
        "    # simple per-example beam search using pointer-mixed logits\n",
        "    results = []\n",
        "    with torch.no_grad():\n",
        "        for text in texts:\n",
        "            enc = tok([text], return_tensors=\"pt\", truncation=True, max_length=MAX_SRC_LEN).to(device)\n",
        "            start_id = getattr(base.config, \"decoder_start_token_id\", None)\n",
        "            if start_id is None:\n",
        "                start_id = tok.bos_token_id if tok.bos_token_id is not None else tok.eos_token_id\n",
        "\n",
        "            # beams: list of (seq_tensor, cum_logprob, finished)\n",
        "            beams = [(torch.tensor([[start_id]], device=device, dtype=torch.long), 0.0, False)]\n",
        "            for step in range(MAX_NEW):\n",
        "                new_beams = []\n",
        "                for seq, score, done in beams:\n",
        "                    if done:\n",
        "                        new_beams.append((seq, score, True))\n",
        "                        continue\n",
        "\n",
        "                    out = model_ptr(\n",
        "                        input_ids=enc.input_ids,\n",
        "                        attention_mask=enc.attention_mask,\n",
        "                        decoder_input_ids=seq,\n",
        "                        use_cache=False\n",
        "                    )\n",
        "                    step_logits = out.logits[:, -1, :].squeeze(0).clone()  # (V,) log-probs\n",
        "\n",
        "                    # enforce min length by masking EOS\n",
        "                    if step + 1 < MIN_NEW and tok.eos_token_id is not None:\n",
        "                        step_logits[tok.eos_token_id] = -1e9\n",
        "\n",
        "                    # no-repeat ngrams (pass 2-D seq; helper supports both)\n",
        "                    _ban_repeating_ngrams(step_logits, seq, NO_REPEAT)\n",
        "\n",
        "                    # top-k = beamsize\n",
        "                    topk_logp, topk_ids = torch.topk(step_logits, k=NUM_BEAMS)\n",
        "                    for k in range(NUM_BEAMS):\n",
        "                        tid  = topk_ids[k].unsqueeze(0).unsqueeze(0)  # (1,1)\n",
        "                        nseq = torch.cat([seq, tid], dim=1)\n",
        "                        nfin = (tid.item() == tok.eos_token_id)\n",
        "                        # length penalty (GNMT-style): higher penalty favors shorter sequences\n",
        "                        raw = score + float(topk_logp[k].item())\n",
        "                        lp  = ((5.0 + nseq.size(1))**LENGTH_PENALTY) / ((5.0 + 1.0)**LENGTH_PENALTY)\n",
        "                        nsc = raw / lp\n",
        "                        new_beams.append((nseq, nsc, nfin))\n",
        "\n",
        "                # prune to top beams\n",
        "                new_beams.sort(key=lambda x: x[1], reverse=True)\n",
        "                beams = new_beams[:NUM_BEAMS]\n",
        "\n",
        "                # stop if all beams finished\n",
        "                if all(b[2] for b in beams):\n",
        "                    break\n",
        "\n",
        "            best_seq = max(beams, key=lambda x: x[1])[0]\n",
        "            results.append(tok.batch_decode(best_seq, skip_special_tokens=True)[0])\n",
        "    return results\n",
        "\n",
        "# ---------------- run on 10 samples (fair) ----------------\n",
        "srcs = df[SRC_COL].astype(str).tolist()\n",
        "refs = df[REF_COL].astype(str).tolist()\n",
        "\n",
        "base_preds = []\n",
        "ptr_preds  = []\n",
        "bs = 4\n",
        "for i in tqdm(range(0, len(srcs), bs), desc=\"Eval\"):\n",
        "    batch = srcs[i:i+bs]\n",
        "    try:\n",
        "        base_preds.extend(generate_baseline_beam_batch(batch))\n",
        "        ptr_preds.extend(generate_pointer_beam_batch(batch))\n",
        "    except RuntimeError as e:\n",
        "        if \"CUDA out of memory\" in str(e) and bs > 1:\n",
        "            torch.cuda.empty_cache(); gc.collect()\n",
        "            bs = max(1, bs//2)\n",
        "            print(\"[OOM] lowering batch_size to\", bs)\n",
        "            for s in batch:\n",
        "                base_preds.extend(generate_baseline_beam_batch([s]))\n",
        "                ptr_preds.extend(generate_pointer_beam_batch([s]))\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "print(\"\\n=== SAMPLE OUTPUTS (10 rows) ===\")\n",
        "for k in range(len(df)):\n",
        "    print(f\"\\n--- idx {k} ---\")\n",
        "    print(\"REF       :\", refs[k][:2500])\n",
        "    print(\"BASE (b5):\", base_preds[k][:2800])\n",
        "    print(\"PTR-BEAM:\", ptr_preds[k][:2800])\n",
        "\n",
        "print(f\"\\n[pointer probe] p_copy_mean={model_ptr._last_p_copy_mean:.3f} | p_gen_mean={model_ptr._last_p_gen_mean:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tiny Sample Search to Find the best decoding Knobs"
      ],
      "metadata": {
        "id": "Xpn8nqlMXjjg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdSunde2Yxno",
        "outputId": "29255e00-6c41-462e-aa7e-a7a194206178"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scoring your current settings on 10-sample slice…\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BASE (b5) ROUGE F1 -> rouge1:43.08, rouge2:23.04, rougeLsum:30.54 | avg words ≈ 65.3\n",
            "PTR-BEAM  ROUGE F1 -> rouge1:41.98, rouge2:19.47, rougeLsum:29.39  | avg words ≈ 71.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "BASE sweep (10 samples) [mn, mx, R1, R2, RLsum, avg_words]\n",
            " 30,  70 -> 40.71, 19.92, 29.57 |  55.5\n",
            " 30,  90 -> 41.93, 22.37, 29.94 |  66.4\n",
            " 30, 110 -> 42.91, 22.83, 30.39 |  65.3\n",
            " 40,  80 -> 42.80, 22.29, 30.42 |  61.3\n",
            " 40, 100 -> 42.67, 22.68, 29.96 |  65.3\n",
            " 40, 120 -> 43.13, 22.74, 30.93 |  65.3\n",
            " 50,  90 -> 42.01, 21.59, 29.86 |  66.4\n",
            " 50, 110 -> 42.34, 22.93, 30.26 |  66.1\n",
            " 50, 130 -> 42.68, 22.48, 30.17 |  66.1\n",
            "\n",
            "BEST BASE: min_new=40, max_new=120 | RLsum=30.93 | avg_words≈65.3\n",
            "\n",
            "PTR-BEAM sweep (10 samples) [mn, mx, R1, R2, RLsum, avg_words]\n",
            " 30,  70 -> 40.14, 17.01, 27.53 |  54.0\n",
            " 30,  90 -> 40.84, 19.20, 28.65 |  66.9\n",
            " 30, 110 -> 41.08, 17.58, 28.03 |  71.0\n",
            " 40,  80 -> 41.46, 18.90, 28.37 |  61.0\n",
            " 40, 100 -> 41.63, 19.06, 28.88 |  70.2\n",
            " 40, 120 -> 41.91, 19.28, 29.26 |  71.2\n",
            " 50,  90 -> 40.73, 18.93, 28.34 |  66.9\n",
            " 50, 110 -> 41.18, 17.40, 27.91 |  71.0\n",
            " 50, 130 -> 41.88, 19.51, 29.14 |  71.2\n",
            "\n",
            "BEST PTR-BEAM: min_new=40, max_new=120 | RLsum=29.26 | avg_words≈71.2\n",
            "\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "# ===== ROUGE + light length sweep for BASE vs PTR-BEAM  =====\n",
        "import math, statistics, torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# --- ROUGE utils ---\n",
        "try:\n",
        "    from rouge_score import rouge_scorer, scoring\n",
        "except ModuleNotFoundError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"rouge-score\", \"-q\"])\n",
        "    from rouge_score import rouge_scorer, scoring\n",
        "\n",
        "def _compute_rouge(preds, refs):\n",
        "    scorer = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\"], use_stemmer=True)\n",
        "    agg = scoring.BootstrapAggregator()\n",
        "    for p, r in zip(preds, refs):\n",
        "        agg.add_scores(scorer.score(r, p))  # signature: score(target, prediction)\n",
        "    res = agg.aggregate()\n",
        "    return {k: {\n",
        "        \"p\": res[k].mid.precision * 100,\n",
        "        \"r\": res[k].mid.recall    * 100,\n",
        "        \"f\": res[k].mid.fmeasure  * 100\n",
        "    } for k in res}\n",
        "\n",
        "def _avg_words(texts):  # rough length proxy\n",
        "    return statistics.mean(len(t.split()) for t in texts) if texts else 0.0\n",
        "\n",
        "# --- reusable decoding helpers that accept min/max (don’t touch your originals) ---\n",
        "def decode_baseline(texts, min_new, max_new, num_beams=5, no_repeat=4, length_penalty=1.8, rep_pen=1.1, max_src_len=400):\n",
        "    enc = tok(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_src_len).to(device)\n",
        "    with torch.no_grad():\n",
        "        out = base.generate(\n",
        "            input_ids=enc.input_ids,\n",
        "            attention_mask=enc.attention_mask,\n",
        "            num_beams=num_beams,\n",
        "            length_penalty=length_penalty,\n",
        "            no_repeat_ngram_size=no_repeat,\n",
        "            min_new_tokens=min_new,\n",
        "            max_new_tokens=max_new,\n",
        "            early_stopping=True,\n",
        "            forced_eos_token_id=tok.eos_token_id,\n",
        "            repetition_penalty=rep_pen,\n",
        "            return_dict_in_generate=False,\n",
        "        )\n",
        "    return tok.batch_decode(out, skip_special_tokens=True)\n",
        "\n",
        "def _ban_repeating_ngrams_inplace(step_logits, seq_1d, n):\n",
        "    if n is None or n <= 0:\n",
        "        return\n",
        "    toks = seq_1d.tolist()\n",
        "    L = len(toks)\n",
        "    if n == 1:\n",
        "        for t in set(toks):\n",
        "            step_logits[t] = -1e9\n",
        "        return\n",
        "    if L < n-1:\n",
        "        return\n",
        "    prefix = tuple(toks[-(n-1):])\n",
        "    banned = set()\n",
        "    for i in range(L - n + 1):\n",
        "        if tuple(toks[i:i+n-1]) == prefix:\n",
        "            banned.add(toks[i+n-1])\n",
        "    for t in banned:\n",
        "        step_logits[t] = -1e9\n",
        "\n",
        "def decode_pointer_beam(texts, min_new, max_new, num_beams=5, no_repeat=4, length_penalty=1.8, max_src_len=400):\n",
        "    results = []\n",
        "    with torch.no_grad():\n",
        "        for text in texts:\n",
        "            enc = tok([text], return_tensors=\"pt\", truncation=True, max_length=max_src_len).to(device)\n",
        "            start_id = getattr(base.config, \"decoder_start_token_id\", None)\n",
        "            if start_id is None:\n",
        "                start_id = tok.bos_token_id if tok.bos_token_id is not None else tok.eos_token_id\n",
        "\n",
        "            beams = [(torch.tensor([[start_id]], device=device, dtype=torch.long), 0.0, False)]\n",
        "            for step in range(max_new):\n",
        "                new_beams = []\n",
        "                for seq, score, done in beams:\n",
        "                    if done:\n",
        "                        new_beams.append((seq, score, True))\n",
        "                        continue\n",
        "                    out = model_ptr(\n",
        "                        input_ids=enc.input_ids,\n",
        "                        attention_mask=enc.attention_mask,\n",
        "                        decoder_input_ids=seq,\n",
        "                        use_cache=False\n",
        "                    )\n",
        "                    step_logits = out.logits[:, -1, :].squeeze(0).clone()  # log-probs\n",
        "                    # enforce min length by masking EOS\n",
        "                    if step + 1 < min_new and tok.eos_token_id is not None:\n",
        "                        step_logits[tok.eos_token_id] = -1e9\n",
        "                    # no-repeat ngrams\n",
        "                    if no_repeat and no_repeat > 0:\n",
        "                        _ban_repeating_ngrams_inplace(step_logits, seq.squeeze(0), no_repeat)\n",
        "                    # expand top-k\n",
        "                    topk_logp, topk_ids = torch.topk(step_logits, k=num_beams)\n",
        "                    for k in range(num_beams):\n",
        "                        tid  = topk_ids[k].view(1,1)\n",
        "                        nseq = torch.cat([seq, tid], dim=1)\n",
        "                        nfin = (tid.item() == tok.eos_token_id)\n",
        "                        raw = score + float(topk_logp[k].item())\n",
        "                        # Google NMT length penalty (applied at *expansion* time for pruning)\n",
        "                        lp  = ((5.0 + nseq.size(1))**length_penalty) / ((5.0 + 1.0)**length_penalty)\n",
        "                        nsc = raw / lp\n",
        "                        new_beams.append((nseq, nsc, nfin))\n",
        "                new_beams.sort(key=lambda x: x[1], reverse=True)\n",
        "                beams = new_beams[:num_beams]\n",
        "                if all(b[2] for b in beams):\n",
        "                    break\n",
        "            best_seq = max(beams, key=lambda x: x[1])[0]\n",
        "            results.append(tok.batch_decode(best_seq, skip_special_tokens=True)[0])\n",
        "    return results\n",
        "\n",
        "# --- pull mini data (reuses your df if present) ---\n",
        "try:\n",
        "    _df = df\n",
        "except NameError:\n",
        "    _df = None\n",
        "    raise RuntimeError(\"df not found; run your sampling cell first.\")\n",
        "\n",
        "srcs = _df.loc[:, \"article\"].astype(str).tolist()\n",
        "refs  = _df.loc[:, \"highlights\"].astype(str).tolist()\n",
        "\n",
        "# --- 1) ROUGE on your current settings (fast sanity check) ---\n",
        "print(\"Scoring your current settings on 10-sample slice…\")\n",
        "curr_base = decode_baseline(srcs, min_new=40, max_new=120)   # match your defaults\n",
        "curr_ptr  = decode_pointer_beam(srcs, min_new=40, max_new=120)\n",
        "\n",
        "r_base = _compute_rouge(curr_base, refs)\n",
        "r_ptr  = _compute_rouge(curr_ptr,  refs)\n",
        "\n",
        "def _fmt(res): return \", \".join([f\"{k}:{res[k]['f']:.2f}\" for k in (\"rouge1\",\"rouge2\",\"rougeLsum\")])\n",
        "\n",
        "print(f\"BASE (b{5}) ROUGE F1 -> {_fmt(r_base)} | avg words ≈ { _avg_words(curr_base):.1f}\")\n",
        "print(f\"PTR-BEAM  ROUGE F1 -> {_fmt(r_ptr)}  | avg words ≈ { _avg_words(curr_ptr):.1f}\")\n",
        "\n",
        "# --- 2) Light grid over min/max new tokens (kept small to stay fast) ---\n",
        "min_grid = [30, 40, 50]\n",
        "max_grow = [40, 60, 80]  # each tried as min + grow\n",
        "\n",
        "def _sweep(decode_fn, name):\n",
        "    best = None\n",
        "    rows = []\n",
        "    for mn in min_grid:\n",
        "        for grow in max_grow:\n",
        "            mx = mn + grow\n",
        "            preds = decode_fn(srcs, min_new=mn, max_new=mx)\n",
        "            rs = _compute_rouge(preds, refs)\n",
        "            rL = rs[\"rougeLsum\"][\"f\"]\n",
        "            rows.append((name, mn, mx, rs[\"rouge1\"][\"f\"], rs[\"rouge2\"][\"f\"], rL, _avg_words(preds)))\n",
        "            if best is None or rL > best[2]:\n",
        "                best = (mn, mx, rL, rs, _avg_words(preds))\n",
        "    # pretty print\n",
        "    print(f\"\\n{name} sweep (10 samples) [mn, mx, R1, R2, RLsum, avg_words]\")\n",
        "    for row in rows:\n",
        "        print(f\"{row[1]:>3}, {row[2]:>3} -> {row[3]:5.2f}, {row[4]:5.2f}, {row[5]:5.2f} | {row[6]:5.1f}\")\n",
        "    print(f\"\\nBEST {name}: min_new={best[0]}, max_new={best[1]} | RLsum={best[2]:.2f} | avg_words≈{best[4]:.1f}\")\n",
        "    return best\n",
        "\n",
        "best_base = _sweep(decode_baseline, \"BASE\")\n",
        "best_ptr  = _sweep(decode_pointer_beam, \"PTR-BEAM\")\n",
        "\n",
        "print(\"\\nDone.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        },
        "id": "SebuuSo7dCdf",
        "outputId": "4b3c1764-e862-42b7-989f-52402ceabb19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entity/number coverage — per-sample:\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "detail"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-9016afd3-01d6-4374-942f-dcf04df8a823\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idx</th>\n",
              "      <th>len_ref</th>\n",
              "      <th>len_base</th>\n",
              "      <th>len_ptr</th>\n",
              "      <th>NE_cov_base_%</th>\n",
              "      <th>NE_cov_ptr_%</th>\n",
              "      <th>NE_prec_base_src_%</th>\n",
              "      <th>NE_prec_ptr_src_%</th>\n",
              "      <th>NUMNER_cov_base_%</th>\n",
              "      <th>NUMNER_cov_ptr_%</th>\n",
              "      <th>...</th>\n",
              "      <th>DIG_cov_base_%</th>\n",
              "      <th>DIG_cov_ptr_%</th>\n",
              "      <th>DIG_prec_base_src_%</th>\n",
              "      <th>DIG_prec_ptr_src_%</th>\n",
              "      <th>miss_NE_base</th>\n",
              "      <th>miss_NE_ptr</th>\n",
              "      <th>miss_NUM_base</th>\n",
              "      <th>miss_NUM_ptr</th>\n",
              "      <th>miss_DIG_base</th>\n",
              "      <th>miss_DIG_ptr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>87</td>\n",
              "      <td>71</td>\n",
              "      <td>64</td>\n",
              "      <td>50.00</td>\n",
              "      <td>25.00</td>\n",
              "      <td>100.00</td>\n",
              "      <td>80.00</td>\n",
              "      <td>16.67</td>\n",
              "      <td>16.67</td>\n",
              "      <td>...</td>\n",
              "      <td>25.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.00</td>\n",
              "      <td>arsene wenger, emirates, european, wenger</td>\n",
              "      <td>arsenal, arsene wenger, emirates, monaco, the ...</td>\n",
              "      <td>2004, 2006, 3, first, last 17 seasons</td>\n",
              "      <td>2004, 2006, 3, first, last 17 seasons</td>\n",
              "      <td>2004, 2006, 3-1</td>\n",
              "      <td>2004, 2006, 3-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>59</td>\n",
              "      <td>51</td>\n",
              "      <td>67</td>\n",
              "      <td>14.29</td>\n",
              "      <td>14.29</td>\n",
              "      <td>100.00</td>\n",
              "      <td>100.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.00</td>\n",
              "      <td>cristiano, galactico, gareth bale, madrid, rea...</td>\n",
              "      <td>cristiano, galactico, gareth bale, madrid, rea...</td>\n",
              "      <td>15.6, 9.5, nearly a year</td>\n",
              "      <td>15.6, 9.5, nearly a year</td>\n",
              "      <td>15.6, 9.5</td>\n",
              "      <td>15.6, 9.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>68</td>\n",
              "      <td>80</td>\n",
              "      <td>63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>50.00</td>\n",
              "      <td>66.67</td>\n",
              "      <td>71.43</td>\n",
              "      <td>25.00</td>\n",
              "      <td>25.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.00</td>\n",
              "      <td>nathan thompson, rspca</td>\n",
              "      <td>rspca</td>\n",
              "      <td>10, six to eight weeks, thursday</td>\n",
              "      <td>10, six to eight weeks, thursday</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>103</td>\n",
              "      <td>67</td>\n",
              "      <td>72</td>\n",
              "      <td>100.00</td>\n",
              "      <td>50.00</td>\n",
              "      <td>100.00</td>\n",
              "      <td>100.00</td>\n",
              "      <td>100.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.67</td>\n",
              "      <td></td>\n",
              "      <td>barwick</td>\n",
              "      <td></td>\n",
              "      <td>31-year-old</td>\n",
              "      <td></td>\n",
              "      <td>31-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>63</td>\n",
              "      <td>76</td>\n",
              "      <td>66</td>\n",
              "      <td>66.67</td>\n",
              "      <td>66.67</td>\n",
              "      <td>100.00</td>\n",
              "      <td>75.00</td>\n",
              "      <td>66.67</td>\n",
              "      <td>33.33</td>\n",
              "      <td>...</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.00</td>\n",
              "      <td>wilson</td>\n",
              "      <td>wilson</td>\n",
              "      <td>up to two years</td>\n",
              "      <td>the 1970s, up to two years</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>76</td>\n",
              "      <td>61</td>\n",
              "      <td>81</td>\n",
              "      <td>66.67</td>\n",
              "      <td>66.67</td>\n",
              "      <td>100.00</td>\n",
              "      <td>100.00</td>\n",
              "      <td>50.00</td>\n",
              "      <td>50.00</td>\n",
              "      <td>...</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.00</td>\n",
              "      <td>james bevel</td>\n",
              "      <td>james bevel</td>\n",
              "      <td>first</td>\n",
              "      <td>first</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>54</td>\n",
              "      <td>51</td>\n",
              "      <td>75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>100.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>100.00</td>\n",
              "      <td>100.00</td>\n",
              "      <td>...</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.00</td>\n",
              "      <td>katherine, northern territory</td>\n",
              "      <td>katherine, northern territory</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>49</td>\n",
              "      <td>60</td>\n",
              "      <td>87</td>\n",
              "      <td>50.00</td>\n",
              "      <td>50.00</td>\n",
              "      <td>100.00</td>\n",
              "      <td>50.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.00</td>\n",
              "      <td>maven</td>\n",
              "      <td>maven</td>\n",
              "      <td>190 miles, 300 km</td>\n",
              "      <td>190 miles, 300 km</td>\n",
              "      <td>190, 300</td>\n",
              "      <td>190, 300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>40</td>\n",
              "      <td>75</td>\n",
              "      <td>51</td>\n",
              "      <td>80.00</td>\n",
              "      <td>60.00</td>\n",
              "      <td>100.00</td>\n",
              "      <td>80.00</td>\n",
              "      <td>50.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.00</td>\n",
              "      <td>the champions league</td>\n",
              "      <td>the champions league, the nou camp</td>\n",
              "      <td>second, wednesday night</td>\n",
              "      <td>2, first, second, wednesday night</td>\n",
              "      <td></td>\n",
              "      <td>2-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>79</td>\n",
              "      <td>61</td>\n",
              "      <td>53</td>\n",
              "      <td>50.00</td>\n",
              "      <td>50.00</td>\n",
              "      <td>100.00</td>\n",
              "      <td>100.00</td>\n",
              "      <td>20.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.00</td>\n",
              "      <td>michael tsokos</td>\n",
              "      <td>alps</td>\n",
              "      <td>24-hour, 600, 95%, next three weeks</td>\n",
              "      <td>24-hour, 600, 78, 95%, next three weeks</td>\n",
              "      <td>24-, 95</td>\n",
              "      <td>24-, 600, 78, 95</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 22 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9016afd3-01d6-4374-942f-dcf04df8a823')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9016afd3-01d6-4374-942f-dcf04df8a823 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9016afd3-01d6-4374-942f-dcf04df8a823');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-55a48be6-f8d2-4fa1-ba09-089f6d870ec3\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-55a48be6-f8d2-4fa1-ba09-089f6d870ec3')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-55a48be6-f8d2-4fa1-ba09-089f6d870ec3 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_81f9b668-bccc-4e76-8e65-d799dd10496a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('detail')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_81f9b668-bccc-4e76-8e65-d799dd10496a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('detail');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   idx  len_ref  len_base  len_ptr  NE_cov_base_%  NE_cov_ptr_%  \\\n",
              "0    0       87        71       64          50.00         25.00   \n",
              "1    1       59        51       67          14.29         14.29   \n",
              "2    2       68        80       63           0.00         50.00   \n",
              "3    3      103        67       72         100.00         50.00   \n",
              "4    4       63        76       66          66.67         66.67   \n",
              "5    5       76        61       81          66.67         66.67   \n",
              "6    6       54        51       75           0.00          0.00   \n",
              "7    7       49        60       87          50.00         50.00   \n",
              "8    8       40        75       51          80.00         60.00   \n",
              "9    9       79        61       53          50.00         50.00   \n",
              "\n",
              "   NE_prec_base_src_%  NE_prec_ptr_src_%  NUMNER_cov_base_%  NUMNER_cov_ptr_%  \\\n",
              "0              100.00              80.00              16.67             16.67   \n",
              "1              100.00             100.00               0.00              0.00   \n",
              "2               66.67              71.43              25.00             25.00   \n",
              "3              100.00             100.00             100.00              0.00   \n",
              "4              100.00              75.00              66.67             33.33   \n",
              "5              100.00             100.00              50.00             50.00   \n",
              "6              100.00               0.00             100.00            100.00   \n",
              "7              100.00              50.00               0.00              0.00   \n",
              "8              100.00              80.00              50.00              0.00   \n",
              "9              100.00             100.00              20.00              0.00   \n",
              "\n",
              "   ...  DIG_cov_base_%  DIG_cov_ptr_%  DIG_prec_base_src_%  \\\n",
              "0  ...            25.0           25.0                100.0   \n",
              "1  ...             0.0            0.0                100.0   \n",
              "2  ...             0.0            0.0                100.0   \n",
              "3  ...           100.0            0.0                100.0   \n",
              "4  ...           100.0          100.0                100.0   \n",
              "5  ...           100.0          100.0                100.0   \n",
              "6  ...           100.0          100.0                100.0   \n",
              "7  ...             0.0            0.0                100.0   \n",
              "8  ...           100.0            0.0                100.0   \n",
              "9  ...            50.0            0.0                100.0   \n",
              "\n",
              "   DIG_prec_ptr_src_%                                       miss_NE_base  \\\n",
              "0              100.00          arsene wenger, emirates, european, wenger   \n",
              "1              100.00  cristiano, galactico, gareth bale, madrid, rea...   \n",
              "2              100.00                             nathan thompson, rspca   \n",
              "3               66.67                                                      \n",
              "4              100.00                                             wilson   \n",
              "5              100.00                                        james bevel   \n",
              "6              100.00                      katherine, northern territory   \n",
              "7              100.00                                              maven   \n",
              "8              100.00                               the champions league   \n",
              "9              100.00                                     michael tsokos   \n",
              "\n",
              "                                         miss_NE_ptr  \\\n",
              "0  arsenal, arsene wenger, emirates, monaco, the ...   \n",
              "1  cristiano, galactico, gareth bale, madrid, rea...   \n",
              "2                                              rspca   \n",
              "3                                            barwick   \n",
              "4                                             wilson   \n",
              "5                                        james bevel   \n",
              "6                      katherine, northern territory   \n",
              "7                                              maven   \n",
              "8                 the champions league, the nou camp   \n",
              "9                                               alps   \n",
              "\n",
              "                           miss_NUM_base  \\\n",
              "0  2004, 2006, 3, first, last 17 seasons   \n",
              "1               15.6, 9.5, nearly a year   \n",
              "2       10, six to eight weeks, thursday   \n",
              "3                                          \n",
              "4                        up to two years   \n",
              "5                                  first   \n",
              "6                                          \n",
              "7                      190 miles, 300 km   \n",
              "8                second, wednesday night   \n",
              "9    24-hour, 600, 95%, next three weeks   \n",
              "\n",
              "                              miss_NUM_ptr    miss_DIG_base      miss_DIG_ptr  \n",
              "0    2004, 2006, 3, first, last 17 seasons  2004, 2006, 3-1   2004, 2006, 3-1  \n",
              "1                 15.6, 9.5, nearly a year        15.6, 9.5         15.6, 9.5  \n",
              "2         10, six to eight weeks, thursday               10                10  \n",
              "3                              31-year-old                                31-  \n",
              "4               the 1970s, up to two years                                     \n",
              "5                                    first                                     \n",
              "6                                                                              \n",
              "7                        190 miles, 300 km         190, 300          190, 300  \n",
              "8        2, first, second, wednesday night                                2-1  \n",
              "9  24-hour, 600, 78, 95%, next three weeks          24-, 95  24-, 600, 78, 95  \n",
              "\n",
              "[10 rows x 22 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Aggregate summary:\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"summary\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"avg_len_ref\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 67.8,\n        \"max\": 67.8,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          67.8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_len_base\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 65.3,\n        \"max\": 65.3,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          65.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_len_ptr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 67.9,\n        \"max\": 67.9,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          67.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NE_cov_base_% (mean)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 47.76,\n        \"max\": 47.76,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          47.76\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NE_cov_ptr_% (mean)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 43.26,\n        \"max\": 43.26,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          43.26\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NE_prec_base_src_% (mean)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 96.67,\n        \"max\": 96.67,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          96.67\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NE_prec_ptr_src_% (mean)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 75.64,\n        \"max\": 75.64,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          75.64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NUMNER_cov_base_% (mean)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 42.83,\n        \"max\": 42.83,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          42.83\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NUMNER_cov_ptr_% (mean)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 22.5,\n        \"max\": 22.5,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          22.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NUMNER_prec_base_src_% (mean)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 100.0,\n        \"max\": 100.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          100.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NUMNER_prec_ptr_src_% (mean)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 89.17,\n        \"max\": 89.17,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          89.17\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DIG_cov_base_% (mean)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 57.5,\n        \"max\": 57.5,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          57.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DIG_cov_ptr_% (mean)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 32.5,\n        \"max\": 32.5,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          32.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DIG_prec_base_src_% (mean)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 100.0,\n        \"max\": 100.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          100.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DIG_prec_ptr_src_% (mean)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 96.67,\n        \"max\": 96.67,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          96.67\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "summary"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-96b5c799-fea1-48fc-881a-7af97a9dbd34\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>avg_len_ref</th>\n",
              "      <th>avg_len_base</th>\n",
              "      <th>avg_len_ptr</th>\n",
              "      <th>NE_cov_base_% (mean)</th>\n",
              "      <th>NE_cov_ptr_% (mean)</th>\n",
              "      <th>NE_prec_base_src_% (mean)</th>\n",
              "      <th>NE_prec_ptr_src_% (mean)</th>\n",
              "      <th>NUMNER_cov_base_% (mean)</th>\n",
              "      <th>NUMNER_cov_ptr_% (mean)</th>\n",
              "      <th>NUMNER_prec_base_src_% (mean)</th>\n",
              "      <th>NUMNER_prec_ptr_src_% (mean)</th>\n",
              "      <th>DIG_cov_base_% (mean)</th>\n",
              "      <th>DIG_cov_ptr_% (mean)</th>\n",
              "      <th>DIG_prec_base_src_% (mean)</th>\n",
              "      <th>DIG_prec_ptr_src_% (mean)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>67.8</td>\n",
              "      <td>65.3</td>\n",
              "      <td>67.9</td>\n",
              "      <td>47.76</td>\n",
              "      <td>43.26</td>\n",
              "      <td>96.67</td>\n",
              "      <td>75.64</td>\n",
              "      <td>42.83</td>\n",
              "      <td>22.5</td>\n",
              "      <td>100.0</td>\n",
              "      <td>89.17</td>\n",
              "      <td>57.5</td>\n",
              "      <td>32.5</td>\n",
              "      <td>100.0</td>\n",
              "      <td>96.67</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-96b5c799-fea1-48fc-881a-7af97a9dbd34')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-96b5c799-fea1-48fc-881a-7af97a9dbd34 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-96b5c799-fea1-48fc-881a-7af97a9dbd34');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_f04d648b-6945-4251-bb62-135f78fd6c2b\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('summary')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f04d648b-6945-4251-bb62-135f78fd6c2b button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('summary');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   avg_len_ref  avg_len_base  avg_len_ptr  NE_cov_base_% (mean)  \\\n",
              "0         67.8          65.3         67.9                 47.76   \n",
              "\n",
              "   NE_cov_ptr_% (mean)  NE_prec_base_src_% (mean)  NE_prec_ptr_src_% (mean)  \\\n",
              "0                43.26                      96.67                     75.64   \n",
              "\n",
              "   NUMNER_cov_base_% (mean)  NUMNER_cov_ptr_% (mean)  \\\n",
              "0                     42.83                     22.5   \n",
              "\n",
              "   NUMNER_prec_base_src_% (mean)  NUMNER_prec_ptr_src_% (mean)  \\\n",
              "0                          100.0                         89.17   \n",
              "\n",
              "   DIG_cov_base_% (mean)  DIG_cov_ptr_% (mean)  DIG_prec_base_src_% (mean)  \\\n",
              "0                   57.5                  32.5                       100.0   \n",
              "\n",
              "   DIG_prec_ptr_src_% (mean)  \n",
              "0                      96.67  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# === spaCy-based entity & number analysis on your 10-sample mini set ===\n",
        "import sys, subprocess, re, statistics, pandas as pd\n",
        "\n",
        "# --- ensure spaCy + model ---\n",
        "try:\n",
        "    import spacy\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"spacy\"])\n",
        "    import spacy\n",
        "\n",
        "# try small model first; fall back to download if missing\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"lemmatizer\",\"textcat\"])\n",
        "except OSError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\", \"-q\"])\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"lemmatizer\",\"textcat\"])\n",
        "\n",
        "# speed: keep only NER (and tok)\n",
        "nlp.enable_pipe(\"ner\")\n",
        "if \"tagger\" in nlp.pipe_names: nlp.disable_pipe(\"tagger\")\n",
        "if \"parser\" in nlp.pipe_names: nlp.disable_pipe(\"parser\")\n",
        "if \"attribute_ruler\" in nlp.pipe_names: nlp.disable_pipe(\"attribute_ruler\")\n",
        "\n",
        "# --- knobs ---\n",
        "TARGET_TYPES = {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"NORP\",\"FAC\",\"EVENT\",\"WORK_OF_ART\",\"PRODUCT\"}\n",
        "NUMERIC_TYPES = {\"DATE\",\"TIME\",\"PERCENT\",\"MONEY\",\"QUANTITY\",\"CARDINAL\",\"ORDINAL\"}\n",
        "\n",
        "def _norm_span(s: str) -> str:\n",
        "    s = s.strip()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    s = re.sub(r\"^[\\\"'“”‘’()\\[\\]{}.,;:]+|[\\\"'“”‘’()\\[\\]{}.,;:]+$\", \"\", s)\n",
        "    return s.lower()\n",
        "\n",
        "def ents_of(text: str, types=None):\n",
        "    if not text: return set()\n",
        "    doc = nlp(text)\n",
        "    out = set()\n",
        "    for e in doc.ents:\n",
        "        if types is None or e.label_ in types:\n",
        "            out.add(_norm_span(e.text))\n",
        "    return out\n",
        "\n",
        "def digits_of(text: str):\n",
        "    if not text: return set()\n",
        "    nums = re.findall(r\"\\b\\d[\\d,.\\-/]*\\b\", text)\n",
        "    clean = set()\n",
        "    for n in nums:\n",
        "        x = n.replace(\",\", \"\").rstrip(\".\")\n",
        "        if x:\n",
        "            clean.add(x)\n",
        "    return clean\n",
        "\n",
        "def coverage(ref_set, pred_set):\n",
        "    if not ref_set: return 1.0\n",
        "    return len(ref_set & pred_set) / len(ref_set)\n",
        "\n",
        "def precision(pred_set, universe):\n",
        "    if not pred_set: return 1.0\n",
        "    return len(pred_set & universe) / len(pred_set)\n",
        "\n",
        "# --- get notebook data ---\n",
        "try:\n",
        "    srcs = df[\"article\"].astype(str).tolist()\n",
        "    refs  = df[\"highlights\"].astype(str).tolist()\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"I can't find `df` with columns article/highlights. Run your generation/sampling cell first.\") from e\n",
        "\n",
        "try:\n",
        "    base = list(base_preds)\n",
        "    ptr  = list(ptr_preds)\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"I can't find `base_preds` / `ptr_preds`. Run your generation cell that creates them.\") from e\n",
        "\n",
        "# --- compute per-sample rows ---\n",
        "rows = []\n",
        "for i, (s,r,b,p) in enumerate(zip(srcs, refs, base, ptr)):\n",
        "    # lengths\n",
        "    lr, lb, lp = len(r.split()), len(b.split()), len(p.split())\n",
        "\n",
        "    # named entities (content) + numeric entities\n",
        "    er = ents_of(r, TARGET_TYPES)\n",
        "    eb = ents_of(b, TARGET_TYPES)\n",
        "    ep = ents_of(p, TARGET_TYPES)\n",
        "    es = ents_of(s, TARGET_TYPES)\n",
        "\n",
        "    # numeric/date entities from spaCy\n",
        "    ner_num_r = ents_of(r, NUMERIC_TYPES)\n",
        "    ner_num_b = ents_of(b, NUMERIC_TYPES)\n",
        "    ner_num_p = ents_of(p, NUMERIC_TYPES)\n",
        "    ner_num_s = ents_of(s, NUMERIC_TYPES)\n",
        "\n",
        "    # raw digits fallback (robust to NER misses)\n",
        "    dig_r = digits_of(r); dig_b = digits_of(b); dig_p = digits_of(p); dig_s = digits_of(s)\n",
        "\n",
        "    rows.append({\n",
        "        \"idx\": i,\n",
        "        \"len_ref\": lr, \"len_base\": lb, \"len_ptr\": lp,\n",
        "\n",
        "        # named-entity coverage/precision (content types)\n",
        "        \"NE_cov_base_%\": round(100*coverage(er, eb), 2),\n",
        "        \"NE_cov_ptr_%\":  round(100*coverage(er, ep), 2),\n",
        "        \"NE_prec_base_src_%\": round(100*precision(eb, es), 2),\n",
        "        \"NE_prec_ptr_src_%\":  round(100*precision(ep, es), 2),\n",
        "\n",
        "        # numeric/date coverage/precision via NER\n",
        "        \"NUMNER_cov_base_%\": round(100*coverage(ner_num_r, ner_num_b), 2),\n",
        "        \"NUMNER_cov_ptr_%\":  round(100*coverage(ner_num_r, ner_num_p), 2),\n",
        "        \"NUMNER_prec_base_src_%\": round(100*precision(ner_num_b, ner_num_s), 2),\n",
        "        \"NUMNER_prec_ptr_src_%\":  round(100*precision(ner_num_p, ner_num_s), 2),\n",
        "\n",
        "        # raw digit coverage/precision (fallback)\n",
        "        \"DIG_cov_base_%\": round(100*coverage(dig_r, dig_b), 2),\n",
        "        \"DIG_cov_ptr_%\":  round(100*coverage(dig_r, dig_p), 2),\n",
        "        \"DIG_prec_base_src_%\": round(100*precision(dig_b, dig_s), 2),\n",
        "        \"DIG_prec_ptr_src_%\":  round(100*precision(dig_p, dig_s), 2),\n",
        "\n",
        "        # quick diffs for inspection (first few missed)\n",
        "        \"miss_NE_base\": \", \".join(sorted(list(er - eb))[:5]),\n",
        "        \"miss_NE_ptr\":  \", \".join(sorted(list(er - ep))[:5]),\n",
        "        \"miss_NUM_base\": \", \".join(sorted(list(ner_num_r - ner_num_b))[:5]),\n",
        "        \"miss_NUM_ptr\":  \", \".join(sorted(list(ner_num_r - ner_num_p))[:5]),\n",
        "        \"miss_DIG_base\": \", \".join(sorted(list(dig_r - dig_b))[:5]),\n",
        "        \"miss_DIG_ptr\":  \", \".join(sorted(list(dig_r - dig_p))[:5]),\n",
        "    })\n",
        "\n",
        "detail = pd.DataFrame(rows)\n",
        "\n",
        "summary = pd.DataFrame([{\n",
        "    \"avg_len_ref\": round(detail[\"len_ref\"].mean(),1),\n",
        "    \"avg_len_base\": round(detail[\"len_base\"].mean(),1),\n",
        "    \"avg_len_ptr\": round(detail[\"len_ptr\"].mean(),1),\n",
        "\n",
        "    \"NE_cov_base_% (mean)\": round(detail[\"NE_cov_base_%\"].mean(),2),\n",
        "    \"NE_cov_ptr_% (mean)\":  round(detail[\"NE_cov_ptr_%\"].mean(),2),\n",
        "    \"NE_prec_base_src_% (mean)\": round(detail[\"NE_prec_base_src_%\"].mean(),2),\n",
        "    \"NE_prec_ptr_src_% (mean)\":  round(detail[\"NE_prec_ptr_src_%\"].mean(),2),\n",
        "\n",
        "    \"NUMNER_cov_base_% (mean)\": round(detail[\"NUMNER_cov_base_%\"].mean(),2),\n",
        "    \"NUMNER_cov_ptr_% (mean)\":  round(detail[\"NUMNER_cov_ptr_%\"].mean(),2),\n",
        "    \"NUMNER_prec_base_src_% (mean)\": round(detail[\"NUMNER_prec_base_src_%\"].mean(),2),\n",
        "    \"NUMNER_prec_ptr_src_% (mean)\":  round(detail[\"NUMNER_prec_ptr_src_%\"].mean(),2),\n",
        "\n",
        "    \"DIG_cov_base_% (mean)\": round(detail[\"DIG_cov_base_%\"].mean(),2),\n",
        "    \"DIG_cov_ptr_% (mean)\":  round(detail[\"DIG_cov_ptr_%\"].mean(),2),\n",
        "    \"DIG_prec_base_src_% (mean)\": round(detail[\"DIG_prec_base_src_%\"].mean(),2),\n",
        "    \"DIG_prec_ptr_src_% (mean)\":  round(detail[\"DIG_prec_ptr_src_%\"].mean(),2),\n",
        "}])\n",
        "\n",
        "print(\"Entity/number coverage — per-sample:\")\n",
        "display(detail)\n",
        "print(\"\\nAggregate summary:\")\n",
        "display(summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b67tYAfEhQR9"
      },
      "source": [
        "# Decoding and Generating 1k summaries for bart baseline and bart pointer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577,
          "referenced_widgets": [
            "66be19cfe4df4666ad1fdefb9b1df83e"
          ]
        },
        "id": "gKq4OC7Oh4O4",
        "outputId": "5a888d75-2aa8-4278-cbe3-f0576f71434d",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[device] cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
            "  self.gen = func(*args, **kwds)\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[pointer_head] loaded=True\n",
            "[data] 1000 samples ready\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "66be19cfe4df4666ad1fdefb9b1df83e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "BASE decode (batched):   0%|          | 0/125 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1729001098.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrcs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBASE_BATCH\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mbase_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_baseline_beam_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA out of memory\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mBASE_BATCH\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1729001098.py\u001b[0m in \u001b[0;36mgenerate_baseline_beam_batch\u001b[0;34m(texts)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_SRC_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         out = base.generate(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2646\u001b[0m             )\n\u001b[1;32m   2647\u001b[0m             \u001b[0;31m# 12. run beam sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m             result = self._beam_search(\n\u001b[0m\u001b[1;32m   2649\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_beam_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   4094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4095\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4096\u001b[0;31m             model_kwargs = self._update_model_kwargs_for_generation(\n\u001b[0m\u001b[1;32m   4097\u001b[0m                 \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4098\u001b[0m                 \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_update_model_kwargs_for_generation\u001b[0;34m(self, outputs, model_kwargs, is_encoder_decoder, num_new_tokens)\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m             \u001b[0mpast_positions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cache_position\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m             new_positions = torch.arange(\n\u001b[0m\u001b[1;32m   1016\u001b[0m                 \u001b[0mpast_positions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_positions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_new_tokens\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_positions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m             ).to(past_positions.device)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ===== 1K summaries (BASE beam-5 vs PTR-BEAM) — save to Drive =====\n",
        "import os, gc, time, math, pandas as pd, torch, torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ---------------- paths ----------------\n",
        "CKPT_DIR = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000\"\n",
        "VAL_CSV  = \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "SRC_COL, REF_COL = \"article\", \"highlights\"\n",
        "\n",
        "# where to save\n",
        "OUT_DIR  = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000\"\n",
        "OUT_FILE = \"1k sum base and pointer.csv\"\n",
        "\n",
        "# ---------------- device & attention mode ----------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"[device]\", device)\n",
        "try:\n",
        "    torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ---------------- load tokenizer + base model ----------------\n",
        "tok  = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
        "base = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR, attn_implementation=\"eager\").to(device).eval()\n",
        "\n",
        "# silence harmless generate() warnings about attentions/hidden states\n",
        "try:\n",
        "    gcfg = base.generation_config\n",
        "    gcfg.output_attentions = False\n",
        "    gcfg.output_hidden_states = False\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ---------------- CopyAwareBart (last-layer attn; prob-space mix; optional gate bias) ----------------\n",
        "class CopyAwareBart(nn.Module):\n",
        "    def __init__(self, base_model, tokenizer, lambda_cov=1.0, eps=1e-8, use_pointer=True, gate_bias=0.0):\n",
        "        super().__init__()\n",
        "        self.base   = base_model\n",
        "        self.tok    = tokenizer\n",
        "        self.lambda_cov = float(lambda_cov)\n",
        "        self.eps    = float(eps)\n",
        "        self.use_ptr= bool(use_pointer)\n",
        "        self.gate_bias = float(gate_bias)  # inference-only bias added to gate pre-activation\n",
        "\n",
        "        H = self.base.config.d_model\n",
        "        self.p_gen_linear = nn.Linear(3*H, 1)  # gate over [dec_hid, context, decoder_input]\n",
        "\n",
        "        # logs\n",
        "        self._last_ce_loss = self._last_cov_loss = None\n",
        "        self._last_p_copy_mean = self._last_p_gen_mean = None\n",
        "\n",
        "    def _mix_pointer(self, out, input_ids, attention_mask, decoder_input_ids):\n",
        "        logits = out.logits  # (B,T,V)\n",
        "        B,T,V  = logits.shape\n",
        "\n",
        "        # use ONLY the last decoder layer cross-attention (mean over heads)\n",
        "        last_ca = out.cross_attentions[-1].mean(1)  # (B,T,S)\n",
        "        attn = last_ca\n",
        "        if attention_mask is not None:\n",
        "            attn = attn.masked_fill(attention_mask[:, None, :] == 0, 0.0)\n",
        "        attn = attn / attn.sum(-1, keepdim=True).clamp_min(self.eps)  # normalize per step\n",
        "\n",
        "        dec_hid = out.decoder_hidden_states[-1]                         # (B,T,H)\n",
        "        ctx     = torch.bmm(attn, out.encoder_last_hidden_state)        # (B,T,H)\n",
        "\n",
        "        # decoder inputs -> embeddings\n",
        "        di = decoder_input_ids\n",
        "        dec_inp = self.base.get_input_embeddings()(di)                  # (B,T,H)\n",
        "\n",
        "        # gate (with optional bias to nudge toward copying)\n",
        "        gate_in = torch.cat([dec_hid, ctx, dec_inp], dim=-1)            # (B,T,3H)\n",
        "        p_gen   = torch.sigmoid(self.p_gen_linear(gate_in) + self.gate_bias).squeeze(-1)  # (B,T)\n",
        "        p_copy  = 1.0 - p_gen\n",
        "\n",
        "        # pointer distribution over vocab (probabilities)\n",
        "        copy_probs = torch.zeros_like(logits)                            # (B,T,V)\n",
        "        idx = input_ids.unsqueeze(1).expand(B, T, input_ids.size(1))     # (B,T,S)\n",
        "        copy_probs.scatter_add_(2, idx, attn)\n",
        "\n",
        "        # generator distribution\n",
        "        vocab_logp = F.log_softmax(logits, dim=-1)                      # (B,T,V)\n",
        "\n",
        "        # final mixture in log-space: log( p_gen*Pvocab + p_copy*Pcopy )\n",
        "        logp_final = torch.logaddexp(\n",
        "            vocab_logp + (p_gen + self.eps).log().unsqueeze(-1),\n",
        "            (copy_probs + self.eps).log() + (p_copy + self.eps).log().unsqueeze(-1)\n",
        "        )\n",
        "\n",
        "        # logs\n",
        "        self._last_p_copy_mean = float(p_copy.detach().mean().cpu())\n",
        "        self._last_p_gen_mean  = float(p_gen.detach().mean().cpu())\n",
        "        return logp_final, attn\n",
        "\n",
        "    def forward(self,\n",
        "        input_ids=None, attention_mask=None, labels=None,\n",
        "        decoder_input_ids=None, decoder_attention_mask=None,\n",
        "        output_attentions=True, output_hidden_states=True,\n",
        "        past_key_values=None, use_cache=False, **kwargs\n",
        "    ):\n",
        "        # prepare teacher-forced decoder inputs if training\n",
        "        if decoder_input_ids is None and labels is not None:\n",
        "            di = labels.clone()\n",
        "            di[di == -100] = self.tok.pad_token_id\n",
        "            di = torch.roll(di, 1, dims=1)\n",
        "            start_id = getattr(self.base.config, \"decoder_start_token_id\", None)\n",
        "            if start_id is None:\n",
        "                start_id = self.tok.bos_token_id if self.tok.bos_token_id is not None else self.tok.eos_token_id\n",
        "            di[:, 0] = start_id\n",
        "        else:\n",
        "            di = decoder_input_ids\n",
        "\n",
        "        out = self.base(\n",
        "            input_ids=input_ids, attention_mask=attention_mask, labels=None,\n",
        "            decoder_input_ids=di, decoder_attention_mask=decoder_attention_mask,\n",
        "            output_attentions=True, output_hidden_states=True,\n",
        "            past_key_values=past_key_values, use_cache=use_cache, return_dict=True,\n",
        "        )\n",
        "\n",
        "        if not self.use_ptr:\n",
        "            if labels is not None:\n",
        "                out.loss = F.cross_entropy(out.logits.view(-1, out.logits.size(-1)),\n",
        "                                           labels.view(-1), ignore_index=-100)\n",
        "            return out\n",
        "\n",
        "        logp_final, attn = self._mix_pointer(out, input_ids, attention_mask, di)\n",
        "        out.logits = logp_final  # replace with pointer-mixed log-probs\n",
        "\n",
        "        if labels is not None:\n",
        "            gold = labels\n",
        "            mask = gold.ne(-100).float()\n",
        "            logp_gold = logp_final.gather(-1, gold.unsqueeze(-1)).squeeze(-1)\n",
        "            nll = -(logp_gold * mask).sum() / mask.sum().clamp_min(1.0)\n",
        "\n",
        "            # coverage loss: use PREVIOUS coverage only (exclusive cumsum)\n",
        "            cov_prev = (attn.cumsum(dim=1) - attn).detach()\n",
        "            cov_loss = torch.min(attn, cov_prev).sum(-1).mean() * self.lambda_cov\n",
        "\n",
        "            out.loss = nll + cov_loss\n",
        "            self._last_ce_loss  = float(nll.detach().cpu())\n",
        "            self._last_cov_loss = float(cov_loss.detach().cpu())\n",
        "        return out\n",
        "\n",
        "def copyaware_generate(self, *args, **kwargs):\n",
        "    return self.base.generate(*args, **kwargs)\n",
        "CopyAwareBart.generate = copyaware_generate\n",
        "\n",
        "# ---------------- attach pointer head from 4k ----------------\n",
        "model_ptr = CopyAwareBart(base, tok, lambda_cov=1.0, use_pointer=True, gate_bias=-0.3).to(device).eval()\n",
        "ph_path = os.path.join(CKPT_DIR, \"pointer_head.pt\")\n",
        "sd = torch.load(ph_path, map_location=\"cpu\")\n",
        "loaded = False\n",
        "if isinstance(sd, dict) and any(k.startswith(\"p_gen_linear.\") for k in sd.keys()):\n",
        "    model_ptr.load_state_dict(sd, strict=False); loaded = True\n",
        "elif isinstance(sd, dict) and all(k in {\"weight\", \"bias\"} for k in sd.keys()):\n",
        "    model_ptr.p_gen_linear.load_state_dict(sd); loaded = True\n",
        "elif isinstance(sd, dict) and \"p_gen_linear\" in sd and all(k in {\"weight\",\"bias\"} for k in sd[\"p_gen_linear\"]):\n",
        "    model_ptr.p_gen_linear.load_state_dict(sd[\"p_gen_linear\"]); loaded = True\n",
        "print(f\"[pointer_head] loaded={loaded}\")\n",
        "\n",
        "# ---------------- data ----------------\n",
        "def safe_read_csv(path, required):\n",
        "    last=None\n",
        "    for enc in (\"utf-8\",\"utf-8-sig\",\"latin1\"):\n",
        "        for eng in (\"c\",\"python\"):\n",
        "            try:\n",
        "                df = pd.read_csv(path, encoding=enc, engine=eng)\n",
        "                for c in required:\n",
        "                    if c not in df.columns: raise ValueError(f\"Missing column '{c}'\")\n",
        "                return df\n",
        "            except Exception as e:\n",
        "                last=e\n",
        "    raise last\n",
        "\n",
        "full_df = safe_read_csv(VAL_CSV, [SRC_COL, REF_COL])\n",
        "N = min(1000, len(full_df))\n",
        "df = full_df.sample(N, random_state=123).reset_index(drop=True)\n",
        "print(f\"[data] {N} samples ready\")\n",
        "\n",
        "# ---------------- decoding knobs (your settings) ----------------\n",
        "NUM_BEAMS      = 5\n",
        "MIN_NEW        = 40\n",
        "MAX_NEW        = 120\n",
        "NO_REPEAT      = 4\n",
        "LENGTH_PENALTY = 1.8\n",
        "MAX_SRC_LEN    = 400\n",
        "\n",
        "# ---------------- helpers ----------------\n",
        "def generate_baseline_beam_batch(texts):\n",
        "    enc = tok(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_SRC_LEN).to(device)\n",
        "    with torch.no_grad():\n",
        "        out = base.generate(\n",
        "            input_ids=enc.input_ids,\n",
        "            attention_mask=enc.attention_mask,\n",
        "            num_beams=NUM_BEAMS,\n",
        "            length_penalty=LENGTH_PENALTY,\n",
        "            no_repeat_ngram_size=NO_REPEAT,\n",
        "            min_new_tokens=MIN_NEW,\n",
        "            max_new_tokens=MAX_NEW,\n",
        "            early_stopping=True,\n",
        "            forced_eos_token_id=tok.eos_token_id,\n",
        "            repetition_penalty=1.1,\n",
        "            return_dict_in_generate=False,\n",
        "        )\n",
        "    return tok.batch_decode(out, skip_special_tokens=True)\n",
        "\n",
        "def _ban_repeating_ngrams(logits: torch.Tensor, seq: torch.Tensor, ngram: int):\n",
        "    if ngram <= 0:\n",
        "        return\n",
        "    toks = seq[0].tolist() if seq.dim()==2 else seq.tolist()\n",
        "    L = len(toks)\n",
        "    if ngram == 1:\n",
        "        for t in set(toks):\n",
        "            logits[t] = -1e9\n",
        "        return\n",
        "    if L < ngram-1:\n",
        "        return\n",
        "    prefix = tuple(toks[-(ngram - 1):])\n",
        "    banned = set()\n",
        "    for i in range(L - ngram + 1):\n",
        "        if tuple(toks[i:i + ngram - 1]) == prefix:\n",
        "            banned.add(toks[i + ngram - 1])\n",
        "    for t in banned:\n",
        "        logits[t] = -1e9\n",
        "\n",
        "def generate_pointer_beam_one(text):\n",
        "    enc = tok([text], return_tensors=\"pt\", truncation=True, max_length=MAX_SRC_LEN).to(device)\n",
        "    start_id = getattr(base.config, \"decoder_start_token_id\", None)\n",
        "    if start_id is None:\n",
        "        start_id = tok.bos_token_id if tok.bos_token_id is not None else tok.eos_token_id\n",
        "    beams = [(torch.tensor([[start_id]], device=device, dtype=torch.long), 0.0, False)]\n",
        "    for step in range(MAX_NEW):\n",
        "        new_beams = []\n",
        "        for seq, score, done in beams:\n",
        "            if done:\n",
        "                new_beams.append((seq, score, True))\n",
        "                continue\n",
        "            out = model_ptr(\n",
        "                input_ids=enc.input_ids,\n",
        "                attention_mask=enc.attention_mask,\n",
        "                decoder_input_ids=seq,\n",
        "                use_cache=False\n",
        "            )\n",
        "            step_logits = out.logits[:, -1, :].squeeze(0).clone()  # (V,) log-probs\n",
        "            if step + 1 < MIN_NEW and tok.eos_token_id is not None:\n",
        "                step_logits[tok.eos_token_id] = -1e9\n",
        "            _ban_repeating_ngrams(step_logits, seq, NO_REPEAT)\n",
        "            topk_logp, topk_ids = torch.topk(step_logits, k=NUM_BEAMS)\n",
        "            for k in range(NUM_BEAMS):\n",
        "                tid  = topk_ids[k].view(1,1)\n",
        "                nseq = torch.cat([seq, tid], dim=1)\n",
        "                nfin = (tid.item() == tok.eos_token_id)\n",
        "                raw  = score + float(topk_logp[k].item())\n",
        "                lp   = ((5.0 + nseq.size(1))**LENGTH_PENALTY) / ((5.0 + 1.0)**LENGTH_PENALTY)\n",
        "                nsc  = raw / lp\n",
        "                new_beams.append((nseq, nsc, nfin))\n",
        "        new_beams.sort(key=lambda x: x[1], reverse=True)\n",
        "        beams = new_beams[:NUM_BEAMS]\n",
        "        if all(b[2] for b in beams):\n",
        "            break\n",
        "    best_seq = max(beams, key=lambda x: x[1])[0]\n",
        "    return tok.batch_decode(best_seq, skip_special_tokens=True)[0]\n",
        "\n",
        "# ---------------- run on 1k samples (fair) ----------------\n",
        "srcs = df[SRC_COL].astype(str).tolist()\n",
        "refs = df[REF_COL].astype(str).tolist()\n",
        "\n",
        "base_preds, ptr_preds = [], []\n",
        "\n",
        "# baseline batched\n",
        "BASE_BATCH = 8  # reduce if OOM\n",
        "for i in tqdm(range(0, len(srcs), BASE_BATCH), desc=\"BASE decode (batched)\"):\n",
        "    batch = srcs[i:i+BASE_BATCH]\n",
        "    try:\n",
        "        base_preds.extend(generate_baseline_beam_batch(batch))\n",
        "    except RuntimeError as e:\n",
        "        if \"CUDA out of memory\" in str(e) and BASE_BATCH > 1:\n",
        "            torch.cuda.empty_cache(); gc.collect()\n",
        "            for s in batch:\n",
        "                base_preds.extend(generate_baseline_beam_batch([s]))\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "# pointer per-example\n",
        "for s in tqdm(srcs, desc=\"PTR-BEAM decode (per-example)\"):\n",
        "    ptr_preds.append(generate_pointer_beam_one(s))\n",
        "\n",
        "print(f\"[probe] p_copy_mean={model_ptr._last_p_copy_mean:.3f} | p_gen_mean={model_ptr._last_p_gen_mean:.3f}\")\n",
        "\n",
        "# ---------------- save to Drive ----------------\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "out_path = os.path.join(OUT_DIR, OUT_FILE)\n",
        "\n",
        "n = min(len(srcs), len(base_preds), len(ptr_preds))\n",
        "out_df = pd.DataFrame({\n",
        "    \"idx\": range(n),\n",
        "    SRC_COL: srcs[:n],\n",
        "    REF_COL: refs[:n],\n",
        "    \"base_pred\": base_preds[:n],\n",
        "    \"ptr_pred\":  ptr_preds[:n],\n",
        "    \"len_base\":  [len(x.split()) for x in base_preds[:n]],\n",
        "    \"len_ptr\":   [len(x.split()) for x in ptr_preds[:n]],\n",
        "})\n",
        "out_df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
        "print(f\"[saved] {out_path}  rows={n}\")\n",
        "print(out_df[[\"len_base\",\"len_ptr\"]].describe().round(1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoding was correctly initialised on the GPU with the pointer head loaded and 1,000 evaluation samples prepared. Informational warnings appeared about deprecated CUDA attention kernels and unused generation flags, both of which are benign. The process was then interrupted manually by a `KeyboardInterrupt` during batched baseline decoding, so no error in the model or data occurred. The interruption simply halted generation partway through, and the notebook was left intact with the model and data unaffected.\n"
      ],
      "metadata": {
        "id": "jeKFXeaGHHxi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pS4v1fnrj5Bw",
        "outputId": "00bb83ac-1ab7-4d43-b4de-06401b95a748"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[saved] slice -> /content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000/slice_1k.csv  (1000 rows)\n",
            "[saved] results -> /content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000/1k sum base and pointer.csv  (BASE ready, PTR empty)\n"
          ]
        }
      ],
      "source": [
        "# === SAVE CURRENT BASE RESULTS + THE EXACT 1K SLICE ===\n",
        "import os, pandas as pd\n",
        "\n",
        "# where to save on Drive\n",
        "OUT_DIR  = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000\"\n",
        "SLICE_CSV = os.path.join(OUT_DIR, \"slice_1k.csv\")                     # exact 1k rows (src+ref)\n",
        "OUT_CSV   = os.path.join(OUT_DIR, \"1k sum base and pointer.csv\")      # results (base done, ptr to fill)\n",
        "\n",
        "assert 'df' in globals(), \"I need your sampled df (1k rows). Re-run the sampling step if missing.\"\n",
        "assert 'base_preds' in globals(), \"I don't see base_preds. Let BASE finish first, then run this cell.\"\n",
        "SRC_COL, REF_COL = \"article\", \"highlights\"  # keep your names\n",
        "\n",
        "# save the slice (so we can regenerate pointers later on the same rows)\n",
        "slice_df = df[[SRC_COL, REF_COL]].copy()\n",
        "slice_df.insert(0, \"idx\", range(len(slice_df)))\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "slice_df.to_csv(SLICE_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "# write results with BASE filled and PTR blank (will be filled later)\n",
        "n = min(len(df), len(base_preds))\n",
        "res = slice_df.iloc[:n].copy()\n",
        "res[\"base_pred\"] = base_preds[:n]\n",
        "res[\"ptr_pred\"]  = \"\"                           # empty for now\n",
        "res[\"len_base\"]  = [len(x.split()) for x in res[\"base_pred\"]]\n",
        "res[\"len_ptr\"]   = 0\n",
        "res.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"[saved] slice -> {SLICE_CSV}  ({len(slice_df)} rows)\")\n",
        "print(f\"[saved] results -> {OUT_CSV}  (BASE ready, PTR empty)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwDWb1yfnAWb"
      },
      "outputs": [],
      "source": [
        "# ===== PTR-BEAM ONLY (beam=5, fp16) — fill ptr_pred in existing CSV and save =====\n",
        "import os, gc, time, math, pandas as pd, torch, torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ---------------- config ----------------\n",
        "CKPT_DIR = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000\"\n",
        "OUT_CSV  = os.path.join(CKPT_DIR, \"1k sum base and pointer.csv\")  # must already exist with BASE results\n",
        "SRC_COL, REF_COL = \"article\", \"highlights\"\n",
        "\n",
        "# decoding knobs (match your baseline sweep)\n",
        "NUM_BEAMS      = 5\n",
        "MIN_NEW        = 40\n",
        "MAX_NEW        = 120\n",
        "NO_REPEAT      = 4\n",
        "LENGTH_PENALTY = 1.8\n",
        "MAX_SRC_LEN    = 400\n",
        "\n",
        "SAVE_EVERY = 50  # how often to checkpoint partial results\n",
        "\n",
        "# ---------------- setup ----------------\n",
        "assert os.path.exists(OUT_CSV), f\"Missing results CSV: {OUT_CSV}\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"[device]\", device)\n",
        "try:\n",
        "    torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True)\n",
        "except Exception:\n",
        "    pass\n",
        "if device == \"cuda\":\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "USE_FP16_PTR = (device == \"cuda\")\n",
        "\n",
        "tok  = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
        "base = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR, attn_implementation=\"eager\").to(device).eval()\n",
        "\n",
        "# ---------------- pointer wrapper ----------------\n",
        "class CopyAwareBart(nn.Module):\n",
        "    def __init__(self, base_model, tokenizer, lambda_cov=1.0, eps=1e-8, use_pointer=True, gate_bias=-0.3):\n",
        "        super().__init__()\n",
        "        self.base   = base_model\n",
        "        self.tok    = tokenizer\n",
        "        self.lambda_cov = float(lambda_cov)\n",
        "        self.eps    = float(eps)\n",
        "        self.use_ptr= bool(use_pointer)\n",
        "        self.gate_bias = float(gate_bias)\n",
        "        H = self.base.config.d_model\n",
        "        self.p_gen_linear = nn.Linear(3*H, 1)\n",
        "        self._last_p_copy_mean = self._last_p_gen_mean = None\n",
        "\n",
        "    def _mix_pointer(self, out, input_ids, attention_mask, decoder_input_ids):\n",
        "        logits = out.logits  # (B,T,V)\n",
        "        B,T,V  = logits.shape\n",
        "        last_ca = out.cross_attentions[-1].mean(1)  # (B,T,S)\n",
        "        if attention_mask is not None:\n",
        "            last_ca = last_ca.masked_fill(attention_mask[:, None, :] == 0, 0.0)\n",
        "        attn = last_ca / last_ca.sum(-1, keepdim=True).clamp_min(self.eps)\n",
        "        dec_hid = out.decoder_hidden_states[-1]                    # (B,T,H)\n",
        "        ctx     = torch.bmm(attn, out.encoder_last_hidden_state)   # (B,T,H)\n",
        "        dec_inp = self.base.get_input_embeddings()(decoder_input_ids)\n",
        "        gate_in = torch.cat([dec_hid, ctx, dec_inp], dim=-1)\n",
        "        p_gen   = torch.sigmoid(self.p_gen_linear(gate_in) + self.gate_bias).squeeze(-1)\n",
        "        p_copy  = 1.0 - p_gen\n",
        "        copy_probs = torch.zeros_like(logits)\n",
        "        idx = input_ids.unsqueeze(1).expand(B, T, input_ids.size(1))\n",
        "        copy_probs.scatter_add_(2, idx, attn)\n",
        "        vocab_logp = F.log_softmax(logits, dim=-1)\n",
        "        logp_final = torch.logaddexp(\n",
        "            vocab_logp + (p_gen + self.eps).log().unsqueeze(-1),\n",
        "            (copy_probs + self.eps).log() + (p_copy + self.eps).log().unsqueeze(-1)\n",
        "        )\n",
        "        self._last_p_copy_mean = float(p_copy.detach().mean().cpu())\n",
        "        self._last_p_gen_mean  = float(p_gen.detach().mean().cpu())\n",
        "        return logp_final\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, decoder_input_ids=None,\n",
        "                output_attentions=True, output_hidden_states=True, use_cache=False, **kwargs):\n",
        "        out = self.base(\n",
        "            input_ids=input_ids, attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            output_attentions=True, output_hidden_states=True,\n",
        "            use_cache=use_cache, return_dict=True,\n",
        "        )\n",
        "        if not self.use_ptr:\n",
        "            return out\n",
        "        out.logits = self._mix_pointer(out, input_ids, attention_mask, decoder_input_ids)\n",
        "        return out\n",
        "\n",
        "def copyaware_generate(self, *args, **kwargs):\n",
        "    return self.base.generate(*args, **kwargs)\n",
        "CopyAwareBart.generate = copyaware_generate\n",
        "\n",
        "model_ptr = CopyAwareBart(base, tok, lambda_cov=1.0, use_pointer=True, gate_bias=-0.3).to(device).eval()\n",
        "ph_path = os.path.join(CKPT_DIR, \"pointer_head.pt\")\n",
        "if os.path.exists(ph_path):\n",
        "    sd = torch.load(ph_path, map_location=\"cpu\")\n",
        "    if isinstance(sd, dict) and any(k.startswith(\"p_gen_linear.\") for k in sd.keys()):\n",
        "        model_ptr.load_state_dict(sd, strict=False)\n",
        "    elif isinstance(sd, dict) and all(k in {\"weight\",\"bias\"} for k in sd.keys()):\n",
        "        model_ptr.p_gen_linear.load_state_dict(sd)\n",
        "    elif isinstance(sd, dict) and \"p_gen_linear\" in sd:\n",
        "        model_ptr.p_gen_linear.load_state_dict(sd[\"p_gen_linear\"])\n",
        "print(\"[pointer_head] loaded=True\")\n",
        "\n",
        "# ---------------- helpers ----------------\n",
        "def _ban_repeating_ngrams(logits: torch.Tensor, seq: torch.Tensor, ngram: int):\n",
        "    if ngram <= 0: return\n",
        "    toks = seq[0].tolist() if seq.dim()==2 else seq.tolist()\n",
        "    L = len(toks)\n",
        "    if ngram == 1:\n",
        "        for t in set(toks): logits[t] = -1e9\n",
        "        return\n",
        "    if L < ngram-1: return\n",
        "    prefix = tuple(toks[-(ngram - 1):])\n",
        "    banned = set()\n",
        "    for i in range(L - ngram + 1):\n",
        "        if tuple(toks[i:i + ngram - 1]) == prefix:\n",
        "            banned.add(toks[i + ngram - 1])\n",
        "    for t in banned: logits[t] = -1e9\n",
        "\n",
        "def generate_pointer_beam_one(text: str) -> str:\n",
        "    enc = tok([text], return_tensors=\"pt\", truncation=True, max_length=MAX_SRC_LEN).to(device)\n",
        "    start_id = getattr(base.config, \"decoder_start_token_id\", None) or (tok.bos_token_id or tok.eos_token_id)\n",
        "    beams = [(torch.tensor([[start_id]], device=device, dtype=torch.long), 0.0, False)]\n",
        "    for step in range(MAX_NEW):\n",
        "        new_beams = []\n",
        "        for seq, score, done in beams:\n",
        "            if done:\n",
        "                new_beams.append((seq, score, True)); continue\n",
        "            with torch.cuda.amp.autocast(dtype=torch.float16, enabled=USE_FP16_PTR):\n",
        "                out = model_ptr(\n",
        "                    input_ids=enc.input_ids,\n",
        "                    attention_mask=enc.attention_mask,\n",
        "                    decoder_input_ids=seq,\n",
        "                    use_cache=False\n",
        "                )\n",
        "            step_logits = out.logits[:, -1, :].squeeze(0).float().clone()  # (V,)\n",
        "            if step + 1 < MIN_NEW and tok.eos_token_id is not None:\n",
        "                step_logits[tok.eos_token_id] = -1e9\n",
        "            _ban_repeating_ngrams(step_logits, seq, NO_REPEAT)\n",
        "            topk_logp, topk_ids = torch.topk(step_logits, k=NUM_BEAMS)\n",
        "            for k in range(NUM_BEAMS):\n",
        "                tid  = topk_ids[k].view(1,1)\n",
        "                nseq = torch.cat([seq, tid], dim=1)\n",
        "                nfin = (tid.item() == tok.eos_token_id)\n",
        "                raw  = score + float(topk_logp[k].item())\n",
        "                lp   = ((5.0 + nseq.size(1))**LENGTH_PENALTY) / ((5.0 + 1.0)**LENGTH_PENALTY)\n",
        "                nsc  = raw / lp\n",
        "                new_beams.append((nseq, nsc, nfin))\n",
        "        new_beams.sort(key=lambda x: x[1], reverse=True)\n",
        "        beams = new_beams[:NUM_BEAMS]\n",
        "        if all(b[2] for b in beams): break\n",
        "    best_seq = max(beams, key=lambda x: x[1])[0]\n",
        "    return tok.batch_decode(best_seq, skip_special_tokens=True)[0]\n",
        "\n",
        "# ---------------- load existing CSV and find rows needing PTR ----------------\n",
        "df = pd.read_csv(OUT_CSV)\n",
        "assert SRC_COL in df.columns and \"base_pred\" in df.columns, \"CSV must contain article/highlights and base_pred.\"\n",
        "if \"ptr_pred\" not in df.columns:\n",
        "    df[\"ptr_pred\"] = \"\"\n",
        "\n",
        "todo_mask = df[\"ptr_pred\"].isna() | (df[\"ptr_pred\"].astype(str) == \"\")\n",
        "todo_idx = df.index[todo_mask].tolist()\n",
        "print(f\"[ptr] remaining: {len(todo_idx)} / {len(df)}\")\n",
        "\n",
        "# ---------------- run pointer decoding only ----------------\n",
        "t0 = time.time()\n",
        "pending = []\n",
        "for j, idx in enumerate(tqdm(todo_idx, desc=\"PTR-BEAM (fp16)\")):\n",
        "    text = str(df.at[idx, SRC_COL])\n",
        "    pred = generate_pointer_beam_one(text)\n",
        "    df.at[idx, \"ptr_pred\"] = pred\n",
        "    df.at[idx, \"len_ptr\"]  = len(pred.split())\n",
        "    pending.append(idx)\n",
        "\n",
        "    if len(pending) >= SAVE_EVERY or j == len(todo_idx) - 1:\n",
        "        df.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
        "        pending.clear()\n",
        "        torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "mins = (time.time() - t0) / 60.0\n",
        "print(f\"[done] updated {OUT_CSV} | elapsed ~{mins:.1f} min\")\n",
        "print(f\"[probe] p_copy_mean={getattr(model_ptr,'_last_p_copy_mean',float('nan')):.3f} | p_gen_mean={getattr(model_ptr,'_last_p_gen_mean',float('nan')):.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418,
          "referenced_widgets": [
            "2cc3bac38aa44b17af8c075865d48359"
          ]
        },
        "id": "7eRHhIauoqFf",
        "outputId": "0aa539a9-159a-49eb-8b0e-6dd6d9e36928"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[device] cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
            "  self.gen = func(*args, **kwds)\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[pointer_head] loaded=True\n",
            "[ptr] to generate: 1000 rows (REGEN_ALL=True)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2cc3bac38aa44b17af8c075865d48359",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "PTR-BEAM (fp16, pre-enc, batched):   0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[done] saved -> /content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000/iksumpointer.csv | elapsed ~28.1 min\n",
            "[probe] p_copy_mean=0.482 | p_gen_mean=0.518\n",
            "count    1000.0\n",
            "mean       74.7\n",
            "std        18.5\n",
            "min        30.0\n",
            "25%        60.0\n",
            "50%        76.0\n",
            "75%        91.0\n",
            "max       111.0\n",
            "Name: len_ptr, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# ===== PTR-BEAM for ALL 1K (beam=5, fp16, pre-encode + beam-batched) =====\n",
        "import os, gc, time, pandas as pd, torch, torch.nn as nn, torch.nn.functional as F\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers.modeling_outputs import BaseModelOutput\n",
        "\n",
        "# ---------- paths ----------\n",
        "CKPT_DIR      = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000\"\n",
        "IN_CSV_PATH   = os.path.join(CKPT_DIR, \"1k sum base and pointer.csv\")  # has BASE preds\n",
        "OUT_CSV_PATH  = os.path.join(CKPT_DIR, \"iksumpointer.csv\")             # write PTR preds here\n",
        "SRC_COL, REF_COL = \"article\", \"highlights\"\n",
        "\n",
        "# ---------- decoding knobs ----------\n",
        "NUM_BEAMS      = 5\n",
        "MIN_NEW        = 40\n",
        "MAX_NEW        = 120\n",
        "NO_REPEAT      = 4\n",
        "LENGTH_PENALTY = 1.8\n",
        "MAX_SRC_LEN    = 400\n",
        "SAVE_EVERY     = 25         # checkpoint frequency\n",
        "REGEN_ALL      = True       # set False to only fill blanks\n",
        "\n",
        "# ---------- setup ----------\n",
        "assert os.path.exists(IN_CSV_PATH), f\"Missing CSV: {IN_CSV_PATH}\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"[device]\", device)\n",
        "try:\n",
        "    torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True)\n",
        "except Exception:\n",
        "    pass\n",
        "if device == \"cuda\":\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "USE_FP16_PTR = (device == \"cuda\")\n",
        "\n",
        "tok  = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
        "base = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR, attn_implementation=\"eager\").to(device).eval()\n",
        "\n",
        "# ---------- pointer wrapper ----------\n",
        "class CopyAwareBart(nn.Module):\n",
        "    def __init__(self, base_model, tokenizer, lambda_cov=1.0, eps=1e-8, use_pointer=True, gate_bias=-0.3):\n",
        "        super().__init__()\n",
        "        self.base   = base_model\n",
        "        self.tok    = tokenizer\n",
        "        self.lambda_cov = float(lambda_cov)\n",
        "        self.eps    = float(eps)\n",
        "        self.use_ptr= bool(use_pointer)\n",
        "        self.gate_bias = float(gate_bias)\n",
        "        H = self.base.config.d_model\n",
        "        self.p_gen_linear = nn.Linear(3*H, 1)\n",
        "        self._last_p_copy_mean = self._last_p_gen_mean = None\n",
        "\n",
        "    def _mix_pointer(self, out, input_ids, attention_mask, decoder_input_ids):\n",
        "        logits = out.logits  # (B,T,V)\n",
        "        last_ca = out.cross_attentions[-1].mean(1)  # (B,T,S)\n",
        "        if attention_mask is not None:\n",
        "            last_ca = last_ca.masked_fill(attention_mask[:, None, :] == 0, 0.0)\n",
        "        attn = last_ca / last_ca.sum(-1, keepdim=True).clamp_min(self.eps)\n",
        "        dec_hid = out.decoder_hidden_states[-1]\n",
        "        ctx     = torch.bmm(attn, out.encoder_last_hidden_state)\n",
        "        dec_inp = self.base.get_input_embeddings()(decoder_input_ids)\n",
        "        gate_in = torch.cat([dec_hid, ctx, dec_inp], dim=-1)\n",
        "        p_gen   = torch.sigmoid(self.p_gen_linear(gate_in) + self.gate_bias).squeeze(-1)\n",
        "        p_copy  = 1.0 - p_gen\n",
        "\n",
        "        copy_probs = torch.zeros_like(logits)\n",
        "        idx = input_ids.unsqueeze(1).expand(input_ids.size(0), logits.size(1), input_ids.size(1))\n",
        "        copy_probs.scatter_add_(2, idx, attn)\n",
        "\n",
        "        vocab_logp = F.log_softmax(logits, dim=-1)\n",
        "        logp_final = torch.logaddexp(\n",
        "            vocab_logp + (p_gen + 1e-8).log().unsqueeze(-1),\n",
        "            (copy_probs + 1e-8).log() + (p_copy + 1e-8).log().unsqueeze(-1)\n",
        "        )\n",
        "        self._last_p_copy_mean = float(p_copy.detach().mean().cpu())\n",
        "        self._last_p_gen_mean  = float(p_gen.detach().mean().cpu())\n",
        "        return logp_final\n",
        "\n",
        "model_ptr = CopyAwareBart(base, tok, lambda_cov=1.0, use_pointer=True, gate_bias=-0.3).to(device).eval()\n",
        "ph_path = os.path.join(CKPT_DIR, \"pointer_head.pt\")\n",
        "if os.path.exists(ph_path):\n",
        "    sd = torch.load(ph_path, map_location=\"cpu\")\n",
        "    if isinstance(sd, dict) and any(k.startswith(\"p_gen_linear.\") for k in sd.keys()):\n",
        "        model_ptr.load_state_dict(sd, strict=False)\n",
        "    elif isinstance(sd, dict) and all(k in {\"weight\",\"bias\"} for k in sd.keys()):\n",
        "        model_ptr.p_gen_linear.load_state_dict(sd)\n",
        "    elif isinstance(sd, dict) and \"p_gen_linear\" in sd:\n",
        "        model_ptr.p_gen_linear.load_state_dict(sd[\"p_gen_linear\"])\n",
        "print(\"[pointer_head] loaded=True\")\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def _ban_repeating_ngrams(logits: torch.Tensor, seq: torch.Tensor, ngram: int):\n",
        "    if ngram <= 0: return\n",
        "    toks = seq[0].tolist() if seq.dim()==2 else seq.tolist()\n",
        "    L = len(toks)\n",
        "    if ngram == 1:\n",
        "        for t in set(toks): logits[t] = -1e9\n",
        "        return\n",
        "    if L < ngram-1: return\n",
        "    prefix = tuple(toks[-(ngram - 1):])\n",
        "    banned = set()\n",
        "    for i in range(L - ngram + 1):\n",
        "        if tuple(toks[i:i + ngram - 1]) == prefix:\n",
        "            banned.add(toks[i + ngram - 1])\n",
        "    for t in banned: logits[t] = -1e9\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_pointer_beam_one_fast(text: str) -> str:\n",
        "    enc = tok([text], return_tensors=\"pt\", truncation=True, max_length=MAX_SRC_LEN).to(device)\n",
        "    src_ids, src_mask = enc.input_ids, enc.attention_mask\n",
        "    # Precompute encoder once\n",
        "    with torch.amp.autocast('cuda', dtype=torch.float16, enabled=USE_FP16_PTR):\n",
        "        enc_out = base.model.get_encoder()(input_ids=src_ids, attention_mask=src_mask, return_dict=True)\n",
        "    start_id = getattr(base.config, \"decoder_start_token_id\", None) or (tok.bos_token_id or tok.eos_token_id)\n",
        "\n",
        "    beams = [(torch.tensor([[start_id]], device=device, dtype=torch.long), 0.0, False)]\n",
        "    for step in range(MAX_NEW):\n",
        "        # collect active beams -> single batched decode\n",
        "        act = [(i,b) for i,b in enumerate(beams) if not b[2]]\n",
        "        if not act: break\n",
        "        seqs = torch.cat([beams[i][0] for i,_ in act], dim=0)  # (K, L)\n",
        "        K = seqs.size(0)\n",
        "        # expand encoder outputs/masks to K\n",
        "        enc_exp = BaseModelOutput(last_hidden_state=enc_out.last_hidden_state.expand(K, -1, -1))\n",
        "        src_mask_exp = src_mask.expand(K, -1)\n",
        "        # run decoder+pointer mix for all beams at once\n",
        "        with torch.amp.autocast('cuda', dtype=torch.float16, enabled=USE_FP16_PTR):\n",
        "            out = base(\n",
        "                encoder_outputs=enc_exp,\n",
        "                attention_mask=src_mask_exp,\n",
        "                decoder_input_ids=seqs,\n",
        "                output_attentions=True, output_hidden_states=True,\n",
        "                use_cache=False, return_dict=True,\n",
        "            )\n",
        "        # mix with true src ids/mask\n",
        "        with torch.amp.autocast('cuda', dtype=torch.float16, enabled=USE_FP16_PTR):\n",
        "            logits = model_ptr._mix_pointer(out, src_ids.expand(K, -1), src_mask_exp, seqs)\n",
        "        step_logits_all = logits[:, -1, :].float()\n",
        "\n",
        "        new_beams = []\n",
        "        for j, (beam_idx, _) in enumerate(act):\n",
        "            step_logits = step_logits_all[j].clone()\n",
        "            if step + 1 < MIN_NEW and tok.eos_token_id is not None:\n",
        "                step_logits[tok.eos_token_id] = -1e9\n",
        "            _ban_repeating_ngrams(step_logits, beams[beam_idx][0], NO_REPEAT)\n",
        "            topk_logp, topk_ids = torch.topk(step_logits, k=NUM_BEAMS)\n",
        "            for k in range(NUM_BEAMS):\n",
        "                tid  = topk_ids[k].view(1,1)\n",
        "                nseq = torch.cat([beams[beam_idx][0], tid], dim=1)\n",
        "                nfin = (tid.item() == tok.eos_token_id)\n",
        "                raw  = beams[beam_idx][1] + float(topk_logp[k].item())\n",
        "                lp   = ((5.0 + nseq.size(1))**LENGTH_PENALTY) / ((5.0 + 1.0)**LENGTH_PENALTY)\n",
        "                nsc  = raw / lp\n",
        "                new_beams.append((nseq, nsc, nfin))\n",
        "        # keep already-finished beams\n",
        "        for (seq, score, done) in beams:\n",
        "            if done: new_beams.append((seq, score, True))\n",
        "        # prune\n",
        "        new_beams.sort(key=lambda x: x[1], reverse=True)\n",
        "        beams = new_beams[:NUM_BEAMS]\n",
        "        if all(b[2] for b in beams): break\n",
        "\n",
        "    best_seq = max(beams, key=lambda x: x[1])[0]\n",
        "    return tok.batch_decode(best_seq, skip_special_tokens=True)[0]\n",
        "\n",
        "# ---------- load CSV & decide which rows to run ----------\n",
        "df = pd.read_csv(IN_CSV_PATH)\n",
        "assert SRC_COL in df.columns and \"base_pred\" in df.columns, \"CSV must contain article/highlights and base_pred.\"\n",
        "\n",
        "if \"ptr_pred\" not in df.columns:\n",
        "    df[\"ptr_pred\"] = \"\"\n",
        "df[\"ptr_pred\"] = df[\"ptr_pred\"].astype(\"object\")\n",
        "\n",
        "if REGEN_ALL:\n",
        "    todo_idx = df.index.tolist()\n",
        "else:\n",
        "    todo_mask = df[\"ptr_pred\"].isna() | (df[\"ptr_pred\"].astype(str).str.strip() == \"\")\n",
        "    todo_idx = df.index[todo_mask].tolist()\n",
        "\n",
        "print(f\"[ptr] to generate: {len(todo_idx)} rows (REGEN_ALL={REGEN_ALL})\")\n",
        "\n",
        "# ---------- run decoding ----------\n",
        "t0 = time.time()\n",
        "pending = []\n",
        "for j, idx in enumerate(tqdm(todo_idx, desc=\"PTR-BEAM (fp16, pre-enc, batched)\")):\n",
        "    text = str(df.at[idx, SRC_COL])\n",
        "    pred = generate_pointer_beam_one_fast(text)\n",
        "    df.at[idx, \"ptr_pred\"] = pred\n",
        "    df.at[idx, \"len_ptr\"]  = int(len(pred.split()))\n",
        "    pending.append(idx)\n",
        "\n",
        "    if len(pending) >= SAVE_EVERY or j == len(todo_idx) - 1:\n",
        "        df.to_csv(OUT_CSV_PATH, index=False, encoding=\"utf-8\")\n",
        "        pending.clear()\n",
        "        torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "mins = (time.time() - t0) / 60.0\n",
        "print(f\"[done] saved -> {OUT_CSV_PATH} | elapsed ~{mins:.1f} min\")\n",
        "print(f\"[probe] p_copy_mean={getattr(model_ptr,'_last_p_copy_mean',float('nan')):.3f} | p_gen_mean={getattr(model_ptr,'_last_p_gen_mean',float('nan')):.3f}\")\n",
        "\n",
        "# quick sanity on lengths\n",
        "print(df[\"len_ptr\"].describe().round(1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F40-2hj1tSdG",
        "outputId": "414121e6-c8ab-4f73-acb3-f900e8ab78fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== LENGTH STATS (words) ===\n",
            "\n",
            "[BASE]\n",
            "count    1000.0\n",
            "mean       61.2\n",
            "std        14.4\n",
            "min        30.0\n",
            "25%        51.0\n",
            "50%        59.0\n",
            "75%        71.0\n",
            "max       102.0\n",
            "Name: len_base, dtype: float64\n",
            "\n",
            "[POINTER]\n",
            "count    1000.0\n",
            "mean       74.7\n",
            "std        18.5\n",
            "min        30.0\n",
            "25%        60.0\n",
            "50%        76.0\n",
            "75%        91.0\n",
            "max       111.0\n",
            "Name: len_ptr, dtype: float64\n",
            "\n",
            "=== ROUGE (mean over examples) ===\n",
            "[BASE] {\n",
            "  \"r1_p\": 0.403,\n",
            "  \"r1_r\": 0.4658,\n",
            "  \"r1_f\": 0.4168,\n",
            "  \"r2_p\": 0.1874,\n",
            "  \"r2_r\": 0.2166,\n",
            "  \"r2_f\": 0.1935,\n",
            "  \"rl_p\": 0.2763,\n",
            "  \"rl_r\": 0.3221,\n",
            "  \"rl_f\": 0.2867\n",
            "}\n",
            "[PTR ] {\n",
            "  \"r1_p\": 0.3603,\n",
            "  \"r1_r\": 0.5037,\n",
            "  \"r1_f\": 0.4046,\n",
            "  \"r2_p\": 0.1631,\n",
            "  \"r2_r\": 0.2293,\n",
            "  \"r2_f\": 0.1836,\n",
            "  \"rl_p\": 0.2394,\n",
            "  \"rl_r\": 0.3367,\n",
            "  \"rl_f\": 0.2693\n",
            "}\n",
            "[PTR - BASE] {\n",
            "  \"r1_p\": -0.0428,\n",
            "  \"r1_r\": 0.0379,\n",
            "  \"r1_f\": -0.0122,\n",
            "  \"r2_p\": -0.0243,\n",
            "  \"r2_r\": 0.0128,\n",
            "  \"r2_f\": -0.01,\n",
            "  \"rl_p\": -0.037,\n",
            "  \"rl_r\": 0.0146,\n",
            "  \"rl_f\": -0.0174\n",
            "}\n",
            "\n",
            "=== ROUGE-L F1: pointer vs base (per-example) ===\n",
            "improved: 405 | worse: 589 | same: 6\n",
            "\n",
            "[save] per-example -> /content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000/1k_rouge_ptr_vs_base_perexample.csv\n",
            "[save] summary    -> /content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000/1k_rouge_ptr_vs_base_summary.json\n",
            "[save] length     -> /content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000/1k_length_stats_ptr_vs_base.csv\n"
          ]
        }
      ],
      "source": [
        "# === Compare BASE vs POINTER on ROUGE + length stats (Colab-ready) ===\n",
        "!pip -q install rouge-score==0.1.2\n",
        "\n",
        "import os, json, math, pandas as pd, numpy as np\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "CKPT_DIR     = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000\"\n",
        "IN_CSV_PATH  = os.path.join(CKPT_DIR, \"iksumpointer.csv\")  # contains columns: article, highlights, base_pred, ptr_pred\n",
        "OUT_PER_EX   = os.path.join(CKPT_DIR, \"1k_rouge_ptr_vs_base_perexample.csv\")\n",
        "OUT_SUMMARY  = os.path.join(CKPT_DIR, \"1k_rouge_ptr_vs_base_summary.json\")\n",
        "OUT_LENSTAT  = os.path.join(CKPT_DIR, \"1k_length_stats_ptr_vs_base.csv\")\n",
        "\n",
        "# --- Load ---\n",
        "df = pd.read_csv(IN_CSV_PATH)\n",
        "required = {\"highlights\", \"base_pred\", \"ptr_pred\"}\n",
        "missing  = required - set(df.columns)\n",
        "assert not missing, f\"Missing columns in CSV: {missing}\"\n",
        "\n",
        "# Clean/normalize text a bit for fair ROUGE\n",
        "def normalize(s):\n",
        "    s = (\"\" if pd.isna(s) else str(s))\n",
        "    # collapse whitespace, strip\n",
        "    return \" \".join(s.split())\n",
        "\n",
        "for c in [\"highlights\",\"base_pred\",\"ptr_pred\"]:\n",
        "    df[c] = df[c].map(normalize)\n",
        "\n",
        "# --- Length stats (word level) ---\n",
        "df[\"len_base\"] = df[\"base_pred\"].str.split().map(len)\n",
        "df[\"len_ptr\"]  = df[\"ptr_pred\"].str.split().map(len)\n",
        "\n",
        "# Print quick stats\n",
        "print(\"\\n=== LENGTH STATS (words) ===\")\n",
        "print(\"\\n[BASE]\"); print(df[\"len_base\"].describe().round(1))\n",
        "print(\"\\n[POINTER]\"); print(df[\"len_ptr\"].describe().round(1))\n",
        "\n",
        "# Save the two series' describe() as a small table\n",
        "len_stats = pd.DataFrame({\n",
        "    \"len_base\": df[\"len_base\"].describe(),\n",
        "    \"len_ptr\":  df[\"len_ptr\"].describe()\n",
        "})\n",
        "len_stats.to_csv(OUT_LENSTAT)\n",
        "\n",
        "# --- ROUGE computation ---\n",
        "# Using Porter stemming + default tokenization for ROUGE-1/2/L\n",
        "scorer = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\"], use_stemmer=True)\n",
        "\n",
        "def rouge_of(preds, refs):\n",
        "    rows = []\n",
        "    for p, r in zip(preds, refs):\n",
        "        sc = scorer.score(r, p)  # (target, prediction) order matches library docs\n",
        "        rows.append({\n",
        "            \"r1_p\": sc[\"rouge1\"].precision, \"r1_r\": sc[\"rouge1\"].recall, \"r1_f\": sc[\"rouge1\"].fmeasure,\n",
        "            \"r2_p\": sc[\"rouge2\"].precision, \"r2_r\": sc[\"rouge2\"].recall, \"r2_f\": sc[\"rouge2\"].fmeasure,\n",
        "            \"rl_p\": sc[\"rougeL\"].precision, \"rl_r\": sc[\"rougeL\"].recall, \"rl_f\": sc[\"rougeL\"].fmeasure,\n",
        "        })\n",
        "    R = pd.DataFrame(rows)\n",
        "    avg = R.mean(numeric_only=True).to_dict()\n",
        "    return R, avg\n",
        "\n",
        "R_base, base_avg = rouge_of(df[\"base_pred\"].tolist(), df[\"highlights\"].tolist())\n",
        "R_ptr,  ptr_avg  = rouge_of(df[\"ptr_pred\"].tolist(),  df[\"highlights\"].tolist())\n",
        "\n",
        "# Per-example deltas\n",
        "per_ex = pd.DataFrame({\n",
        "    \"id\": df.index,\n",
        "    \"len_base\": df[\"len_base\"], \"len_ptr\": df[\"len_ptr\"],\n",
        "    **{f\"base_{k}\": v for k,v in R_base.to_dict(orient=\"list\").items()},\n",
        "    **{f\"ptr_{k}\":  v for k,v in R_ptr.to_dict(orient=\"list\").items()},\n",
        "})\n",
        "for k in [\"r1_f\",\"r2_f\",\"rl_f\"]:\n",
        "    per_ex[f\"Δ_{k}\"] = per_ex[f\"ptr_{k}\"] - per_ex[f\"base_{k}\"]\n",
        "\n",
        "# Quick win/loss counts on ROUGE-L F1\n",
        "improved = (per_ex[\"Δ_rl_f\"] >  0).sum()\n",
        "worse    = (per_ex[\"Δ_rl_f\"] <  0).sum()\n",
        "same     = (per_ex[\"Δ_rl_f\"] == 0).sum()\n",
        "\n",
        "# --- Print concise summary ---\n",
        "def fmt(d):\n",
        "    return {k: float(f\"{v:.4f}\") for k,v in d.items()}\n",
        "\n",
        "summary = {\n",
        "    \"n\": int(len(df)),\n",
        "    \"length_stats\": {\n",
        "        \"base\": df[\"len_base\"].describe().to_dict(),\n",
        "        \"ptr\":  df[\"len_ptr\"].describe().to_dict(),\n",
        "    },\n",
        "    \"rouge_base_avg\": fmt(base_avg),\n",
        "    \"rouge_ptr_avg\":  fmt(ptr_avg),\n",
        "    \"rouge_ptr_minus_base\": fmt({k: ptr_avg[k]-base_avg[k] for k in base_avg}),\n",
        "    \"per_example_rl_f\": {\"improved\": int(improved), \"worse\": int(worse), \"same\": int(same)},\n",
        "    \"inputs\": {\"csv\": IN_CSV_PATH}\n",
        "}\n",
        "\n",
        "print(\"\\n=== ROUGE (mean over examples) ===\")\n",
        "print(\"[BASE]\", json.dumps(fmt(base_avg), indent=2))\n",
        "print(\"[PTR ]\", json.dumps(fmt(ptr_avg),  indent=2))\n",
        "print(\"[PTR - BASE]\", json.dumps(fmt({k: ptr_avg[k]-base_avg[k] for k in base_avg}), indent=2))\n",
        "\n",
        "print(\"\\n=== ROUGE-L F1: pointer vs base (per-example) ===\")\n",
        "print(f\"improved: {improved} | worse: {worse} | same: {same}\")\n",
        "\n",
        "# --- Save artifacts ---\n",
        "per_ex.to_csv(OUT_PER_EX, index=False)\n",
        "with open(OUT_SUMMARY, \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"\\n[save] per-example -> {OUT_PER_EX}\")\n",
        "print(f\"[save] summary    -> {OUT_SUMMARY}\")\n",
        "print(f\"[save] length     -> {OUT_LENSTAT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waUUh5Zw1a1-",
        "outputId": "345cc71b-9cbd-410d-c58f-38a312eeb374"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== LENGTH STATS (VAL highlights, words) ===\n",
            "count    13368.0\n",
            "mean        57.9\n",
            "std         25.6\n",
            "min         10.0\n",
            "25%         41.0\n",
            "50%         54.0\n",
            "75%         69.0\n",
            "max       1440.0\n",
            "Name: len_ref, dtype: float64\n",
            "\n",
            "=== LENGTH STATS (TRAIN highlights, words) ===\n",
            "count    283861.0\n",
            "mean         51.6\n",
            "std          21.3\n",
            "min           4.0\n",
            "25%          38.0\n",
            "50%          49.0\n",
            "75%          60.0\n",
            "max        1296.0\n",
            "Name: len_ref, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "VAL_PATH   = \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "TRAIN_PATH = \"/content/drive/MyDrive/cleaned_outputs/naa/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast_applied.csv\"\n",
        "\n",
        "for split, path in [(\"VAL\", VAL_PATH), (\"TRAIN\", TRAIN_PATH)]:\n",
        "    df = pd.read_csv(path)\n",
        "    assert \"highlights\" in df.columns, f\"'highlights' column missing in {path}\"\n",
        "    df[\"highlights\"] = df[\"highlights\"].fillna(\"\").astype(str)\n",
        "    df[\"len_ref\"] = df[\"highlights\"].str.split().map(len)\n",
        "\n",
        "    print(f\"\\n=== LENGTH STATS ({split} highlights, words) ===\")\n",
        "    print(df[\"len_ref\"].describe().round(1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2RqcLZM3LXN",
        "outputId": "dc5512fe-c34a-4afb-848d-fe9658a8b661"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[loaded] rows=1000 from /content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000/iksumpointer.csv\n",
            "\n",
            "=== [BASE_PRED] ENTITY (overall) ===\n",
            "Precision: 0.378463\n",
            "Recall:    0.380854\n",
            "F1:        0.379655\n",
            "\n",
            "=== [BASE_PRED] ENTITY (by type) ===\n",
            "PERSON       P=0.508 R=0.473 F1=0.490\n",
            "DATE         P=0.307 R=0.314 F1=0.311\n",
            "ORG          P=0.340 R=0.335 F1=0.338\n",
            "GPE          P=0.466 R=0.463 F1=0.465\n",
            "CARDINAL     P=0.306 R=0.324 F1=0.315\n",
            "NORP         P=0.292 R=0.358 F1=0.322\n",
            "MONEY        P=0.327 R=0.333 F1=0.330\n",
            "ORDINAL      P=0.251 R=0.350 F1=0.293\n",
            "TIME         P=0.122 R=0.136 F1=0.129\n",
            "LOC          P=0.356 R=0.363 F1=0.359\n",
            "FAC          P=0.208 R=0.283 F1=0.240\n",
            "QUANTITY     P=0.255 R=0.233 F1=0.243\n",
            "EVENT        P=0.360 R=0.305 F1=0.330\n",
            "WORK_OF_ART  P=0.237 R=0.184 F1=0.207\n",
            "PRODUCT      P=0.273 R=0.364 F1=0.312\n",
            "PERCENT      P=0.278 R=0.192 F1=0.227\n",
            "LAW          P=0.091 R=0.077 F1=0.083\n",
            "LANGUAGE     P=0.667 R=0.444 F1=0.533\n",
            "\n",
            "=== [BASE_PRED] OTHER METRICS ===\n",
            "Number Coverage: 0.452363\n",
            "Repetition Rate (3-gram): 0.001629\n",
            "Repetition Rate (4-gram): 0.000015\n",
            "Copy Rate (token coverage vs article): 0.993829\n",
            "\n",
            "=== [PTR_PRED] ENTITY (overall) ===\n",
            "Precision: 0.336422\n",
            "Recall:    0.403928\n",
            "F1:        0.367097\n",
            "\n",
            "=== [PTR_PRED] ENTITY (by type) ===\n",
            "PERSON       P=0.431 R=0.460 F1=0.445\n",
            "DATE         P=0.271 R=0.346 F1=0.304\n",
            "ORG          P=0.322 R=0.390 F1=0.353\n",
            "GPE          P=0.421 R=0.481 F1=0.449\n",
            "CARDINAL     P=0.304 R=0.365 F1=0.332\n",
            "NORP         P=0.290 R=0.429 F1=0.346\n",
            "MONEY        P=0.276 R=0.345 F1=0.307\n",
            "ORDINAL      P=0.233 R=0.371 F1=0.287\n",
            "TIME         P=0.142 R=0.242 F1=0.179\n",
            "LOC          P=0.319 R=0.373 F1=0.344\n",
            "FAC          P=0.154 R=0.250 F1=0.191\n",
            "QUANTITY     P=0.235 R=0.317 F1=0.270\n",
            "EVENT        P=0.333 R=0.305 F1=0.319\n",
            "WORK_OF_ART  P=0.190 R=0.224 F1=0.206\n",
            "PRODUCT      P=0.298 R=0.515 F1=0.378\n",
            "PERCENT      P=0.077 R=0.038 F1=0.051\n",
            "LAW          P=0.231 R=0.231 F1=0.231\n",
            "LANGUAGE     P=0.333 R=0.222 F1=0.267\n",
            "\n",
            "=== [PTR_PRED] OTHER METRICS ===\n",
            "Number Coverage: 0.497289\n",
            "Repetition Rate (3-gram): 0.002394\n",
            "Repetition Rate (4-gram): 0.000059\n",
            "Copy Rate (token coverage vs article): 0.995341\n"
          ]
        }
      ],
      "source": [
        "# === Entity coverage & diagnostics (PRINT ONLY; no saving) ===\n",
        "# Compares predictions to references (highlights) using spaCy NER.\n",
        "# Prints: overall P/R/F1, by-type P/R/F1, number coverage, repetition, copy rate.\n",
        "\n",
        "!pip -q install spacy==3.7.5\n",
        "import os, re, numpy as np, pandas as pd, spacy\n",
        "\n",
        "# --------- paths ---------\n",
        "CSV_PATH = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000/iksumpointer.csv\"  # change if needed\n",
        "\n",
        "# --------- load spaCy NER ---------\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "except OSError:\n",
        "    from spacy.cli import download\n",
        "    download(\"en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "\n",
        "# --------- helpers ---------\n",
        "def normalize(s):\n",
        "    return \" \".join(str(s if s is not None else \"\").split())\n",
        "\n",
        "_num_re = re.compile(r\"\\b\\d[\\d,]*(?:\\.\\d+)?%?\\b\")\n",
        "_tok_re = re.compile(r\"[A-Za-z0-9]+\")\n",
        "\n",
        "def ents_of(text):\n",
        "    doc = nlp(text)\n",
        "    # deduplicate within a doc: (label, normalized surface)\n",
        "    return {(ent.label_, ent.text.strip().lower()) for ent in doc.ents if ent.text.strip()}\n",
        "\n",
        "def prf1(pred_set, gold_set):\n",
        "    tp = len(pred_set & gold_set)\n",
        "    fp = len(pred_set - gold_set)\n",
        "    fn = len(gold_set - pred_set)\n",
        "    p = tp / (tp+fp) if (tp+fp) else 0.0\n",
        "    r = tp / (tp+fn) if (tp+fn) else 0.0\n",
        "    f = 2*p*r/(p+r) if (p+r) else 0.0\n",
        "    return tp, fp, fn, p, r, f\n",
        "\n",
        "def numbers_in(text):\n",
        "    # normalize by removing commas (e.g., \"1,000\" -> \"1000\")\n",
        "    return {re.sub(r\",\", \"\", m.group(0)) for m in _num_re.finditer(text)}\n",
        "\n",
        "def rep_rate(text, n=3):\n",
        "    toks = text.split()\n",
        "    if len(toks) < n: return 0.0\n",
        "    ngrams = [tuple(toks[i:i+n]) for i in range(len(toks)-n+1)]\n",
        "    total = len(ngrams)\n",
        "    uniq  = len(set(ngrams))\n",
        "    # fraction of n-gram positions that are repetitions beyond the first occurrence\n",
        "    return max(0.0, (total - uniq) / total)\n",
        "\n",
        "def copy_rate(pred, article):\n",
        "    # token-level coverage: fraction of pred tokens found in article\n",
        "    p = [t.lower() for t in _tok_re.findall(pred)]\n",
        "    a = set(t.lower() for t in _tok_re.findall(article))\n",
        "    return (sum(1 for t in p if t in a) / len(p)) if p else 0.0\n",
        "\n",
        "def evaluate_system(df, pred_col):\n",
        "    preds = df[pred_col].tolist()\n",
        "    refs  = df[\"highlights\"].tolist()\n",
        "\n",
        "    # --- ENTITIES (overall + by type), micro-averaged ---\n",
        "    overall = {\"tp\":0,\"fp\":0,\"fn\":0}\n",
        "    by_type = {}\n",
        "    for p, r in zip(preds, refs):\n",
        "        P, R = ents_of(p), ents_of(r)\n",
        "        tp, fp, fn, p_, r_, f_ = prf1(P, R)\n",
        "        overall[\"tp\"] += tp; overall[\"fp\"] += fp; overall[\"fn\"] += fn\n",
        "\n",
        "        # per type accumulators\n",
        "        gold_by = {}\n",
        "        for lab, surf in R: gold_by.setdefault(lab, set()).add((lab, surf))\n",
        "        pred_by = {}\n",
        "        for lab, surf in P: pred_by.setdefault(lab, set()).add((lab, surf))\n",
        "        all_labs = set(gold_by) | set(pred_by)\n",
        "        for lab in all_labs:\n",
        "            k = lab\n",
        "            tp_, fp_, fn_, *_ = prf1(pred_by.get(lab,set()), gold_by.get(lab,set()))\n",
        "            d = by_type.get(k, {\"tp\":0,\"fp\":0,\"fn\":0})\n",
        "            d[\"tp\"] += tp_; d[\"fp\"] += fp_; d[\"fn\"] += fn_\n",
        "            by_type[k] = d\n",
        "\n",
        "    def finalize_counts(d):\n",
        "        tp, fp, fn = d[\"tp\"], d[\"fp\"], d[\"fn\"]\n",
        "        p = tp/(tp+fp) if (tp+fp) else 0.0\n",
        "        r = tp/(tp+fn) if (tp+fn) else 0.0\n",
        "        f = 2*p*r/(p+r) if (p+r) else 0.0\n",
        "        return p, r, f\n",
        "\n",
        "    p_overall, r_overall, f_overall = finalize_counts(overall)\n",
        "\n",
        "    # --- OTHER METRICS ---\n",
        "    # Number \"accuracy\": micro recall of numbers in reference covered by prediction\n",
        "    gold_nums_total = 0\n",
        "    gold_nums_hit   = 0\n",
        "    rep3 = []; rep4 = []; copy = []\n",
        "    for art, pred, ref in zip(df[\"article\"], preds, refs):\n",
        "        gnums = numbers_in(ref)\n",
        "        pnums = numbers_in(pred)\n",
        "        gold_nums_total += len(gnums)\n",
        "        gold_nums_hit   += len(gnums & pnums)\n",
        "        rep3.append(rep_rate(pred, n=3))\n",
        "        rep4.append(rep_rate(pred, n=4))\n",
        "        copy.append(copy_rate(pred, art))\n",
        "    num_acc = (gold_nums_hit / gold_nums_total) if gold_nums_total else 0.0\n",
        "\n",
        "    # --- PRINT RESULTS ---\n",
        "    print(f\"\\n=== [{pred_col.upper()}] ENTITY (overall) ===\")\n",
        "    print(f\"Precision: {p_overall:.6f}\")\n",
        "    print(f\"Recall:    {r_overall:.6f}\")\n",
        "    print(f\"F1:        {f_overall:.6f}\")\n",
        "\n",
        "    print(f\"\\n=== [{pred_col.upper()}] ENTITY (by type) ===\")\n",
        "    # Order by support (tp+fn) descending\n",
        "    rows = []\n",
        "    for lab, d in by_type.items():\n",
        "        p, r, f = finalize_counts(d)\n",
        "        rows.append((lab, p, r, f, d[\"tp\"]+d[\"fn\"]))\n",
        "    rows.sort(key=lambda x: (-x[4], x[0]))\n",
        "    if not rows:\n",
        "        print(\"(no entities)\")\n",
        "    else:\n",
        "        for lab, p, r, f, _ in rows:\n",
        "            print(f\"{lab:<12} P={p:.3f} R={r:.3f} F1={f:.3f}\")\n",
        "\n",
        "    print(f\"\\n=== [{pred_col.upper()}] OTHER METRICS ===\")\n",
        "    print(f\"Number Coverage: {num_acc:.6f}\")\n",
        "    print(f\"Repetition Rate (3-gram): {float(np.mean(rep3)):.6f}\")\n",
        "    print(f\"Repetition Rate (4-gram): {float(np.mean(rep4)):.6f}\")\n",
        "    print(f\"Copy Rate (token coverage vs article): {float(np.mean(copy)):.6f}\")\n",
        "\n",
        "# --------- run ---------\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "for c in [\"article\",\"highlights\",\"base_pred\",\"ptr_pred\"]:\n",
        "    if c in df.columns: df[c] = df[c].map(normalize)\n",
        "\n",
        "print(f\"[loaded] rows={len(df)} from {CSV_PATH}\")\n",
        "\n",
        "if \"base_pred\" in df.columns:\n",
        "    evaluate_system(df, \"base_pred\")\n",
        "if \"ptr_pred\" in df.columns:\n",
        "    evaluate_system(df, \"ptr_pred\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lybhoHfw33EG",
        "outputId": "ea5f7035-89dc-4a5d-9445-a85d919d8a51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== BASE ===\n",
            "Precision: 0.378463\n",
            "Recall:    0.380854\n",
            "F1:        0.379655\n",
            "\n",
            "(by type)\n",
            "PERSON       P=0.508 R=0.473 F1=0.490\n",
            "DATE         P=0.307 R=0.314 F1=0.311\n",
            "ORG          P=0.340 R=0.335 F1=0.338\n",
            "GPE          P=0.466 R=0.463 F1=0.465\n",
            "CARDINAL     P=0.306 R=0.324 F1=0.315\n",
            "NORP         P=0.292 R=0.358 F1=0.322\n",
            "MONEY        P=0.327 R=0.333 F1=0.330\n",
            "ORDINAL      P=0.251 R=0.350 F1=0.293\n",
            "TIME         P=0.122 R=0.136 F1=0.129\n",
            "LOC          P=0.356 R=0.363 F1=0.359\n",
            "FAC          P=0.208 R=0.283 F1=0.240\n",
            "QUANTITY     P=0.255 R=0.233 F1=0.243\n",
            "EVENT        P=0.360 R=0.305 F1=0.330\n",
            "WORK_OF_ART  P=0.237 R=0.184 F1=0.207\n",
            "PRODUCT      P=0.273 R=0.364 F1=0.312\n",
            "PERCENT      P=0.278 R=0.192 F1=0.227\n",
            "LAW          P=0.091 R=0.077 F1=0.083\n",
            "LANGUAGE     P=0.667 R=0.444 F1=0.533\n",
            "\n",
            "=== PTR (original) ===\n",
            "Precision: 0.336422\n",
            "Recall:    0.403928\n",
            "F1:        0.367097\n",
            "\n",
            "(by type)\n",
            "PERSON       P=0.431 R=0.460 F1=0.445\n",
            "DATE         P=0.271 R=0.346 F1=0.304\n",
            "ORG          P=0.322 R=0.390 F1=0.353\n",
            "GPE          P=0.421 R=0.481 F1=0.449\n",
            "CARDINAL     P=0.304 R=0.365 F1=0.332\n",
            "NORP         P=0.290 R=0.429 F1=0.346\n",
            "MONEY        P=0.276 R=0.345 F1=0.307\n",
            "ORDINAL      P=0.233 R=0.371 F1=0.287\n",
            "TIME         P=0.142 R=0.242 F1=0.179\n",
            "LOC          P=0.319 R=0.373 F1=0.344\n",
            "FAC          P=0.154 R=0.250 F1=0.191\n",
            "QUANTITY     P=0.235 R=0.317 F1=0.270\n",
            "EVENT        P=0.333 R=0.305 F1=0.319\n",
            "WORK_OF_ART  P=0.190 R=0.224 F1=0.206\n",
            "PRODUCT      P=0.298 R=0.515 F1=0.378\n",
            "PERCENT      P=0.077 R=0.038 F1=0.051\n",
            "LAW          P=0.231 R=0.231 F1=0.231\n",
            "LANGUAGE     P=0.333 R=0.222 F1=0.267\n",
            "\n",
            "=== PTR @ base-length ===\n",
            "Precision: 0.365834\n",
            "Recall:    0.365884\n",
            "F1:        0.365859\n",
            "\n",
            "(by type)\n",
            "PERSON       P=0.467 R=0.422 F1=0.443\n",
            "DATE         P=0.293 R=0.311 F1=0.302\n",
            "ORG          P=0.348 R=0.343 F1=0.346\n",
            "GPE          P=0.455 R=0.456 F1=0.456\n",
            "CARDINAL     P=0.326 R=0.314 F1=0.320\n",
            "NORP         P=0.315 R=0.398 F1=0.352\n",
            "MONEY        P=0.327 R=0.304 F1=0.315\n",
            "ORDINAL      P=0.257 R=0.336 F1=0.291\n",
            "TIME         P=0.160 R=0.220 F1=0.185\n",
            "LOC          P=0.351 R=0.324 F1=0.337\n",
            "FAC          P=0.173 R=0.239 F1=0.201\n",
            "QUANTITY     P=0.194 R=0.200 F1=0.197\n",
            "EVENT        P=0.319 R=0.254 F1=0.283\n",
            "WORK_OF_ART  P=0.200 R=0.204 F1=0.202\n",
            "PRODUCT      P=0.333 R=0.485 F1=0.395\n",
            "PERCENT      P=0.100 R=0.038 F1=0.056\n",
            "LAW          P=0.273 R=0.231 F1=0.250\n",
            "LANGUAGE     P=0.400 R=0.222 F1=0.286\n"
          ]
        }
      ],
      "source": [
        "# Entity metrics with pointer truncated to base length to check if the length is the problem\n",
        "!pip -q install spacy==3.7.5\n",
        "import pandas as pd, re, spacy\n",
        "\n",
        "CSV = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000/iksumpointer.csv\"\n",
        "\n",
        "# spaCy NER\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "except OSError:\n",
        "    from spacy.cli import download\n",
        "    download(\"en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "\n",
        "def norm(s): return \" \".join(str(s or \"\").split())\n",
        "\n",
        "def ents_of(text):\n",
        "    doc = nlp(text)\n",
        "    return {(e.label_, e.text.strip().lower()) for e in doc.ents if e.text.strip()}\n",
        "\n",
        "def prf1(pred_set, gold_set):\n",
        "    tp = len(pred_set & gold_set); fp = len(pred_set - gold_set); fn = len(gold_set - pred_set)\n",
        "    P = tp/(tp+fp) if (tp+fp) else 0.0\n",
        "    R = tp/(tp+fn) if (tp+fn) else 0.0\n",
        "    F = 2*P*R/(P+R) if (P+R) else 0.0\n",
        "    return tp, fp, fn, P, R, F\n",
        "\n",
        "def eval_entities(preds, refs, by_type=True):\n",
        "    overall = {\"tp\":0,\"fp\":0,\"fn\":0}\n",
        "    types = {}\n",
        "    for p, r in zip(preds, refs):\n",
        "        P, R = ents_of(p), ents_of(r)\n",
        "        tp, fp, fn, *_ = prf1(P, R)\n",
        "        overall[\"tp\"] += tp; overall[\"fp\"] += fp; overall[\"fn\"] += fn\n",
        "        if by_type:\n",
        "            gb, pb = {}, {}\n",
        "            for lab, s in R: gb.setdefault(lab,set()).add((lab,s))\n",
        "            for lab, s in P: pb.setdefault(lab,set()).add((lab,s))\n",
        "            for lab in set(gb)|set(pb):\n",
        "                tp_, fp_, fn_, *_ = prf1(pb.get(lab,set()), gb.get(lab,set()))\n",
        "                d = types.get(lab, {\"tp\":0,\"fp\":0,\"fn\":0})\n",
        "                d[\"tp\"]+=tp_; d[\"fp\"]+=fp_; d[\"fn\"]+=fn_\n",
        "                types[lab]=d\n",
        "    def finish(c):\n",
        "        tp,fp,fn=c[\"tp\"],c[\"fp\"],c[\"fn\"]\n",
        "        P=tp/(tp+fp) if (tp+fp) else 0.0\n",
        "        R=tp/(tp+fn) if (tp+fn) else 0.0\n",
        "        F=2*P*R/(P+R) if (P+R) else 0.0\n",
        "        return P,R,F\n",
        "    P,R,F = finish(overall)\n",
        "    print(f\"Precision: {P:.6f}\\nRecall:    {R:.6f}\\nF1:        {F:.6f}\")\n",
        "    if by_type and types:\n",
        "        rows=[]\n",
        "        for lab,d in types.items():\n",
        "            P,R,F = finish(d); supp = d[\"tp\"]+d[\"fn\"]\n",
        "            rows.append((supp, lab, P,R,F))\n",
        "        rows.sort(reverse=True)\n",
        "        print(\"\\n(by type)\")\n",
        "        for _,lab,P,R,F in rows[:20]:\n",
        "            print(f\"{lab:<12} P={P:.3f} R={R:.3f} F1={F:.3f}\")\n",
        "\n",
        "# Load\n",
        "df = pd.read_csv(CSV)\n",
        "for c in [\"article\",\"highlights\",\"base_pred\",\"ptr_pred\"]:\n",
        "    if c in df.columns: df[c] = df[c].map(norm)\n",
        "\n",
        "# Truncate pointer to base word count per example\n",
        "base_lens = df[\"base_pred\"].str.split().map(len)\n",
        "def first_n_words(s, n):\n",
        "    w = s.split(); return \" \".join(w[:int(n)]) if n>0 else \"\"\n",
        "\n",
        "ptr_trunc = [first_n_words(p, n) for p,n in zip(df[\"ptr_pred\"], base_lens)]\n",
        "\n",
        "print(\"=== BASE ===\")\n",
        "eval_entities(df[\"base_pred\"].tolist(), df[\"highlights\"].tolist())\n",
        "print(\"\\n=== PTR (original) ===\")\n",
        "eval_entities(df[\"ptr_pred\"].tolist(), df[\"highlights\"].tolist())\n",
        "print(\"\\n=== PTR @ base-length ===\")\n",
        "eval_entities(ptr_trunc, df[\"highlights\"].tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQ599ysm45u0",
        "outputId": "72d91127-4e28-4720-b47d-83ba4f44f6c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== BASE — entity errors by type ===\n",
            "PERSON       tp= 916 fp= 886 fn=1019  P=0.508 R=0.473 F1=0.490\n",
            "DATE         tp= 405 fp= 914 fn= 884  P=0.307 R=0.314 F1=0.311\n",
            "ORG          tp= 382 fp= 740 fn= 757  P=0.340 R=0.335 F1=0.338\n",
            "GPE          tp= 512 fp= 586 fn= 593  P=0.466 R=0.463 F1=0.465\n",
            "CARDINAL     tp= 228 fp= 516 fn= 476  P=0.306 R=0.324 F1=0.315\n",
            "NORP         tp=  81 fp= 196 fn= 145  P=0.292 R=0.358 F1=0.322\n",
            "MONEY        tp=  56 fp= 115 fn= 112  P=0.327 R=0.333 F1=0.330\n",
            "ORDINAL      tp=  49 fp= 146 fn=  91  P=0.251 R=0.350 F1=0.293\n",
            "TIME         tp=  18 fp= 130 fn= 114  P=0.122 R=0.136 F1=0.129\n",
            "LOC          tp=  37 fp=  67 fn=  65  P=0.356 R=0.363 F1=0.359\n",
            "FAC          tp=  26 fp=  99 fn=  66  P=0.208 R=0.283 F1=0.240\n",
            "QUANTITY     tp=  14 fp=  41 fn=  46  P=0.255 R=0.233 F1=0.243\n",
            "EVENT        tp=  18 fp=  32 fn=  41  P=0.360 R=0.305 F1=0.330\n",
            "WORK_OF_ART  tp=   9 fp=  29 fn=  40  P=0.237 R=0.184 F1=0.207\n",
            "PRODUCT      tp=  12 fp=  32 fn=  21  P=0.273 R=0.364 F1=0.312\n",
            "PERCENT      tp=   5 fp=  13 fn=  21  P=0.278 R=0.192 F1=0.227\n",
            "LAW          tp=   1 fp=  10 fn=  12  P=0.091 R=0.077 F1=0.083\n",
            "LANGUAGE     tp=   4 fp=   2 fn=   5  P=0.667 R=0.444 F1=0.533\n",
            "\n",
            "=== PTR — entity errors by type ===\n",
            "PERSON       tp= 891 fp=1175 fn=1044  P=0.431 R=0.460 F1=0.445\n",
            "DATE         tp= 446 fp=1200 fn= 843  P=0.271 R=0.346 F1=0.304\n",
            "ORG          tp= 444 fp= 933 fn= 695  P=0.322 R=0.390 F1=0.353\n",
            "GPE          tp= 532 fp= 732 fn= 573  P=0.421 R=0.481 F1=0.449\n",
            "CARDINAL     tp= 257 fp= 588 fn= 447  P=0.304 R=0.365 F1=0.332\n",
            "NORP         tp=  97 fp= 238 fn= 129  P=0.290 R=0.429 F1=0.346\n",
            "MONEY        tp=  58 fp= 152 fn= 110  P=0.276 R=0.345 F1=0.307\n",
            "ORDINAL      tp=  52 fp= 171 fn=  88  P=0.233 R=0.371 F1=0.287\n",
            "TIME         tp=  32 fp= 194 fn= 100  P=0.142 R=0.242 F1=0.179\n",
            "LOC          tp=  38 fp=  81 fn=  64  P=0.319 R=0.373 F1=0.344\n",
            "FAC          tp=  23 fp= 126 fn=  69  P=0.154 R=0.250 F1=0.191\n",
            "QUANTITY     tp=  19 fp=  62 fn=  41  P=0.235 R=0.317 F1=0.270\n",
            "EVENT        tp=  18 fp=  36 fn=  41  P=0.333 R=0.305 F1=0.319\n",
            "WORK_OF_ART  tp=  11 fp=  47 fn=  38  P=0.190 R=0.224 F1=0.206\n",
            "PRODUCT      tp=  17 fp=  40 fn=  16  P=0.298 R=0.515 F1=0.378\n",
            "PERCENT      tp=   1 fp=  12 fn=  25  P=0.077 R=0.038 F1=0.051\n",
            "LAW          tp=   3 fp=  10 fn=  10  P=0.231 R=0.231 F1=0.231\n",
            "LANGUAGE     tp=   2 fp=   4 fn=   7  P=0.333 R=0.222 F1=0.267\n",
            "\n",
            "=== PTR@base — entity errors by type ===\n",
            "PERSON       tp= 816 fp= 932 fn=1119  P=0.467 R=0.422 F1=0.443\n",
            "DATE         tp= 401 fp= 966 fn= 888  P=0.293 R=0.311 F1=0.302\n",
            "ORG          tp= 391 fp= 732 fn= 748  P=0.348 R=0.343 F1=0.346\n",
            "GPE          tp= 504 fp= 603 fn= 601  P=0.455 R=0.456 F1=0.456\n",
            "CARDINAL     tp= 221 fp= 456 fn= 483  P=0.326 R=0.314 F1=0.320\n",
            "NORP         tp=  90 fp= 196 fn= 136  P=0.315 R=0.398 F1=0.352\n",
            "MONEY        tp=  51 fp= 105 fn= 117  P=0.327 R=0.304 F1=0.315\n",
            "ORDINAL      tp=  47 fp= 136 fn=  93  P=0.257 R=0.336 F1=0.291\n",
            "TIME         tp=  29 fp= 152 fn= 103  P=0.160 R=0.220 F1=0.185\n",
            "LOC          tp=  33 fp=  61 fn=  69  P=0.351 R=0.324 F1=0.337\n",
            "FAC          tp=  22 fp= 105 fn=  70  P=0.173 R=0.239 F1=0.201\n",
            "QUANTITY     tp=  12 fp=  50 fn=  48  P=0.194 R=0.200 F1=0.197\n",
            "EVENT        tp=  15 fp=  32 fn=  44  P=0.319 R=0.254 F1=0.283\n",
            "WORK_OF_ART  tp=  10 fp=  40 fn=  39  P=0.200 R=0.204 F1=0.202\n",
            "PRODUCT      tp=  16 fp=  32 fn=  17  P=0.333 R=0.485 F1=0.395\n",
            "PERCENT      tp=   1 fp=   9 fn=  25  P=0.100 R=0.038 F1=0.056\n",
            "LAW          tp=   3 fp=   8 fn=  10  P=0.273 R=0.231 F1=0.250\n",
            "LANGUAGE     tp=   2 fp=   3 fn=   7  P=0.400 R=0.222 F1=0.286\n"
          ]
        }
      ],
      "source": [
        "# FP/FN by entity type for BASE, PTR, and PTR@base-length (PRINT ONLY)\n",
        "!pip -q install spacy==3.7.5\n",
        "import pandas as pd, spacy\n",
        "\n",
        "CSV = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000/iksumpointer.csv\"\n",
        "\n",
        "# spaCy NER\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "except OSError:\n",
        "    from spacy.cli import download\n",
        "    download(\"en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "\n",
        "def norm(s): return \" \".join(str(s or \"\").split())\n",
        "\n",
        "def ents(text):\n",
        "    d = nlp(text)\n",
        "    by = {}\n",
        "    for e in d.ents:\n",
        "        t = e.text.strip()\n",
        "        if t:\n",
        "            by.setdefault(e.label_, set()).add((e.label_, t.lower()))\n",
        "    return by\n",
        "\n",
        "def prf_counts(pred_by, gold_by):\n",
        "    labs = set(pred_by) | set(gold_by)\n",
        "    out = {}\n",
        "    for L in labs:\n",
        "        P, G = pred_by.get(L, set()), gold_by.get(L, set())\n",
        "        tp = len(P & G); fp = len(P - G); fn = len(G - P)\n",
        "        out[L] = (tp, fp, fn)\n",
        "    return out\n",
        "\n",
        "def first_n(s, n):\n",
        "    w = s.split()\n",
        "    return \" \".join(w[:int(n)]) if n and n > 0 else \"\"\n",
        "\n",
        "# Load & normalize\n",
        "df = pd.read_csv(CSV)\n",
        "for c in [\"article\",\"highlights\",\"base_pred\",\"ptr_pred\"]:\n",
        "    if c in df.columns: df[c] = df[c].map(norm)\n",
        "\n",
        "# Pointer truncated to base word count\n",
        "base_lens = df[\"base_pred\"].str.split().map(len)\n",
        "ptr_trunc = [first_n(p, n) for p, n in zip(df[\"ptr_pred\"], base_lens)]\n",
        "\n",
        "def aggregate(preds, refs):\n",
        "    preds = list(preds)\n",
        "    refs  = list(refs)\n",
        "    agg = {}\n",
        "    for p, r in zip(preds, refs):\n",
        "        pc, rc = ents(p), ents(r)\n",
        "        d = prf_counts(pc, rc)\n",
        "        for L, (tp, fp, fn) in d.items():\n",
        "            a = agg.get(L, [0, 0, 0])\n",
        "            a[0] += tp; a[1] += fp; a[2] += fn\n",
        "            agg[L] = a\n",
        "    return agg\n",
        "\n",
        "for name, preds in [(\"BASE\", df[\"base_pred\"]),\n",
        "                    (\"PTR\", df[\"ptr_pred\"]),\n",
        "                    (\"PTR@base\", ptr_trunc)]:\n",
        "    agg = aggregate(preds, df[\"highlights\"])\n",
        "    print(f\"\\n=== {name} — entity errors by type ===\")\n",
        "    rows = []\n",
        "    for L, (tp, fp, fn) in agg.items():\n",
        "        P = tp/(tp+fp) if (tp+fp) else 0.0\n",
        "        R = tp/(tp+fn) if (tp+fn) else 0.0\n",
        "        F = 2*P*R/(P+R) if (P+R) else 0.0\n",
        "        rows.append((tp+fn, L, tp, fp, fn, P, R, F))\n",
        "    rows.sort(reverse=True)\n",
        "    for _, L, tp, fp, fn, P, R, F in rows[:20]:\n",
        "        print(f\"{L:<12} tp={tp:4d} fp={fp:4d} fn={fn:4d}  P={P:.3f} R={R:.3f} F1={F:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9FNsFrI9eNo"
      },
      "source": [
        "#Decoding exp for tunning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194,
          "referenced_widgets": [
            "80e0b99372de4549adee2a7ea10ead26"
          ]
        },
        "id": "Kr8aAFee56jw",
        "outputId": "e0a325d4-2c38-4344-a134-4f9c3b308eb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[device] cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[pointer_head] loaded=True\n",
            "[loaded] 100 rows from base csv\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80e0b99372de4549adee2a7ea10ead26",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "PTR-BEAM (exp2):   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[done] saved -> /content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000/100exp2sum.csv | elapsed ~9.8 min\n",
            "[probe] p_copy_mean=0.319 | p_gen_mean=0.681\n"
          ]
        }
      ],
      "source": [
        "# === 100 new POINTER summaries (shorter/preciser), matching first 100 base rows ===\n",
        "import os, gc, time, pandas as pd, torch, torch.nn as nn, torch.nn.functional as F\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers.modeling_outputs import BaseModelOutput\n",
        "\n",
        "# ---------- paths ----------\n",
        "CKPT_DIR     = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000\"\n",
        "BASE_CSV     = os.path.join(CKPT_DIR, \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000/iksumpointer.csv\")  # existing base summaries\n",
        "OUT_CSV      = os.path.join(CKPT_DIR, \"100exp2sum.csv\")               # NEW file to save\n",
        "SRC_COL, REF_COL = \"article\", \"highlights\"\n",
        "N = 100  # first 100 rows\n",
        "\n",
        "# ---------- decoding knobs (tighter than before) ----------\n",
        "NUM_BEAMS      = 5\n",
        "MIN_NEW        = 34\n",
        "MAX_NEW        = 100\n",
        "NO_REPEAT      = 5\n",
        "LENGTH_PENALTY = 2.2\n",
        "MAX_SRC_LEN    = 400\n",
        "SAVE_EVERY     = 25\n",
        "GATE_BIAS      = +0.30\n",
        "\n",
        "# ---------- setup ----------\n",
        "assert os.path.exists(BASE_CSV), f\"Missing: {BASE_CSV}\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"[device]\", device)\n",
        "if device == \"cuda\":\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "USE_FP16_PTR = (device == \"cuda\")\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "tok  = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
        "base = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR, attn_implementation=\"eager\").to(device).eval()\n",
        "\n",
        "# ---------- pointer wrapper (same architecture that worked, just gate_bias=+0.30) ----------\n",
        "class CopyAwareBart(nn.Module):\n",
        "    def __init__(self, base_model, tokenizer, lambda_cov=1.0, eps=1e-8, use_pointer=True, gate_bias=0.0):\n",
        "        super().__init__()\n",
        "        self.base   = base_model\n",
        "        self.tok    = tokenizer\n",
        "        self.lambda_cov = float(lambda_cov)\n",
        "        self.eps    = float(eps)\n",
        "        self.use_ptr= bool(use_pointer)\n",
        "        self.gate_bias = float(gate_bias)\n",
        "        H = self.base.config.d_model\n",
        "        self.p_gen_linear = nn.Linear(3*H, 1)\n",
        "        self._last_p_copy_mean = self._last_p_gen_mean = None\n",
        "\n",
        "    def _mix_pointer(self, out, input_ids, attention_mask, decoder_input_ids):\n",
        "        logits = out.logits  # (B,T,V)\n",
        "        last_ca = out.cross_attentions[-1].mean(1)  # (B,T,S)\n",
        "        if attention_mask is not None:\n",
        "            last_ca = last_ca.masked_fill(attention_mask[:, None, :] == 0, 0.0)\n",
        "        attn = last_ca / last_ca.sum(-1, keepdim=True).clamp_min(self.eps)\n",
        "        dec_hid = out.decoder_hidden_states[-1]\n",
        "        ctx     = torch.bmm(attn, out.encoder_last_hidden_state)\n",
        "        dec_inp = self.base.get_input_embeddings()(decoder_input_ids)\n",
        "        gate_in = torch.cat([dec_hid, ctx, dec_inp], dim=-1)\n",
        "        p_gen   = torch.sigmoid(self.p_gen_linear(gate_in) + self.gate_bias).squeeze(-1)\n",
        "        p_copy  = 1.0 - p_gen\n",
        "\n",
        "        copy_probs = torch.zeros_like(logits)\n",
        "        idx = input_ids.unsqueeze(1).expand(input_ids.size(0), logits.size(1), input_ids.size(1))\n",
        "        copy_probs.scatter_add_(2, idx, attn)\n",
        "\n",
        "        vocab_logp = F.log_softmax(logits, dim=-1)\n",
        "        logp_final = torch.logaddexp(\n",
        "            vocab_logp + (p_gen + 1e-8).log().unsqueeze(-1),\n",
        "            (copy_probs + 1e-8).log() + (p_copy + 1e-8).log().unsqueeze(-1)\n",
        "        )\n",
        "        self._last_p_copy_mean = float(p_copy.detach().mean().cpu())\n",
        "        self._last_p_gen_mean  = float(p_gen.detach().mean().cpu())\n",
        "        return logp_final\n",
        "\n",
        "model_ptr = CopyAwareBart(base, tok, use_pointer=True, gate_bias=GATE_BIAS).to(device).eval()\n",
        "ph_path = os.path.join(CKPT_DIR, \"pointer_head.pt\")\n",
        "if os.path.exists(ph_path):\n",
        "    sd = torch.load(ph_path, map_location=\"cpu\")\n",
        "    if isinstance(sd, dict) and any(k.startswith(\"p_gen_linear.\") for k in sd.keys()):\n",
        "        model_ptr.load_state_dict(sd, strict=False)\n",
        "    elif isinstance(sd, dict) and all(k in {\"weight\",\"bias\"} for k in sd.keys()):\n",
        "        model_ptr.p_gen_linear.load_state_dict(sd)\n",
        "    elif isinstance(sd, dict) and \"p_gen_linear\" in sd:\n",
        "        model_ptr.p_gen_linear.load_state_dict(sd[\"p_gen_linear\"])\n",
        "print(\"[pointer_head] loaded=True\")\n",
        "\n",
        "def _ban_repeating_ngrams(logits: torch.Tensor, seq: torch.Tensor, ngram: int):\n",
        "    if ngram <= 0: return\n",
        "    toks = seq[0].tolist() if seq.dim()==2 else seq.tolist()\n",
        "    L = len(toks)\n",
        "    if L < ngram-1: return\n",
        "    prefix = tuple(toks[-(ngram - 1):])\n",
        "    banned = set()\n",
        "    for i in range(L - ngram + 1):\n",
        "        if tuple(toks[i:i + ngram - 1]) == prefix:\n",
        "            banned.add(toks[i + ngram - 1])\n",
        "    for t in banned: logits[t] = -1e9\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_pointer_beam_one_fast(text: str) -> str:\n",
        "    enc = tok([text], return_tensors=\"pt\", truncation=True, max_length=MAX_SRC_LEN).to(device)\n",
        "    src_ids, src_mask = enc.input_ids, enc.attention_mask\n",
        "    with torch.amp.autocast('cuda', dtype=torch.float16, enabled=USE_FP16_PTR):\n",
        "        enc_out = base.model.get_encoder()(input_ids=src_ids, attention_mask=src_mask, return_dict=True)\n",
        "    start_id = getattr(base.config, \"decoder_start_token_id\", None) or (tok.bos_token_id or tok.eos_token_id)\n",
        "    beams = [(torch.tensor([[start_id]], device=device, dtype=torch.long), 0.0, False)]\n",
        "    for step in range(MAX_NEW):\n",
        "        act = [(i,b) for i,b in enumerate(beams) if not b[2]]\n",
        "        if not act: break\n",
        "        seqs = torch.cat([beams[i][0] for i,_ in act], dim=0)  # (K, L)\n",
        "        K = seqs.size(0)\n",
        "        enc_exp = BaseModelOutput(last_hidden_state=enc_out.last_hidden_state.expand(K, -1, -1))\n",
        "        src_mask_exp = src_mask.expand(K, -1)\n",
        "        with torch.amp.autocast('cuda', dtype=torch.float16, enabled=USE_FP16_PTR):\n",
        "            out = base(\n",
        "                encoder_outputs=enc_exp,\n",
        "                attention_mask=src_mask_exp,\n",
        "                decoder_input_ids=seqs,\n",
        "                output_attentions=True, output_hidden_states=True,\n",
        "                use_cache=False, return_dict=True,\n",
        "            )\n",
        "        with torch.amp.autocast('cuda', dtype=torch.float16, enabled=USE_FP16_PTR):\n",
        "            logits = model_ptr._mix_pointer(out, src_ids.expand(K, -1), src_mask_exp, seqs)\n",
        "        step_logits_all = logits[:, -1, :].float()\n",
        "\n",
        "        new_beams = []\n",
        "        for j, (beam_idx, _) in enumerate(act):\n",
        "            step_logits = step_logits_all[j].clone()\n",
        "            if step + 1 < MIN_NEW and tok.eos_token_id is not None:\n",
        "                step_logits[tok.eos_token_id] = -1e9\n",
        "            _ban_repeating_ngrams(step_logits, beams[beam_idx][0], NO_REPEAT)\n",
        "            topk_logp, topk_ids = torch.topk(step_logits, k=NUM_BEAMS)\n",
        "            for k in range(NUM_BEAMS):\n",
        "                tid  = topk_ids[k].view(1,1)\n",
        "                nseq = torch.cat([beams[beam_idx][0], tid], dim=1)\n",
        "                nfin = (tid.item() == tok.eos_token_id)\n",
        "                raw  = beams[beam_idx][1] + float(topk_logp[k].item())\n",
        "                lp   = ((5.0 + nseq.size(1))**LENGTH_PENALTY) / ((5.0 + 1.0)**LENGTH_PENALTY)\n",
        "                nsc  = raw / lp\n",
        "                new_beams.append((nseq, nsc, nfin))\n",
        "        for (seq, score, done) in beams:\n",
        "            if done: new_beams.append((seq, score, True))\n",
        "        new_beams.sort(key=lambda x: x[1], reverse=True)\n",
        "        beams = new_beams[:NUM_BEAMS]\n",
        "        if all(b[2] for b in beams): break\n",
        "        if (step & 7) == 0:\n",
        "            torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "    best_seq = max(beams, key=lambda x: x[1])[0]\n",
        "    return tok.batch_decode(best_seq, skip_special_tokens=True)[0]\n",
        "\n",
        "# ---------- load first 100 rows from base CSV ----------\n",
        "base_df = pd.read_csv(BASE_CSV)\n",
        "base_100 = base_df.head(N).copy()\n",
        "print(f\"[loaded] {len(base_100)} rows from base csv\")\n",
        "\n",
        "# ---------- generate new pointer for these exact rows ----------\n",
        "t0 = time.time()\n",
        "ptr_preds, lens = [], []\n",
        "for i, text in enumerate(tqdm(base_100[SRC_COL].astype(str).tolist(), desc=\"PTR-BEAM (exp2)\")):\n",
        "    pred = generate_pointer_beam_one_fast(text)\n",
        "    ptr_preds.append(pred)\n",
        "    lens.append(len(pred.split()))\n",
        "    if (i+1) % SAVE_EVERY == 0:\n",
        "        torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "out_df = pd.DataFrame({\n",
        "    \"idx\": base_100.index.values,\n",
        "    SRC_COL: base_100[SRC_COL].values,\n",
        "    REF_COL: base_100[REF_COL].values,\n",
        "    \"base_pred\": base_100[\"base_pred\"].values,\n",
        "    \"ptr_pred_exp2\": ptr_preds,\n",
        "    \"len_base\": base_100[\"base_pred\"].astype(str).str.split().map(len).values,\n",
        "    \"len_ptr_exp2\": lens,\n",
        "})\n",
        "out_df.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
        "mins = (time.time() - t0) / 60.0\n",
        "print(f\"[done] saved -> {OUT_CSV} | elapsed ~{mins:.1f} min\")\n",
        "print(f\"[probe] p_copy_mean={getattr(model_ptr,'_last_p_copy_mean',float('nan')):.3f} | p_gen_mean={getattr(model_ptr,'_last_p_gen_mean',float('nan')):.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fP0KZR2-hrR",
        "outputId": "59668679-cc46-4cd7-a644-43ce1624e505"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== ROUGE (mean over matched 100) ===\n",
            "[BASE] {'r1_p': 0.4092, 'r1_r': 0.4721, 'r1_f': 0.4227, 'r2_p': 0.1958, 'r2_r': 0.2283, 'r2_f': 0.2035, 'rl_p': 0.2873, 'rl_r': 0.3334, 'rl_f': 0.2978}\n",
            "[PTR exp2] {'r1_p': 0.3873, 'r1_r': 0.4859, 'r1_f': 0.418, 'r2_p': 0.1754, 'r2_r': 0.2253, 'r2_f': 0.1916, 'rl_p': 0.2579, 'rl_r': 0.3285, 'rl_f': 0.2801}\n",
            "[PTR exp2 - BASE] {'r1_p': -0.0219, 'r1_r': 0.0138, 'r1_f': -0.0047, 'r2_p': -0.0204, 'r2_r': -0.003, 'r2_f': -0.0119, 'rl_p': -0.0294, 'rl_r': -0.0049, 'rl_f': -0.0177}\n"
          ]
        }
      ],
      "source": [
        "# Fix ROUGE aggregation and print results for the already-loaded base_100/new100\n",
        "from rouge_score import rouge_scorer\n",
        "import pandas as pd\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\"], use_stemmer=True)\n",
        "\n",
        "def rouge_mean(preds, refs):\n",
        "    rows = []\n",
        "    for p, r in zip(preds, refs):\n",
        "        sc = scorer.score(r, p)\n",
        "        rows.append({\n",
        "            \"r1_p\": sc[\"rouge1\"].precision, \"r1_r\": sc[\"rouge1\"].recall, \"r1_f\": sc[\"rouge1\"].fmeasure,\n",
        "            \"r2_p\": sc[\"rouge2\"].precision, \"r2_r\": sc[\"rouge2\"].recall, \"r2_f\": sc[\"rouge2\"].fmeasure,\n",
        "            \"rl_p\": sc[\"rougeL\"].precision, \"rl_r\": sc[\"rougeL\"].recall, \"rl_f\": sc[\"rougeL\"].fmeasure,\n",
        "        })\n",
        "    m = pd.DataFrame(rows).mean(numeric_only=True)\n",
        "    return {k: float(f\"{v:.4f}\") for k, v in m.items()}  # <-- use .items()\n",
        "\n",
        "refs = base_100[REF_COL].tolist()\n",
        "rb   = rouge_mean(base_100[\"base_pred\"].tolist(), refs)\n",
        "rp   = rouge_mean(new100[\"ptr_pred_exp2\"].tolist(), refs)\n",
        "diff = {k: float(f\"{rp[k]-rb[k]:.4f}\") for k in rb}\n",
        "\n",
        "print(\"\\n=== ROUGE (mean over matched 100) ===\")\n",
        "print(\"[BASE]\", rb)\n",
        "print(\"[PTR exp2]\", rp)\n",
        "print(\"[PTR exp2 - BASE]\", diff)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpXt4Bb2AWNx",
        "outputId": "8bed19f5-3d1b-41ae-d141-a3847b1101ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================================================================================================\n",
            "[1] row_id=26   len_base=48   len_ptr=39\n",
            "- REF  : Mesut Ozil reveals he brings moisturiser to every game . Olivier Giroud says it is a pleasure when fans imitate his hair style . Giroud admires 'top man' David Beckahm and 'elegant' Andriy Shevchenko . Ozil, Giroud and Alexis Sanchez posed for GQ Style magazine . CLICK HERE for all the latest Arsenal news .\n",
            "- BASE : Arsenal face Manchester United at Old Trafford on Monday to reach the FA Cup semi-finals. Mesut Ozil reveals he has a personal grooming kit. Olivier Giroud claims he is inspired by David Beckham and Andriy Shevchenko. The Gunners have won seven of their last eight Premier League games.\n",
            "- PTR2 : Arsenal beat Manchester United 3-0 in the FA Cup semi-finals on Monday. Mesut Ozil revealed he has a personal grooming kit that he brings to every game. Olivier Giroud, Alexis Sanchez and Mesut Ozil pose for GQ Style shoot.\n",
            "========================================================================================================================\n",
            "[2] row_id=86   len_base=59   len_ptr=56\n",
            "- REF  : Charley Saturmin Robinet, 39, killed in struggle which was caught on film . Police say Robinet tried to grab a gun, but footage sparked angry protests . Police were trying to arrest him for not reporting to probation officials . He stole $33,500 in 2000 to fund acting classes at Beverly Hills Playhouse . He served 13 years' jail for armed robbery and had mental health issues . Originally thought to be French, authorities now say he stole his identity . He gained passport in 1990s under name and …\n",
            "- BASE : Charley Saturmin Robinet, 39, was killed in a scuffle with police on Skid Row on Sunday. He allegedly tried to grab the holstered pistol of an officer while police arrested him. The shooting was caught by an onlooker on a video that went viral. The shooting sparked calls for a special police commission hearing on the use of force.\n",
            "- PTR2 : .'Charley Saturmin Robinet, 39, was killed on Los Angeles' Skid Row on Sunday. He allegedly tried to grab the holstered pistol of an officer while police arrested him. The shooting was caught by an onlooker on a video that went viral. The shooting sparked calls for a special police commission hearing on the use of force.\n",
            "========================================================================================================================\n",
            "[3] row_id=2   len_base=80   len_ptr=64\n",
            "- REF  : Nathan Thompson allegedly killed 10 bull terriers aged six to eight weeks . He pleaded guilty to nine extra animal cruelty charges on Thursday . Police say he took them to bushland where he hit their heads with rocks . He was released on bail but is banned from being around animals alone . RSPCA confirmed the only pup that survived is doing well in their care .\n",
            "- BASE : A man from the NSW Hunter region killed a litter of puppies by smashing them over the head with a rock. The only pup to survive the attack has been named 'Lucky' and is being cared for by the RSPCA. Police allege a witness saw the 25-year-old man begin to kill the puppies. He drove away from the area and found five dead pups and another two still clinging to life. He pleaded guilty to nine extra animal cruelty charges.\n",
            "- PTR2 : Police allege Nathan Thompson, 25, killed a litter of puppies by hitting them over the head with a rock. The only pup to survive the attack, nicknamed 'Lucky', is'stable' Police allege a witness saw the 25-year-old man begin to kill a number of the puppies. The man drove away from the area and found five dead pups and another two still clinging to life.\n",
            "========================================================================================================================\n",
            "[4] row_id=55   len_base=43   len_ptr=92\n",
            "- REF  : A short circuit occurred in the rover's arm as it was moving . It had been transferring rock powder into a sample analysis container . However an electrical problem then caused the rover to stop operations . Nasa engineers in California are now investigating the problem . If it can be solved the rover should begin operations again in a few days .\n",
            "- BASE : A short circuit occurred in the rover's arm as it was moving. The rover's top speed is 1.5 inches (3.8 centimetres) per second. The rover is the fourth rover to visit Mars. It took around seven minutes to land on the Red Planet.\n",
            "- PTR2 : Engineers in California are investigating the problem. The rover stopped work when an electrical problem was discovered while it was using its arm to move powder into its sample container. The severity of the problem is not yet known, but Nasa said they expected to resume operations in several days if it can be resolved. The short circuit occurred while the rover was moving its arm on 27 February. When the rover detected the anomaly, it stopped the movement of its arm. Nasa said that such a brie …\n",
            "========================================================================================================================\n",
            "[5] row_id=75   len_base=84   len_ptr=74\n",
            "- REF  : Roberto Di Matteo has conceded that it will be difficult for Schalke to sign Real Madrid's Germany international midfielder Sami Khedira . Khedira is out of contract at the end of the season and is wanted by several top clubs including Manchester United and Arsenal . Di Matteo believes that Khedira will have plenty of options to pursue . READ: Khedira confirms Real Madrid exit .\n",
            "- BASE : Schalke boss Roberto Di Matteo has admitted that the club could miss out on Real Madrid's defensive midfielder Sami Khedira due to the stiff competition they will face in the transfer market. The 27-year-old anchorman is out of contract at the Santiago Bernabeu in the summer and will be available on a free transfer. But factoring in Khedira's obvious quality and reported interest from Arsenal and Manchester United, it is not hard to see why Di Matteo feels signing him will be a challenge.\n",
            "- PTR2 : Schalke boss Roberto Di Matteo has admitted that the club could miss out on Real Madrid's defensive midfielder Sami Khedira. The 27-year-old anchorman is out of contract at the Santiago Bernabeu in the summer. but will be available on a free transfer. The former Chelsea manager told Sport1 on Friday that 'There is not a single coach in the world who would not want to have a player of his calibre in their team'\n",
            "========================================================================================================================\n",
            "[6] row_id=93   len_base=64   len_ptr=54\n",
            "- REF  : Juan Mata scored two goals as Manchester United beat Liverpool 2-1 . Second strike from Mata at Anfield on Sunday was superb scissor kick . Paolo di Canio, Gus Poyet and Peter Crouch have netted similar strikes . Mark Hughes, Luis Suarez and Dennis Bergkamp also make the list . READ: Top 50 most shocking moments in Premier League history (50-41)\n",
            "- BASE : Manchester United beat Liverpool 2-1 at Anfield on Sunday. The Spanish midfielder latched onto Angel di Maria's lofted pass, and twisted his body to volley a spectacular effort past despairing goalkeeper Simon Mignolet. We're not calling it an 'overhead kick', but his finish was sublime. The Spaniard's goal was the second of his double and secured a 2-1 victory for Louis van Gaal's side.\n",
            "- PTR2 : Manchester United beat Liverpool 2-1 at Anfield on Sunday. Juan Mata scored a spectacular effort for Manchester United. The Spaniard's goal was the second of his double and secured a 2-1 victory for Louis van Gaal's side. Paolo di Canio's scissor kick was named goal of the season in the 1999/00 Premier League campaign.\n",
            "========================================================================================================================\n",
            "[7] row_id=16   len_base=95   len_ptr=64\n",
            "- REF  : Bradford face Reading in FA Cup quarter-final replay on Monday night . Royals were beaten by Watford on Sunday, while Bantams secured draw . Phil Parkinson and Reading boss Steve Clarke unhappy with scheduling .\n",
            "- BASE : Bradford boss Phil Parkinson believes his side have already won one psychological battle ahead of Monday night's FA Cup quarter-final replay against Reading. Both sides made wholesale changes to their Saturday line-ups in order to combat the frantic fixture scheduling. It was the second-string Bantams who came out best with a gutsy 1-1 draw at Notts County. Bradford's fortunes came in contrast to their Reading rivals. Steve Clarke sent out a team entirely unrecognisable from the one that earned …\n",
            "- PTR2 : Both sides made wholesale changes to their Saturday line-ups. Both teams missed chances as Reading and Bradford drew 0-0 in the quarter-final last weekend. Bradford manager Phil Parkinson believes his side have already won one psychological battle ahead of Monday night's FA Cup quarter-final replay against Reading. Reading manager Steve Clarke is bemused by the decision to host a quarter-final replay on a Monday.\n",
            "========================================================================================================================\n",
            "[8] row_id=73   len_base=57   len_ptr=82\n",
            "- REF  : Eleven of the past 14 years have been droughts for Lake Powell reservoir . Reservoir at Arizona-Utah border is 45 per cent below 'full pool' capacity . Lake will drop even further as it gives water to Hoover Dam's Lake Mead .\n",
            "- BASE : Lake Powell, a reservoir at the Arizona-Utah border, is 45 per cent below its capacity. The lake has lost 4.4 trillion gallons of water in a recent drought. The river's basin has been experiencing the drought for eleven of the last 14 years. Seven states and 40million people get water from different parts of the river's basin.\n",
            "- PTR2 : A drought in the western United States has left water levels in the Colorado River basin far below their normal levels. Lake Powell is the country's second-largest reservoir when it is at 'full pool' capacity. The lake, from which the Colorado eventually snakes through Grand Canyon National Park, has lost 4.4 trillion gallons of water in a recent drought. The river's basin has been experiencing the drought for eleven of the last 14 years. The Colorado's basin has been one-fourth the size\n",
            "========================================================================================================================\n",
            "[9] row_id=54   len_base=57   len_ptr=63\n",
            "- REF  : Valerie Cadman-Khan was left 'humiliated' when she was arrested in 2008 . She had left Aimee, then 12, at boyfriend's house while she was at work . Detective Sergeant Colin Helyer arrived at property over unpaid tax bill . But he banged on the door of property in Middlesbrough and 'lost temper' When Mrs Cadman-Khan arrived home she was arrested and led away . Police officer claimed the child had been left in the cold for 45 minutes . His version of events was described as 'freestyle lying' in co …\n",
            "- BASE : Valerie Cadman-Khan was handcuffed in front of her daughter Aimee when she was 12. The 56-year-old was left 'humiliated' when officers led her away from her former boyfriend's house. Her mother launched a wrongful arrest suit and has won her case against the force. Her claim for £34,000 in compensation will be assessed at a later date.\n",
            "- PTR2 : Valerie Cadman-Khan was handcuffed in front of her daughter Aimee when she was 12. The 56-year-old was left 'humiliated' when officers led her away from her Middlesbrough home in front of her crying daughter Aimee. Her mother launched a wrongful arrest suit and has won her case against the force. Her claim for £34,000 in compensation will be assessed at a later date.\n",
            "========================================================================================================================\n",
            "[10] row_id=95   len_base=94   len_ptr=52\n",
            "- REF  : Spain face Ukraine in a crunch Pool C clash in Sevilla on Friday night . Sergio Ramos made his debut for Spain on March 26, 2005 . Xavi set to seal €10million per year deal with Qatari side Al-Sadd .\n",
            "- BASE : Real Madrid centre back Sergio Ramos will mark 10 years in a Spain jersey when he lines up for Vicente del Bosque's side against Ukraine on Friday night. Ramos made his first appearance for his country on March 26, 2005. He will be hoping to celebrate his 10th anniversary with a victory against his side's Group C rivals in a crucial Euro 2016 qualifier at the Estadio Ramón Sánchez Pizjuán in Sevilla. Spanish newspaper AS lead with the headline 'Ramos' Friday' as the veteran Spaniard also gets re …\n",
            "- PTR2 : Ramos will mark 10 years in a Spain jersey when he lines up for Vicente del Bosque's side against Ukraine. Ramos made his first appearance for his country on March 26, 2005. Spain face Ukraine on Friday night. Corriere dello Sport carries a story on Ciro Immobile's proposed move to AC Milan.\n"
          ]
        }
      ],
      "source": [
        "# === Show 10 side-by-side samples (REF vs BASE vs PTR exp2) ===\n",
        "import pandas as pd\n",
        "\n",
        "PATH = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000/100exp2sum.csv\"\n",
        "N_SHOW = 10          # how many to display\n",
        "RANDOM = True        # set False to take the first N rows\n",
        "\n",
        "df = pd.read_csv(PATH)\n",
        "need = {\"article\",\"highlights\",\"base_pred\",\"ptr_pred_exp2\"}\n",
        "missing = need - set(df.columns)\n",
        "assert not missing, f\"Missing columns in 100exp2sum.csv: {missing}\"\n",
        "\n",
        "# pick rows\n",
        "rows = df.sample(n=N_SHOW, random_state=0).reset_index(drop=True) if RANDOM else df.head(N_SHOW).reset_index(drop=True)\n",
        "\n",
        "def trunc(s, n=500):\n",
        "    s = str(s or \"\").strip()\n",
        "    return s if len(s) <= n else s[:n].rstrip() + \" …\"\n",
        "\n",
        "for j, row in rows.iterrows():\n",
        "    base = str(row[\"base_pred\"]).strip()\n",
        "    ptr2 = str(row[\"ptr_pred_exp2\"]).strip()\n",
        "    ref  = str(row[\"highlights\"]).strip()\n",
        "    ridx = row[\"idx\"] if \"idx\" in row else j\n",
        "\n",
        "    print(\"=\"*120)\n",
        "    print(f\"[{j+1}] row_id={ridx}   len_base={len(base.split())}   len_ptr={len(ptr2.split())}\")\n",
        "    print(\"- REF  :\", trunc(ref))\n",
        "    print(\"- BASE :\", trunc(base))\n",
        "    print(\"- PTR2 :\", trunc(ptr2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gxf89e5uHZC1",
        "outputId": "1836f0d2-6462-40c7-e8ef-9acf810af21b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== LENGTH STATS (words) — matched 100 ===\n",
            "[BASE]\n",
            " count    100.0\n",
            "mean      61.0\n",
            "std       15.3\n",
            "min       35.0\n",
            "25%       50.0\n",
            "50%       59.0\n",
            "75%       70.2\n",
            "max       99.0\n",
            "Name: base_pred, dtype: float64\n",
            "\n",
            "[PTR exp2]\n",
            " count    100.0\n",
            "mean      66.1\n",
            "std       14.8\n",
            "min       35.0\n",
            "25%       53.8\n",
            "50%       67.5\n",
            "75%       79.0\n",
            "max       92.0\n",
            "Name: ptr_pred_exp2, dtype: float64\n",
            "\n",
            "[PTR exp2 - BASE] length delta (words)\n",
            "count    100.0\n",
            "mean       5.1\n",
            "std       14.8\n",
            "min      -42.0\n",
            "25%       -2.2\n",
            "50%        4.0\n",
            "75%       15.0\n",
            "max       49.0\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# === LENGTH STATS (words) for matched 100: BASE vs PTR exp2 (PRINT ONLY) ===\n",
        "import os, pandas as pd\n",
        "\n",
        "CKPT_DIR      = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000\"\n",
        "BASE_PATH     = os.path.join(CKPT_DIR, \"iksumpointer.csv\")\n",
        "NEW100_PATH   = os.path.join(CKPT_DIR, \"100exp2sum.csv\")\n",
        "SRC_COL, REF_COL = \"article\", \"highlights\"\n",
        "\n",
        "def norm(s): return \" \".join(str(s or \"\").split())\n",
        "\n",
        "base_all = pd.read_csv(BASE_PATH)\n",
        "new100   = pd.read_csv(NEW100_PATH)\n",
        "assert \"ptr_pred_exp2\" in new100.columns, \"100exp2sum.csv must have 'ptr_pred_exp2'.\"\n",
        "\n",
        "# align to exact same rows\n",
        "if \"idx\" in new100.columns:\n",
        "    base_100 = base_all.iloc[new100[\"idx\"].tolist()].copy()\n",
        "else:\n",
        "    base_100 = base_all.head(len(new100)).copy()\n",
        "    assert (base_100[SRC_COL].astype(str).values == new100[SRC_COL].astype(str).values).all(), \\\n",
        "        \"Sources not aligned and no 'idx' column.\"\n",
        "\n",
        "# normalize and compute lengths\n",
        "base_len = base_100[\"base_pred\"].map(norm).str.split().map(len)\n",
        "ptr2_len = new100[\"ptr_pred_exp2\"].map(norm).str.split().map(len)\n",
        "\n",
        "print(\"=== LENGTH STATS (words) — matched 100 ===\")\n",
        "print(\"[BASE]\\n\", base_len.describe().round(1))\n",
        "print(\"\\n[PTR exp2]\\n\", ptr2_len.describe().round(1))\n",
        "\n",
        "# quick deltas\n",
        "delta = (ptr2_len - base_len)\n",
        "print(\"\\n[PTR exp2 - BASE] length delta (words)\")\n",
        "print(delta.describe().round(1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3d1zntPHJjp",
        "outputId": "9c3cc690-2108-41b9-92ca-bf2aa7a8e086"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== [BASE] ENTITY (overall) ===\n",
            "TP=265  FP=470  FN=465\n",
            "Precision: 0.360544\n",
            "Recall:    0.363014\n",
            "F1:        0.361775\n",
            "\n",
            "=== [BASE] ENTITY (by type) ===\n",
            "PERSON       tp= 85 fp= 83 fn=102  P=0.506 R=0.455 F1=0.479\n",
            "DATE         tp= 36 fp= 98 fn= 97  P=0.269 R=0.271 F1=0.270\n",
            "GPE          tp= 46 fp= 63 fn= 71  P=0.422 R=0.393 F1=0.407\n",
            "ORG          tp= 38 fp= 77 fn= 72  P=0.330 R=0.345 F1=0.338\n",
            "CARDINAL     tp= 25 fp= 54 fn= 48  P=0.316 R=0.342 F1=0.329\n",
            "NORP         tp=  8 fp= 20 fn= 15  P=0.286 R=0.348 F1=0.314\n",
            "LOC          tp=  6 fp=  9 fn=  9  P=0.400 R=0.400 F1=0.400\n",
            "MONEY        tp=  6 fp= 13 fn=  9  P=0.316 R=0.400 F1=0.353\n",
            "ORDINAL      tp=  4 fp= 17 fn=  7  P=0.190 R=0.364 F1=0.250\n",
            "TIME         tp=  2 fp= 10 fn=  8  P=0.167 R=0.200 F1=0.182\n",
            "EVENT        tp=  4 fp=  2 fn=  5  P=0.667 R=0.444 F1=0.533\n",
            "FAC          tp=  3 fp= 12 fn=  4  P=0.200 R=0.429 F1=0.273\n",
            "QUANTITY     tp=  0 fp=  6 fn=  7  P=0.000 R=0.000 F1=0.000\n",
            "PERCENT      tp=  0 fp=  0 fn=  3  P=0.000 R=0.000 F1=0.000\n",
            "PRODUCT      tp=  1 fp=  2 fn=  2  P=0.333 R=0.333 F1=0.333\n",
            "WORK_OF_ART  tp=  1 fp=  2 fn=  2  P=0.333 R=0.333 F1=0.333\n",
            "LANGUAGE     tp=  0 fp=  0 fn=  2  P=0.000 R=0.000 F1=0.000\n",
            "LAW          tp=  0 fp=  2 fn=  2  P=0.000 R=0.000 F1=0.000\n",
            "\n",
            "=== [PTR exp2] ENTITY (overall) ===\n",
            "TP=272  FP=529  FN=458\n",
            "Precision: 0.339576\n",
            "Recall:    0.372603\n",
            "F1:        0.355323\n",
            "\n",
            "=== [PTR exp2] ENTITY (by type) ===\n",
            "PERSON       tp= 83 fp= 89 fn=104  P=0.483 R=0.444 F1=0.462\n",
            "DATE         tp= 42 fp=127 fn= 91  P=0.249 R=0.316 F1=0.278\n",
            "GPE          tp= 50 fp= 72 fn= 67  P=0.410 R=0.427 F1=0.418\n",
            "ORG          tp= 39 fp= 92 fn= 71  P=0.298 R=0.355 F1=0.324\n",
            "CARDINAL     tp= 23 fp= 54 fn= 50  P=0.299 R=0.315 F1=0.307\n",
            "NORP         tp=  7 fp= 16 fn= 16  P=0.304 R=0.304 F1=0.304\n",
            "LOC          tp=  3 fp=  9 fn= 12  P=0.250 R=0.200 F1=0.222\n",
            "MONEY        tp=  6 fp= 16 fn=  9  P=0.273 R=0.400 F1=0.324\n",
            "ORDINAL      tp=  3 fp= 12 fn=  8  P=0.200 R=0.273 F1=0.231\n",
            "TIME         tp=  1 fp= 11 fn=  9  P=0.083 R=0.100 F1=0.091\n",
            "EVENT        tp=  5 fp=  7 fn=  4  P=0.417 R=0.556 F1=0.476\n",
            "FAC          tp=  5 fp=  9 fn=  2  P=0.357 R=0.714 F1=0.476\n",
            "QUANTITY     tp=  2 fp=  7 fn=  5  P=0.222 R=0.286 F1=0.250\n",
            "PERCENT      tp=  0 fp=  1 fn=  3  P=0.000 R=0.000 F1=0.000\n",
            "PRODUCT      tp=  2 fp=  2 fn=  1  P=0.500 R=0.667 F1=0.571\n",
            "WORK_OF_ART  tp=  1 fp=  4 fn=  2  P=0.200 R=0.333 F1=0.250\n",
            "LANGUAGE     tp=  0 fp=  0 fn=  2  P=0.000 R=0.000 F1=0.000\n",
            "LAW          tp=  0 fp=  1 fn=  2  P=0.000 R=0.000 F1=0.000\n",
            "\n",
            "=== [PTR exp2 - BASE] ENTITY (overall delta) ===\n",
            "ΔPrecision=-0.020969  ΔRecall=+0.009589  ΔF1=-0.006451\n"
          ]
        }
      ],
      "source": [
        "# === ENTITIES for matched 100 rows: BASE vs PTR-exp2 (PRINT ONLY) ===\n",
        "!pip -q install spacy==3.7.5\n",
        "import os, pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "CKPT_DIR   = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000\"\n",
        "BASE_PATH  = os.path.join(CKPT_DIR, \"iksumpointer.csv\")   # has base + old ptr, and the references\n",
        "NEW100_PATH= os.path.join(CKPT_DIR, \"100exp2sum.csv\")     # new 100 pointer summaries\n",
        "SRC_COL, REF_COL = \"article\", \"highlights\"\n",
        "\n",
        "def norm(s):\n",
        "    s = \" \".join(str(s or \"\").split())\n",
        "    return s\n",
        "\n",
        "# --- load & align (prefer idx to guarantee exact same rows) ---\n",
        "base_all = pd.read_csv(BASE_PATH)\n",
        "new100   = pd.read_csv(NEW100_PATH)\n",
        "\n",
        "assert \"ptr_pred_exp2\" in new100.columns, \"100exp2sum.csv must have column 'ptr_pred_exp2'.\"\n",
        "\n",
        "if \"idx\" in new100.columns:\n",
        "    base_100 = base_all.iloc[new100[\"idx\"].tolist()].copy()\n",
        "else:\n",
        "    base_100 = base_all.head(len(new100)).copy()\n",
        "    # sanity if no idx: require exact article alignment\n",
        "    assert (base_100[SRC_COL].astype(str).values == new100[SRC_COL].astype(str).values).all(), \\\n",
        "        \"Sources not aligned and no `idx` column found.\"\n",
        "\n",
        "# normalize text\n",
        "for c in [SRC_COL, REF_COL, \"base_pred\"]:\n",
        "    base_100[c] = base_100[c].map(norm)\n",
        "new100[\"ptr_pred_exp2\"] = new100[\"ptr_pred_exp2\"].map(norm)\n",
        "\n",
        "# --- spaCy NER ---\n",
        "import spacy\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "except OSError:\n",
        "    from spacy.cli import download\n",
        "    download(\"en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "\n",
        "def ents_of(text):\n",
        "    d = nlp(text)\n",
        "    # normalize surface form by lowercasing & stripping\n",
        "    return {(e.label_, e.text.strip().lower()) for e in d.ents if e.text.strip()}\n",
        "\n",
        "def prf_counts(preds, refs):\n",
        "    # returns overall TP, FP, FN and per-type dict of counts\n",
        "    TP=FP=FN=0\n",
        "    per = {}  # label -> dict(tp, fp, fn)\n",
        "    for p, r in zip(preds, refs):\n",
        "        P, R = ents_of(p), ents_of(r)\n",
        "        TP_i = len(P & R); FP_i = len(P - R); FN_i = len(R - P)\n",
        "        TP += TP_i; FP += FP_i; FN += FN_i\n",
        "        # accumulate by type\n",
        "        for lab,_ in (P & R):\n",
        "            per.setdefault(lab, {\"tp\":0,\"fp\":0,\"fn\":0}); per[lab][\"tp\"] += 1\n",
        "        for lab,_ in (P - R):\n",
        "            per.setdefault(lab, {\"tp\":0,\"fp\":0,\"fn\":0}); per[lab][\"fp\"] += 1\n",
        "        for lab,_ in (R - P):\n",
        "            per.setdefault(lab, {\"tp\":0,\"fp\":0,\"fn\":0}); per[lab][\"fn\"] += 1\n",
        "    def prf(tp,fp,fn):\n",
        "        P = tp/(tp+fp) if (tp+fp)>0 else 0.0\n",
        "        R = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
        "        F = 2*P*R/(P+R) if (P+R)>0 else 0.0\n",
        "        return P,R,F\n",
        "    P,R,F = prf(TP,FP,FN)\n",
        "    # per-type table sorted by reference support (tp+fn desc), then name\n",
        "    rows=[]\n",
        "    for lab, c in per.items():\n",
        "        p,r,f = prf(c[\"tp\"], c[\"fp\"], c[\"fn\"])\n",
        "        rows.append((lab, c[\"tp\"], c[\"fp\"], c[\"fn\"], p, r, f, c[\"tp\"]+c[\"fn\"]))\n",
        "    rows.sort(key=lambda x:(-x[7], x[0]))\n",
        "    return {\"overall\": (TP,FP,FN,P,R,F), \"per_type\": rows}\n",
        "\n",
        "# compute\n",
        "refs = base_100[REF_COL].tolist()\n",
        "base_preds = base_100[\"base_pred\"].tolist()\n",
        "ptr_preds  = new100[\"ptr_pred_exp2\"].tolist()\n",
        "\n",
        "B = prf_counts(base_preds, refs)\n",
        "P2= prf_counts(ptr_preds,  refs)\n",
        "\n",
        "# --- print nicely ---\n",
        "def fmt_overall(name, O):\n",
        "    TP,FP,FN,P,R,F = O\n",
        "    print(f\"=== [{name}] ENTITY (overall) ===\")\n",
        "    print(f\"TP={TP}  FP={FP}  FN={FN}\")\n",
        "    print(f\"Precision: {P:.6f}\\nRecall:    {R:.6f}\\nF1:        {F:.6f}\\n\")\n",
        "\n",
        "def fmt_table(name, rows):\n",
        "    print(f\"=== [{name}] ENTITY (by type) ===\")\n",
        "    for lab,tp,fp,fn,p,r,f,_support in rows:\n",
        "        print(f\"{lab:<12s} tp={tp:3d} fp={fp:3d} fn={fn:3d}  P={p:.3f} R={r:.3f} F1={f:.3f}\")\n",
        "    print()\n",
        "\n",
        "fmt_overall(\"BASE\", B[\"overall\"])\n",
        "fmt_table(\"BASE\", B[\"per_type\"])\n",
        "\n",
        "fmt_overall(\"PTR exp2\", P2[\"overall\"])\n",
        "fmt_table(\"PTR exp2\", P2[\"per_type\"])\n",
        "\n",
        "# quick delta summary overall\n",
        "_,_,_, Pb,Rb,Fb = B[\"overall\"]\n",
        "_,_,_, Pp,Rp,Fp = P2[\"overall\"]\n",
        "print(\"=== [PTR exp2 - BASE] ENTITY (overall delta) ===\")\n",
        "print(f\"ΔPrecision={Pp-Pb:+.6f}  ΔRecall={Rp-Rb:+.6f}  ΔF1={Fp-Fb:+.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337,
          "referenced_widgets": [
            "1463742a79a94da4aa978f45d7b4255a"
          ]
        },
        "id": "Zt7fMObXIbzk",
        "outputId": "9f35585c-b894-44d6-b122-4608a9a73592"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1463742a79a94da4aa978f45d7b4255a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "PTR-cov+rerank (100):   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== LENGTH (words) ===\n",
            "[BASE] mean= 61.02 median= 59.0\n",
            "[PTR*] mean= 71.56 median= 72.0\n",
            "[Δ] mean= 10.54\n",
            "\n",
            "=== ROUGE (mean over 100) ===\n",
            "[BASE] {'r1_p': 0.4092, 'r1_r': 0.4721, 'r1_f': 0.4227, 'r2_p': 0.1958, 'r2_r': 0.2283, 'r2_f': 0.2035, 'rl_p': 0.2873, 'rl_r': 0.3334, 'rl_f': 0.2978}\n",
            "[PTR*] {'r1_p': 0.3697, 'r1_r': 0.5012, 'r1_f': 0.4108, 'r2_p': 0.1765, 'r2_r': 0.2408, 'r2_f': 0.197, 'rl_p': 0.2538, 'rl_r': 0.3501, 'rl_f': 0.2837}\n",
            "[PTR* - BASE] {'r1_p': -0.0395, 'r1_r': 0.0291, 'r1_f': -0.0119, 'r2_p': -0.0193, 'r2_r': 0.0125, 'r2_f': -0.0065, 'rl_p': -0.0335, 'rl_r': 0.0167, 'rl_f': -0.0141}\n",
            "\n",
            "=== ENTITIES (overall, micro) ===\n",
            "[BASE] {'precision': 0.3605, 'recall': 0.363, 'f1': 0.3618}\n",
            "[PTR*] {'precision': 0.3429, 'recall': 0.4247, 'f1': 0.3794}\n"
          ]
        }
      ],
      "source": [
        "# === Pointer decode with coverage penalty + entity-aware rerank (first 100 of iksumpointer.csv) ===\n",
        "!pip -q install rouge-score==0.1.2 spacy==3.7.5\n",
        "import os, gc, re, math, pandas as pd, torch, torch.nn as nn, torch.nn.functional as F\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers.modeling_outputs import BaseModelOutput\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# ---------- paths ----------\n",
        "CKPT_DIR = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000\"\n",
        "BASE_CSV = os.path.join(CKPT_DIR, \"iksumpointer.csv\")   # has base + refs + sources\n",
        "SRC_COL, REF_COL = \"article\", \"highlights\"\n",
        "N = 100\n",
        "\n",
        "# ---------- knobs ----------\n",
        "NUM_BEAMS         = 5\n",
        "MIN_NEW           = 34\n",
        "MAX_NEW           = 100\n",
        "NO_REPEAT         = 5\n",
        "LENGTH_PENALTY    = 2.2\n",
        "MAX_SRC_LEN       = 400\n",
        "GATE_BIAS         = +0.30\n",
        "\n",
        "COV_ALPHA         = 0.10   # coverage penalty weight (decode-time)\n",
        "RERANK_K          = 5      # how many finished beams to rerank (up to NUM_BEAMS)\n",
        "PEN_UNSUP_ENT     = 0.60   # penalty per unsupported named entity\n",
        "PEN_UNSUP_NUM     = 0.40   # penalty per unsupported number/percent\n",
        "BONUS_SUP_PERSONGPE = 0.05 # tiny bonus for supported PERSON/GPE covered\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device == \"cuda\":\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "USE_FP16 = (device == \"cuda\")\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "tok  = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
        "base = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR, attn_implementation=\"eager\").to(device).eval()\n",
        "\n",
        "# ---------- pointer wrapper (same as your working one) ----------\n",
        "class CopyAwareBart(nn.Module):\n",
        "    def __init__(self, base_model, tokenizer, eps=1e-8, gate_bias=0.0):\n",
        "        super().__init__()\n",
        "        self.base = base_model\n",
        "        self.tok  = tokenizer\n",
        "        self.eps  = float(eps)\n",
        "        H = self.base.config.d_model\n",
        "        self.p_gen_linear = nn.Linear(3*H, 1)\n",
        "        self.gate_bias = float(gate_bias)\n",
        "        self._last_p_copy_mean = self._last_p_gen_mean = None\n",
        "\n",
        "    def _mix_pointer(self, out, input_ids, attention_mask, decoder_input_ids):\n",
        "        logits = out.logits\n",
        "        ca = out.cross_attentions[-1].mean(1)          # [B,T,S]\n",
        "        if attention_mask is not None:\n",
        "            ca = ca.masked_fill(attention_mask[:,None,:]==0, 0.0)\n",
        "        attn = ca / ca.sum(-1, keepdim=True).clamp_min(self.eps)\n",
        "\n",
        "        dec_hid = out.decoder_hidden_states[-1]\n",
        "        ctx     = torch.bmm(attn, out.encoder_last_hidden_state)\n",
        "        dec_inp = self.base.get_input_embeddings()(decoder_input_ids)\n",
        "        gate_in = torch.cat([dec_hid, ctx, dec_inp], dim=-1)\n",
        "        p_gen   = torch.sigmoid(self.p_gen_linear(gate_in) + self.gate_bias).squeeze(-1)\n",
        "        p_copy  = 1.0 - p_gen\n",
        "\n",
        "        copy_probs = torch.zeros_like(logits)\n",
        "        idx = input_ids.unsqueeze(1).expand(input_ids.size(0), logits.size(1), input_ids.size(1))\n",
        "        copy_probs.scatter_add_(2, idx, attn)\n",
        "\n",
        "        vocab_logp = F.log_softmax(logits, dim=-1)\n",
        "        logp_final = torch.logaddexp(\n",
        "            vocab_logp + (p_gen + 1e-8).log().unsqueeze(-1),\n",
        "            (copy_probs + 1e-8).log() + (p_copy + 1e-8).log().unsqueeze(-1)\n",
        "        )\n",
        "        self._last_p_copy_mean = float(p_copy.detach().mean().cpu())\n",
        "        self._last_p_gen_mean  = float(p_gen.detach().mean().cpu())\n",
        "        return logp_final, attn\n",
        "\n",
        "model_ptr = CopyAwareBart(base, tok, gate_bias=GATE_BIAS).to(device).eval()\n",
        "\n",
        "# load pointer head if present\n",
        "ph_path = os.path.join(CKPT_DIR, \"pointer_head.pt\")\n",
        "if os.path.exists(ph_path):\n",
        "    sd = torch.load(ph_path, map_location=\"cpu\")\n",
        "    if isinstance(sd, dict) and \"p_gen_linear\" in sd:\n",
        "        model_ptr.p_gen_linear.load_state_dict(sd[\"p_gen_linear\"])\n",
        "    elif isinstance(sd, dict) and all(k in {\"weight\",\"bias\"} for k in sd):\n",
        "        model_ptr.p_gen_linear.load_state_dict(sd)\n",
        "\n",
        "def _ban_repeating_ngrams(logits: torch.Tensor, seq: torch.Tensor, ngram: int):\n",
        "    if ngram <= 0: return\n",
        "    toks = seq[0].tolist() if seq.dim()==2 else seq.tolist()\n",
        "    if len(toks) < ngram-1: return\n",
        "    prefix = tuple(toks[-(ngram - 1):])\n",
        "    banned = set()\n",
        "    for i in range(len(toks) - ngram + 1):\n",
        "        if tuple(toks[i:i+ngram-1]) == prefix:\n",
        "            banned.add(toks[i+ngram-1])\n",
        "    for t in banned: logits[t] = -1e9\n",
        "\n",
        "# ---------- entity utilities for reranking ----------\n",
        "import spacy, numpy as np, re\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "except OSError:\n",
        "    from spacy.cli import download\n",
        "    download(\"en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "\n",
        "def ents_set(text):\n",
        "    d = nlp(text)\n",
        "    return {(e.label_, e.text.strip().lower()) for e in d.ents if e.text.strip()}\n",
        "\n",
        "num_re = re.compile(r\"(\\d[\\d,\\.]*%?)\")\n",
        "\n",
        "def rerank_score(hyp, base_norm_score, src_ents, src_text):\n",
        "    hyp_ents = ents_set(hyp)\n",
        "    unsup = len(hyp_ents - src_ents)\n",
        "    sup   = len(hyp_ents & src_ents)\n",
        "\n",
        "    # numbers unsupported\n",
        "    src_nums = set(num_re.findall(src_text))\n",
        "    hyp_nums = set(num_re.findall(hyp))\n",
        "    unsup_num = len([n for n in hyp_nums if n not in src_nums])\n",
        "\n",
        "    # small positive for PERSON/GPE that are supported\n",
        "    sup_person_gpe = sum(1 for (lab,_) in (hyp_ents & src_ents) if lab in {\"PERSON\",\"GPE\"})\n",
        "\n",
        "    return (base_norm_score\n",
        "            - PEN_UNSUP_ENT*unsup\n",
        "            - PEN_UNSUP_NUM*unsup_num\n",
        "            + BONUS_SUP_PERSONGPE*sup_person_gpe)\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_pointer_one_with_cov_rerank(text: str) -> str:\n",
        "    # pre-encode once\n",
        "    enc = tok([text], return_tensors=\"pt\", truncation=True, max_length=MAX_SRC_LEN).to(device)\n",
        "    src_ids, src_mask = enc.input_ids, enc.attention_mask\n",
        "    with torch.amp.autocast('cuda', dtype=torch.float16, enabled=USE_FP16):\n",
        "        enc_out = base.model.get_encoder()(input_ids=src_ids, attention_mask=src_mask, return_dict=True)\n",
        "\n",
        "    start_id = getattr(base.config, \"decoder_start_token_id\", None) or (tok.bos_token_id or tok.eos_token_id)\n",
        "    beams = [(torch.tensor([[start_id]], device=device, dtype=torch.long),  # seq\n",
        "              0.0,   # norm score (raw_sum / lp)\n",
        "              0.0,   # raw logprob sum (for accurate len-norm updates)\n",
        "              False, # done\n",
        "              torch.zeros(src_ids.size(1), device=device, dtype=enc_out.last_hidden_state.dtype))]  # coverage [S]\n",
        "\n",
        "    finished = []  # (seq, norm_score)\n",
        "\n",
        "    for step in range(MAX_NEW):\n",
        "        act = [(i, b) for i, b in enumerate(beams) if not b[3]]\n",
        "        if not act: break\n",
        "\n",
        "        seqs = torch.cat([beams[i][0] for i,_ in act], dim=0)  # [K, L]\n",
        "        K = seqs.size(0)\n",
        "        enc_exp = BaseModelOutput(last_hidden_state=enc_out.last_hidden_state.expand(K, -1, -1))\n",
        "        src_mask_exp = src_mask.expand(K, -1)\n",
        "\n",
        "        with torch.amp.autocast('cuda', dtype=torch.float16, enabled=USE_FP16):\n",
        "            out = base(\n",
        "                encoder_outputs=enc_exp,\n",
        "                attention_mask=src_mask_exp,\n",
        "                decoder_input_ids=seqs,\n",
        "                output_attentions=True, output_hidden_states=True,\n",
        "                use_cache=False, return_dict=True,\n",
        "            )\n",
        "            logp_final, attn = model_ptr._mix_pointer(out, src_ids.expand(K, -1), src_mask_exp, seqs)\n",
        "        step_logits_all = logp_final[:, -1, :]  # log-probs\n",
        "        attn_t_all      = attn[:, -1, :]        # [K, S] attention at current step\n",
        "\n",
        "        new_beams = []\n",
        "        for j, (beam_idx, _) in enumerate(act):\n",
        "            step_logits = step_logits_all[j].clone().float()\n",
        "            if step + 1 < MIN_NEW and tok.eos_token_id is not None:\n",
        "                step_logits[tok.eos_token_id] = -1e9\n",
        "            _ban_repeating_ngrams(step_logits, beams[beam_idx][0], NO_REPEAT)\n",
        "\n",
        "            topk_logp, topk_ids = torch.topk(step_logits, k=NUM_BEAMS)\n",
        "            prev_seq, prev_norm, prev_raw, prev_done, prev_cov = beams[beam_idx]\n",
        "            attn_t = attn_t_all[j]\n",
        "\n",
        "            # coverage penalty for this extension\n",
        "            cov_pen = COV_ALPHA * torch.min(attn_t, prev_cov).sum().item()\n",
        "            for k in range(NUM_BEAMS):\n",
        "                tid  = topk_ids[k].view(1,1)\n",
        "                nseq = torch.cat([prev_seq, tid], dim=1)\n",
        "                nfin = (tid.item() == tok.eos_token_id)\n",
        "                raw  = prev_raw + float(topk_logp[k].item()) - cov_pen\n",
        "                lp   = ((5.0 + nseq.size(1))**LENGTH_PENALTY) / ((5.0 + 1.0)**LENGTH_PENALTY)\n",
        "                nsc  = raw / lp\n",
        "                ncov = prev_cov + attn_t\n",
        "                new_beams.append((nseq, nsc, raw, nfin, ncov))\n",
        "\n",
        "        # keep best beams\n",
        "        new_beams.sort(key=lambda x: x[1], reverse=True)\n",
        "        beams = new_beams[:NUM_BEAMS]\n",
        "\n",
        "        # collect finished for reranking\n",
        "        for seq, nsc, raw, done, cov in beams:\n",
        "            if done:\n",
        "                finished.append((seq, nsc))\n",
        "        finished = sorted(finished, key=lambda x: x[1], reverse=True)[:RERANK_K]\n",
        "\n",
        "        if all(b[3] for b in beams): break\n",
        "        if (step & 7) == 0:\n",
        "            del out, logp_final, attn, step_logits_all, attn_t_all\n",
        "            torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "    # if nothing finished, take the best ongoing\n",
        "    if not finished:\n",
        "        finished = [(max(beams, key=lambda x: x[1])[0], max(beams, key=lambda x: x[1])[1])]\n",
        "\n",
        "    # entity-aware rerank among finished\n",
        "    src_txt  = tok.batch_decode(src_ids, skip_special_tokens=True)[0]\n",
        "    src_ents = ents_set(src_txt)\n",
        "    cands = []\n",
        "    for seq, nsc in finished:\n",
        "        hyp = tok.batch_decode(seq.detach().cpu(), skip_special_tokens=True)[0]\n",
        "        s = rerank_score(hyp, nsc, src_ents, src_txt)\n",
        "        cands.append((s, hyp))\n",
        "    cands.sort(key=lambda x: x[0], reverse=True)\n",
        "    return cands[0][1]\n",
        "\n",
        "# ---------- load data & run ----------\n",
        "df_all = pd.read_csv(BASE_CSV).head(N).copy()\n",
        "for c in [SRC_COL, REF_COL, \"base_pred\"]:\n",
        "    df_all[c] = df_all[c].fillna(\"\").astype(str).str.replace(r\"\\s+\",\" \",regex=True).str.strip()\n",
        "\n",
        "ptr_new = []\n",
        "for art in tqdm(df_all[SRC_COL].tolist(), desc=\"PTR-cov+rerank (100)\"):\n",
        "    ptr_new.append(generate_pointer_one_with_cov_rerank(art))\n",
        "\n",
        "# ---------- evaluate (PRINT ONLY) ----------\n",
        "def norm(s): return \" \".join(str(s or \"\").split())\n",
        "refs = df_all[REF_COL].map(norm).tolist()\n",
        "base_preds = df_all[\"base_pred\"].map(norm).tolist()\n",
        "ptr_preds  = list(map(norm, ptr_new))\n",
        "\n",
        "# Length\n",
        "import numpy as np\n",
        "bl = [len(x.split()) for x in base_preds]\n",
        "pl = [len(x.split()) for x in ptr_preds]\n",
        "print(\"=== LENGTH (words) ===\")\n",
        "print(\"[BASE] mean=\", np.mean(bl), \"median=\", np.median(bl))\n",
        "print(\"[PTR*] mean=\", np.mean(pl), \"median=\", np.median(pl))\n",
        "print(\"[Δ] mean=\", float(np.mean(np.array(pl)-np.array(bl))))\n",
        "\n",
        "# ROUGE\n",
        "scorer = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\"], use_stemmer=True)\n",
        "def rouge_mean(preds, refs):\n",
        "    rows=[]\n",
        "    for p,r in zip(preds, refs):\n",
        "        sc = scorer.score(r, p)\n",
        "        rows.append({\n",
        "            \"r1_p\": sc[\"rouge1\"].precision, \"r1_r\": sc[\"rouge1\"].recall, \"r1_f\": sc[\"rouge1\"].fmeasure,\n",
        "            \"r2_p\": sc[\"rouge2\"].precision, \"r2_r\": sc[\"rouge2\"].recall, \"r2_f\": sc[\"rouge2\"].fmeasure,\n",
        "            \"rl_p\": sc[\"rougeL\"].precision, \"rl_r\": sc[\"rougeL\"].recall, \"rl_f\": sc[\"rougeL\"].fmeasure,\n",
        "        })\n",
        "    R = pd.DataFrame(rows).mean(numeric_only=True)\n",
        "    return {k: float(f\"{v:.4f}\") for k,v in R.items()}\n",
        "\n",
        "rb = rouge_mean(base_preds, refs)\n",
        "rp = rouge_mean(ptr_preds,  refs)\n",
        "diff = {k: float(f\"{rp[k]-rb[k]:.4f}\") for k in rb}\n",
        "print(\"\\n=== ROUGE (mean over 100) ===\")\n",
        "print(\"[BASE]\", rb)\n",
        "print(\"[PTR*]\", rp)\n",
        "print(\"[PTR* - BASE]\", diff)\n",
        "\n",
        "# Entities (overall micro)\n",
        "def ents(text):\n",
        "    d = nlp(text);\n",
        "    return {(e.label_, e.text.strip().lower()) for e in d.ents if e.text.strip()}\n",
        "def prf(preds, refs):\n",
        "    TP=FP=FN=0\n",
        "    for p,r in zip(preds, refs):\n",
        "        P, R = ents(p), ents(r)\n",
        "        TP += len(P & R); FP += len(P - R); FN += len(R - P)\n",
        "    P = TP/(TP+FP) if (TP+FP) else 0.0\n",
        "    R = TP/(TP+FN) if (TP+FN) else 0.0\n",
        "    F = 2*P*R/(P+R) if (P+R) else 0.0\n",
        "    return {\"precision\":float(f\"{P:.4f}\"), \"recall\":float(f\"{R:.4f}\"), \"f1\":float(f\"{F:.4f}\")}\n",
        "print(\"\\n=== ENTITIES (overall, micro) ===\")\n",
        "print(\"[BASE]\", prf(base_preds, refs))\n",
        "print(\"[PTR*]\", prf(ptr_preds,  refs))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viqrML50mL-p"
      },
      "source": [
        "# Tunning decoding exp 2 (seq copy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372,
          "referenced_widgets": [
            "bcd18de43a714e4bb5fc80788bda38cb"
          ]
        },
        "id": "_tEzy2zPq6n5",
        "outputId": "4f642c26-711f-4a53-f169-0747f2c2c697"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "/usr/local/lib/python3.12/dist-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bcd18de43a714e4bb5fc80788bda38cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "PTR-cov+priors (100):   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== LENGTH (words) ===\n",
            "[BASE] mean= 61.02 median= 59.0\n",
            "[PTR*] mean= 63.99 median= 66.0\n",
            "[Δ] mean= 2.97\n",
            "\n",
            "=== ROUGE (mean over 100) ===\n",
            "[BASE] {'r1_p': 0.4092, 'r1_r': 0.4721, 'r1_f': 0.4227, 'r2_p': 0.1958, 'r2_r': 0.2283, 'r2_f': 0.2035, 'rl_p': 0.2873, 'rl_r': 0.3334, 'rl_f': 0.2978}\n",
            "[PTR*] {'r1_p': 0.392, 'r1_r': 0.4707, 'r1_f': 0.4103, 'r2_p': 0.1833, 'r2_r': 0.2159, 'r2_f': 0.1902, 'rl_p': 0.2682, 'rl_r': 0.3238, 'rl_f': 0.2808}\n",
            "[PTR* - BASE] {'r1_p': -0.0172, 'r1_r': -0.0014, 'r1_f': -0.0124, 'r2_p': -0.0125, 'r2_r': -0.0124, 'r2_f': -0.0133, 'rl_p': -0.0191, 'rl_r': -0.0096, 'rl_f': -0.017}\n",
            "\n",
            "=== ENTITIES (overall, micro) ===\n",
            "[BASE] {'precision': 0.3605, 'recall': 0.363, 'f1': 0.3618}\n",
            "[PTR*] {'precision': 0.3373, 'recall': 0.4219, 'f1': 0.3749}\n"
          ]
        }
      ],
      "source": [
        "# === Pointer decode with coverage + entity/number priors + CopyNext (100 only) ===\n",
        "!pip -q install rouge-score==0.1.2 spacy==3.7.5\n",
        "import os, gc, re, math, pandas as pd, numpy as np, torch, torch.nn as nn, torch.nn.functional as F\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers.modeling_outputs import BaseModelOutput\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# ---------- paths ----------\n",
        "CKPT_DIR = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000\"\n",
        "BASE_CSV = os.path.join(CKPT_DIR, \"iksumpointer.csv\")   # has refs, sources, and base_pred\n",
        "SRC_COL, REF_COL = \"article\", \"highlights\"\n",
        "N = 100  # only regenerate pointer for first 100\n",
        "\n",
        "# ---------- decode knobs (precision-oriented) ----------\n",
        "NUM_BEAMS          = 5\n",
        "MIN_NEW            = 34\n",
        "MAX_NEW            = 100\n",
        "NO_REPEAT          = 4\n",
        "LENGTH_PENALTY     = 2.2\n",
        "MAX_SRC_LEN        = 400\n",
        "GATE_BIAS          = +0.30\n",
        "\n",
        "# coverage (decode-time) + rerank\n",
        "COV_ALPHA          = 0.10\n",
        "RERANK_K           = 5\n",
        "PEN_UNSUP_ENT      = 0.60   # penalty per unsupported NER\n",
        "PEN_UNSUP_NUM      = 0.40   # penalty per unsupported number/percent\n",
        "BONUS_SUP_PERSONGPE= 0.05   # small bonus for supported PERSON/GPE\n",
        "\n",
        "# NEW: copy priors (logit bonuses in log-prob space)\n",
        "COPYNEXT_BONUS     = 1.20   # span-continuation bump toward src[i+1]\n",
        "ENTCOPY_BONUS      = 0.85   # boost for tokens aligned to NE positions\n",
        "NUMCOPY_BONUS      = 0.95   # boost for tokens aligned to numeric positions\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device == \"cuda\":\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "USE_FP16 = (device == \"cuda\")\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "tok  = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
        "base = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR, attn_implementation=\"eager\").to(device).eval()\n",
        "\n",
        "# ---------- pointer wrapper (as before) ----------\n",
        "class CopyAwareBart(nn.Module):\n",
        "    def __init__(self, base_model, tokenizer, eps=1e-8, gate_bias=0.0):\n",
        "        super().__init__()\n",
        "        self.base = base_model\n",
        "        self.tok  = tokenizer\n",
        "        self.eps  = float(eps)\n",
        "        H = self.base.config.d_model\n",
        "        self.p_gen_linear = nn.Linear(3*H, 1)\n",
        "        self.gate_bias = float(gate_bias)\n",
        "        self._last_p_copy_mean = self._last_p_gen_mean = None\n",
        "\n",
        "    def _mix_pointer(self, out, input_ids, attention_mask, decoder_input_ids):\n",
        "        logits = out.logits                                  # [B,T,V]\n",
        "        ca = out.cross_attentions[-1].mean(1)               # [B,T,S]\n",
        "        if attention_mask is not None:\n",
        "            ca = ca.masked_fill(attention_mask[:,None,:]==0, 0.0)\n",
        "        attn = ca / ca.sum(-1, keepdim=True).clamp_min(self.eps)\n",
        "\n",
        "        dec_hid = out.decoder_hidden_states[-1]             # [B,T,H]\n",
        "        ctx     = torch.bmm(attn, out.encoder_last_hidden_state)  # [B,T,H]\n",
        "        dec_inp = self.base.get_input_embeddings()(decoder_input_ids)  # [B,T,H]\n",
        "        gate_in = torch.cat([dec_hid, ctx, dec_inp], dim=-1)          # [B,T,3H]\n",
        "        p_gen   = torch.sigmoid(self.p_gen_linear(gate_in) + self.gate_bias).squeeze(-1)  # [B,T]\n",
        "        p_copy  = 1.0 - p_gen\n",
        "\n",
        "        # pointer distribution over vocab via scatter_add on source ids\n",
        "        copy_probs = torch.zeros_like(logits)\n",
        "        idx = input_ids.unsqueeze(1).expand(input_ids.size(0), logits.size(1), input_ids.size(1))\n",
        "        copy_probs.scatter_add_(2, idx, attn)\n",
        "\n",
        "        vocab_logp = F.log_softmax(logits, dim=-1)\n",
        "        logp_final = torch.logaddexp(\n",
        "            vocab_logp + (p_gen + 1e-8).log().unsqueeze(-1),\n",
        "            (copy_probs + 1e-8).log() + (p_copy + 1e-8).log().unsqueeze(-1)\n",
        "        )\n",
        "        self._last_p_copy_mean = float(p_copy.detach().mean().cpu())\n",
        "        self._last_p_gen_mean  = float(p_gen.detach().mean().cpu())\n",
        "        return logp_final, attn\n",
        "\n",
        "model_ptr = CopyAwareBart(base, tok, gate_bias=GATE_BIAS).to(device).eval()\n",
        "\n",
        "# load pointer head if present\n",
        "ph_path = os.path.join(CKPT_DIR, \"pointer_head.pt\")\n",
        "if os.path.exists(ph_path):\n",
        "    sd = torch.load(ph_path, map_location=\"cpu\")\n",
        "    if isinstance(sd, dict) and \"p_gen_linear\" in sd:\n",
        "        model_ptr.p_gen_linear.load_state_dict(sd[\"p_gen_linear\"])\n",
        "    elif isinstance(sd, dict) and all(k in {\"weight\",\"bias\"} for k in sd):\n",
        "        model_ptr.p_gen_linear.load_state_dict(sd)\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def _ban_repeating_ngrams(logits: torch.Tensor, seq: torch.Tensor, ngram: int):\n",
        "    if ngram <= 0: return\n",
        "    toks = seq[0].tolist() if seq.dim()==2 else seq.tolist()\n",
        "    if len(toks) < ngram-1: return\n",
        "    prefix = tuple(toks[-(ngram - 1):])\n",
        "    banned = set()\n",
        "    for i in range(len(toks) - ngram + 1):\n",
        "        if tuple(toks[i:i+ngram-1]) == prefix:\n",
        "            banned.add(toks[i+ngram-1])\n",
        "    for t in banned: logits[t] = -1e9\n",
        "\n",
        "import spacy, re\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "except OSError:\n",
        "    from spacy.cli import download\n",
        "    download(\"en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "\n",
        "def ents_set(text):\n",
        "    d = nlp(text)\n",
        "    return {(e.label_, e.text.strip().lower()) for e in d.ents if e.text.strip()}\n",
        "\n",
        "num_re = re.compile(r\"(\\d[\\d,\\.]*%?)\")\n",
        "\n",
        "def rerank_score(hyp, base_norm_score, src_ents, src_text):\n",
        "    hyp_ents = ents_set(hyp)\n",
        "    unsup = len(hyp_ents - src_ents)\n",
        "    sup   = len(hyp_ents & src_ents)\n",
        "    src_nums = set(num_re.findall(src_text))\n",
        "    hyp_nums = set(num_re.findall(hyp))\n",
        "    unsup_num = len([n for n in hyp_nums if n not in src_nums])\n",
        "    sup_person_gpe = sum(1 for (lab,_) in (hyp_ents & src_ents) if lab in {\"PERSON\",\"GPE\"})\n",
        "    return (base_norm_score\n",
        "            - PEN_UNSUP_ENT*unsup\n",
        "            - PEN_UNSUP_NUM*unsup_num\n",
        "            + BONUS_SUP_PERSONGPE*sup_person_gpe)\n",
        "\n",
        "# ---------- NEW: build source-aware priors (NE/NUM masks and position maps) ----------\n",
        "def _build_source_priors(src_ids_1xS, src_txt):\n",
        "    \"\"\"Return dicts keyed by token id -> list[source positions] for NE and NUM, and a general pos map.\"\"\"\n",
        "    S = src_ids_1xS.size(1)\n",
        "    ids = src_ids_1xS[0].tolist()\n",
        "\n",
        "    # NER spans and numbers from raw text\n",
        "    doc = nlp(src_txt)\n",
        "    ne_texts = [e.text for e in doc.ents if e.text.strip()]\n",
        "    nums     = list(set(num_re.findall(src_txt)))\n",
        "\n",
        "    # token-id sets for NE and NUM\n",
        "    ne_tids = set()\n",
        "    for t in ne_texts:\n",
        "        ne_tids.update(tok(t, add_special_tokens=False).input_ids)\n",
        "    num_tids = set()\n",
        "    for t in nums:\n",
        "        num_tids.update(tok(t, add_special_tokens=False).input_ids)\n",
        "\n",
        "    # maps: tid -> [positions]\n",
        "    pos_by_tid      = {}\n",
        "    ne_pos_by_tid   = {}\n",
        "    num_pos_by_tid  = {}\n",
        "    for i, tid in enumerate(ids):\n",
        "        pos_by_tid.setdefault(tid, []).append(i)\n",
        "        if tid in ne_tids:\n",
        "            ne_pos_by_tid.setdefault(tid, []).append(i)\n",
        "        if tid in num_tids:\n",
        "            num_pos_by_tid.setdefault(tid, []).append(i)\n",
        "    return pos_by_tid, ne_pos_by_tid, num_pos_by_tid\n",
        "\n",
        "# --- Guarded span-copying (caps, decay, attention gating, cooldown) ---\n",
        "SPAN_MAX            = 6      # hard cap on consecutive CopyNext tokens\n",
        "COPYNEXT_DECAY      = 0.65   # geometric decay within a span\n",
        "COPYNEXT_MIN_ATTN   = 0.30   # require strong local attention to continue\n",
        "ATTN_ENTROPY_MAX    = 2.5    # require concentrated attention\n",
        "COOLDOWN_STEPS      = 2      # after span ends, wait a bit before re-trying\n",
        "\n",
        "def _attn_entropy(p):\n",
        "    p = p.clamp_min(1e-8)\n",
        "    return float((-p * p.log()).sum().item())\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_pointer_one_with_cov_prec(text: str) -> str:\n",
        "    enc = tok([text], return_tensors=\"pt\", truncation=True, max_length=MAX_SRC_LEN).to(device)\n",
        "    src_ids, src_mask = enc.input_ids, enc.attention_mask\n",
        "    with torch.amp.autocast('cuda', dtype=torch.float16, enabled=USE_FP16):\n",
        "        enc_out = base.model.get_encoder()(input_ids=src_ids, attention_mask=src_mask,\n",
        "                                           output_attentions=False, return_dict=True)\n",
        "\n",
        "    # source priors (as in your previous cell)\n",
        "    src_txt = tok.batch_decode(src_ids, skip_special_tokens=True)[0]\n",
        "    pos_by_tid, ne_pos_by_tid, num_pos_by_tid = _build_source_priors(src_ids, src_txt)\n",
        "\n",
        "    start_id = getattr(base.config, \"decoder_start_token_id\", None) or (tok.bos_token_id or tok.eos_token_id)\n",
        "\n",
        "    # add two integers to beam state: copy_run_len, cooldown_left\n",
        "    beams = [(torch.tensor([[start_id]], device=device, dtype=torch.long),  # seq\n",
        "              0.0, 0.0, False,\n",
        "              torch.zeros(src_ids.size(1), device=device, dtype=enc_out.last_hidden_state.dtype),\n",
        "              0,   # copy_run_len\n",
        "              0)]  # cooldown_left\n",
        "    finished = []\n",
        "\n",
        "    for step in range(MAX_NEW):\n",
        "        act = [(i, b) for i, b in enumerate(beams) if not b[3]]\n",
        "        if not act: break\n",
        "\n",
        "        seqs = torch.cat([beams[i][0] for i,_ in act], dim=0)\n",
        "        K = seqs.size(0)\n",
        "        enc_exp = BaseModelOutput(last_hidden_state=enc_out.last_hidden_state.expand(K, -1, -1))\n",
        "        src_mask_exp = src_mask.expand(K, -1)\n",
        "\n",
        "        with torch.amp.autocast('cuda', dtype=torch.float16, enabled=USE_FP16):\n",
        "            out = base(\n",
        "                encoder_outputs=enc_exp,\n",
        "                attention_mask=src_mask_exp,\n",
        "                decoder_input_ids=seqs,\n",
        "                output_attentions=True, output_hidden_states=True,\n",
        "                use_cache=False, return_dict=True,\n",
        "            )\n",
        "            logp_final, attn = model_ptr._mix_pointer(out, src_ids.expand(K, -1), src_mask_exp, seqs)\n",
        "\n",
        "        step_logits_all = logp_final[:, -1, :].clone().float()\n",
        "        attn_t_all      = attn[:, -1, :]\n",
        "\n",
        "        new_beams = []\n",
        "        for j, (beam_idx, _) in enumerate(act):\n",
        "            step_logits = step_logits_all[j]\n",
        "            if step + 1 < MIN_NEW and tok.eos_token_id is not None:\n",
        "                step_logits[tok.eos_token_id] = -1e9\n",
        "\n",
        "            _ban_repeating_ngrams(step_logits, beams[beam_idx][0], NO_REPEAT)\n",
        "\n",
        "            # ---- guarded CopyNext bonus ----\n",
        "            prev_seq, prev_norm, prev_raw, prev_done, prev_cov, copy_run, cooldown = beams[beam_idx]\n",
        "            attn_t = attn_t_all[j]\n",
        "            attn_t = attn_t / attn_t.sum().clamp_min(1e-8)      # normalize for entropy and weighting\n",
        "            ent = _attn_entropy(attn_t)\n",
        "\n",
        "            next_tid_for_span = None\n",
        "            # allow span continuation only if: not cooling down, under cap, high-confidence attention\n",
        "            if (cooldown == 0) and (copy_run < SPAN_MAX) and (ent <= ATTN_ENTROPY_MAX):\n",
        "                last_tid = prev_seq[0, -1].item()\n",
        "                if last_tid in pos_by_tid:\n",
        "                    # choose the most-attended occurrence of last token in source\n",
        "                    poss = pos_by_tid[last_tid]\n",
        "                    if poss:\n",
        "                        poss_t = torch.tensor(poss, device=attn_t.device)\n",
        "                        weights = attn_t[poss_t]\n",
        "                        best_idx = int(poss_t[torch.argmax(weights)].item())\n",
        "                        best_w   = float(weights.max().item())\n",
        "                        if best_w >= COPYNEXT_MIN_ATTN and (best_idx + 1) < src_ids.size(1):\n",
        "                            next_tid_for_span = int(src_ids[0, best_idx + 1].item())\n",
        "                            # decayed bonus within span\n",
        "                            step_logits[next_tid_for_span] += COPYNEXT_BONUS * (COPYNEXT_DECAY ** copy_run) * best_w\n",
        "\n",
        "            # ---- entity/number priors (unchanged) ----\n",
        "            if ne_pos_by_tid:\n",
        "                for tid, poss in ne_pos_by_tid.items():\n",
        "                    w = float(attn_t[torch.tensor(poss, device=attn_t.device)].sum().item())\n",
        "                    if w > 0:\n",
        "                        step_logits[tid] += ENTCOPY_BONUS * w\n",
        "            if num_pos_by_tid:\n",
        "                for tid, poss in num_pos_by_tid.items():\n",
        "                    w = float(attn_t[torch.tensor(poss, device=attn_t.device)].sum().item())\n",
        "                    if w > 0:\n",
        "                        step_logits[tid] += NUMCOPY_BONUS * w\n",
        "\n",
        "            # expand\n",
        "            topk_logp, topk_ids = torch.topk(step_logits, k=NUM_BEAMS)\n",
        "            cov_pen = COV_ALPHA * torch.min(attn_t, prev_cov).sum().item()\n",
        "\n",
        "            for k in range(NUM_BEAMS):\n",
        "                tid  = topk_ids[k].view(1,1)\n",
        "                nseq = torch.cat([prev_seq, tid], dim=1)\n",
        "                nfin = (tid.item() == tok.eos_token_id)\n",
        "                raw  = prev_raw + float(topk_logp[k].item()) - cov_pen\n",
        "                lp   = ((5.0 + nseq.size(1))**LENGTH_PENALTY) / ((5.0 + 1.0)**LENGTH_PENALTY)\n",
        "                nsc  = raw / lp\n",
        "                ncov = prev_cov + attn_t\n",
        "\n",
        "                # update copy-run state\n",
        "                if next_tid_for_span is not None and tid.item() == next_tid_for_span:\n",
        "                    n_copy_run  = copy_run + 1\n",
        "                    n_cooldown  = 0 if n_copy_run < SPAN_MAX else COOLDOWN_STEPS\n",
        "                else:\n",
        "                    n_copy_run  = 0\n",
        "                    # if we were in a span and broke it, start cooldown\n",
        "                    n_cooldown  = max(cooldown - 1, 0) if copy_run == 0 else COOLDOWN_STEPS\n",
        "\n",
        "                new_beams.append((nseq, nsc, raw, nfin, ncov, n_copy_run, n_cooldown))\n",
        "\n",
        "        new_beams.sort(key=lambda x: x[1], reverse=True)\n",
        "        beams = new_beams[:NUM_BEAMS]\n",
        "\n",
        "        for seq, nsc, raw, done, cov, cr, cd in beams:\n",
        "            if done:\n",
        "                finished.append((seq, nsc))\n",
        "        finished = sorted(finished, key=lambda x: x[1], reverse=True)[:RERANK_K]\n",
        "\n",
        "        if all(b[3] for b in beams): break\n",
        "        if (step & 7) == 0:\n",
        "            del out, logp_final, attn, step_logits_all, attn_t_all\n",
        "            torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "    if not finished:\n",
        "        finished = [(max(beams, key=lambda x: x[1])[0], max(beams, key=lambda x: x[1])[1])]\n",
        "\n",
        "    # rerank (as before)\n",
        "    src_ents = ents_set(src_txt)\n",
        "    cands = []\n",
        "    for seq, nsc in finished:\n",
        "        hyp = tok.batch_decode(seq.detach().cpu(), skip_special_tokens=True)[0]\n",
        "        s = rerank_score(hyp, nsc, src_ents, src_txt)\n",
        "        cands.append((s, hyp))\n",
        "    cands.sort(key=lambda x: x[0], reverse=True)\n",
        "    return cands[0][1]\n",
        "\n",
        "\n",
        "# ---------- load, run, evaluate (PRINT ONLY) ----------\n",
        "df_all = pd.read_csv(BASE_CSV).head(N).copy()\n",
        "for c in [SRC_COL, REF_COL, \"base_pred\"]:\n",
        "    df_all[c] = df_all[c].fillna(\"\").astype(str).str.replace(r\"\\s+\",\" \",regex=True).str.strip()\n",
        "\n",
        "ptr_new = []\n",
        "for art in tqdm(df_all[SRC_COL].tolist(), desc=\"PTR-cov+priors (100)\"):\n",
        "    ptr_new.append(generate_pointer_one_with_cov_prec(art))\n",
        "\n",
        "# evaluation\n",
        "def norm(s): return \" \".join(str(s or \"\").split())\n",
        "refs = df_all[REF_COL].map(norm).tolist()\n",
        "base_preds = df_all[\"base_pred\"].map(norm).tolist()\n",
        "ptr_preds  = list(map(norm, ptr_new))\n",
        "\n",
        "# Length\n",
        "bl = [len(x.split()) for x in base_preds]\n",
        "pl = [len(x.split()) for x in ptr_preds]\n",
        "print(\"=== LENGTH (words) ===\")\n",
        "print(\"[BASE] mean=\", np.mean(bl), \"median=\", np.median(bl))\n",
        "print(\"[PTR*] mean=\", np.mean(pl), \"median=\", np.median(pl))\n",
        "print(\"[Δ] mean=\", float(np.mean(np.array(pl)-np.array(bl))))\n",
        "\n",
        "# ROUGE\n",
        "scorer = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\"], use_stemmer=True)\n",
        "def rouge_mean(preds, refs):\n",
        "    rows=[]\n",
        "    for p,r in zip(preds, refs):\n",
        "        sc = scorer.score(r, p)\n",
        "        rows.append({\n",
        "            \"r1_p\": sc[\"rouge1\"].precision, \"r1_r\": sc[\"rouge1\"].recall, \"r1_f\": sc[\"rouge1\"].fmeasure,\n",
        "            \"r2_p\": sc[\"rouge2\"].precision, \"r2_r\": sc[\"rouge2\"].recall, \"r2_f\": sc[\"rouge2\"].fmeasure,\n",
        "            \"rl_p\": sc[\"rougeL\"].precision, \"rl_r\": sc[\"rougeL\"].recall, \"rl_f\": sc[\"rougeL\"].fmeasure,\n",
        "        })\n",
        "    R = pd.DataFrame(rows).mean(numeric_only=True)\n",
        "    return {k: float(f\"{v:.4f}\") for k,v in R.items()}\n",
        "\n",
        "rb = rouge_mean(base_preds, refs)\n",
        "rp = rouge_mean(ptr_preds,  refs)\n",
        "diff = {k: float(f\"{rp[k]-rb[k]:.4f}\") for k in rb}\n",
        "print(\"\\n=== ROUGE (mean over 100) ===\")\n",
        "print(\"[BASE]\", rb)\n",
        "print(\"[PTR*]\", rp)\n",
        "print(\"[PTR* - BASE]\", diff)\n",
        "\n",
        "# Entities (overall micro)\n",
        "def ents(text):\n",
        "    d = nlp(text);\n",
        "    return {(e.label_, e.text.strip().lower()) for e in d.ents if e.text.strip()}\n",
        "def prf(preds, refs):\n",
        "    TP=FP=FN=0\n",
        "    for p,r in zip(preds, refs):\n",
        "        P, R = ents(p), ents(r)\n",
        "        TP += len(P & R); FP += len(P - R); FN += len(R - P)\n",
        "    P = TP/(TP+FP) if (TP+FP) else 0.0\n",
        "    R = TP/(TP+FN) if (TP+FN) else 0.0\n",
        "    F = 2*P*R/(P+R) if (P+R) else 0.0\n",
        "    return {\"precision\":float(f\"{P:.4f}\"), \"recall\":float(f\"{R:.4f}\"), \"f1\":float(f\"{F:.4f}\")}\n",
        "\n",
        "print(\"\\n=== ENTITIES (overall, micro) ===\")\n",
        "print(\"[BASE]\", prf(base_preds, refs))\n",
        "print(\"[PTR*]\", prf(ptr_preds,  refs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8Fg3V7-uAp5",
        "outputId": "d1aa550b-97f1-4016-c459-6cbda3cb6f45"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[saved] /content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000/ptr_exp2_vs_base_samples_100.csv\n",
            "\n",
            "[0] =======================\n",
            "ARTICLE   : Arsene Wenger's Champions League record is as phenomenal as it is deeply disappointing. The paradox is apparent is Wenger's own mantra, oft repeated, that the club's record of qualifying for the tournament for 17 consecutive years and having reached the equiva\n",
            "REFERENCE : Arsene Wenger has qualified for the Champions League for last 17 seasons . But despite his managerial pedigree he boasts no European trophies . Iconic boss has only reached on final and three semi-finals . Monaco game is hugely poignant after 3-1 defeat in fir\n",
            "BASE      : Arsene Wenger's Champions League record is as phenomenal as it is deeply disappointing. The club's record of qualifying for the tournament for 17 consecutive years is extraordinary. Yet having contested the Champions League and its predecessor the European Cup\n",
            "PTR-exp2  : Arsenal face Monaco in the Champions League quarter-finals on Tuesday. Arsene Wenger's record of qualifying for the tournament for 17 consecutive years is extraordinary. Yet having contested the Champions League and its predecessor the European Cup 18 times no\n",
            "  ENTITIES — BASE  sup=6 fp=5 fn=13\n",
            "    sup: [('CARDINAL', 'three'), ('NUMBER', '17'), ('ORG', 'arsenal'), ('ORG', 'champions league'), ('ORG', 'monaco'), ('ORG', 'the champions league')]\n",
            "    fp : [('DATE', '17 consecutive years'), ('GPE', 'monaco'), ('LAW', 'the european cup 18'), ('NUMBER', '18'), ('PERSON', \"arsene wenger's\")]\n",
            "    fn : [('CARDINAL', '3'), ('DATE', '2004'), ('DATE', '2006'), ('DATE', 'last 17 seasons'), ('GPE', 'emirates'), ('NORP', 'european'), ('NUMBER', '1'), ('NUMBER', '2004'), ('NUMBER', '2006'), ('NUMBER', '3'), ('ORDINAL', 'first'), ('PERSON', 'arsene wenger'), ('PERSON', 'wenger')]\n",
            "  ENTITIES — PTR2  sup=5 fp=6 fn=14\n",
            "    sup: [('CARDINAL', 'three'), ('NUMBER', '17'), ('ORG', 'arsenal'), ('ORG', 'monaco'), ('ORG', 'the champions league')]\n",
            "    fp : [('DATE', '17 consecutive years'), ('DATE', 'tuesday'), ('GPE', 'monaco'), ('LAW', 'the european cup 18'), ('NUMBER', '18'), ('PERSON', \"arsene wenger's\")]\n",
            "    fn : [('CARDINAL', '3'), ('DATE', '2004'), ('DATE', '2006'), ('DATE', 'last 17 seasons'), ('GPE', 'emirates'), ('NORP', 'european'), ('NUMBER', '1'), ('NUMBER', '2004'), ('NUMBER', '2006'), ('NUMBER', '3'), ('ORDINAL', 'first'), ('ORG', 'champions league'), ('PERSON', 'arsene wenger'), ('PERSON', 'wenger')]\n",
            "\n",
            "[1] =======================\n",
            "ARTICLE   : Cristiano Ronaldo has gone almost a year without scoring a free-kick in La Liga despite being Real Madrid's designated taker, and supporters are now asking if the ball shouldn't be taken off the Portuguese and given to Gareth Bale. The 30-year-old last hit the\n",
            "REFERENCE : Cristiano Ronaldo gone nearly a year without scoring a free-kick in La Liga . Real Madrid fans asking whether Gareth Bale should take over set-pieces . Bale scores a free-kick every 9.5 attempts while Ronaldo is every 15.6 . Gareth Bale: 'It is surreal to be c\n",
            "BASE      : Cristiano Ronaldo has gone almost a year without scoring a free-kick in La Liga. The 30-year-old last hit the back of the net from a set-piece in the Spanish league on March 26 last year. He has now failed to score a single goal with any of his last 51 free-ki\n",
            "PTR-exp2  : Ricardo Ronaldo has gone almost a year without scoring a free-kick in La Liga. The 30-year-old last hit the back of the net from a set-piece in the Spanish league on March 26 last year. His last goal from a free-kick came in last season's Champions League agai\n",
            "  ENTITIES — BASE  sup=1 fp=9 fn=11\n",
            "    sup: [('FAC', 'la liga')]\n",
            "    fp : [('CARDINAL', '51'), ('DATE', '30-year-old'), ('DATE', 'almost a year'), ('DATE', 'march 26 last year'), ('NORP', 'spanish'), ('NUMBER', '26'), ('NUMBER', '30'), ('NUMBER', '51'), ('PERSON', 'cristiano ronaldo')]\n",
            "    fn : [('CARDINAL', '15.6'), ('CARDINAL', '9.5'), ('DATE', 'nearly a year'), ('GPE', 'madrid'), ('NUMBER', '15.6'), ('NUMBER', '9.5'), ('ORG', 'cristiano'), ('ORG', 'galactico'), ('ORG', 'real madrid'), ('PERSON', 'gareth bale'), ('PERSON', 'ronaldo')]\n",
            "  ENTITIES — PTR2  sup=1 fp=10 fn=11\n",
            "    sup: [('FAC', 'la liga')]\n",
            "    fp : [('DATE', '30-year-old'), ('DATE', 'almost a year'), ('DATE', \"last season's\"), ('DATE', 'march 26 last year'), ('LOC', 'bayern munich'), ('NORP', 'spanish'), ('NUMBER', '26'), ('NUMBER', '30'), ('ORG', 'champions league'), ('PERSON', 'ricardo ronaldo')]\n",
            "    fn : [('CARDINAL', '15.6'), ('CARDINAL', '9.5'), ('DATE', 'nearly a year'), ('GPE', 'madrid'), ('NUMBER', '15.6'), ('NUMBER', '9.5'), ('ORG', 'cristiano'), ('ORG', 'galactico'), ('ORG', 'real madrid'), ('PERSON', 'gareth bale'), ('PERSON', 'ronaldo')]\n",
            "\n",
            "[2] =======================\n",
            "ARTICLE   : A man from the NSW Hunter region who killed a litter of puppies by smashing them over the head with a rock has been slapped with an extra nine animal cruelty charges. It comes amidst news that the sole pup to survive the attack, nicknamed 'Lucky', is 'stable'.\n",
            "REFERENCE : Nathan Thompson allegedly killed 10 bull terriers aged six to eight weeks . He pleaded guilty to nine extra animal cruelty charges on Thursday . Police say he took them to bushland where he hit their heads with rocks . He was released on bail but is banned fro\n",
            "BASE      : A man from the NSW Hunter region killed a litter of puppies by smashing them over the head with a rock. The only pup to survive the attack has been named 'Lucky' and is being cared for by the RSPCA. Police allege a witness saw the 25-year-old man begin to kill\n",
            "PTR-exp2  : Police allege Nathan Thompson, 25, killed a litter of puppies by hitting them over the head with a rock. The only pup to survive the attack, nicknamed 'Lucky', is'stable' Police allege a witness saw the 25-year-old man begin to kill a number of the puppies. Th\n",
            "  ENTITIES — BASE  sup=1 fp=7 fn=6\n",
            "    sup: [('CARDINAL', 'nine')]\n",
            "    fp : [('CARDINAL', 'five'), ('CARDINAL', 'two'), ('DATE', '25-year-old'), ('NORP', 'hunter'), ('NUMBER', '25'), ('ORG', 'nsw'), ('WORK_OF_ART', \"'lucky'\")]\n",
            "    fn : [('CARDINAL', '10'), ('DATE', 'six to eight weeks'), ('DATE', 'thursday'), ('NUMBER', '10'), ('PERSON', 'nathan thompson'), ('PERSON', 'rspca')]\n",
            "  ENTITIES — PTR2  sup=1 fp=7 fn=6\n",
            "    sup: [('PERSON', 'nathan thompson')]\n",
            "    fp : [('CARDINAL', 'five'), ('CARDINAL', 'two'), ('DATE', '25'), ('DATE', '25-year-old'), ('NUMBER', '25'), ('NUMBER', '25,'), ('WORK_OF_ART', \"'lucky', is'stable' police\")]\n",
            "    fn : [('CARDINAL', '10'), ('CARDINAL', 'nine'), ('DATE', 'six to eight weeks'), ('DATE', 'thursday'), ('NUMBER', '10'), ('PERSON', 'rspca')]\n",
            "\n",
            "[3] =======================\n",
            "ARTICLE   : A woman raised by two lesbians has provoked controversy after publicly announcing that she is against gay marriage because she says she missed out on having her father in her life when growing up. Heather Barwick, a 31-year-old mother-of-four from South Caroli\n",
            "REFERENCE : Heather Barwick says she is against gay marriage because she missed out on having her father around when she was growing up . The 31-year-old mother-of-four admits her mother's partner 'treated me as if I was her own daughter' She also says that her biological\n",
            "BASE      : Heather Barwick, a 31-year-old mother-of-four from South Carolina, says her mother left her father when she was 2 or 3 so that she could move in with the woman she loved. She also admits that her biological father 'wasn't a great guy' and 'didn't bother coming\n",
            "PTR-exp2  : Hather Barwick, 31, says her mother left her father when she was 2 or 3 so that she could move in with the woman she loved. She says her mom raised me with her same-sex partner back in the '80s and '90s,' writes Barwick for the conservative publication The Fed\n",
            "  ENTITIES — BASE  sup=5 fp=5 fn=0\n",
            "    sup: [('DATE', '31-year-old'), ('NUMBER', '20'), ('NUMBER', '31'), ('PERSON', 'barwick'), ('PERSON', 'heather barwick')]\n",
            "    fp : [('CARDINAL', '2'), ('CARDINAL', '3'), ('GPE', 'south carolina'), ('NUMBER', '2'), ('NUMBER', '3')]\n",
            "    fn : []\n",
            "  ENTITIES — PTR2  sup=1 fp=10 fn=4\n",
            "    sup: [('PERSON', 'barwick')]\n",
            "    fp : [('CARDINAL', '2'), ('CARDINAL', '3'), ('DATE', '31'), ('DATE', \"the '80s and '90s\"), ('NUMBER', '2'), ('NUMBER', '3'), ('NUMBER', '31,'), ('NUMBER', '80'), ('NUMBER', '90'), ('PERSON', 'hather barwick')]\n",
            "    fn : [('DATE', '31-year-old'), ('NUMBER', '20'), ('NUMBER', '31'), ('PERSON', 'heather barwick')]\n",
            "\n",
            "[4] =======================\n",
            "ARTICLE   : The Archbishop of Adelaide has been charged with covering up child sex abuse. It is alleged Philip Wilson tried to hide abuse carried out by another priest in the 1970s, The Australian reported. Wilson was appointed archbishop for the South Australian capital \n",
            "REFERENCE : Adelaide Archbishop Philip Wilson has been charged with failing to report child sex abuse in his diocese in the 1970s . He has been the archbishop of the South Australian capital since 2011 . In the 1970s, Wilson did not report abuse carried out by another pri\n",
            "BASE      : The Archbishop of Adelaide has been charged with covering up child sex abuse. It is alleged Philip Wilson tried to hide abuse carried out by another priest in the 1970s. He was appointed archbishop for the South Australian capital in 2011. The charge comes aft\n",
            "PTR-exp2  : Archbishop of Adelaide charged with covering up child sex abuse. It is alleged Philip Wilson tried to hide abuse carried out by another priest in the 1970s. The charge comes after it was revealed the senior figure in the Catholic Church did not report priest J\n",
            "  ENTITIES — BASE  sup=5 fp=8 fn=3\n",
            "    sup: [('DATE', '2011'), ('DATE', 'the 1970s'), ('NORP', 'south australian'), ('NUMBER', '1970'), ('PERSON', 'philip wilson')]\n",
            "    fp : [('DATE', '13-year-old'), ('DATE', '1989'), ('NUMBER', '13'), ('NUMBER', '1989.'), ('NUMBER', '2011.'), ('ORG', 'the archbishop of adelaide'), ('ORG', 'the catholic church'), ('PERSON', 'jim fletcher')]\n",
            "    fn : [('DATE', 'up to two years'), ('NUMBER', '2011'), ('ORG', 'wilson')]\n",
            "  ENTITIES — PTR2  sup=3 fp=11 fn=5\n",
            "    sup: [('DATE', 'the 1970s'), ('NUMBER', '1970'), ('PERSON', 'philip wilson')]\n",
            "    fp : [('DATE', '13-year-old'), ('DATE', '1989 to 1991'), ('DATE', 'five years later'), ('DATE', 'last year'), ('NUMBER', '13'), ('NUMBER', '1989'), ('NUMBER', '1991'), ('ORG', 'archbishop of adelaide'), ('ORG', 'the catholic church'), ('PERSON', 'fletcher'), ('PERSON', 'jim fletcher')]\n",
            "    fn : [('DATE', '2011'), ('DATE', 'up to two years'), ('NORP', 'south australian'), ('NUMBER', '2011'), ('ORG', 'wilson')]\n",
            "\n",
            "[5] =======================\n",
            "ARTICLE   : Civil rights activist Diane Nash has said that she didn't participate in a march across the Edmund Pettus bridge in Selma, Alabama on Saturday because of former President George W. Bush. Nash revealed her reason to opt out of the Saturday event in an interview\n",
            "REFERENCE : Diane Nash has said that she didn't participate in a march across the Edmund Pettus bridge on Saturday because of the former president . She's said she made the choice 'when it was apparent that he was going to be part of it . Nash was a leader in the Freedom \n",
            "BASE      : Civil rights activist Diane Nash has said that she didn't participate in a march across the Edmund Pettus bridge in Selma, Alabama on Saturday. She said she made the choice 'when it was apparent that he was going to be part of it' She continued, claiming 'And \n",
            "PTR-exp2  : Diane Nash said that she didn't participate in a march across the Edmund Pettus bridge in Selma, Alabama on Saturday. She said she made the choice 'when it was apparent that he was going to be part of it' She continued, claiming 'And George Bush stands for jus\n",
            "  ENTITIES — BASE  sup=2 fp=3 fn=3\n",
            "    sup: [('DATE', 'saturday'), ('PERSON', 'diane nash')]\n",
            "    fp : [('GPE', 'alabama'), ('GPE', 'selma'), ('PERSON', 'george bush')]\n",
            "    fn : [('ORDINAL', 'first'), ('ORG', 'selma'), ('PERSON', 'james bevel')]\n",
            "  ENTITIES — PTR2  sup=2 fp=9 fn=3\n",
            "    sup: [('DATE', 'saturday'), ('PERSON', 'diane nash')]\n",
            "    fp : [('DATE', '2014'), ('GPE', 'alabama'), ('GPE', 'selma'), ('NUMBER', '2014'), ('ORG', \"king's\"), ('ORG', 'newsone'), ('PERSON', 'george bush'), ('PERSON', 'nash'), ('PERSON', 'tessa thompson')]\n",
            "    fn : [('ORDINAL', 'first'), ('ORG', 'selma'), ('PERSON', 'james bevel')]\n",
            "\n",
            "[6] =======================\n",
            "ARTICLE   : A young driver has escaped serious injury after rolling his vehicle on a remote Northern Territory highway - only to be stung by a swarm of angry bees that escaped his trailer. Police say the 18-year-old was towing a trailer with crates full of bees when the t\n",
            "REFERENCE : A teenager has been attacked by an angry swarm of bees after rolling his car on a Northern Territory highway . He was transporting a trailer-full of bees towards Katherine when he drove off the road, flipping the vehicle . The young driver escaped unhurt, but \n",
            "BASE      : The 18-year-old was towing a trailer with crates full of bees when the trailer's wheel clipped the edge of the road. The driver lost control, the vehicle fish-tailed and flipped about 10 metres into scrub land. The driver managed to crawl out with minor swelli\n",
            "PTR-exp2  : A young driver was towing a trailer with crates full of bees when the trailer's wheel clipped the edge of the road, 17 km north of Katherine. The driver lost control, the vehicle fish-tailed and flipped about 10 metres into scrub land. The young driver managed\n",
            "  ENTITIES — BASE  sup=0 fp=4 fn=2\n",
            "    sup: []\n",
            "    fp : [('DATE', '18-year-old'), ('NUMBER', '10'), ('NUMBER', '18'), ('QUANTITY', 'about 10 metres')]\n",
            "    fn : [('LOC', 'northern territory'), ('PERSON', 'katherine')]\n",
            "  ENTITIES — PTR2  sup=0 fp=5 fn=2\n",
            "    sup: []\n",
            "    fp : [('GPE', 'katherine'), ('NUMBER', '10'), ('NUMBER', '17'), ('QUANTITY', '17 km'), ('QUANTITY', 'about 10 metres')]\n",
            "    fn : [('LOC', 'northern territory'), ('PERSON', 'katherine')]\n",
            "\n",
            "[7] =======================\n",
            "ARTICLE   : Earth may have been witness to a stunning aurora displays, but they are nothing compared to what can be seen on Mars. Because Mars has a very thin atmosphere, the sun's energetic particles hit it directly and penetrate deeper creating incredibly bright and vas\n",
            "REFERENCE : Maven probe spotted ultraviolet auroral glow in northern hemisphere . What was suprising was how deep in the atmosphere aurora occurred . Dust was also found up to 190 miles (300 km) above planet's surface . No known process on Mars can explain the appearance \n",
            "BASE      : Mars has a very thin atmosphere, and the sun's energetic particles hit it directly and penetrate deeper creating incredibly bright and vast light shows. But their behaviour doesn't always follow existing rules. Nasa scientists were recently stunned to discover\n",
            "PTR-exp2  : NASA scientists were stunned to discover aurora that reaches deep into the Martian atmosphere. The aurora on Mars is similar to Earth's \"Northern Lights\" but penetrates deep into the atmosphere. The sun's energetic particles hit it directly and penetrate deepe\n",
            "  ENTITIES — BASE  sup=1 fp=4 fn=5\n",
            "    sup: [('LOC', 'mars')]\n",
            "    fp : [('LOC', 'earth'), ('NORP', 'martian'), ('ORG', 'nasa'), ('WORK_OF_ART', 'northern lights')]\n",
            "    fn : [('NUMBER', '190'), ('NUMBER', '300'), ('ORG', 'maven'), ('QUANTITY', '190 miles'), ('QUANTITY', '300 km')]\n",
            "  ENTITIES — PTR2  sup=1 fp=4 fn=5\n",
            "    sup: [('LOC', 'mars')]\n",
            "    fp : [('LOC', 'earth'), ('NORP', 'martian'), ('ORG', 'nasa'), ('WORK_OF_ART', 'northern lights')]\n",
            "    fn : [('NUMBER', '190'), ('NUMBER', '300'), ('ORG', 'maven'), ('QUANTITY', '190 miles'), ('QUANTITY', '300 km')]\n",
            "\n",
            "[8] =======================\n",
            "ARTICLE   : Manchester City playmaker David Silva believes the Barclays Premier League champions have got to stop Barcelona scoring if they are to have any chance of pulling off a miracle at the Nou Camp on Wednesday night. Silva and his team-mates are 2-1 down after the \n",
            "REFERENCE : Manchester City face Barcelona on Wednesday night at the Nou Camp . David Silva feels City must stop Barca from scoring in the second leg . City are 2-1 down after the first leg of the Champions League tie .\n",
            "BASE      : Manchester City playmaker David Silva believes they have to stop Barcelona scoring if they are to have any chance of pulling off a miracle. Silva and his team-mates are 2-1 down after the first leg of their Champions League last-16 tie. It is a task easier sai\n",
            "PTR-exp2  : Manchester City playmaker David Silva believes the Barclays Premier League champions have got to stop Barcelona scoring. Silva and his City team-mates are 2-1 down after the first leg of their Champions League last-16 tie. It is a task easier said than done to\n",
            "  ENTITIES — BASE  sup=7 fp=6 fn=4\n",
            "    sup: [('CARDINAL', '2'), ('GPE', 'barcelona'), ('GPE', 'manchester city'), ('NUMBER', '1'), ('NUMBER', '2'), ('ORDINAL', 'first'), ('PERSON', 'david silva')]\n",
            "    fp : [('GPE', 'city'), ('LOC', 'the nou camp'), ('NUMBER', '16'), ('ORG', 'champions league last-16'), ('PERSON', 'lionel messi'), ('PERSON', 'silva')]\n",
            "    fn : [('FAC', 'the nou camp'), ('ORDINAL', 'second'), ('ORG', 'the champions league'), ('TIME', 'wednesday night')]\n",
            "  ENTITIES — PTR2  sup=8 fp=8 fn=3\n",
            "    sup: [('CARDINAL', '2'), ('FAC', 'the nou camp'), ('GPE', 'barcelona'), ('GPE', 'manchester city'), ('NUMBER', '1'), ('NUMBER', '2'), ('ORDINAL', 'first'), ('PERSON', 'david silva')]\n",
            "    fp : [('DATE', 'wednesday'), ('GPE', 'city'), ('NUMBER', '16'), ('ORG', 'champions league last-16'), ('ORG', 'the barclays premier league'), ('PERSON', 'lionel messi'), ('PERSON', 'messi'), ('PERSON', 'silva')]\n",
            "    fn : [('ORDINAL', 'second'), ('ORG', 'the champions league'), ('TIME', 'wednesday night')]\n",
            "\n",
            "[9] =======================\n",
            "ARTICLE   : A professor helping to identify 600 body parts belonging to the Germanwings crash victims said today he would be haunted forever by the grim task. Remains of the 150 passengers and crew are being categorised after co-pilot Andreas Lubitz deliberately flew the \n",
            "REFERENCE : Forensic experts painstakingly try to identify 600 body parts of victims and have isolated 78 distinct DNA strands . Remains will be photographed and scanned in 3D before being placed in morgue until identification has occurred . It is hoped 95% will be identi\n",
            "BASE      : Remains of the 150 passengers and crew are being categorised after co-pilot Andreas Lubitz deliberately flew the Airbus A320 into the French Alps on Tuesday. Investigators at the crash site have so far retrieved about 600 body parts. They have managed to isola\n",
            "PTR-exp2  : Remains of the 150 passengers and crew are being categorised. Germany's most prestigious forensic scientist Michael Tsokos spoke of the work ahead as it emerged Lubitz's remains have already been found. Investigators at the Germanwings crash site have so far r\n",
            "  ENTITIES — BASE  sup=4 fp=10 fn=8\n",
            "    sup: [('CARDINAL', '78'), ('LOC', 'alps'), ('NUMBER', '600'), ('NUMBER', '78')]\n",
            "    fp : [('CARDINAL', '150'), ('CARDINAL', 'about 600'), ('CARDINAL', 'one'), ('DATE', 'tuesday'), ('NORP', 'french'), ('NUMBER', '150'), ('NUMBER', '320'), ('ORG', 'airbus'), ('PERSON', 'andreas lubitz'), ('PRODUCT', 'a320')]\n",
            "    fn : [('CARDINAL', '600'), ('DATE', 'next three weeks'), ('NUMBER', '24'), ('NUMBER', '3'), ('NUMBER', '95%'), ('PERCENT', '95%'), ('PERSON', 'michael tsokos'), ('TIME', '24-hour')]\n",
            "  ENTITIES — PTR2  sup=4 fp=7 fn=8\n",
            "    sup: [('CARDINAL', '78'), ('NUMBER', '600'), ('NUMBER', '78'), ('PERSON', 'michael tsokos')]\n",
            "    fp : [('CARDINAL', '150'), ('CARDINAL', 'about 600'), ('CARDINAL', 'one'), ('GPE', 'germany'), ('NUMBER', '150'), ('PERSON', 'germanwings'), ('PERSON', 'lubitz')]\n",
            "    fn : [('CARDINAL', '600'), ('DATE', 'next three weeks'), ('LOC', 'alps'), ('NUMBER', '24'), ('NUMBER', '3'), ('NUMBER', '95%'), ('PERCENT', '95%'), ('TIME', '24-hour')]\n",
            "\n",
            "=== [BASE] ENTITY (overall) ===\n",
            "TP=317  FP=580  FN=574\n",
            "Precision: 0.353400\n",
            "Recall:    0.355780\n",
            "F1:        0.354586\n",
            "\n",
            "=== [BASE] ENTITY (by type) ===\n",
            "PERSON       tp= 85 fp= 83 fn=102  P=0.506 R=0.455 F1=0.479\n",
            "NUMBER       tp= 52 fp=110 fn=109  P=0.321 R=0.323 F1=0.322\n",
            "DATE         tp= 36 fp= 98 fn= 97  P=0.269 R=0.271 F1=0.270\n",
            "GPE          tp= 46 fp= 63 fn= 71  P=0.422 R=0.393 F1=0.407\n",
            "ORG          tp= 38 fp= 77 fn= 72  P=0.330 R=0.345 F1=0.338\n",
            "CARDINAL     tp= 25 fp= 54 fn= 48  P=0.316 R=0.342 F1=0.329\n",
            "NORP         tp=  8 fp= 20 fn= 15  P=0.286 R=0.348 F1=0.314\n",
            "LOC          tp=  6 fp=  9 fn=  9  P=0.400 R=0.400 F1=0.400\n",
            "MONEY        tp=  6 fp= 13 fn=  9  P=0.316 R=0.400 F1=0.353\n",
            "ORDINAL      tp=  4 fp= 17 fn=  7  P=0.190 R=0.364 F1=0.250\n",
            "TIME         tp=  2 fp= 10 fn=  8  P=0.167 R=0.200 F1=0.182\n",
            "EVENT        tp=  4 fp=  2 fn=  5  P=0.667 R=0.444 F1=0.533\n",
            "FAC          tp=  3 fp= 12 fn=  4  P=0.200 R=0.429 F1=0.273\n",
            "QUANTITY     tp=  0 fp=  6 fn=  7  P=0.000 R=0.000 F1=0.000\n",
            "PERCENT      tp=  0 fp=  0 fn=  3  P=0.000 R=0.000 F1=0.000\n",
            "PRODUCT      tp=  1 fp=  2 fn=  2  P=0.333 R=0.333 F1=0.333\n",
            "WORK_OF_ART  tp=  1 fp=  2 fn=  2  P=0.333 R=0.333 F1=0.333\n",
            "LANGUAGE     tp=  0 fp=  0 fn=  2  P=0.000 R=0.000 F1=0.000\n",
            "LAW          tp=  0 fp=  2 fn=  2  P=0.000 R=0.000 F1=0.000\n",
            "\n",
            "=== [PTR exp2] ENTITY (overall) ===\n",
            "TP=326  FP=654  FN=565\n",
            "Precision: 0.332653\n",
            "Recall:    0.365881\n",
            "F1:        0.348477\n",
            "\n",
            "=== [PTR exp2] ENTITY (by type) ===\n",
            "PERSON       tp= 83 fp= 89 fn=104  P=0.483 R=0.444 F1=0.462\n",
            "NUMBER       tp= 54 fp=125 fn=107  P=0.302 R=0.335 F1=0.318\n",
            "DATE         tp= 42 fp=127 fn= 91  P=0.249 R=0.316 F1=0.278\n",
            "GPE          tp= 50 fp= 72 fn= 67  P=0.410 R=0.427 F1=0.418\n",
            "ORG          tp= 39 fp= 92 fn= 71  P=0.298 R=0.355 F1=0.324\n",
            "CARDINAL     tp= 23 fp= 54 fn= 50  P=0.299 R=0.315 F1=0.307\n",
            "NORP         tp=  7 fp= 16 fn= 16  P=0.304 R=0.304 F1=0.304\n",
            "LOC          tp=  3 fp=  9 fn= 12  P=0.250 R=0.200 F1=0.222\n",
            "MONEY        tp=  6 fp= 16 fn=  9  P=0.273 R=0.400 F1=0.324\n",
            "ORDINAL      tp=  3 fp= 12 fn=  8  P=0.200 R=0.273 F1=0.231\n",
            "TIME         tp=  1 fp= 11 fn=  9  P=0.083 R=0.100 F1=0.091\n",
            "EVENT        tp=  5 fp=  7 fn=  4  P=0.417 R=0.556 F1=0.476\n",
            "FAC          tp=  5 fp=  9 fn=  2  P=0.357 R=0.714 F1=0.476\n",
            "QUANTITY     tp=  2 fp=  7 fn=  5  P=0.222 R=0.286 F1=0.250\n",
            "PERCENT      tp=  0 fp=  1 fn=  3  P=0.000 R=0.000 F1=0.000\n",
            "PRODUCT      tp=  2 fp=  2 fn=  1  P=0.500 R=0.667 F1=0.571\n",
            "WORK_OF_ART  tp=  1 fp=  4 fn=  2  P=0.200 R=0.333 F1=0.250\n",
            "LANGUAGE     tp=  0 fp=  0 fn=  2  P=0.000 R=0.000 F1=0.000\n",
            "LAW          tp=  0 fp=  1 fn=  2  P=0.000 R=0.000 F1=0.000\n",
            "\n",
            "=== [PTR exp2 - BASE] ENTITY (overall delta) ===\n",
            "ΔPrecision=-0.020747  ΔRecall=+0.010101  ΔF1=-0.006109\n"
          ]
        }
      ],
      "source": [
        "# === SIDE-BY-SIDE samples + ENTITY PRF for matched 100 rows (BASE vs PTR-exp2) ===\n",
        "!pip -q install spacy==3.7.5\n",
        "import os, re, pandas as pd, numpy as np\n",
        "\n",
        "CKPT_DIR    = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000\"\n",
        "BASE_PATH   = os.path.join(CKPT_DIR, \"iksumpointer.csv\")   # has base_pred + refs + source (+ maybe old ptr)\n",
        "NEW100_PATH = os.path.join(CKPT_DIR, \"100exp2sum.csv\")     # must contain 'ptr_pred_exp2' (+ optional 'idx')\n",
        "SRC_COL, REF_COL = \"article\", \"highlights\"\n",
        "TRUNC_ART  = 260   # print truncation for article display\n",
        "TRUNC_SUM  = 260   # print truncation for summaries\n",
        "\n",
        "def norm(s):\n",
        "    return \" \".join(str(s or \"\").split())\n",
        "\n",
        "# --- load & align (prefer 'idx' to guarantee exact same rows) ---\n",
        "base_all = pd.read_csv(BASE_PATH)\n",
        "new100   = pd.read_csv(NEW100_PATH)\n",
        "assert \"ptr_pred_exp2\" in new100.columns, \"100exp2sum.csv must have column 'ptr_pred_exp2'.\"\n",
        "\n",
        "if \"idx\" in new100.columns:\n",
        "    base_100 = base_all.iloc[new100[\"idx\"].tolist()].copy()\n",
        "    base_100[\"idx\"] = new100[\"idx\"].tolist()\n",
        "else:\n",
        "    base_100 = base_all.head(len(new100)).copy()\n",
        "    assert (base_100[SRC_COL].astype(str).values == new100[SRC_COL].astype(str).values).all(), \\\n",
        "        \"Sources not aligned and no `idx` column found.\"\n",
        "    base_100[\"idx\"] = np.arange(len(base_100))\n",
        "\n",
        "# normalize text\n",
        "for c in [SRC_COL, REF_COL, \"base_pred\"]:\n",
        "    base_100[c] = base_100[c].map(norm)\n",
        "new100[\"ptr_pred_exp2\"] = new100[\"ptr_pred_exp2\"].map(norm)\n",
        "\n",
        "# merge aligned predictions\n",
        "comb = base_100[[\"idx\", SRC_COL, REF_COL, \"base_pred\"]].copy()\n",
        "comb[\"ptr_pred_exp2\"] = new100[\"ptr_pred_exp2\"].values\n",
        "\n",
        "# --- spaCy NER ---\n",
        "import spacy\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "except OSError:\n",
        "    from spacy.cli import download\n",
        "    download(\"en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "\n",
        "def ents_of(text):\n",
        "    d = nlp(text)\n",
        "    return {(e.label_, e.text.strip().lower()) for e in d.ents if e.text.strip()}\n",
        "\n",
        "# optional: treat numbers/percents as entities for analysis\n",
        "num_re = re.compile(r\"(\\d[\\d,\\.]*%?)\")\n",
        "def nums_of(text):\n",
        "    return {(\"NUMBER\", n) for n in num_re.findall(text)}\n",
        "\n",
        "INCLUDE_NUMBERS = True\n",
        "\n",
        "def entset(text):\n",
        "    S = ents_of(text)\n",
        "    if INCLUDE_NUMBERS:\n",
        "        S |= nums_of(text)\n",
        "    return S\n",
        "\n",
        "def prf_counts(preds, refs):\n",
        "    TP=FP=FN=0\n",
        "    per = {}\n",
        "    for p, r in zip(preds, refs):\n",
        "        P, R = entset(p), entset(r)\n",
        "        inter = P & R\n",
        "        TP_i, FP_i, FN_i = len(inter), len(P - R), len(R - P)\n",
        "        TP += TP_i; FP += FP_i; FN += FN_i\n",
        "        # accumulate by type\n",
        "        for lab,_ in inter:\n",
        "            per.setdefault(lab, {\"tp\":0,\"fp\":0,\"fn\":0}); per[lab][\"tp\"] += 1\n",
        "        for lab,_ in (P - R):\n",
        "            per.setdefault(lab, {\"tp\":0,\"fp\":0,\"fn\":0}); per[lab][\"fp\"] += 1\n",
        "        for lab,_ in (R - P):\n",
        "            per.setdefault(lab, {\"tp\":0,\"fp\":0,\"fn\":0}); per[lab][\"fn\"] += 1\n",
        "    def prf(tp,fp,fn):\n",
        "        P = tp/(tp+fp) if (tp+fp)>0 else 0.0\n",
        "        R = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
        "        F = 2*P*R/(P+R) if (P+R)>0 else 0.0\n",
        "        return P,R,F\n",
        "    Pm,Rm,Fm = prf(TP,FP,FN)\n",
        "    rows=[]\n",
        "    for lab, c in per.items():\n",
        "        p,r,f = prf(c[\"tp\"], c[\"fp\"], c[\"fn\"])\n",
        "        rows.append((lab, c[\"tp\"], c[\"fp\"], c[\"fn\"], p, r, f, c[\"tp\"]+c[\"fn\"]))\n",
        "    rows.sort(key=lambda x:(-x[7], x[0]))\n",
        "    return {\"overall\": (TP,FP,FN,Pm,Rm,Fm), \"per_type\": rows}\n",
        "\n",
        "# --- per-sample entity diffs & a compact side-by-side view ---\n",
        "def ent_diff(pred, ref):\n",
        "    P, R = entset(pred), entset(ref)\n",
        "    sup = sorted(list(P & R))\n",
        "    fp  = sorted(list(P - R))\n",
        "    fn  = sorted(list(R - P))\n",
        "    return sup, fp, fn\n",
        "\n",
        "side_rows = []\n",
        "for _, row in comb.iterrows():\n",
        "    sup_b, fp_b, fn_b  = ent_diff(row[\"base_pred\"], row[REF_COL])\n",
        "    sup_p, fp_p, fn_p  = ent_diff(row[\"ptr_pred_exp2\"], row[REF_COL])\n",
        "    side_rows.append({\n",
        "        \"idx\": int(row[\"idx\"]),\n",
        "        \"article_trim\": row[SRC_COL][:TRUNC_ART],\n",
        "        \"reference_trim\": row[REF_COL][:TRUNC_SUM],\n",
        "        \"base_pred_trim\": row[\"base_pred\"][:TRUNC_SUM],\n",
        "        \"ptr_exp2_trim\":  row[\"ptr_pred_exp2\"][:TRUNC_SUM],\n",
        "        \"B_sup\": len(sup_b), \"B_fp\": len(fp_b), \"B_fn\": len(fn_b),\n",
        "        \"P_sup\": len(sup_p), \"P_fp\": len(fp_p), \"P_fn\": len(fn_p),\n",
        "        \"B_sup_list\": sup_b, \"B_fp_list\": fp_b, \"B_fn_list\": fn_b,\n",
        "        \"P_sup_list\": sup_p, \"P_fp_list\": fp_p, \"P_fn_list\": fn_p,\n",
        "        \"ref_ent_list\": sorted(list(entset(row[REF_COL])))\n",
        "    })\n",
        "\n",
        "side = pd.DataFrame(side_rows).sort_values(\"idx\").reset_index(drop=True)\n",
        "\n",
        "# save full per-sample comparison CSV (includes entity lists)\n",
        "OUT_CSV = os.path.join(CKPT_DIR, \"ptr_exp2_vs_base_samples_100.csv\")\n",
        "side.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
        "print(f\"[saved] {OUT_CSV}\")\n",
        "\n",
        "# print a readable side-by-side for the first K samples (adjust K)\n",
        "K = 10\n",
        "for i in range(min(K, len(side))):\n",
        "    r = side.iloc[i]\n",
        "    print(f\"\\n[{int(r['idx'])}] =======================\")\n",
        "    print(f\"ARTICLE   : {r['article_trim']}\")\n",
        "    print(f\"REFERENCE : {r['reference_trim']}\")\n",
        "    print(f\"BASE      : {r['base_pred_trim']}\")\n",
        "    print(f\"PTR-exp2  : {r['ptr_exp2_trim']}\")\n",
        "    print(f\"  ENTITIES — BASE  sup={r['B_sup']} fp={r['B_fp']} fn={r['B_fn']}\")\n",
        "    print(f\"    sup: {r['B_sup_list']}\")\n",
        "    print(f\"    fp : {r['B_fp_list']}\")\n",
        "    print(f\"    fn : {r['B_fn_list']}\")\n",
        "    print(f\"  ENTITIES — PTR2  sup={r['P_sup']} fp={r['P_fp']} fn={r['P_fn']}\")\n",
        "    print(f\"    sup: {r['P_sup_list']}\")\n",
        "    print(f\"    fp : {r['P_fp_list']}\")\n",
        "    print(f\"    fn : {r['P_fn_list']}\")\n",
        "\n",
        "# === ENTITIES for matched 100 rows: BASE vs PTR-exp2 (overall & per-type) ===\n",
        "refs       = comb[REF_COL].tolist()\n",
        "base_preds = comb[\"base_pred\"].tolist()\n",
        "ptr_preds  = comb[\"ptr_pred_exp2\"].tolist()\n",
        "\n",
        "B  = prf_counts(base_preds, refs)\n",
        "P2 = prf_counts(ptr_preds,  refs)\n",
        "\n",
        "def fmt_overall(name, O):\n",
        "    TP,FP,FN,Pm,Rm,Fm = O\n",
        "    print(f\"\\n=== [{name}] ENTITY (overall) ===\")\n",
        "    print(f\"TP={TP}  FP={FP}  FN={FN}\")\n",
        "    print(f\"Precision: {Pm:.6f}\\nRecall:    {Rm:.6f}\\nF1:        {Fm:.6f}\")\n",
        "\n",
        "def fmt_table(name, rows):\n",
        "    print(f\"\\n=== [{name}] ENTITY (by type) ===\")\n",
        "    for lab,tp,fp,fn,p,r,f,_support in rows:\n",
        "        print(f\"{lab:<12s} tp={tp:3d} fp={fp:3d} fn={fn:3d}  P={p:.3f} R={r:.3f} F1={f:.3f}\")\n",
        "\n",
        "fmt_overall(\"BASE\", B[\"overall\"])\n",
        "fmt_table(\"BASE\", B[\"per_type\"])\n",
        "fmt_overall(\"PTR exp2\", P2[\"overall\"])\n",
        "fmt_table(\"PTR exp2\", P2[\"per_type\"])\n",
        "\n",
        "# quick delta summary overall\n",
        "_,_,_, Pb,Rb,Fb = B[\"overall\"]\n",
        "_,_,_, Pp,Rp,Fp = P2[\"overall\"]\n",
        "print(\"\\n=== [PTR exp2 - BASE] ENTITY (overall delta) ===\")\n",
        "print(f\"ΔPrecision={Pp-Pb:+.6f}  ΔRecall={Rp-Rb:+.6f}  ΔF1={Fp-Fb:+.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cup415dsz9Pv"
      },
      "source": [
        "Your entity metric changed vs the earlier batch: this run includes numbers as entities (INCLUDE_NUMBERS=True in the viewer), which increases FPs and depresses precision.\n",
        "\n",
        "Decoding still allows capitalized tokens and numbers not present in the source, causing errors like “Ricardo Ronaldo” and spurious dates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V5Q48mP0Gx6"
      },
      "source": [
        "# Exp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158,
          "referenced_widgets": [
            "2809acea0633460eb90c0d4fac6357ce"
          ]
        },
        "id": "diALimd7z89z",
        "outputId": "995b46f0-a717-44f8-cb3d-2d224a8a053a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2809acea0633460eb90c0d4fac6357ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "PTR-exp5 (100):   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[saved] /content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000/100exp5sum.csv\n",
            "=== LENGTH (words) — PTR-exp5 (100) ===\n",
            "mean= 66.31  median= 67.0\n"
          ]
        }
      ],
      "source": [
        "# === Pointer decode — precision-first + length control (exp5) ===\n",
        "# Saves /ckpt_step4000/100exp5sum.csv with column 'ptr_pred_exp5'\n",
        "import os, re, gc, math, torch, torch.nn as nn, torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers.modeling_outputs import BaseModelOutput\n",
        "\n",
        "CKPT_DIR = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000\"\n",
        "BASE_CSV = os.path.join(CKPT_DIR, \"iksumpointer.csv\")\n",
        "SRC_COL, REF_COL = \"article\", \"highlights\"\n",
        "N = 100\n",
        "\n",
        "# ---- precision + length knobs ----\n",
        "NUM_BEAMS          = 5\n",
        "MIN_NEW            = 24          # shorter minimum -> closer to median 54\n",
        "MAX_NEW            = 90\n",
        "NO_REPEAT          = 6\n",
        "LENGTH_PENALTY     = 3.0         # stronger brevity pressure\n",
        "MAX_SRC_LEN        = 400\n",
        "GATE_BIAS          = -0.10       # tilt toward copy (lower p_gen)\n",
        "\n",
        "COV_ALPHA          = 0.15        # more anti-drift\n",
        "RERANK_K           = 5\n",
        "PEN_UNSUP_ENT      = 1.6         # heavy precision bias\n",
        "PEN_UNSUP_NUM      = 1.4\n",
        "BONUS_SUP_PERSONGPE= 0.05\n",
        "\n",
        "# HARD precision constraints\n",
        "HARD_OOS_PROPN     = True        # block Titlecase tokens not in source\n",
        "HARD_OOS_NUM       = True        # block digit tokens not in source\n",
        "\n",
        "# Length prior (word-count window ~ refs: mean≈58, median≈54)\n",
        "LEN_TARGET_LOW     = 48          # start encouraging EOS\n",
        "LEN_TARGET_MID     = 54          # median-ish\n",
        "LEN_TARGET_HIGH    = 62          # end of \"good\" window\n",
        "EOS_BONUS_MAX      = 3.0         # max EOS boost inside window\n",
        "EARLY_EOS_PEN      = 2.5         # discourage EOS before LEN_TARGET_LOW\n",
        "EOS_PUNCT_BONUS    = 0.9         # extra EOS boost if last token ends a sentence\n",
        "\n",
        "# light span-copy (kept modest so length prior can act)\n",
        "SPAN_MAX           = 6\n",
        "COPY_ENT_MIN_ATTN  = 0.35\n",
        "ATTN_ENTROPY_MAX   = 2.3\n",
        "COOLDOWN_STEPS     = 2\n",
        "COPY_BONUS         = 1.0         # \"hard-ish\": big bonus to next source token in span\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device == \"cuda\":\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "USE_FP16 = (device == \"cuda\")\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "tok  = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
        "base = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR, attn_implementation=\"eager\").to(device).eval()\n",
        "\n",
        "# ---- pointer wrapper ----\n",
        "class CopyAwareBart(nn.Module):\n",
        "    def __init__(self, base_model, tokenizer, eps=1e-8, gate_bias=0.0):\n",
        "        super().__init__()\n",
        "        self.base = base_model\n",
        "        self.tok  = tokenizer\n",
        "        self.eps  = float(eps)\n",
        "        H = self.base.config.d_model\n",
        "        self.p_gen_linear = nn.Linear(3*H, 1)\n",
        "        self.gate_bias = float(gate_bias)\n",
        "        self._last_p_copy_mean = self._last_p_gen_mean = None\n",
        "\n",
        "    def _mix_pointer(self, out, input_ids, attention_mask, decoder_input_ids):\n",
        "        logits = out.logits\n",
        "        ca = out.cross_attentions[-1].mean(1)          # [B,T,S]\n",
        "        if attention_mask is not None:\n",
        "            ca = ca.masked_fill(attention_mask[:,None,:]==0, 0.0)\n",
        "        attn = ca / ca.sum(-1, keepdim=True).clamp_min(self.eps)\n",
        "\n",
        "        dec_hid = out.decoder_hidden_states[-1]\n",
        "        ctx     = torch.bmm(attn, out.encoder_last_hidden_state)\n",
        "        dec_inp = self.base.get_input_embeddings()(decoder_input_ids)\n",
        "        gate_in = torch.cat([dec_hid, ctx, dec_inp], dim=-1)\n",
        "        p_gen   = torch.sigmoid(self.p_gen_linear(gate_in) + self.gate_bias).squeeze(-1)\n",
        "        p_copy  = 1.0 - p_gen\n",
        "\n",
        "        copy_probs = torch.zeros_like(logits)\n",
        "        idx = input_ids.unsqueeze(1).expand(input_ids.size(0), logits.size(1), input_ids.size(1))\n",
        "        copy_probs.scatter_add_(2, idx, attn)\n",
        "\n",
        "        vocab_logp = torch.log_softmax(logits, dim=-1)\n",
        "        logp_final = torch.logaddexp(\n",
        "            vocab_logp + (p_gen + 1e-8).log().unsqueeze(-1),\n",
        "            (copy_probs + 1e-8).log() + (p_copy + 1e-8).log().unsqueeze(-1)\n",
        "        )\n",
        "        self._last_p_copy_mean = float(p_copy.detach().mean().cpu())\n",
        "        self._last_p_gen_mean  = float(p_gen.detach().mean().cpu())\n",
        "        return logp_final, attn\n",
        "\n",
        "model_ptr = CopyAwareBart(base, tok, gate_bias=GATE_BIAS).to(device).eval()\n",
        "\n",
        "# optional pointer head load\n",
        "ph_path = os.path.join(CKPT_DIR, \"pointer_head.pt\")\n",
        "if os.path.exists(ph_path):\n",
        "    sd = torch.load(ph_path, map_location=\"cpu\")\n",
        "    if isinstance(sd, dict) and \"p_gen_linear\" in sd:\n",
        "        model_ptr.p_gen_linear.load_state_dict(sd[\"p_gen_linear\"])\n",
        "    elif isinstance(sd, dict) and all(k in {\"weight\",\"bias\"} for k in sd):\n",
        "        model_ptr.p_gen_linear.load_state_dict(sd)\n",
        "\n",
        "# ---- helpers ----\n",
        "def _ban_repeating_ngrams(logits: torch.Tensor, seq: torch.Tensor, ngram: int):\n",
        "    if ngram <= 0: return\n",
        "    toks = seq[0].tolist() if seq.dim()==2 else seq.tolist()\n",
        "    if len(toks) < ngram-1: return\n",
        "    prefix = tuple(toks[-(ngram - 1):])\n",
        "    banned = set()\n",
        "    for i in range(len(toks) - ngram + 1):\n",
        "        if tuple(toks[i:i+ngram-1]) == prefix:\n",
        "            banned.add(toks[i+ngram-1])\n",
        "    for t in banned: logits[t] = -1e9\n",
        "\n",
        "# vocabulary properties\n",
        "V = tok.vocab_size\n",
        "TOKS = tok.convert_ids_to_tokens(list(range(V)))\n",
        "def _clean(s): return s.replace(\"Ġ\",\"\").replace(\"▁\",\"\")\n",
        "SPECIAL = torch.zeros(V, dtype=torch.bool, device=device)\n",
        "for sp in tok.all_special_ids: SPECIAL[sp] = True\n",
        "IS_PUNCT = torch.tensor([bool(re.fullmatch(r\"[.,;:!?]+\", _clean(t))) for t in TOKS],\n",
        "                        dtype=torch.bool, device=device)\n",
        "IS_TITLE = torch.tensor([(lambda s: (s[:1].isupper() and not s.isupper()))(_clean(t)) for t in TOKS],\n",
        "                        dtype=torch.bool, device=device)\n",
        "HAS_DIGIT= torch.tensor([any(ch.isdigit() for ch in _clean(t)) for t in TOKS],\n",
        "                        dtype=torch.bool, device=device)\n",
        "BEGINS_WORD = torch.tensor([(t.startswith(\"Ġ\") or t.startswith(\"▁\")) and (not IS_PUNCT[i]) and (not SPECIAL[i])\n",
        "                            for i,t in enumerate(TOKS)], dtype=torch.bool, device=device)\n",
        "\n",
        "# presence masks from source\n",
        "num_re = re.compile(r\"(\\d[\\d,\\.]*%?)\")\n",
        "def _src_presence(src_ids_1xS, src_txt):\n",
        "    ids = src_ids_1xS[0].tolist()\n",
        "    present = torch.zeros(V, dtype=torch.bool, device=device)\n",
        "    if ids:\n",
        "        present[torch.tensor(list(set(ids)), device=device)] = True\n",
        "    num_tids = set()\n",
        "    for n in set(num_re.findall(src_txt)):\n",
        "        num_tids.update(tok(n, add_special_tokens=False).input_ids)\n",
        "    num_present = torch.zeros(V, dtype=torch.bool, device=device)\n",
        "    if num_tids:\n",
        "        num_present[torch.tensor(list(num_tids), device=device)] = True\n",
        "    return present, num_present\n",
        "\n",
        "# approximate word count from token ids (fast, no decode)\n",
        "def approx_wc(seq_ids_1xL):\n",
        "    ids = seq_ids_1xL[0]\n",
        "    wc = int(BEGINS_WORD[ids].sum().item())\n",
        "    if wc == 0:\n",
        "        # fallback: count 1 for first non-special/non-punct content token\n",
        "        for tid in ids.tolist():\n",
        "            if not SPECIAL[tid] and not IS_PUNCT[tid]:\n",
        "                wc = 1; break\n",
        "    return wc\n",
        "\n",
        "def last_is_sentence_end(seq_ids_1xL):\n",
        "    tid = seq_ids_1xL[0, -1].item()\n",
        "    s = _clean(TOKS[tid])\n",
        "    return s in {\".\",\"!\",\"?\"}\n",
        "\n",
        "# spaCy-only for rerank\n",
        "import spacy\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "except OSError:\n",
        "    from spacy.cli import download\n",
        "    download(\"en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "\n",
        "def ents_set(text):\n",
        "    d = nlp(text)\n",
        "    return {(e.label_, e.text.strip().lower()) for e in d.ents if e.text.strip()}\n",
        "\n",
        "def rerank_score(hyp, base_norm_score, src_ents, src_text):\n",
        "    hyp_ents = ents_set(hyp)\n",
        "    unsup = len(hyp_ents - src_ents)\n",
        "    sup   = len(hyp_ents & src_ents)\n",
        "    src_nums = set(num_re.findall(src_text))\n",
        "    hyp_nums = set(num_re.findall(hyp))\n",
        "    unsup_num = len([n for n in hyp_nums if n not in src_nums])\n",
        "    sup_person_gpe = sum(1 for (lab,_) in (hyp_ents & src_ents) if lab in {\"PERSON\",\"GPE\"})\n",
        "    return (base_norm_score\n",
        "            - PEN_UNSUP_ENT*unsup\n",
        "            - PEN_UNSUP_NUM*unsup_num\n",
        "            + BONUS_SUP_PERSONGPE*sup_person_gpe)\n",
        "\n",
        "# ---- decode: precision guards + EOS length prior + \"hard-ish\" span copy ----\n",
        "@torch.no_grad()\n",
        "def generate_prec_len(text: str) -> str:\n",
        "    enc = tok([text], return_tensors=\"pt\", truncation=True, max_length=MAX_SRC_LEN).to(device)\n",
        "    src_ids, src_mask = enc.input_ids, enc.attention_mask\n",
        "    with torch.amp.autocast('cuda', dtype=torch.float16, enabled=USE_FP16):\n",
        "        enc_out = base.model.get_encoder()(input_ids=src_ids, attention_mask=src_mask,\n",
        "                                           output_attentions=False, return_dict=True)\n",
        "    src_txt = tok.batch_decode(src_ids, skip_special_tokens=True)[0]\n",
        "    src_present, src_num_present = _src_presence(src_ids, src_txt)\n",
        "\n",
        "    start_id = getattr(base.config, \"decoder_start_token_id\", None) or (tok.bos_token_id or tok.eos_token_id)\n",
        "    eos_id   = tok.eos_token_id\n",
        "\n",
        "    # beam state: (seq, norm, raw, done, cov, copy_run_len, next_src_pos, cooldown)\n",
        "    beams = [(torch.tensor([[start_id]], device=device, dtype=torch.long),\n",
        "              0.0, 0.0, False,\n",
        "              torch.zeros(src_ids.size(1), device=device, dtype=enc_out.last_hidden_state.dtype),\n",
        "              0, -1, 0)]\n",
        "    finished = []\n",
        "\n",
        "    for step in range(MAX_NEW):\n",
        "        act = [(i,b) for i,b in enumerate(beams) if not b[3]]\n",
        "        if not act: break\n",
        "\n",
        "        seqs = torch.cat([beams[i][0] for i,_ in act], dim=0)\n",
        "        K = seqs.size(0)\n",
        "        enc_exp = BaseModelOutput(last_hidden_state=enc_out.last_hidden_state.expand(K, -1, -1))\n",
        "        src_mask_exp = src_mask.expand(K, -1)\n",
        "\n",
        "        with torch.amp.autocast('cuda', dtype=torch.float16, enabled=USE_FP16):\n",
        "            out = base(\n",
        "                encoder_outputs=enc_exp,\n",
        "                attention_mask=src_mask_exp,\n",
        "                decoder_input_ids=seqs,\n",
        "                output_attentions=True, output_hidden_states=True,\n",
        "                use_cache=False, return_dict=True,\n",
        "            )\n",
        "            logp_final, attn = model_ptr._mix_pointer(out, src_ids.expand(K, -1), src_mask_exp, seqs)\n",
        "\n",
        "        step_logits_all = logp_final[:, -1, :].clone().float()\n",
        "        attn_t_all      = attn[:, -1, :]  # [K,S]\n",
        "\n",
        "        new_beams = []\n",
        "        for j, (beam_idx, _) in enumerate(act):\n",
        "            step_logits = step_logits_all[j]\n",
        "            prev_seq, prev_norm, prev_raw, prev_done, prev_cov, copy_run, next_pos, cooldown = beams[beam_idx]\n",
        "\n",
        "            # min length\n",
        "            if step + 1 < MIN_NEW and eos_id is not None:\n",
        "                step_logits[eos_id] = -1e9\n",
        "\n",
        "            _ban_repeating_ngrams(step_logits, prev_seq, NO_REPEAT)\n",
        "\n",
        "            # normalize attention for confidence checks\n",
        "            attn_t = attn_t_all[j]\n",
        "            attn_t = attn_t / attn_t.sum().clamp_min(1e-8)\n",
        "            ent = float((-attn_t.clamp_min(1e-8) * attn_t.clamp_min(1e-8).log()).sum().item())\n",
        "\n",
        "            # --------- precision: HARD OOS blocks ----------\n",
        "            if HARD_OOS_PROPN:\n",
        "                oos_propn = (~src_present) & IS_TITLE & (~SPECIAL)\n",
        "                step_logits[oos_propn] = -1e9\n",
        "            if HARD_OOS_NUM:\n",
        "                oos_num = (~src_num_present) & HAS_DIGIT & (~SPECIAL)\n",
        "                step_logits[oos_num] = -1e9\n",
        "\n",
        "            # --------- \"hard-ish\" span copy mode ----------\n",
        "            forced_tid = None\n",
        "            if next_pos >= 0 and copy_run < SPAN_MAX:\n",
        "                # continue current span if still valid\n",
        "                if next_pos < src_ids.size(1):\n",
        "                    forced_tid = int(src_ids[0, next_pos].item())\n",
        "                    step_logits[forced_tid] += COPY_BONUS  # big push to stay in-span\n",
        "                else:\n",
        "                    next_pos = -1\n",
        "\n",
        "            if (next_pos < 0) and (cooldown == 0) and (ent <= ATTN_ENTROPY_MAX):\n",
        "                last_tid = prev_seq[0, -1].item()\n",
        "                poss = (src_ids[0] == last_tid).nonzero(as_tuple=True)[0]\n",
        "                if len(poss) > 0:\n",
        "                    # pick occurrence with max current attention\n",
        "                    best_idx = int(poss[torch.argmax(attn_t[poss])].item())\n",
        "                    best_w   = float(attn_t[best_idx].item())\n",
        "                    if best_w >= COPY_ENT_MIN_ATTN and best_idx + 1 < src_ids.size(1):\n",
        "                        forced_tid = int(src_ids[0, best_idx + 1].item())\n",
        "                        step_logits[forced_tid] += COPY_BONUS\n",
        "                        next_pos = best_idx + 1\n",
        "\n",
        "            # --------- length prior: EOS shaping ----------\n",
        "            wc = approx_wc(prev_seq)\n",
        "            if eos_id is not None:\n",
        "                if wc < LEN_TARGET_LOW:\n",
        "                    # too early to stop\n",
        "                    step_logits[eos_id] -= EARLY_EOS_PEN\n",
        "                else:\n",
        "                    # ramp EOS bonus through [LOW..HIGH]\n",
        "                    t = min(1.0, max(0.0, (wc - LEN_TARGET_LOW) / max(1.0, (LEN_TARGET_HIGH - LEN_TARGET_LOW))))\n",
        "                    step_logits[eos_id] += EOS_BONUS_MAX * t\n",
        "                    # extra nudge if we just ended a sentence and we're around the median\n",
        "                    if wc >= LEN_TARGET_MID and last_is_sentence_end(prev_seq):\n",
        "                        step_logits[eos_id] += EOS_PUNCT_BONUS\n",
        "\n",
        "            # expand beams\n",
        "            topk_logp, topk_ids = torch.topk(step_logits, k=NUM_BEAMS)\n",
        "            cov_pen = COV_ALPHA * torch.min(attn_t, prev_cov).sum().item()\n",
        "\n",
        "            for k in range(NUM_BEAMS):\n",
        "                tid  = topk_ids[k].view(1,1)\n",
        "                nseq = torch.cat([prev_seq, tid], dim=1)\n",
        "                nfin = (tid.item() == eos_id)\n",
        "                raw  = prev_raw + float(topk_logp[k].item()) - cov_pen\n",
        "                lp   = ((5.0 + nseq.size(1))**LENGTH_PENALTY) / ((5.0 + 1.0)**LENGTH_PENALTY)\n",
        "                nsc  = raw / lp\n",
        "                ncov = prev_cov + attn_t\n",
        "\n",
        "                # update span-copy state\n",
        "                n_copy_run, n_next_pos, n_cooldown = 0, -1, max(cooldown-1, 0)\n",
        "                if forced_tid is not None and tid.item() == forced_tid:\n",
        "                    n_copy_run = copy_run + 1\n",
        "                    n_next_pos = next_pos + 1\n",
        "                    n_cooldown = 0 if n_copy_run < SPAN_MAX else COOLDOWN_STEPS\n",
        "                else:\n",
        "                    # span broken -> cooldown\n",
        "                    if copy_run > 0:\n",
        "                        n_cooldown = COOLDOWN_STEPS\n",
        "\n",
        "                new_beams.append((nseq, nsc, raw, nfin, ncov, n_copy_run, n_next_pos, n_cooldown))\n",
        "\n",
        "        new_beams.sort(key=lambda x: x[1], reverse=True)\n",
        "        beams = new_beams[:NUM_BEAMS]\n",
        "\n",
        "        for seq, nsc, raw, done, cov, cr, np_, cd in beams:\n",
        "            if done:\n",
        "                finished.append((seq, nsc))\n",
        "        finished = sorted(finished, key=lambda x: x[1], reverse=True)[:RERANK_K]\n",
        "\n",
        "        if all(b[3] for b in beams): break\n",
        "        if (step & 7) == 0:\n",
        "            del out, logp_final, attn, step_logits_all, attn_t_all\n",
        "            torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "    if not finished:\n",
        "        finished = [(max(beams, key=lambda x: x[1])[0], max(beams, key=lambda x: x[1])[1])]\n",
        "\n",
        "    # rerank with strong unsupported penalties\n",
        "    src_ents = ents_set(src_txt)\n",
        "    cands = []\n",
        "    for seq, nsc in finished:\n",
        "        hyp = tok.batch_decode(seq.detach().cpu(), skip_special_tokens=True)[0]\n",
        "        s = rerank_score(hyp, nsc, src_ents, src_txt)\n",
        "        cands.append((s, hyp))\n",
        "    cands.sort(key=lambda x: x[0], reverse=True)\n",
        "    return cands[0][1]\n",
        "\n",
        "# ---- run on first 100 & save + print length stats ----\n",
        "def norm(s): return \" \".join(str(s or \"\").split())\n",
        "\n",
        "df = pd.read_csv(BASE_CSV).head(N).copy()\n",
        "df[SRC_COL] = df[SRC_COL].fillna(\"\").astype(str).str.replace(r\"\\s+\",\" \",regex=True).str.strip()\n",
        "\n",
        "outs = []\n",
        "for art in tqdm(df[SRC_COL].tolist(), desc=\"PTR-exp5 (100)\"):\n",
        "    outs.append(generate_prec_len(art))\n",
        "\n",
        "out_path = os.path.join(CKPT_DIR, \"100exp5sum.csv\")\n",
        "pd.DataFrame({\"idx\": list(range(len(outs))), SRC_COL: df[SRC_COL].tolist(), \"ptr_pred_exp5\": outs}).to_csv(out_path, index=False, encoding=\"utf-8\")\n",
        "print(f\"[saved] {out_path}\")\n",
        "\n",
        "# quick length summary (words)\n",
        "lens = [len(norm(x).split()) for x in outs]\n",
        "import numpy as np\n",
        "print(\"=== LENGTH (words) — PTR-exp5 (100) ===\")\n",
        "print(\"mean=\", float(np.mean(lens)), \" median=\", float(np.median(lens)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXRM-wM_yjj9",
        "outputId": "7e95a0b7-f6ea-4d79-fa7d-ea6e4ef1b61c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== LENGTH (words) ===\n",
            "[BASE] mean= 61.02 median= 59.0\n",
            "[PTR*] mean= 66.31 median= 67.0\n",
            "[Δ] mean= 5.29\n",
            "\n",
            "=== ROUGE (mean over 100) ===\n",
            "[BASE] {'r1_p': 0.4092, 'r1_r': 0.4721, 'r1_f': 0.4227, 'r2_p': 0.1958, 'r2_r': 0.2283, 'r2_f': 0.2035, 'rl_p': 0.2873, 'rl_r': 0.3334, 'rl_f': 0.2978}\n",
            "[PTR*] {'r1_p': 0.3632, 'r1_r': 0.4557, 'r1_f': 0.392, 'r2_p': 0.1539, 'r2_r': 0.1925, 'r2_f': 0.1669, 'rl_p': 0.2422, 'rl_r': 0.3087, 'rl_f': 0.2637}\n",
            "[PTR* - BASE] {'r1_p': -0.046, 'r1_r': -0.0164, 'r1_f': -0.0307, 'r2_p': -0.0419, 'r2_r': -0.0358, 'r2_f': -0.0366, 'rl_p': -0.0451, 'rl_r': -0.0247, 'rl_f': -0.0341}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== ENTITIES (overall, micro) — NER only ===\n",
            "[BASE] {'precision': 0.3605, 'recall': 0.363, 'f1': 0.3618}\n",
            "[PTR*] {'precision': 0.3277, 'recall': 0.3452, 'f1': 0.3362}\n",
            "[PTR* - BASE] {'precision': -0.0328, 'recall': -0.0178, 'f1': -0.0256}\n",
            "\n",
            "=== ENTITIES (overall, micro) — NER + NUMBERS ===\n",
            "[BASE] {'precision': 0.3534, 'recall': 0.3558, 'f1': 0.3546}\n",
            "[PTR*] {'precision': 0.3221, 'recall': 0.2907, 'f1': 0.3056}\n",
            "[PTR* - BASE] {'precision': -0.0313, 'recall': -0.0651, 'f1': -0.049}\n"
          ]
        }
      ],
      "source": [
        "# === EVAL: ROUGE + ENTITY (BASE vs PTR-exp5) on matched 100 ===\n",
        "!pip -q install rouge-score==0.1.2 spacy==3.7.5\n",
        "import os, re, pandas as pd, numpy as np\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "CKPT_DIR     = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000\"\n",
        "BASE_PATH    = os.path.join(CKPT_DIR, \"iksumpointer.csv\")     # has source, reference, base_pred\n",
        "NEW_PATH     = os.path.join(CKPT_DIR, \"100exp5sum.csv\")       # has ptr_pred_exp5 (+ optional idx)\n",
        "SRC_COL, REF_COL = \"article\", \"highlights\"\n",
        "PTR_COL      = \"ptr_pred_exp5\"\n",
        "\n",
        "def norm(s): return \" \".join(str(s or \"\").split())\n",
        "\n",
        "# ---- load & align (prefer idx if present) ----\n",
        "base_all = pd.read_csv(BASE_PATH)\n",
        "new      = pd.read_csv(NEW_PATH)\n",
        "assert PTR_COL in new.columns, f\"{os.path.basename(NEW_PATH)} must have column '{PTR_COL}'.\"\n",
        "\n",
        "if \"idx\" in new.columns:\n",
        "    base_100 = base_all.iloc[new[\"idx\"].tolist()].copy()\n",
        "else:\n",
        "    base_100 = base_all.head(len(new)).copy()\n",
        "    # sanity if no idx: require exact article alignment\n",
        "    assert (base_100[SRC_COL].astype(str).values == new[SRC_COL].astype(str).values).all(), \\\n",
        "        \"Sources not aligned and no `idx` column found.\"\n",
        "\n",
        "for c in [SRC_COL, REF_COL, \"base_pred\"]:\n",
        "    base_100[c] = base_100[c].map(norm)\n",
        "new[PTR_COL] = new[PTR_COL].map(norm)\n",
        "\n",
        "refs       = base_100[REF_COL].tolist()\n",
        "base_preds = base_100[\"base_pred\"].tolist()\n",
        "ptr_preds  = new[PTR_COL].tolist()\n",
        "\n",
        "# ---- LENGTH ----\n",
        "def lens(words): return [len(x.split()) for x in words]\n",
        "bl, pl = lens(base_preds), lens(ptr_preds)\n",
        "\n",
        "print(\"=== LENGTH (words) ===\")\n",
        "print(f\"[BASE] mean= {np.mean(bl):.2f} median= {np.median(bl):.1f}\")\n",
        "print(f\"[PTR*] mean= {np.mean(pl):.2f} median= {np.median(pl):.1f}\")\n",
        "print(f\"[Δ] mean= {np.mean(np.array(pl)-np.array(bl)):.2f}\\n\")\n",
        "\n",
        "# ---- ROUGE ----\n",
        "scorer = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\"], use_stemmer=True)\n",
        "\n",
        "def rouge_mean(preds, refs):\n",
        "    rows=[]\n",
        "    for p,r in zip(preds, refs):\n",
        "        sc = scorer.score(r, p)\n",
        "        rows.append({\n",
        "            \"r1_p\": sc[\"rouge1\"].precision, \"r1_r\": sc[\"rouge1\"].recall, \"r1_f\": sc[\"rouge1\"].fmeasure,\n",
        "            \"r2_p\": sc[\"rouge2\"].precision, \"r2_r\": sc[\"rouge2\"].recall, \"r2_f\": sc[\"rouge2\"].fmeasure,\n",
        "            \"rl_p\": sc[\"rougeL\"].precision, \"rl_r\": sc[\"rougeL\"].recall, \"rl_f\": sc[\"rougeL\"].fmeasure,\n",
        "        })\n",
        "    R = pd.DataFrame(rows).mean(numeric_only=True)\n",
        "    return {k: float(f\"{v:.4f}\") for k,v in R.items()}\n",
        "\n",
        "rb = rouge_mean(base_preds, refs)\n",
        "rp = rouge_mean(ptr_preds,  refs)\n",
        "diff = {k: float(f\"{rp[k]-rb[k]:.4f}\") for k in rb}\n",
        "\n",
        "print(\"=== ROUGE (mean over 100) ===\")\n",
        "print(\"[BASE]\", rb)\n",
        "print(\"[PTR*]\", rp)\n",
        "print(\"[PTR* - BASE]\", diff)\n",
        "\n",
        "# ---- ENTITIES ----\n",
        "import spacy\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "except OSError:\n",
        "    from spacy.cli import download\n",
        "    download(\"en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "\n",
        "def ents(text):\n",
        "    d = nlp(text);\n",
        "    return {(e.label_, e.text.strip().lower()) for e in d.ents if e.text.strip()}\n",
        "\n",
        "num_re = re.compile(r\"(\\d[\\d,\\.]*%?)\")\n",
        "def nums(text): return {(\"NUMBER\", n) for n in num_re.findall(text)}\n",
        "\n",
        "def prf_overall(preds, refs, include_numbers=False):\n",
        "    TP=FP=FN=0\n",
        "    for p,r in zip(preds, refs):\n",
        "        P = ents(p); R = ents(r)\n",
        "        if include_numbers:\n",
        "            P |= nums(p); R |= nums(r)\n",
        "        TP += len(P & R); FP += len(P - R); FN += len(R - P)\n",
        "    Prec = TP/(TP+FP) if (TP+FP)>0 else 0.0\n",
        "    Rec  = TP/(TP+FN) if (TP+FN)>0 else 0.0\n",
        "    F1   = 2*Prec*Rec/(Prec+Rec) if (Prec+Rec)>0 else 0.0\n",
        "    return {\"precision\": float(f\"{Prec:.4f}\"), \"recall\": float(f\"{Rec:.4f}\"), \"f1\": float(f\"{F1:.4f}\")}\n",
        "\n",
        "print(\"\\n=== ENTITIES (overall, micro) — NER only ===\")\n",
        "B_ner = prf_overall(base_preds, refs, include_numbers=False)\n",
        "P_ner = prf_overall(ptr_preds,  refs, include_numbers=False)\n",
        "print(\"[BASE]\", B_ner)\n",
        "print(\"[PTR*]\", P_ner)\n",
        "print(\"[PTR* - BASE]\", {k: float(f\"{P_ner[k]-B_ner[k]:.4f}\") for k in B_ner})\n",
        "\n",
        "print(\"\\n=== ENTITIES (overall, micro) — NER + NUMBERS ===\")\n",
        "B_num = prf_overall(base_preds, refs, include_numbers=True)\n",
        "P_num = prf_overall(ptr_preds,  refs, include_numbers=True)\n",
        "print(\"[BASE]\", B_num)\n",
        "print(\"[PTR*]\", P_num)\n",
        "print(\"[PTR* - BASE]\", {k: float(f\"{P_num[k]-B_num[k]:.4f}\") for k in B_num})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_pSXdfi8gaM"
      },
      "source": [
        "# Increase precision while ranking in decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsGxbBMEO8HB"
      },
      "source": [
        "Entity-and-copy–aware re-ranking (new objective).\n",
        "Each article now decodes K beams, then we pick the winner by\n",
        "logP(hyp) + α·EntF1(source,hyp) + β·CopyRate(source,hyp) − γ·RepeatFrac(hyp).\n",
        "Earlier you either took the model’s top beam or penalized a couple of things ad-hoc. Here, entity overlap with the source is rewarded (not just penalized when missing), lexical reuse is modestly rewarded, and n-gram repetition is explicitly penalized. This shifts selection toward beams that mention only supported entities and fewer hallucinations.\n",
        "\n",
        "Optional decode-time gate bias (pointer-specific; no training).\n",
        "We temporarily lower the generation gate bias (p_gen), so the pointer copies slightly more when attention is confident. You didn’t adjust the gate at inference before; this is a safe, reversible nudge that mainly improves entity precision.\n",
        "\n",
        "Stricter length discipline.\n",
        "min_new≈55, max_new≈100, length_penalty=2.0, no_repeat_ngram_size=3. Previously your pointer outputs skewed long; trimming verbosity reduces spurious/extra entities and repetitive fragments—again boosting precision.\n",
        "\n",
        "Side-by-side A/B on a fixed slice.\n",
        "For the same 100 validation articles, you now log metrics for top-beam vs re-ranked (and with/without gate bias). Earlier runs mixed settings or samples, making the precision deltas hard to isolate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHalWJ91MdbC",
        "outputId": "bb185cf5-3374-445a-a03c-b208bc50a898"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Cannot install numpy==2.0.2 and scipy==1.11.4 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# --- ENV REPAIR (recommended if TensorFlow is installed) ---\n",
        "%pip install -q --upgrade --force-reinstall \"numpy==2.0.2\" \"scipy==1.11.4\"\n",
        "import IPython, os, sys\n",
        "IPython.Application.instance().kernel.do_shutdown(True)  # restarts the kernel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "791qDWS2KXIM"
      },
      "outputs": [],
      "source": [
        "# Cell 1 — Imports without SciPy dependencies\n",
        "import os, re, math, random, json, gc\n",
        "from collections import Counter\n",
        "from typing import List, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Determinism\n",
        "SEED = 0\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def clear_gpu():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dnA45z-M7M6"
      },
      "outputs": [],
      "source": [
        "# === Fixed strict reader & strict saver ===\n",
        "import os, csv\n",
        "import pandas as pd\n",
        "\n",
        "def _strict_read(path: str, required_cols=None, nrows=None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    RFC-4180-friendly strict CSV reader with fallbacks.\n",
        "    - Try utf-8/latin-1 × C/Python engines.\n",
        "    - Do NOT silently skip bad lines, unless we must drop to Python engine.\n",
        "    - Validate required columns when provided.\n",
        "    \"\"\"\n",
        "    attempts = []\n",
        "    # 1) Fast path: utf-8 / C engine\n",
        "    for enc in (\"utf-8\", \"latin-1\"):\n",
        "        # C engine (no on_bad_lines control here)\n",
        "        try:\n",
        "            df = pd.read_csv(path, encoding=enc, engine=\"c\", nrows=nrows)\n",
        "            if required_cols:\n",
        "                missing = [c for c in required_cols if c not in df.columns]\n",
        "                if missing:\n",
        "                    raise ValueError(f\"Missing required cols: {missing}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            attempts.append(f\"[{enc}/c] {type(e).__name__}: {e}\")\n",
        "\n",
        "        # Python engine, strict (error on bad lines)\n",
        "        try:\n",
        "            df = pd.read_csv(path, encoding=enc, engine=\"python\",\n",
        "                             on_bad_lines=\"error\", nrows=nrows)\n",
        "            if required_cols:\n",
        "                missing = [c for c in required_cols if c not in df.columns]\n",
        "                if missing:\n",
        "                    raise ValueError(f\"Missing required cols: {missing}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            attempts.append(f\"[{enc}/python,error] {type(e).__name__}: {e}\")\n",
        "\n",
        "        # Python engine, last resort (skip bad lines)\n",
        "        try:\n",
        "            df = pd.read_csv(path, encoding=enc, engine=\"python\",\n",
        "                             on_bad_lines=\"skip\", nrows=nrows)\n",
        "            if required_cols:\n",
        "                missing = [c for c in required_cols if c not in df.columns]\n",
        "                if missing:\n",
        "                    raise ValueError(f\"Missing required cols: {missing}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            attempts.append(f\"[{enc}/python,skip] {type(e).__name__}: {e}\")\n",
        "\n",
        "    raise RuntimeError(\"Strict read failed. Attempts:\\n\" + \"\\n\".join(attempts))\n",
        "\n",
        "\n",
        "def save_csv_strict(df: pd.DataFrame, path: str):\n",
        "    \"\"\"\n",
        "    Atomic save with RFC-4180 settings + immediate strict re-open to verify.\n",
        "    \"\"\"\n",
        "    tmp = path + \".tmp\"\n",
        "    df.to_csv(\n",
        "        tmp,\n",
        "        index=False,\n",
        "        encoding=\"utf-8\",\n",
        "        quoting=csv.QUOTE_ALL,   # stdlib csv constants\n",
        "        escapechar=\"\\\\\",\n",
        "        lineterminator=\"\\n\"\n",
        "    )\n",
        "    # Re-open strictly to verify integrity (same columns)\n",
        "    _ = _strict_read(tmp, required_cols=list(df.columns))\n",
        "    os.replace(tmp, path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2Tvr4TL_vfO"
      },
      "outputs": [],
      "source": [
        "import os, re, math, random, json, gc\n",
        "from collections import Counter\n",
        "from typing import List, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Determinism (as per your project standard)\n",
        "SEED = 0\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def clear_gpu():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TO7eb8iiL_U9",
        "outputId": "1cba2ce8-73f3-4806-f2f9-e0639d894a6a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer vocab size: 50265\n",
            "Model params (M): 139.420416\n"
          ]
        }
      ],
      "source": [
        "# Configure one of the following:\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000\"\n",
        "HF_MODEL_NAME  = \"facebook/bart-base\"\n",
        "\n",
        "def load_tok_model(checkpoint_dir: str = \"\", hf_model_name: str = \"facebook/bart-base\"):\n",
        "    \"\"\"\n",
        "    Loads tokenizer and model from a local checkpoint (if provided) or from HF Hub.\n",
        "    Works with standard BART; if your pointer head exists in the checkpoint, it will be loaded.\n",
        "    \"\"\"\n",
        "    if checkpoint_dir and os.path.isdir(checkpoint_dir):\n",
        "        tok = AutoTokenizer.from_pretrained(checkpoint_dir, use_fast=True)\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_dir)\n",
        "    else:\n",
        "        tok = AutoTokenizer.from_pretrained(hf_model_name, use_fast=True)\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(hf_model_name)\n",
        "\n",
        "    model = model.to(DEVICE)\n",
        "    model.eval()\n",
        "    return tok, model\n",
        "\n",
        "tok, model = load_tok_model(CHECKPOINT_DIR, HF_MODEL_NAME)\n",
        "\n",
        "print(\"Tokenizer vocab size:\", len(tok))\n",
        "print(\"Model params (M):\", sum(p.numel() for p in model.parameters()) / 1e6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmiZhJcJMq31"
      },
      "outputs": [],
      "source": [
        "# --------- Optional spaCy NER  ----------\n",
        "try:\n",
        "    import spacy\n",
        "    try:\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "    except Exception:\n",
        "        nlp = None\n",
        "except Exception:\n",
        "    nlp = None\n",
        "\n",
        "_WORD_RE = re.compile(r\"[A-Za-z0-9]+(?:[-_/][A-Za-z0-9]+)*\")\n",
        "_NUM_RE  = re.compile(r\"\\d[\\d,.\\-/]*\")\n",
        "\n",
        "def _words(txt: str) -> List[str]:\n",
        "    return _WORD_RE.findall(txt.lower())\n",
        "\n",
        "def _spans_simple(txt: str) -> List[str]:\n",
        "    # Heuristic entities: numbers + title-cased multiword chunks\n",
        "    spans = set()\n",
        "    # numbers\n",
        "    for m in _NUM_RE.finditer(txt):\n",
        "        spans.add(m.group(0))\n",
        "    # title-cased multiwords as proxy for PERSON/ORG/LOC\n",
        "    toks = re.findall(r\"\\b[A-Z][a-z]+\\b\", txt)\n",
        "    for i in range(len(toks)-1):\n",
        "        if toks[i][0].isupper() and toks[i+1][0].isupper():\n",
        "            spans.add(f\"{toks[i]} {toks[i+1]}\")\n",
        "    return list(spans)\n",
        "\n",
        "def extract_entities(txt: str) -> List[str]:\n",
        "    if nlp is not None:\n",
        "        doc = nlp(txt)\n",
        "        return [ent.text for ent in doc.ents]\n",
        "    return _spans_simple(txt)\n",
        "\n",
        "def entity_prf_against_source(source: str, hyp: str) -> Tuple[float, float, float]:\n",
        "    src_ents = set([s.strip() for s in extract_entities(source) if s.strip()])\n",
        "    hyp_ents = set([s.strip() for s in extract_entities(hyp)    if s.strip()])\n",
        "    if not src_ents and not hyp_ents:\n",
        "        return 1.0, 1.0, 1.0\n",
        "    if not hyp_ents:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    tp = len(src_ents & hyp_ents)\n",
        "    prec = tp / max(1, len(hyp_ents))\n",
        "    rec  = tp / max(1, len(src_ents))\n",
        "    f1   = 0.0 if (prec + rec) == 0 else 2 * prec * rec / (prec + rec)\n",
        "    return prec, rec, f1\n",
        "\n",
        "def copy_rate(source: str, hyp: str) -> float:\n",
        "    src = set(_words(source))\n",
        "    hyp_w = _words(hyp)\n",
        "    if not hyp_w:\n",
        "        return 0.0\n",
        "    copied = sum(1 for w in hyp_w if w in src)\n",
        "    return copied / len(hyp_w)\n",
        "\n",
        "def repeat_fraction(hyp: str, n: int = 3) -> float:\n",
        "    toks = _words(hyp)\n",
        "    if len(toks) < n + 1:\n",
        "        return 0.0\n",
        "    grams = [tuple(toks[i:i+n]) for i in range(len(toks)-n+1)]\n",
        "    c = Counter(grams)\n",
        "    reps = sum(v - 1 for v in c.values() if v > 1)\n",
        "    return reps / max(1, len(grams))\n",
        "\n",
        "# ---------- Optional: gate bias context for pointer models ----------\n",
        "import contextlib\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def pointer_gate_bias(model: nn.Module, delta: float = -0.8):\n",
        "    \"\"\"\n",
        "    Lowers p_gen bias so p_copy increases during decoding (delta < 0).\n",
        "    If no gate layer is found, does nothing.\n",
        "    \"\"\"\n",
        "    target_mod = None\n",
        "    for name, mod in model.named_modules():\n",
        "        if isinstance(mod, nn.Linear) and getattr(mod, \"out_features\", None) == 1:\n",
        "            lname = name.lower()\n",
        "            if any(k in lname for k in [\"pgen\", \"p_gen\", \"gate\", \"copy_gate\", \"copygate\", \"pointer_gate\"]):\n",
        "                target_mod = mod\n",
        "                break\n",
        "    orig = None\n",
        "    try:\n",
        "        if target_mod is not None and target_mod.bias is not None:\n",
        "            orig = target_mod.bias.detach().clone()\n",
        "            with torch.no_grad():\n",
        "                target_mod.bias.data.add_(delta)\n",
        "        yield\n",
        "    finally:\n",
        "        if target_mod is not None and orig is not None:\n",
        "            with torch.no_grad():\n",
        "                target_mod.bias.data.copy_(orig)\n",
        "\n",
        "# ---------- Beam decode + rerank ----------\n",
        "@torch.no_grad()\n",
        "def decode_with_rerank(\n",
        "    model, tok, sources: List[str],\n",
        "    beams: int = 8,\n",
        "    min_new_tokens: int = 55,\n",
        "    max_new_tokens: int = 100,\n",
        "    length_penalty: float = 2.0,\n",
        "    no_repeat_ngram_size: int = 3,\n",
        "    a_ent: float = 1.2,     # entity F1 weight vs source\n",
        "    b_copy: float = 0.3,    # copy-rate weight\n",
        "    c_rep: float = 3.0      # repetition penalty weight\n",
        ") -> List[dict]:\n",
        "    model.eval()\n",
        "    out = []\n",
        "    for src in sources:\n",
        "        enc = tok([src], return_tensors=\"pt\", truncation=True, max_length=1024).to(DEVICE)\n",
        "        gen = model.generate(\n",
        "            **enc,\n",
        "            num_beams=beams,\n",
        "            num_return_sequences=beams,\n",
        "            min_new_tokens=min_new_tokens,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            length_penalty=length_penalty,\n",
        "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True\n",
        "        )\n",
        "        seqs = gen.sequences\n",
        "        scores = gen.sequences_scores  # length-normalized log-probs in HF\n",
        "\n",
        "        # top beam by HF score\n",
        "        base_idx = int(torch.argmax(scores).item())\n",
        "        hyps = [tok.decode(seqs[i], skip_special_tokens=True) for i in range(seqs.size(0))]\n",
        "\n",
        "        rerank_scores, meta = [], []\n",
        "        for hyp, logp in zip(hyps, scores.tolist()):\n",
        "            p, r, f1 = entity_prf_against_source(src, hyp)\n",
        "            cr  = copy_rate(src, hyp)\n",
        "            rep = repeat_fraction(hyp, n=3)\n",
        "            score = float(logp) + a_ent * f1 + b_copy * cr - c_rep * rep\n",
        "            rerank_scores.append(score)\n",
        "            meta.append(dict(logp=float(logp), ent_p=p, ent_r=r, ent_f1=f1, copy_rate=cr, rep_frac=rep))\n",
        "        best_idx = int(max(range(len(rerank_scores)), key=rerank_scores.__getitem__))\n",
        "\n",
        "        out.append({\n",
        "            \"source\": src,\n",
        "            \"hyp_topbeam\": hyps[base_idx],\n",
        "            \"hyp_rerank\":  hyps[best_idx],\n",
        "            \"top_logp\":    meta[base_idx][\"logp\"],\n",
        "            \"rerank_logp\": meta[best_idx][\"logp\"],\n",
        "            \"top_entP\":    meta[base_idx][\"ent_p\"],\n",
        "            \"top_entR\":    meta[base_idx][\"ent_r\"],\n",
        "            \"top_entF1\":   meta[base_idx][\"ent_f1\"],\n",
        "            \"top_copy\":    meta[base_idx][\"copy_rate\"],\n",
        "            \"top_rep\":     meta[base_idx][\"rep_frac\"],\n",
        "            \"rerank_entP\": meta[best_idx][\"ent_p\"],\n",
        "            \"rerank_entR\": meta[best_idx][\"ent_r\"],\n",
        "            \"rerank_entF1\":meta[best_idx][\"ent_f1\"],\n",
        "            \"rerank_copy\": meta[best_idx][\"copy_rate\"],\n",
        "            \"rerank_rep\":  meta[best_idx][\"rep_frac\"],\n",
        "        })\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlrrS7YTMrVL",
        "outputId": "e75e8b49-9ac7-44b4-8cc7-8d5391e0d0b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampled 100 articles. Reference column present: True\n"
          ]
        }
      ],
      "source": [
        "VAL_PATH = \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "\n",
        "df_val = _strict_read(VAL_PATH)\n",
        "assert \"article\" in df_val.columns, df_val.columns\n",
        "has_ref = \"highlights\" in df_val.columns\n",
        "\n",
        "# Deterministic sample of N articles\n",
        "N = 100\n",
        "rng = np.random.default_rng(SEED)\n",
        "idx = rng.choice(len(df_val), size=N, replace=False)\n",
        "articles = df_val.loc[idx, \"article\"].astype(str).tolist()\n",
        "refs = df_val.loc[idx, \"highlights\"].astype(str).tolist() if has_ref else None\n",
        "\n",
        "print(f\"Sampled {len(articles)} articles. Reference column present: {has_ref}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SX88qSiMysP",
        "outputId": "ac1c47af-c0a8-4baf-8347-b58da77765c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`sdpa` attention does not support `output_attentions=True` or `head_mask`. Please set your attention to `eager` if you want any of these features.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoding completed.\n"
          ]
        }
      ],
      "source": [
        "# Decoding hyperparameters (precision-first discipline)\n",
        "BEAMS = 8\n",
        "MIN_NEW = 55\n",
        "MAX_NEW = 100\n",
        "LEN_P = 2.0\n",
        "NR_NGRAM = 3\n",
        "\n",
        "# Rerank weights\n",
        "A_ENT = 1.2\n",
        "B_COPY = 0.3\n",
        "C_REP  = 3.0\n",
        "\n",
        "clear_gpu()\n",
        "\n",
        "# Pass A: rerank only\n",
        "results_A = decode_with_rerank(\n",
        "    model, tok, articles,\n",
        "    beams=BEAMS,\n",
        "    min_new_tokens=MIN_NEW,\n",
        "    max_new_tokens=MAX_NEW,\n",
        "    length_penalty=LEN_P,\n",
        "    no_repeat_ngram_size=NR_NGRAM,\n",
        "    a_ent=A_ENT, b_copy=B_COPY, c_rep=C_REP\n",
        ")\n",
        "\n",
        "# Pass B: optional gate bias (effective if your model has a p_gen gate)\n",
        "clear_gpu()\n",
        "with pointer_gate_bias(model, delta=-0.8):\n",
        "    results_B = decode_with_rerank(\n",
        "        model, tok, articles,\n",
        "        beams=BEAMS,\n",
        "        min_new_tokens=MIN_NEW,\n",
        "        max_new_tokens=MAX_NEW,\n",
        "        length_penalty=LEN_P,\n",
        "        no_repeat_ngram_size=NR_NGRAM,\n",
        "        a_ent=A_ENT, b_copy=B_COPY, c_rep=C_REP\n",
        "    )\n",
        "\n",
        "print(\"Decoding completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05lJo7D1NPD8",
        "outputId": "65078e00-08fa-4408-e9b5-6529a9f1db84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Pass A: Rerank only ===\n",
            "Top-beam  : {'entP': 0.9674, 'entR': 0.2286, 'entF1': 0.3543, 'copy_rate': 0.9929, 'repeat_frac': 0.001}\n",
            "Reranked  : {'entP': 0.975, 'entR': 0.2517, 'entF1': 0.3836, 'copy_rate': 0.994, 'repeat_frac': 0.0004}\n",
            "Δ (re-rank - top): {'entP': 0.0077, 'entR': 0.0232, 'entF1': 0.0293, 'copy_rate': 0.0011, 'repeat_frac': -0.0006}\n",
            "ROUGE (top): {'rouge1': 0.3927, 'rouge2': 0.1803, 'rougeLsum': 0.2686}\n",
            "ROUGE (rr) : {'rouge1': 0.3969, 'rouge2': 0.1814, 'rougeLsum': 0.2697}\n",
            "Δ ROUGE    : {'rouge1': 0.0042, 'rouge2': 0.0011, 'rougeLsum': 0.0011}\n",
            "\n",
            "=== Pass B: Gate-biased + rerank ===\n",
            "Top-beam  : {'entP': 0.9674, 'entR': 0.2286, 'entF1': 0.3543, 'copy_rate': 0.9929, 'repeat_frac': 0.001}\n",
            "Reranked  : {'entP': 0.975, 'entR': 0.2517, 'entF1': 0.3836, 'copy_rate': 0.994, 'repeat_frac': 0.0004}\n",
            "Δ (re-rank - top): {'entP': 0.0077, 'entR': 0.0232, 'entF1': 0.0293, 'copy_rate': 0.0011, 'repeat_frac': -0.0006}\n",
            "ROUGE (top): {'rouge1': 0.3927, 'rouge2': 0.1803, 'rougeLsum': 0.2686}\n",
            "ROUGE (rr) : {'rouge1': 0.3969, 'rouge2': 0.1814, 'rougeLsum': 0.2697}\n",
            "Δ ROUGE    : {'rouge1': 0.0042, 'rouge2': 0.0011, 'rougeLsum': 0.0011}\n"
          ]
        }
      ],
      "source": [
        "def _agg_metrics(res_list: List[dict], pick_key: str = \"hyp_rerank\") -> dict:\n",
        "    entP, entR, entF1, copyR, repF = [], [], [], [], []\n",
        "    for r in res_list:\n",
        "        if pick_key == \"hyp_topbeam\":\n",
        "            entP.append(r[\"top_entP\"]); entR.append(r[\"top_entR\"]); entF1.append(r[\"top_entF1\"])\n",
        "            copyR.append(r[\"top_copy\"]); repF.append(r[\"top_rep\"])\n",
        "        else:\n",
        "            entP.append(r[\"rerank_entP\"]); entR.append(r[\"rerank_entR\"]); entF1.append(r[\"rerank_entF1\"])\n",
        "            copyR.append(r[\"rerank_copy\"]); repF.append(r[\"rerank_rep\"])\n",
        "    return {\n",
        "        \"entP\": float(np.mean(entP)),\n",
        "        \"entR\": float(np.mean(entR)),\n",
        "        \"entF1\": float(np.mean(entF1)),\n",
        "        \"copy_rate\": float(np.mean(copyR)),\n",
        "        \"repeat_frac\": float(np.mean(repF)),\n",
        "    }\n",
        "\n",
        "def _rouge_scores(hyps: List[str], refs: List[str]) -> dict:\n",
        "    sc = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\"], use_stemmer=True)\n",
        "    r1, r2, rl = [], [], []\n",
        "    for h, g in zip(hyps, refs):\n",
        "        s = sc.score(g, h)\n",
        "        r1.append(s[\"rouge1\"].fmeasure)\n",
        "        r2.append(s[\"rouge2\"].fmeasure)\n",
        "        rl.append(s[\"rougeLsum\"].fmeasure)\n",
        "    return {\"rouge1\": float(np.mean(r1)), \"rouge2\": float(np.mean(r2)), \"rougeLsum\": float(np.mean(rl))}\n",
        "\n",
        "def summarize_run(name: str, res: List[dict], refs: Optional[List[str]] = None):\n",
        "    base   = _agg_metrics(res, \"hyp_topbeam\")\n",
        "    rerank = _agg_metrics(res, \"hyp_rerank\")\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(\"Top-beam  :\", {k: round(v, 4) for k,v in base.items()})\n",
        "    print(\"Reranked  :\", {k: round(v, 4) for k,v in rerank.items()})\n",
        "    print(\"Δ (re-rank - top):\", {k: round(rerank[k]-base[k], 4) for k in base.keys()})\n",
        "    if refs is not None:\n",
        "        hyps_top = [r[\"hyp_topbeam\"] for r in res]\n",
        "        hyps_rr  = [r[\"hyp_rerank\"]  for r in res]\n",
        "        rouge_top = _rouge_scores(hyps_top, refs)\n",
        "        rouge_rr  = _rouge_scores(hyps_rr,  refs)\n",
        "        print(\"ROUGE (top):\", {k: round(v, 4) for k,v in rouge_top.items()})\n",
        "        print(\"ROUGE (rr) :\", {k: round(v, 4) for k,v in rouge_rr.items()})\n",
        "        print(\"Δ ROUGE    :\", {k: round(rouge_rr[k]-rouge_top[k], 4) for k in rouge_top.keys()})\n",
        "\n",
        "# Summaries\n",
        "summarize_run(\"Pass A: Rerank only\", results_A, refs)\n",
        "summarize_run(\"Pass B: Gate-biased + rerank\", results_B, refs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqgQ2vKiVXS2"
      },
      "source": [
        "Gate was not defined so gate A=B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmbZt8OEXGXW",
        "outputId": "c110cd23-f4e8-451d-dd82-6c788980fa66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== OVERALL vs HIGHLIGHTS ===\n",
            "Top-beam: {'entP_ref': 0.3971, 'entR_ref': 0.4338, 'entF1_ref': 0.3924}\n",
            "Rerank  : {'entP_ref': 0.3717, 'entR_ref': 0.451, 'entF1_ref': 0.3889}\n",
            "Δ (rr - top): {'entP_ref': -0.0254, 'entR_ref': 0.0172, 'entF1_ref': -0.0035}\n",
            "\n",
            "=== MEAN PER-TYPE F1 (vs highlights) ===\n",
            "PERSON    top=0.4290  rr=0.4306  Δ=0.0015\n",
            "ORG       top=0.2124  rr=0.2323  Δ=0.0289\n",
            "GPE       top=0.3851  rr=0.3923  Δ=0.0073\n",
            "LOC       top=0.3125  rr=0.3333  Δ=0.0000\n",
            "DATE      top=0.2164  rr=0.2310  Δ=0.0236\n",
            "CARDINAL  top=0.3129  rr=0.2851  Δ=-0.0040\n",
            "MONEY     top=0.1966  rr=0.1725  Δ=-0.0159\n",
            "PERCENT   top=0.0000  rr=0.0000  Δ=0.0000\n",
            "\n",
            "Saved: /content/drive/MyDrive/decoded_precision_phase1/val_100_entities_vs_highlights_typed.csv\n"
          ]
        }
      ],
      "source": [
        "# === ENTITIES vs GOLD HIGHLIGHTS (overall + by type) ===\n",
        "import re, numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "# 1) NER helpers ---------------------------------------------------------------\n",
        "try:\n",
        "    ents_by_type\n",
        "except NameError:\n",
        "    # Optional spaCy; else fallback (NUMBER/DATE/PROPER)\n",
        "    try:\n",
        "        import spacy\n",
        "        try:\n",
        "            _nlp = spacy.load(\"en_core_web_sm\")\n",
        "        except Exception:\n",
        "            _nlp = None\n",
        "    except Exception:\n",
        "        _nlp = None\n",
        "\n",
        "    _NUM_RE   = re.compile(r\"\\b\\d[\\d,.\\-/]*\\b\")\n",
        "    _DATE_RE  = re.compile(r\"\\b(?:\\d{1,2}\\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*|\"\n",
        "                           r\"(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{1,2}|\"\n",
        "                           r\"\\d{4})\\b\", re.I)\n",
        "    _TITLE_RE = re.compile(r\"\\b(?:[A-Z][a-z]+(?:\\s+[A-Z][a-z]+){1,3})\\b\")\n",
        "\n",
        "    def ents_by_type(text: str):\n",
        "        d = defaultdict(set)\n",
        "        if _nlp is not None:\n",
        "            doc = _nlp(text)\n",
        "            for ent in doc.ents:\n",
        "                d[ent.label_].add(ent.text.strip())\n",
        "            return d\n",
        "        # fallback\n",
        "        for m in _NUM_RE.finditer(text):  d[\"NUMBER\"].add(m.group(0))\n",
        "        for m in _DATE_RE.finditer(text): d[\"DATE\"].add(m.group(0))\n",
        "        for m in _TITLE_RE.finditer(text): d[\"PROPER\"].add(m.group(0))\n",
        "        return d\n",
        "\n",
        "def _norm_set(ss):  # normalize for set ops\n",
        "    return set(s.strip().lower() for s in ss if s and s.strip())\n",
        "\n",
        "def _prf(h, r):\n",
        "    tp = len(h & r)\n",
        "    p  = tp / max(1, len(h))\n",
        "    rc = tp / max(1, len(r))\n",
        "    f1 = 0.0 if (p+rc)==0 else 2*p*rc/(p+rc)\n",
        "    return p, rc, f1\n",
        "\n",
        "def typed_prf_vs_ref(hyp: str, ref: str):\n",
        "    H = {lab: _norm_set(spans) for lab, spans in ents_by_type(hyp).items()}\n",
        "    R = {lab: _norm_set(spans) for lab, spans in ents_by_type(ref).items()}\n",
        "    labels = sorted(set(H.keys()) | set(R.keys()))\n",
        "    out = {}\n",
        "    for lab in labels:\n",
        "        p,r,f = _prf(H.get(lab, set()), R.get(lab, set()))\n",
        "        out[lab] = (p,r,f, len(H.get(lab,set())), len(R.get(lab,set())))\n",
        "    # overall (micro over all entity strings)\n",
        "    H_all = _norm_set({s for v in H.values() for s in v})\n",
        "    R_all = _norm_set({s for v in R.values() for s in v})\n",
        "    p,r,f = _prf(H_all, R_all)\n",
        "    return out, (p,r,f)\n",
        "\n",
        "# 2) Pick results object -------------------------------------------------------\n",
        "RES = results_A  # or results_B\n",
        "\n",
        "# 3) Aggregate overall vs reference ------------------------------------------\n",
        "hyps_top = [r[\"hyp_topbeam\"] for r in RES]\n",
        "hyps_rr  = [r[\"hyp_rerank\"]  for r in RES]\n",
        "\n",
        "def overall_ent_ref(hyps, refs):\n",
        "    Ps, Rs, Fs = [], [], []\n",
        "    for h, g in zip(hyps, refs):\n",
        "        _, (p,r,f) = typed_prf_vs_ref(h, g)\n",
        "        Ps.append(p); Rs.append(r); Fs.append(f)\n",
        "    return {\"entP_ref\": float(np.mean(Ps)),\n",
        "            \"entR_ref\": float(np.mean(Rs)),\n",
        "            \"entF1_ref\": float(np.mean(Fs))}\n",
        "\n",
        "ov_top = overall_ent_ref(hyps_top, refs)\n",
        "ov_rr  = overall_ent_ref(hyps_rr,  refs)\n",
        "delta  = {k: round(ov_rr[k]-ov_top[k], 4) for k in ov_top}\n",
        "print(\"=== OVERALL vs HIGHLIGHTS ===\")\n",
        "print(\"Top-beam:\", {k: round(v,4) for k,v in ov_top.items()})\n",
        "print(\"Rerank  :\", {k: round(v,4) for k,v in ov_rr.items()})\n",
        "print(\"Δ (rr - top):\", delta)\n",
        "\n",
        "# 4) Per-type table (mean over the set) ---------------------------------------\n",
        "rows = []\n",
        "for h_top, h_rr, g in zip(hyps_top, hyps_rr, refs):\n",
        "    tmap_top, _ = typed_prf_vs_ref(h_top, g)\n",
        "    tmap_rr,  _ = typed_prf_vs_ref(h_rr,  g)\n",
        "    labels = sorted(set(tmap_top.keys()) | set(tmap_rr.keys()))\n",
        "    row = {}\n",
        "    for lab in labels:\n",
        "        p1,r1,f1,hn1,sn1 = tmap_top.get(lab, (np.nan,)*5)\n",
        "        p2,r2,f2,hn2,sn2 = tmap_rr.get(lab,  (np.nan,)*5)\n",
        "        row.update({\n",
        "            f\"{lab}_P_top\": p1, f\"{lab}_R_top\": r1, f\"{lab}_F1_top\": f1,\n",
        "            f\"{lab}_P_rr\":  p2, f\"{lab}_R_rr\":  r2, f\"{lab}_F1_rr\":  f2,\n",
        "            f\"{lab}_ΔF1\":   (f2 - f1) if (not np.isnan(f1) and not np.isnan(f2)) else np.nan,\n",
        "            f\"{lab}_HypN\":  hn2,  # show rr hyp count (rough guide)\n",
        "            f\"{lab}_RefN\":  sn2,\n",
        "        })\n",
        "    rows.append(row)\n",
        "\n",
        "typed_df = pd.DataFrame(rows)\n",
        "typed_mean = typed_df.mean(numeric_only=True).to_dict()\n",
        "\n",
        "# Display a compact view for common labels if present\n",
        "common = [c for base in [\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"DATE\",\"NUMBER\",\"CARDINAL\",\"MONEY\",\"PERCENT\"]\n",
        "          for c in (f\"{base}_F1_top\", f\"{base}_F1_rr\", f\"{base}_ΔF1\") if c in typed_df.columns]\n",
        "print(\"\\n=== MEAN PER-TYPE F1 (vs highlights) ===\")\n",
        "for base in [\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"DATE\",\"NUMBER\",\"CARDINAL\",\"MONEY\",\"PERCENT\"]:\n",
        "    t, r, d = f\"{base}_F1_top\", f\"{base}_F1_rr\", f\"{base}_ΔF1\"\n",
        "    if t in typed_mean:\n",
        "        print(f\"{base:8s}  top={typed_mean[t]:.4f}  rr={typed_mean[r]:.4f}  Δ={typed_mean[d]:.4f}\")\n",
        "\n",
        "# 5) Save full per-type table --------------------------------------------------\n",
        "OUT_DIR = \"/content/drive/MyDrive/decoded_precision_phase1\"\n",
        "import os; os.makedirs(OUT_DIR, exist_ok=True)\n",
        "OUT = os.path.join(OUT_DIR, \"val_100_entities_vs_highlights_typed.csv\")\n",
        "typed_df.to_csv(OUT, index=False)\n",
        "print(\"\\nSaved:\", OUT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y49xl9t2NTeY",
        "outputId": "32dcf5aa-9808-49bd-e964-5ee17d70e0b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved:\n",
            "/content/drive/MyDrive/decoded_precision_phase1/val_passA_rerank.csv\n",
            "/content/drive/MyDrive/decoded_precision_phase1/val_passB_gatebias_rerank.csv\n"
          ]
        }
      ],
      "source": [
        "OUT_DIR = \"/content/drive/MyDrive/decoded_precision_phase1\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "def _to_df(res: List[dict]):\n",
        "    return pd.DataFrame(res)\n",
        "\n",
        "dfA = _to_df(results_A)\n",
        "dfB = _to_df(results_B)\n",
        "\n",
        "save_csv_strict(dfA, os.path.join(OUT_DIR, \"val_passA_rerank.csv\"))\n",
        "save_csv_strict(dfB, os.path.join(OUT_DIR, \"val_passB_gatebias_rerank.csv\"))\n",
        "\n",
        "print(\"Saved:\")\n",
        "print(os.path.join(OUT_DIR, \"val_passA_rerank.csv\"))\n",
        "print(os.path.join(OUT_DIR, \"val_passB_gatebias_rerank.csv\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUuBZmHQQ-YB",
        "outputId": "a843dc14-da3b-4997-8d69-f952e3497aed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gate candidates: NONE FOUND\n"
          ]
        }
      ],
      "source": [
        "# Run this once to see if a 1-dim \"gate\" Linear exists\n",
        "import torch.nn as nn\n",
        "cands = []\n",
        "for name, mod in model.named_modules():\n",
        "    if isinstance(mod, nn.Linear) and getattr(mod, \"out_features\", None) == 1:\n",
        "        cands.append(name)\n",
        "print(\"Gate candidates:\", cands or \"NONE FOUND\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqEFJabxQ-1p"
      },
      "outputs": [],
      "source": [
        "def entity_prf_hyp_vs_ref(hyps, refs):\n",
        "    Ps, Rs, Fs = [], [], []\n",
        "    for h, r in zip(hyps, refs):\n",
        "        H = set(s.strip() for s in extract_entities(h) if s.strip())\n",
        "        R = set(s.strip() for s in extract_entities(r) if s.strip())\n",
        "        if not H and not R:\n",
        "            Ps.append(1.0); Rs.append(1.0); Fs.append(1.0); continue\n",
        "        if not H:\n",
        "            Ps.append(0.0); Rs.append(0.0); Fs.append(0.0); continue\n",
        "        tp = len(H & R)\n",
        "        p = tp / max(1, len(H))\n",
        "        r = tp / max(1, len(R))\n",
        "        f = 0.0 if (p+r)==0 else 2*p*r/(p+r)\n",
        "        Ps.append(p); Rs.append(r); Fs.append(f)\n",
        "    return {\"entP_ref\": float(np.mean(Ps)), \"entR_ref\": float(np.mean(Rs)), \"entF1_ref\": float(np.mean(Fs))}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS_G_yGVREh_",
        "outputId": "757aaae9-8dd6-4eac-a61a-6e0e60ee1e52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'entP_ref': 0.39034220028337674, 'entR_ref': 0.4255211177711178, 'entF1_ref': 0.38510455514987035} {'entP_ref': 0.366040182040182, 'entR_ref': 0.44521567321567324, 'entF1_ref': 0.3830576522744099} {'entP_ref': -0.0243, 'entR_ref': 0.0197, 'entF1_ref': -0.002}\n"
          ]
        }
      ],
      "source": [
        "hyps_top = [r[\"hyp_topbeam\"] for r in results_A]\n",
        "hyps_rr  = [r[\"hyp_rerank\"]  for r in results_A]\n",
        "ref_metrics_top = entity_prf_hyp_vs_ref(hyps_top, refs)\n",
        "ref_metrics_rr  = entity_prf_hyp_vs_ref(hyps_rr,  refs)\n",
        "print(ref_metrics_top, ref_metrics_rr, {k: round(ref_metrics_rr[k]-ref_metrics_top[k], 4) for k in ref_metrics_top})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a_7jGJsZn8q"
      },
      "source": [
        "# PGC + Gate + Precision Reranking Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQCSp5g0WsLM"
      },
      "outputs": [],
      "source": [
        "# Cell 0\n",
        "import os\n",
        "os.environ[\"TRANSFORMERS_ATTENTION_IMPLEMENTATION\"] = \"eager\"  # HF will default to eager\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FXZtlo_dFDg",
        "outputId": "5f5a4781-4efe-4536-a195-0cb0954bb50e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformers: 4.55.4\n",
            "Torch: 2.8.0+cu126\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Cell 1 — env & imports\n",
        "!pip -q install --upgrade transformers accelerate sentencepiece\n",
        "\n",
        "import os, math, random, gc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "from transformers import BartForConditionalGeneration, BartTokenizerFast, PretrainedConfig\n",
        "try:\n",
        "    from transformers.modeling_outputs import Seq2SeqLMOutput\n",
        "except Exception:\n",
        "    from transformers.utils import ModelOutput\n",
        "    class Seq2SeqLMOutput(ModelOutput):\n",
        "        loss: Optional[torch.Tensor] = None\n",
        "        logits: torch.FloatTensor = None\n",
        "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
        "        decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
        "        decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
        "        cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
        "        encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n",
        "        encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
        "        encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
        "\n",
        "SEED = 0\n",
        "random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import transformers, torch\n",
        "print(\"Transformers:\", transformers.__version__)\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"Device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kD3OZ_QFT_Kc",
        "outputId": "6f29766e-dd68-435d-c3e0-ab91fdd6a37c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 50265\n"
          ]
        }
      ],
      "source": [
        "# Cell 2 — paths + tok\n",
        "BASE_MODEL_NAME = \"facebook/bart-base\"    # or \"facebook/bart-large-cnn\"\n",
        "CKPT_DIR = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000\"  # adjust if needed\n",
        "\n",
        "tok = BartTokenizerFast.from_pretrained(BASE_MODEL_NAME, use_fast=True)\n",
        "print(\"Vocab size:\", tok.vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IKX6-OVUCga"
      },
      "outputs": [],
      "source": [
        "# Cell 3 — CopyAwareBart with encoder-ID caching for generate()\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import BartForConditionalGeneration, PretrainedConfig, BartTokenizerFast\n",
        "from typing import Optional\n",
        "\n",
        "class CopyAwareBart(BartForConditionalGeneration):\n",
        "    def __init__(self, config: PretrainedConfig, tokenizer: BartTokenizerFast,\n",
        "                 lambda_cov: float = 1.0, gate_bias: float = 0.0, use_pointer: bool = True):\n",
        "        super().__init__(config)\n",
        "        self.tok = tokenizer\n",
        "        self.lambda_cov = float(lambda_cov)\n",
        "        self.gate_bias = float(gate_bias)\n",
        "        self.use_pointer = bool(use_pointer)\n",
        "\n",
        "        d_model = config.d_model\n",
        "        self.gate_linear = nn.Linear(d_model * 3, 1)\n",
        "        nn.init.zeros_(self.gate_linear.bias)\n",
        "\n",
        "        # Caches\n",
        "        self._enc_ids_cache: Optional[torch.Tensor] = None\n",
        "        self._last_p_copy_mean: float = 0.0\n",
        "        self._last_p_gen_mean: float = 1.0\n",
        "\n",
        "    # ---------- helpers ----------\n",
        "    @staticmethod\n",
        "    def _safe_log(x: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
        "        return torch.log(torch.clamp(x, min=eps))\n",
        "\n",
        "    def _resolve_enc_ids(self, input_ids: Optional[torch.Tensor], B_current: int) -> Optional[torch.Tensor]:\n",
        "        \"\"\"Ensure we have encoder token ids for scatter; expand to B_current for beams.\"\"\"\n",
        "        if input_ids is not None:\n",
        "            self._enc_ids_cache = input_ids\n",
        "            base = input_ids\n",
        "        else:\n",
        "            base = self._enc_ids_cache\n",
        "            if base is None:\n",
        "                return None\n",
        "        # Expand to match beam-expanded batch if needed\n",
        "        if base.size(0) != B_current:\n",
        "            if B_current % base.size(0) == 0:\n",
        "                base = base.repeat_interleave(B_current // base.size(0), dim=0)\n",
        "            else:\n",
        "                base = base.expand(B_current, -1)  # fallback\n",
        "        return base\n",
        "\n",
        "    def _get_attn_with_proxy(self, outputs, attention_mask: Optional[torch.Tensor]) -> Optional[torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Return [B,T,S] attention. If real cross-attn exists, use it; otherwise build proxy:\n",
        "        softmax( (dec_h @ enc_h^T)/sqrt(H) ) with pad masking.\n",
        "        \"\"\"\n",
        "        ca = getattr(outputs, \"cross_attentions\", None)\n",
        "        if ca and any(x is not None for x in ca):\n",
        "            last = next(x for x in reversed(ca) if x is not None)   # [B,H,T,S]\n",
        "            attn = last.mean(dim=1)                                 # [B,T,S]\n",
        "            if attention_mask is not None:\n",
        "                attn = attn * attention_mask[:, None, :]\n",
        "            attn = attn / (attn.sum(dim=-1, keepdim=True) + 1e-8)\n",
        "            return attn\n",
        "\n",
        "        # Proxy attention\n",
        "        enc_h = outputs.encoder_last_hidden_state                  # [B,S,H]\n",
        "        dec_h = outputs.decoder_hidden_states[-1]                  # [B,T,H]\n",
        "        logits = torch.bmm(dec_h, enc_h.transpose(1, 2)) / math.sqrt(dec_h.size(-1))  # [B,T,S]\n",
        "        if attention_mask is not None:\n",
        "            logits = logits.masked_fill(attention_mask[:, None, :] == 0, float(\"-inf\"))\n",
        "        return torch.softmax(logits, dim=-1)\n",
        "\n",
        "    def _coverage_loss(self, attn: Optional[torch.Tensor]) -> torch.Tensor:\n",
        "        if attn is None or not self.use_pointer or self.lambda_cov <= 0.0:\n",
        "            return torch.zeros((), device=attn.device if isinstance(attn, torch.Tensor) else None)\n",
        "        cov = attn.cumsum(dim=1) - attn\n",
        "        return torch.minimum(attn, cov).sum(dim=(-1, -2)).mean()\n",
        "\n",
        "    # ---------- forward ----------\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        labels=None,\n",
        "        decoder_input_ids=None,\n",
        "        decoder_attention_mask=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=True,   # needed for proxy\n",
        "        use_cache=True,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        need_atts = bool(output_attentions) or self.use_pointer\n",
        "        outputs = super().forward(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=None,  # CE on our mixed logits\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            output_attentions=need_atts,\n",
        "            output_hidden_states=True,\n",
        "            use_cache=use_cache,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        logits_gen = outputs.logits  # [B,T,V]\n",
        "        B, T, V = logits_gen.shape\n",
        "\n",
        "        if not self.use_pointer:\n",
        "            loss = None\n",
        "            if labels is not None:\n",
        "                loss = F.cross_entropy(logits_gen.view(-1, V), labels.view(-1), ignore_index=-100, reduction=\"mean\")\n",
        "            return outputs.__class__(\n",
        "                loss=loss, logits=logits_gen, **{k: v for k, v in outputs.items() if k != \"logits\"}\n",
        "            )\n",
        "\n",
        "        # Resolve encoder ids for scatter (handles beam-expanded batches)\n",
        "        enc_input_ids = self._resolve_enc_ids(input_ids, B)\n",
        "        if enc_input_ids is None:\n",
        "            # Fallback to vanilla if we really can't get source ids\n",
        "            loss = None\n",
        "            if labels is not None:\n",
        "                loss = F.cross_entropy(logits_gen.view(-1, V), labels.view(-1), ignore_index=-100, reduction=\"mean\")\n",
        "            self._last_p_gen_mean = 0.999; self._last_p_copy_mean = 0.001\n",
        "            return outputs.__class__(loss=loss, logits=logits_gen, **{k: v for k, v in outputs.items() if k != \"logits\"})\n",
        "\n",
        "        # Attention (real or proxy)\n",
        "        attn = self._get_attn_with_proxy(outputs, attention_mask)        # [B,T,S]\n",
        "\n",
        "        # Copy distribution over vocab\n",
        "        copy_dist_vocab = torch.zeros(B, T, V, device=logits_gen.device, dtype=logits_gen.dtype)\n",
        "        scatter_index = enc_input_ids[:, None, :].expand(B, T, enc_input_ids.size(1))\n",
        "        copy_dist_vocab = copy_dist_vocab.scatter_add(2, scatter_index, attn)\n",
        "\n",
        "        # Context + gate inputs\n",
        "        enc_hidden = outputs.encoder_last_hidden_state                   # [B,S,H]\n",
        "        context = torch.bmm(attn, enc_hidden)                            # [B,T,H]\n",
        "        dec_h = outputs.decoder_hidden_states[-1]                        # [B,T,H]\n",
        "\n",
        "        # Previous token embeddings\n",
        "        if decoder_input_ids is None:\n",
        "            if labels is not None:\n",
        "                bos_id = self.config.decoder_start_token_id or self.tok.bos_token_id\n",
        "                y_in = torch.full((B, 1), bos_id, dtype=torch.long, device=logits_gen.device)\n",
        "                di = labels.masked_fill(labels == -100, self.tok.pad_token_id)\n",
        "                decoder_input_ids = torch.cat([y_in, di[:, :-1]], dim=1)\n",
        "            else:\n",
        "                decoder_input_ids = kwargs.get(\"decoder_input_ids\", None)\n",
        "                if decoder_input_ids is None:\n",
        "                    decoder_input_ids = torch.full((B, T), self.tok.pad_token_id,\n",
        "                                                   device=logits_gen.device, dtype=torch.long)\n",
        "        y_prev_emb = self.model.decoder.embed_tokens(decoder_input_ids)\n",
        "\n",
        "        # Gate & mixture\n",
        "        gate_inp = torch.cat([dec_h, context, y_prev_emb], dim=-1)\n",
        "        p_gen = torch.sigmoid(self.gate_linear(gate_inp).squeeze(-1) + self.gate_bias).clamp_(1e-4, 1 - 1e-4)\n",
        "        p_copy = 1.0 - p_gen\n",
        "\n",
        "        logp_gen = F.log_softmax(logits_gen, dim=-1)\n",
        "        logp_copy = self._safe_log(copy_dist_vocab)\n",
        "        mix_logp = torch.logaddexp(self._safe_log(p_gen)[:, :, None] + logp_gen,\n",
        "                                   self._safe_log(p_copy)[:, :, None] + logp_copy)\n",
        "\n",
        "        self._last_p_gen_mean = float(p_gen.mean().detach().cpu())\n",
        "        self._last_p_copy_mean = float(p_copy.mean().detach().cpu())\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            ce = F.cross_entropy(mix_logp.view(-1, V), labels.view(-1), ignore_index=-100, reduction=\"mean\")\n",
        "            cov = self._coverage_loss(attn)\n",
        "            loss = ce + self.lambda_cov * cov\n",
        "\n",
        "        return outputs.__class__(loss=loss, logits=mix_logp, **{k: v for k, v in outputs.items() if k != \"logits\"})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mxoo2zhkVctM",
        "outputId": "40906eac-acda-474b-c7c7-123df38f646d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[attn] Using eager/math kernels.\n"
          ]
        }
      ],
      "source": [
        "# Cell 3.5 — optional: make sure torch SDPA fused kernels are off\n",
        "try:\n",
        "    from torch.backends.cuda import sdp_kernel\n",
        "    sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False)\n",
        "    print(\"[attn] Using eager/math kernels.\")\n",
        "except Exception as e:\n",
        "    print(\"[attn] sdp_kernel not available:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8R_16EaUeIA",
        "outputId": "94bd9efe-9fec-41ce-c157-2c409abc6705"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[load] /content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000/model.safetensors\n",
            "[load] wrapper ckpt | missing=5 unexpected=0\n",
            "Model ready. gate_bias = 0.9\n"
          ]
        }
      ],
      "source": [
        "# Cell F2 — rebuild model with fixed class and reload weights\n",
        "\n",
        "from transformers import BartConfig, BartForConditionalGeneration\n",
        "import os, glob\n",
        "from safetensors.torch import load_file as load_safetensors\n",
        "\n",
        "def build_wrapper_with_eager_config(base_model_name: str):\n",
        "    cfg = BartConfig.from_pretrained(base_model_name)\n",
        "    if hasattr(cfg, \"attn_implementation\"):\n",
        "        cfg.attn_implementation = \"eager\"\n",
        "    return CopyAwareBart(cfg, tokenizer=tok, lambda_cov=1.0, gate_bias=0.90, use_pointer=True)\n",
        "\n",
        "def load_state_into_wrapper(model, ckpt_dir_or_model_name: str):\n",
        "    if os.path.isdir(ckpt_dir_or_model_name):\n",
        "        cands = []\n",
        "        for pat in [\"model.safetensors\",\"model-*.safetensors\",\"pytorch_model.bin\",\"pytorch_model-*.bin\"]:\n",
        "            cands += glob.glob(os.path.join(ckpt_dir_or_model_name, pat))\n",
        "        if cands:\n",
        "            path = sorted(cands)[0]\n",
        "            print(f\"[load] {path}\")\n",
        "            sd = load_safetensors(path) if path.endswith(\".safetensors\") else torch.load(path, map_location=\"cpu\")\n",
        "            missing, unexpected = model.load_state_dict(sd, strict=False)\n",
        "            print(f\"[load] wrapper ckpt | missing={len(missing)} unexpected={len(unexpected)}\")\n",
        "            return\n",
        "    print(f\"[load] fallback base weights from '{ckpt_dir_or_model_name}'\")\n",
        "    base = BartForConditionalGeneration.from_pretrained(ckpt_dir_or_model_name)\n",
        "    missing, unexpected = model.load_state_dict(base.state_dict(), strict=False)\n",
        "    print(f\"[load] base→wrapper | missing={len(missing)} unexpected={len(unexpected)}\")\n",
        "\n",
        "# Rebuild & load\n",
        "model = build_wrapper_with_eager_config(BASE_MODEL_NAME)\n",
        "load_state_into_wrapper(model, CKPT_DIR if os.path.isdir(CKPT_DIR) else BASE_MODEL_NAME)\n",
        "model = model.to(device).eval()\n",
        "\n",
        "# Ensure generation config returns dict; attentions optional (proxy covers us)\n",
        "gencfg = model.generation_config\n",
        "gencfg.return_dict_in_generate = True\n",
        "gencfg.output_attentions = True\n",
        "gencfg.output_hidden_states = False\n",
        "\n",
        "print(\"Model ready. gate_bias =\", model.gate_bias)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FT42ahLb4VU",
        "outputId": "8068ffe8-8166-4797-ae7e-c6aeb763501e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[patch] _get_last_cross_attn now tolerates None entries.\n"
          ]
        }
      ],
      "source": [
        "# Cell 4.1 — make cross-attn extraction robust to None entries\n",
        "\n",
        "def _get_last_cross_attn_safe(self, outputs):\n",
        "    ca = getattr(outputs, \"cross_attentions\", None)\n",
        "    if not ca:\n",
        "        return None\n",
        "    # Find the last non-None layer\n",
        "    for layer_ca in reversed(ca):\n",
        "        if layer_ca is not None:\n",
        "            return layer_ca.mean(dim=1)  # [B, H, T, S] -> [B, T, S]\n",
        "    return None\n",
        "\n",
        "# Monkey-patch both the class and the existing instance to be extra safe\n",
        "CopyAwareBart._get_last_cross_attn = _get_last_cross_attn_safe\n",
        "try:\n",
        "    model._get_last_cross_attn = _get_last_cross_attn_safe.__get__(model, CopyAwareBart)\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "print(\"[patch] _get_last_cross_attn now tolerates None entries.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8-O3axTfsdA"
      },
      "outputs": [],
      "source": [
        "# Calibrate gate bias on real (source, reference) pairs\n",
        "\n",
        "def measure_pcopy_on_pairs(model, src_texts, ref_texts, src_max=400, tgt_max=100):\n",
        "\n",
        "    enc = tok(src_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=src_max).to(device)\n",
        "    tgt = tok(ref_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=tgt_max).to(device)\n",
        "    bos_id = model.config.decoder_start_token_id or tok.bos_token_id\n",
        "    bos = torch.full((tgt.input_ids.size(0), 1), bos_id, dtype=torch.long, device=device)\n",
        "    dec_inp = torch.cat([bos, tgt.input_ids[:, :-1]], dim=1)  # shift-right\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(**enc, decoder_input_ids=dec_inp, output_attentions=True, output_hidden_states=False)\n",
        "    return model._last_p_copy_mean  # mean over all steps in the sequence\n",
        "\n",
        "def calibrate_gate_bias_on_pairs(model, src_texts, ref_texts, target=(0.30, 0.35), step=0.1, max_iters=8):\n",
        "    lo, hi = target\n",
        "    for k in range(max_iters):\n",
        "        p = measure_pcopy_on_pairs(model, src_texts, ref_texts)\n",
        "        print(f\"[calib] iter {k} | gate_bias={model.gate_bias:+.2f} | p_copy_mean={p:.3f}\")\n",
        "        if lo <= p <= hi:\n",
        "            print(\"[calib] done.\")\n",
        "            break\n",
        "        # Too copy-heavy -> increase bias (more generate). Too low -> decrease bias.\n",
        "        model.gate_bias += 0.10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qN7I7yDid2X",
        "outputId": "47b155ff-865e-491a-c471-fcd5c65a32c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[calib] iter 0 | gate_bias=+0.80 | p_copy_mean=0.360\n",
            "[calib] iter 1 | gate_bias=+0.90 | p_copy_mean=0.338\n",
            "[calib] done.\n"
          ]
        }
      ],
      "source": [
        "# Cell 5.7 — pick a val slice and run calibration on gold refs (highlights)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "VAL_CSV = \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "\n",
        "if \"df\" in globals() and {\"article\",\"highlights\"}.issubset(set(df.columns)):\n",
        "    df_val = df.copy()\n",
        "else:\n",
        "    df_val = pd.read_csv(VAL_CSV)\n",
        "\n",
        "# Clean & take a small slice for quick calibration\n",
        "df_val = df_val[[\"article\",\"highlights\"]].dropna().reset_index(drop=True)\n",
        "N = min(100, len(df_val))   # 50–100 is plenty\n",
        "src_batch = df_val[\"article\"].iloc[:N].astype(str).tolist()\n",
        "ref_batch = df_val[\"highlights\"].iloc[:N].astype(str).tolist()\n",
        "\n",
        "# Use the helpers you already defined just above:\n",
        "# - measure_pcopy_on_pairs(...)\n",
        "# - calibrate_gate_bias_on_pairs(...)\n",
        "calibrate_gate_bias_on_pairs(\n",
        "    model,\n",
        "    src_batch,\n",
        "    ref_batch,\n",
        "    target=(0.30, 0.35),   # selective copying\n",
        "    step=0.1,\n",
        "    max_iters=8\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc4q8_R-eZCu"
      },
      "outputs": [],
      "source": [
        "# Keep the calibrated gate\n",
        "model.gate_bias = 0.90   # your calibrated setting\n",
        "\n",
        "# Decoding params (same as your earlier run)\n",
        "BEAMS    = 8\n",
        "MIN_NEW  = 18\n",
        "MAX_NEW  = 44\n",
        "LEN_P    = 0.75\n",
        "NR_NGRAM = 4\n",
        "EARLY_STOP = True\n",
        "\n",
        "# Rerank weights\n",
        "A_UNSUP_ENT = 1.2   # penalty per unsupported capitalized span (entity-like)\n",
        "B_UNSUP_NUM = 1.0   # penalty per unsupported number/percent/year\n",
        "C_REPEAT    = 3.0   # penalty × repetition fraction\n",
        "D_LEN       = 0.8   # penalty × relative length deviation from target\n",
        "\n",
        "\n",
        "LENGTH_TGT  = 31\n",
        "# Device guard\n",
        "try:\n",
        "    device\n",
        "except NameError:\n",
        "    device = next(model.parameters()).device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JYDD1bhmsNe"
      },
      "outputs": [],
      "source": [
        "# Cell 6.1 — imports + lightweight features + improved scorer\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.amp import autocast\n",
        "\n",
        "# If you want LENGTH_TGT from val refs, compute it once here (fallback 58 otherwise)\n",
        "if \"df_val\" in globals() and \"highlights\" in df_val.columns:\n",
        "    LENGTH_TGT = int(np.median([len(str(x).split()) for x in df_val[\"highlights\"].dropna().iloc[:1000]]))\n",
        "else:\n",
        "    LENGTH_TGT = 31\n",
        "\n",
        "# very lightweight feature extractors\n",
        "_NUM_RE   = re.compile(r\"\\d[\\d,.\\-/]*%?\")\n",
        "_CAP_SPAN = re.compile(r\"(?:\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)+)\")\n",
        "\n",
        "def _find_numbers(s: str): return set(_NUM_RE.findall(s))\n",
        "def _find_caps(s: str):    return set(_CAP_SPAN.findall(s))\n",
        "\n",
        "def _repeat_fraction(hyp: str, n: int = 3) -> float:\n",
        "    toks = re.findall(r\"[A-Za-z0-9]+(?:[-_/][A-Za-z0-9]+)*\", hyp.lower())\n",
        "    if len(toks) < n + 1: return 0.0\n",
        "    grams = [tuple(toks[i:i+n]) for i in range(len(toks)-n+1)]\n",
        "    uniq = len(set(grams))\n",
        "    return max(0.0, 1.0 - uniq / len(grams))\n",
        "\n",
        "def improved_rerank_scores(\n",
        "    src: str,\n",
        "    hyps: list[str],\n",
        "    base_scores: np.ndarray,\n",
        "    w_unsup_ent: float = A_UNSUP_ENT,\n",
        "    w_unsup_num: float = B_UNSUP_NUM,\n",
        "    w_repeat: float    = C_REPEAT,\n",
        "    w_len: float       = D_LEN,\n",
        "    length_tgt: int    = LENGTH_TGT,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Score = base logP\n",
        "          - w_unsup_ent * (# cap spans not substrings of source)\n",
        "          - w_unsup_num * (# numbers not in source's number set)\n",
        "          - w_repeat    * repetition_fraction\n",
        "          - w_len       * relative_length_deviation_from_target\n",
        "    \"\"\"\n",
        "    src_nums = _find_numbers(src)\n",
        "    scores = []\n",
        "\n",
        "    for hyp, base in zip(hyps, base_scores):\n",
        "        hyp_nums = _find_numbers(hyp)\n",
        "        hyp_caps = _find_caps(hyp)\n",
        "\n",
        "        unsup_ent = sum(1 for e in hyp_caps if e not in src)      # substring check ok here\n",
        "        unsup_num = sum(1 for n in hyp_nums if n not in src_nums)\n",
        "        repeat_f  = _repeat_fraction(hyp, n=3)\n",
        "\n",
        "        # length regularizer (relative deviation)\n",
        "        L = len(hyp.split())\n",
        "        rel_len_dev = abs(L - length_tgt) / max(1, length_tgt)\n",
        "\n",
        "        score = float(base) - w_unsup_ent*unsup_ent - w_unsup_num*unsup_num - w_repeat*repeat_f - w_len*rel_len_dev\n",
        "        scores.append(score)\n",
        "\n",
        "    return np.array(scores, dtype=np.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-rnYtd-mw70"
      },
      "outputs": [],
      "source": [
        "# Cell 6.2 — decode + rerank\n",
        "\n",
        "GEN_KW = dict(\n",
        "    num_beams=BEAMS,\n",
        "    num_return_sequences=BEAMS,\n",
        "    do_sample=False,\n",
        "    min_new_tokens=MIN_NEW,\n",
        "    max_new_tokens=MAX_NEW,\n",
        "    no_repeat_ngram_size=NR_NGRAM,\n",
        "    length_penalty=LEN_P,\n",
        "    early_stopping=EARLY_STOP,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,  # base logP per beam\n",
        ")\n",
        "\n",
        "@torch.no_grad()\n",
        "def decode_with_rerank(articles: list[str], batch_size: int = 8):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      top_beam : HF top hypothesis per example\n",
        "      reranked : our precision-first reranked hypothesis per example\n",
        "    \"\"\"\n",
        "    top_beam, reranked = [], []\n",
        "\n",
        "    for i in range(0, len(articles), batch_size):\n",
        "        batch_src = articles[i:i+batch_size]\n",
        "        enc = tok(batch_src, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
        "\n",
        "        # AMP-safe generation\n",
        "        ctx = autocast(device_type=\"cuda\", dtype=torch.float16) if torch.cuda.is_available() else torch.no_grad()\n",
        "        with ctx:\n",
        "            gen = model.generate(**enc, **GEN_KW)\n",
        "\n",
        "        B = enc.input_ids.size(0)\n",
        "        beams = GEN_KW[\"num_return_sequences\"]\n",
        "        seqs = gen.sequences.view(B, beams, -1)                          # [B, beams, T_dec]\n",
        "        base_scores = gen.sequences_scores.view(B, beams).cpu().numpy()  # [B, beams]\n",
        "\n",
        "        # decode all beams to text\n",
        "        all_texts = tok.batch_decode(seqs.view(-1, seqs.size(-1)), skip_special_tokens=True)\n",
        "        texts_per_ex = [all_texts[j*beams:(j+1)*beams] for j in range(B)]\n",
        "\n",
        "        for j in range(B):\n",
        "            # HF’s top beam (highest base logP)\n",
        "            top_beam.append(texts_per_ex[j][0])\n",
        "\n",
        "            # Our precision-first rerank\n",
        "            rerank = improved_rerank_scores(batch_src[j], texts_per_ex[j], base_scores[j])\n",
        "            best = int(np.argmax(rerank))\n",
        "            reranked.append(texts_per_ex[j][best])\n",
        "\n",
        "    return top_beam, reranked\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF-KWZ1JnBBd",
        "outputId": "2c7d9d57-1280-4a6f-93d7-3d82390cb28e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==== Example 0 ====\n",
            "SOURCE  : Share, and your gift will be multiplied. That may sound like an esoteric adage, but when Zully Broussard selflessly decided to give one of her kidneys to a stranger, her generosity paired up with big data. It resulted in six patients receiving transplants. That surprised and wowed her. \"I thought I ...\n",
            "TOP     : Zully Broussard's generosity paired up with big data . It resulted in six patients receiving transplants . That surprised and wowed her . \"I thought I was going to help this one person who\n",
            "RERANKED: Zully Broussard's generosity paired up with big data . It resulted in six patients receiving transplants . That surprised and wowed her . \"I thought I was going to help this one person who\n",
            "\n",
            "==== Example 1 ====\n",
            "SOURCE  : On the 6th of April 1996, San Jose Clash and DC United strode out in front of 31,683 expectant fans at the Spartan Stadium in San Jose, California. The historic occasion was the first ever Major League Soccer match -- a brave new dawn for the world's favorite sport in a land its charms had yet to co...\n",
            "TOP     : On the 6th of April 1996, San Jose Clash and DC United strode out in front of 31,683 expectant fans . The historic occasion was a brave new dawn for the world's favorite sport in\n",
            "RERANKED: On the 6th of April 1996, San Jose Clash and DC United strode out in front of 31,683 expectant fans . The historic occasion was a brave new dawn for the world's favorite sport in\n",
            "\n",
            "==== Example 2 ====\n",
            "SOURCE  : French striker Bafetimbi Gomis, who has a history of fainting, said he is now \"feeling well\" after collapsing during Swansea's 3-2 loss at Tottenham in the Premier League on Wednesday. The worrying incident occurred in the first half at White Hart Lane -- after Tottenham scored in the seventh minute...\n",
            "TOP     : French striker Bafetimbi Gomis collapses during Swansea's 3-2 loss at Tottenham . The 29-year-old left the pitch conscious following about five minutes of treatment . The match was temporarily\n",
            "RERANKED: French striker Bafetimbi Gomis collapses during Swansea's 3-2 loss at Tottenham . The 29-year-old left the pitch conscious following about five minutes of treatment . The match was temporarily\n",
            "\n",
            "==== Example 3 ====\n",
            "SOURCE  : It was an act of frustration perhaps more commonly associated with golf's fictional anti-hero Happy Gilmore than the world's reigning No 1. player. But when Rory McIlroy pulled his second shot on the eighth hole of the WGC Cadillac Championship into a lake Friday, he might as well have been channeli...\n",
            "TOP     : Rory McIlroy pulled his second shot on the eighth hole of the WGC Cadillac Championship into a lake . The four-time major winner launched the 3-iron used to play the offending shot into the\n",
            "RERANKED: Rory McIlroy pulled his second shot on the eighth hole of the WGC Cadillac Championship into a lake . The four-time major winner launched the 3-iron used to play the offending shot into the\n",
            "\n",
            "==== Example 4 ====\n",
            "SOURCE  : A Pennsylvania community is pulling together to search for an eighth-grade student who has been missing since Wednesday. The search has drawn hundreds of volunteers on foot and online. The parents of Cayman Naib, 13, have been communicating through the Facebook group \"Find Cayman\" since a day after ...\n",
            "TOP     : Cayman Naib, 13, was last seen wearing a gray down winter jacket, black ski pants and hiking boots . He could be in the Radnor-Wayne area, roughly 20 miles from Philadelphia\n",
            "RERANKED: Cayman Naib, 13, was last seen wearing a gray down winter jacket, black ski pants and hiking boots . He could be in the Radnor-Wayne area, roughly 20 miles from Philadelphia\n"
          ]
        }
      ],
      "source": [
        "# Cell 6.3 — run & peek\n",
        "\n",
        "if \"df_val\" not in globals():\n",
        "    import pandas as pd\n",
        "    VAL_CSV = \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "    df_val = pd.read_csv(VAL_CSV)[[\"article\",\"highlights\"]].dropna().reset_index(drop=True)\n",
        "\n",
        "K = 50\n",
        "articles_eval = df_val[\"article\"].iloc[:K].astype(str).tolist()\n",
        "\n",
        "top_out, rr_out = decode_with_rerank(articles_eval)\n",
        "\n",
        "for i in range(min(5, K)):\n",
        "    print(f\"\\n==== Example {i} ====\")\n",
        "    print(\"SOURCE  :\", articles_eval[i][:300].replace(\"\\n\",\" \") + (\"...\" if len(articles_eval[i]) > 300 else \"\"))\n",
        "    print(\"TOP     :\", top_out[i])\n",
        "    print(\"RERANKED:\", rr_out[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3_vJ9DeoV1-",
        "outputId": "ca023c6f-097c-4c4f-8230-e9672e19ddb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating K=50 examples\n",
            "\n",
            "=== Top-beam ===\n",
            "{'entP': 0.9543, 'entR': 0.173, 'entF1': 0.2805, 'copy_rate': 0.9806, 'repeat_frac': 0.001, 'rouge1': 0.3556, 'rouge2': 0.1421, 'rougeLsum': 0.2517}\n",
            "\n",
            "=== Reranked ===\n",
            "{'entP': 0.9578, 'entR': 0.1781, 'entF1': 0.2879, 'copy_rate': 0.9795, 'repeat_frac': 0.0006, 'rouge1': 0.3546, 'rouge2': 0.1398, 'rougeLsum': 0.2486}\n",
            "\n",
            "=== Δ (reranked - top) ===\n",
            "{'entP': 0.0035, 'entR': 0.005, 'entF1': 0.0074, 'copy_rate': -0.001, 'repeat_frac': -0.0004, 'rouge1': -0.001, 'rouge2': -0.0023, 'rougeLsum': -0.0031}\n"
          ]
        }
      ],
      "source": [
        "# Cell 6.4 — Evaluate top-beam vs reranked (entity P/R/F1 vs source, copy/repeat, ROUGE vs gold)\n",
        "import re, numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# ---- optional spaCy NER (falls back to heuristic if unavailable) ----\n",
        "try:\n",
        "    import spacy\n",
        "    try:\n",
        "        _nlp = spacy.load(\"en_core_web_sm\")\n",
        "    except Exception:\n",
        "        _nlp = None\n",
        "except Exception:\n",
        "    _nlp = None\n",
        "\n",
        "# ---- helpers (entity/copy/repeat) ----\n",
        "_WORD_RE = re.compile(r\"[A-Za-z0-9]+(?:[-_/][A-Za-z0-9]+)*\")\n",
        "_NUM_RE  = re.compile(r\"\\d[\\d,.\\-/]*\")\n",
        "\n",
        "def _words(txt: str):\n",
        "    return _WORD_RE.findall(str(txt).lower())\n",
        "\n",
        "def _spans_simple(txt: str):\n",
        "    spans = set()\n",
        "    # numbers\n",
        "    for m in _NUM_RE.finditer(str(txt)):\n",
        "        spans.add(m.group(0))\n",
        "    # title-cased multiwords as proxy for PERSON/ORG/LOC\n",
        "    toks = re.findall(r\"\\b[A-Z][a-z]+\\b\", str(txt))\n",
        "    for i in range(len(toks)-1):\n",
        "        if toks[i][0].isupper() and toks[i+1][0].isupper():\n",
        "            spans.add(f\"{toks[i]} {toks[i+1]}\")\n",
        "    return list(spans)\n",
        "\n",
        "def extract_entities(txt: str):\n",
        "    if _nlp is not None:\n",
        "        doc = _nlp(str(txt))\n",
        "        return [ent.text for ent in doc.ents]\n",
        "    return _spans_simple(txt)\n",
        "\n",
        "def entity_prf_against_source(source: str, hyp: str):\n",
        "    src_ents = set(s.strip() for s in extract_entities(source) if s.strip())\n",
        "    hyp_ents = set(s.strip() for s in extract_entities(hyp) if s.strip())\n",
        "    if not src_ents and not hyp_ents: return 1.0, 1.0, 1.0\n",
        "    if not hyp_ents: return 0.0, 0.0, 0.0\n",
        "    tp = len(src_ents & hyp_ents)\n",
        "    prec = tp / max(1, len(hyp_ents))\n",
        "    rec  = tp / max(1, len(src_ents))\n",
        "    f1   = 0.0 if (prec+rec)==0 else 2*prec*rec/(prec+rec)\n",
        "    return prec, rec, f1\n",
        "\n",
        "def copy_rate(source: str, hyp: str) -> float:\n",
        "    src = set(_words(source))\n",
        "    hyp_w = _words(hyp)\n",
        "    if not hyp_w: return 0.0\n",
        "    copied = sum(1 for w in hyp_w if w in src)\n",
        "    return copied / len(hyp_w)\n",
        "\n",
        "def repeat_fraction(hyp: str, n: int = 3) -> float:\n",
        "    toks = _words(hyp)\n",
        "    if len(toks) < n + 1: return 0.0\n",
        "    grams = [tuple(toks[i:i+n]) for i in range(len(toks)-n+1)]\n",
        "    c = Counter(grams)\n",
        "    reps = sum(v - 1 for v in c.values() if v > 1)\n",
        "    return reps / max(1, len(grams))\n",
        "\n",
        "# ---- ROUGE (uses rouge-score; installs if missing) ----\n",
        "try:\n",
        "    from rouge_score import rouge_scorer\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"rouge-score\"])\n",
        "    from rouge_score import rouge_scorer\n",
        "\n",
        "_sc = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\"], use_stemmer=True)\n",
        "\n",
        "def rouge_f1_batch(refs: list[str], hyps: list[str]):\n",
        "    r1=r2=rl=0.0\n",
        "    n = len(hyps)\n",
        "    for ref, hyp in zip(refs, hyps):\n",
        "        sc = _sc.score(str(ref), str(hyp))\n",
        "        r1 += sc[\"rouge1\"].fmeasure\n",
        "        r2 += sc[\"rouge2\"].fmeasure\n",
        "        rl += sc[\"rougeLsum\"].fmeasure\n",
        "    return {\"rouge1\": r1/n, \"rouge2\": r2/n, \"rougeLsum\": rl/n}\n",
        "\n",
        "def eval_set(sources: list[str], hyps: list[str], refs: list[str]):\n",
        "    assert len(sources)==len(hyps)==len(refs)\n",
        "    n = len(hyps)\n",
        "    entP=entR=entF1=cr=rep=0.0\n",
        "    for s,h in zip(sources, hyps):\n",
        "        p,r,f1 = entity_prf_against_source(s, h)\n",
        "        entP+=p; entR+=r; entF1+=f1\n",
        "        cr+=copy_rate(s,h)\n",
        "        rep+=repeat_fraction(h, n=3)\n",
        "    ent = {\"entP\": entP/n, \"entR\": entR/n, \"entF1\": entF1/n,\n",
        "           \"copy_rate\": cr/n, \"repeat_frac\": rep/n}\n",
        "    rg  = rouge_f1_batch(refs, hyps)\n",
        "    return ent | rg\n",
        "\n",
        "def _fmt(d):\n",
        "    return {k: round(float(v), 4) for k,v in d.items()}\n",
        "\n",
        "# --------- collect inputs ----------\n",
        "K = len(top_out) if 'top_out' in globals() else len(rr_out)\n",
        "articles_eval = df_val[\"article\"].iloc[:K].astype(str).tolist()\n",
        "refs_eval     = df_val[\"highlights\"].iloc[:K].astype(str).tolist()\n",
        "tops          = top_out if 'top_out' in globals() else rr_out\n",
        "rrs           = rr_out  # your reranked outputs\n",
        "\n",
        "# --------- compute ---------\n",
        "print(f\"Evaluating K={K} examples\")\n",
        "\n",
        "top_metrics = eval_set(articles_eval, tops, refs_eval)\n",
        "rr_metrics  = eval_set(articles_eval, rrs,  refs_eval)\n",
        "\n",
        "delta = {k: rr_metrics[k]-top_metrics[k] for k in top_metrics.keys()}\n",
        "\n",
        "print(\"\\n=== Top-beam ===\")\n",
        "print(_fmt(top_metrics))\n",
        "print(\"\\n=== Reranked ===\")\n",
        "print(_fmt(rr_metrics))\n",
        "print(\"\\n=== Δ (reranked - top) ===\")\n",
        "print(_fmt(delta))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J2cPfIuqMX-",
        "outputId": "4f567f31-60f4-4c59-ee3b-3f3838b7746d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating vs **reference** on K=50 examples\n",
            "\n",
            "=== Top-beam (entities vs ref + ROUGE) ===\n",
            "{'entP_ref': 0.3307, 'entR_ref': 0.349, 'entF1_ref': 0.3098, 'rouge1': 0.3413, 'rouge2': 0.135, 'rougeLsum': 0.2542}\n",
            "\n",
            "=== Reranked (entities vs ref + ROUGE) ===\n",
            "{'entP_ref': 0.3431, 'entR_ref': 0.362, 'entF1_ref': 0.3244, 'rouge1': 0.3518, 'rouge2': 0.1429, 'rougeLsum': 0.2613}\n",
            "\n",
            "=== Δ (reranked - top) ===\n",
            "{'entP_ref': 0.0123, 'entR_ref': 0.013, 'entF1_ref': 0.0146, 'rouge1': 0.0105, 'rouge2': 0.0079, 'rougeLsum': 0.0071}\n"
          ]
        }
      ],
      "source": [
        "# Cell 6.5 — Entities vs **reference** (and ROUGE). Uses the same extract_entities() helper as before.\n",
        "\n",
        "def entity_prf_against_reference(reference: str, hyp: str):\n",
        "    ref_ents = set(s.strip() for s in extract_entities(reference) if s.strip())\n",
        "    hyp_ents = set(s.strip() for s in extract_entities(hyp) if s.strip())\n",
        "    if not ref_ents and not hyp_ents: return 1.0, 1.0, 1.0\n",
        "    if not hyp_ents: return 0.0, 0.0, 0.0\n",
        "    tp = len(ref_ents & hyp_ents)\n",
        "    prec = tp / max(1, len(hyp_ents))\n",
        "    rec  = tp / max(1, len(ref_ents))\n",
        "    f1   = 0.0 if (prec+rec)==0 else 2*prec*rec/(prec+rec)\n",
        "    return prec, rec, f1\n",
        "\n",
        "def eval_entities_vs_ref(refs: list[str], hyps: list[str]):\n",
        "    n = len(hyps)\n",
        "    entP=entR=entF1=0.0\n",
        "    for r,h in zip(refs, hyps):\n",
        "        p,r1,f1 = entity_prf_against_reference(r, h)\n",
        "        entP+=p; entR+=r1; entF1+=f1\n",
        "    return {\"entP_ref\": entP/n, \"entR_ref\": entR/n, \"entF1_ref\": entF1/n}\n",
        "\n",
        "# Inputs from earlier cells:\n",
        "K = len(top_out)\n",
        "articles_eval = df_val[\"article\"].iloc[:K].astype(str).tolist()\n",
        "refs_eval     = df_val[\"highlights\"].iloc[:K].astype(str).tolist()\n",
        "tops          = top_out\n",
        "rrs           = rr_out\n",
        "\n",
        "# ROUGE (same as before)\n",
        "top_rouge = rouge_f1_batch(refs_eval, tops)\n",
        "rr_rouge  = rouge_f1_batch(refs_eval, rrs)\n",
        "\n",
        "# Entities vs **reference**\n",
        "top_ent_ref = eval_entities_vs_ref(refs_eval, tops)\n",
        "rr_ent_ref  = eval_entities_vs_ref(refs_eval, rrs)\n",
        "\n",
        "def _fmt(d): return {k: round(float(v), 4) for k,v in d.items()}\n",
        "\n",
        "print(f\"Evaluating vs **reference** on K={K} examples\")\n",
        "print(\"\\n=== Top-beam (entities vs ref + ROUGE) ===\")\n",
        "print(_fmt(top_ent_ref | top_rouge))\n",
        "print(\"\\n=== Reranked (entities vs ref + ROUGE) ===\")\n",
        "print(_fmt(rr_ent_ref  | rr_rouge))\n",
        "print(\"\\n=== Δ (reranked - top) ===\")\n",
        "delta = {k:(rr_ent_ref|rr_rouge)[k] - (top_ent_ref|top_rouge)[k] for k in (top_ent_ref|top_rouge)}\n",
        "print(_fmt(delta))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hSYTs5-q4Qd",
        "outputId": "52ddcfe3-c828-4e91-a7f7-1398b94dd323"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== top_beam: length stats vs references (K=50) ===\n",
            "Words (ref) : {'n': 50, 'mean': 32.28, 'median': 31.0, 'p25': 24.0, 'p75': 39.0, 'std': 10.25, 'min': 13, 'max': 59}\n",
            "Words (pred): {'n': 50, 'mean': 34.78, 'median': 35.0, 'p25': 32.25, 'p75': 37.75, 'std': 3.71, 'min': 22, 'max': 42}\n",
            "Δ Words (pred - ref): mean=2.50, median=4.00\n",
            "Chars (ref) : {'n': 50, 'mean': 185.78, 'median': 175.0, 'p25': 148.5, 'p75': 229.5, 'std': 57.17, 'min': 77, 'max': 312}\n",
            "Chars (pred): {'n': 50, 'mean': 195.1, 'median': 190.5, 'p25': 180.75, 'p75': 212.25, 'std': 22.89, 'min': 145, 'max': 246}\n",
            "Δ Chars (pred - ref): mean=9.32, median=15.00\n",
            "BPE (ref)  : {'n': 50, 'mean': 38.18, 'median': 37.0, 'p25': 28.0, 'p75': 48.0, 'std': 11.9, 'min': 16, 'max': 62}\n",
            "BPE (pred) : {'n': 50, 'mean': 41.92, 'median': 42.0, 'p25': 42.0, 'p75': 42.0, 'std': 0.44, 'min': 39, 'max': 42}\n",
            "Δ BPE (pred - ref): mean=3.74, median=5.00\n",
            "\n",
            "=== reranked: length stats vs references (K=50) ===\n",
            "Words (ref) : {'n': 50, 'mean': 32.28, 'median': 31.0, 'p25': 24.0, 'p75': 39.0, 'std': 10.25, 'min': 13, 'max': 59}\n",
            "Words (pred): {'n': 50, 'mean': 34.48, 'median': 34.0, 'p25': 33.0, 'p75': 37.0, 'std': 3.31, 'min': 22, 'max': 40}\n",
            "Δ Words (pred - ref): mean=2.20, median=3.50\n",
            "Chars (ref) : {'n': 50, 'mean': 185.78, 'median': 175.0, 'p25': 148.5, 'p75': 229.5, 'std': 57.17, 'min': 77, 'max': 312}\n",
            "Chars (pred): {'n': 50, 'mean': 194.86, 'median': 189.5, 'p25': 180.0, 'p75': 209.0, 'std': 21.01, 'min': 146, 'max': 239}\n",
            "Δ Chars (pred - ref): mean=9.08, median=16.00\n",
            "BPE (ref)  : {'n': 50, 'mean': 38.18, 'median': 37.0, 'p25': 28.0, 'p75': 48.0, 'std': 11.9, 'min': 16, 'max': 62}\n",
            "BPE (pred) : {'n': 50, 'mean': 41.84, 'median': 42.0, 'p25': 42.0, 'p75': 42.0, 'std': 0.81, 'min': 37, 'max': 42}\n",
            "Δ BPE (pred - ref): mean=3.66, median=5.00\n"
          ]
        }
      ],
      "source": [
        "# Cell 6.6 — Length stats: words / characters / tokenizer subwords\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def _word_len_list(strs):\n",
        "    return np.array([len(str(s).split()) for s in strs], dtype=np.int32)\n",
        "\n",
        "def _char_len_list(strs):\n",
        "    return np.array([len(str(s)) for s in strs], dtype=np.int32)\n",
        "\n",
        "def _bpe_len_list(strs, tok):\n",
        "    lens = []\n",
        "    for s in strs:\n",
        "        ids = tok(str(s), add_special_tokens=False, return_tensors=None).input_ids\n",
        "        lens.append(len(ids))\n",
        "    return np.array(lens, dtype=np.int32)\n",
        "\n",
        "def _stats(arr):\n",
        "    arr = np.array(arr)\n",
        "    return dict(\n",
        "        n=int(arr.size),\n",
        "        mean=float(np.mean(arr)),\n",
        "        median=float(np.median(arr)),\n",
        "        p25=float(np.percentile(arr, 25)),\n",
        "        p75=float(np.percentile(arr, 75)),\n",
        "        std=float(np.std(arr)),\n",
        "        min=int(np.min(arr)),\n",
        "        max=int(np.max(arr)),\n",
        "    )\n",
        "\n",
        "# Collect prediction sets available\n",
        "pred_sets = {}\n",
        "if 'top_out' in globals():\n",
        "    pred_sets['top_beam'] = top_out\n",
        "if 'rr_out' in globals():\n",
        "    pred_sets['reranked'] = rr_out\n",
        "\n",
        "assert len(pred_sets) > 0, \"No predictions found. Run decode first.\"\n",
        "\n",
        "# Align K across refs & predictions\n",
        "K = min(*(len(v) for v in pred_sets.values()), len(df_val))\n",
        "refs = df_val[\"highlights\"].iloc[:K].astype(str).tolist()\n",
        "\n",
        "for name, preds in pred_sets.items():\n",
        "    preds = preds[:K]\n",
        "\n",
        "    ref_w  = _word_len_list(refs)\n",
        "    hyp_w  = _word_len_list(preds)\n",
        "    ref_c  = _char_len_list(refs)\n",
        "    hyp_c  = _char_len_list(preds)\n",
        "    ref_bp = _bpe_len_list(refs, tok)\n",
        "    hyp_bp = _bpe_len_list(preds, tok)\n",
        "\n",
        "    print(f\"\\n=== {name}: length stats vs references (K={K}) ===\")\n",
        "\n",
        "    w_ref = {k: (round(v,2) if isinstance(v,float) else v) for k,v in _stats(ref_w).items()}\n",
        "    w_hyp = {k: (round(v,2) if isinstance(v,float) else v) for k,v in _stats(hyp_w).items()}\n",
        "    print(\"Words (ref) :\", w_ref)\n",
        "    print(\"Words (pred):\", w_hyp)\n",
        "    print(\"Δ Words (pred - ref): mean={:.2f}, median={:.2f}\".format(\n",
        "        float(np.mean(hyp_w - ref_w)), float(np.median(hyp_w - ref_w))\n",
        "    ))\n",
        "\n",
        "    c_ref = {k: (round(v,2) if isinstance(v,float) else v) for k,v in _stats(ref_c).items()}\n",
        "    c_hyp = {k: (round(v,2) if isinstance(v,float) else v) for k,v in _stats(hyp_c).items()}\n",
        "    print(\"Chars (ref) :\", c_ref)\n",
        "    print(\"Chars (pred):\", c_hyp)\n",
        "    print(\"Δ Chars (pred - ref): mean={:.2f}, median={:.2f}\".format(\n",
        "        float(np.mean(hyp_c - ref_c)), float(np.median(hyp_c - ref_c))\n",
        "    ))\n",
        "\n",
        "    b_ref = {k: (round(v,2) if isinstance(v,float) else v) for k,v in _stats(ref_bp).items()}\n",
        "    b_hyp = {k: (round(v,2) if isinstance(v,float) else v) for k,v in _stats(hyp_bp).items()}\n",
        "    print(\"BPE (ref)  :\", b_ref)\n",
        "    print(\"BPE (pred) :\", b_hyp)\n",
        "    print(\"Δ BPE (pred - ref): mean={:.2f}, median={:.2f}\".format(\n",
        "        float(np.mean(hyp_bp - ref_bp)), float(np.median(hyp_bp - ref_bp))\n",
        "    ))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sthc24ckxbYN"
      },
      "source": [
        "> reranker improve with beams reusing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5RoZXu9sxoV"
      },
      "outputs": [],
      "source": [
        "# Cell A1 — decode ONCE and cache beams+scores for fast reranking\n",
        "import numpy as np, torch\n",
        "from torch.amp import autocast\n",
        "\n",
        "@torch.no_grad()\n",
        "def decode_collect_beams(articles, batch_size=8):\n",
        "    bundles = []  # list of dicts with: src, texts (list[str]), base_scores (np.array)\n",
        "    for i in range(0, len(articles), batch_size):\n",
        "        batch_src = articles[i:i+batch_size]\n",
        "        enc = tok(batch_src, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
        "        ctx = autocast(device_type=\"cuda\", dtype=torch.float16) if torch.cuda.is_available() else torch.no_grad()\n",
        "        with ctx:\n",
        "            gen = model.generate(**enc, **GEN_KW)\n",
        "\n",
        "        B = enc.input_ids.size(0)\n",
        "        beams = GEN_KW[\"num_return_sequences\"]\n",
        "        seqs = gen.sequences.view(B, beams, -1)\n",
        "        base_scores = gen.sequences_scores.view(B, beams).cpu().numpy()\n",
        "        all_texts = tok.batch_decode(seqs.view(-1, seqs.size(-1)), skip_special_tokens=True)\n",
        "\n",
        "        for j, src in enumerate(batch_src):\n",
        "            texts = all_texts[j*beams:(j+1)*beams]\n",
        "            bundles.append({\"src\": src, \"texts\": texts, \"base_scores\": base_scores[j]})\n",
        "    return bundles\n",
        "\n",
        "# run it on your K=50 set (same list you used)\n",
        "K = 50\n",
        "articles_eval = df_val[\"article\"].iloc[:K].astype(str).tolist()\n",
        "beam_bundles = decode_collect_beams(articles_eval)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa0eryRQxqQE"
      },
      "outputs": [],
      "source": [
        "# Cell A2 — improved reranker (penalize unsup; reward supported; soft length)\n",
        "import re, numpy as np\n",
        "\n",
        "# weights (start here; tweak live)\n",
        "A_UNSUP_ENT = 0.9     # penalty per unsupported capitalized span\n",
        "B_UNSUP_NUM = 0.8     # penalty per unseen number\n",
        "C_REPEAT    = 3.0     # repetition penalty\n",
        "D_LEN       = 0.25    # softer length reg (generation already aligned)\n",
        "W_SUP_ENT   = 0.12    # reward per supported cap span\n",
        "W_SUP_NUM   = 0.06    # reward per supported number\n",
        "LENGTH_TGT_WORDS = 31\n",
        "\n",
        "_NUM_RE    = re.compile(r\"\\d[\\d,.\\-/]*%?\")\n",
        "_CAP_SPAN  = re.compile(r\"(?:\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)+)\")\n",
        "_TOKEN_RE  = re.compile(r\"[A-Za-z0-9]+(?:[-_/][A-Za-z0-9]+)*\")\n",
        "\n",
        "def _find_numbers(s): return set(_NUM_RE.findall(s))\n",
        "def _find_caps(s):    return set(_CAP_SPAN.findall(s))\n",
        "def _repeat_fraction(h, n=3):\n",
        "    toks = _TOKEN_RE.findall(h.lower())\n",
        "    if len(toks) < n+1: return 0.0\n",
        "    grams = [tuple(toks[i:i+n]) for i in range(len(toks)-n+1)]\n",
        "    return max(0.0, 1.0 - len(set(grams))/len(grams))\n",
        "\n",
        "def _norm(s): return re.sub(r\"[^A-Za-z0-9 ]+\", \"\", s).lower().strip()\n",
        "def _acronym(s):\n",
        "    ws = re.findall(r\"\\b[A-Za-z]+\\b\", s)\n",
        "    return \"\".join(w[0].upper() for w in ws) if len(ws) >= 2 else None\n",
        "def _supported_in_source(span, src_raw, src_norm):\n",
        "    if span in src_raw: return True\n",
        "    cn = _norm(span)\n",
        "    if cn and cn in src_norm: return True\n",
        "    ac = _acronym(span)\n",
        "    return bool(ac and ac in src_raw.upper())\n",
        "\n",
        "def rerank_score(src, hyp, base, length_tgt=LENGTH_TGT_WORDS):\n",
        "    src_nums = _find_numbers(src)\n",
        "    src_norm = _norm(src); src_raw = src\n",
        "    hyp_nums = _find_numbers(hyp)\n",
        "    hyp_caps = _find_caps(hyp)\n",
        "\n",
        "    unsup_ent = sup_ent = 0\n",
        "    for e in hyp_caps:\n",
        "        if _supported_in_source(e, src_raw, src_norm): sup_ent += 1\n",
        "        else: unsup_ent += 1\n",
        "\n",
        "    sup_num   = sum(1 for n in hyp_nums if n in src_nums)\n",
        "    unsup_num = sum(1 for n in hyp_nums if n not in src_nums)\n",
        "\n",
        "    repeat_f  = _repeat_fraction(hyp, n=3)\n",
        "    L         = len(hyp.split())\n",
        "    rel_len   = abs(L - length_tgt) / max(1, length_tgt)\n",
        "\n",
        "    return ( float(base)\n",
        "           - A_UNSUP_ENT*unsup_ent\n",
        "           - B_UNSUP_NUM*unsup_num\n",
        "           - C_REPEAT   *repeat_f\n",
        "           - D_LEN      *rel_len\n",
        "           + W_SUP_ENT  *sup_ent\n",
        "           + W_SUP_NUM  *sup_num )\n",
        "\n",
        "def apply_reranker(bundles):\n",
        "    tops, rrs = [], []\n",
        "    for b in bundles:\n",
        "        src, texts, base = b[\"src\"], b[\"texts\"], b[\"base_scores\"]\n",
        "        # top-beam is index 0 when do_sample=False\n",
        "        tops.append(texts[0])\n",
        "        scores = [rerank_score(src, h, bs) for h, bs in zip(texts, base)]\n",
        "        rrs.append(texts[int(np.argmax(scores))])\n",
        "    return tops, rrs\n",
        "\n",
        "# Apply (no re-generation needed)\n",
        "top_out, rr_out = apply_reranker(beam_bundles)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOme4jGYxync",
        "outputId": "52b9a3b5-1ad3-4082-f8ac-8fe8a3c70d2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beams reranked for K=50 examples.\n",
            "\n",
            "==== Example 0 ====\n",
            "SOURCE  : Share, and your gift will be multiplied. That may sound like an esoteric adage, but when Zully Broussard selflessly decided to give one of her kidneys to a stranger, her generosity paired up with big data. It resulted in six patients receiving transplants. That surprised and wowed her. \"I thought I ...\n",
            "TOP     : Zully Broussard's generosity paired up with big data . It resulted in six patients receiving transplants . That surprised and wowed her . \"I thought I was going to help this one person who\n",
            "RERANKED: Zully Broussard's generosity paired up with big data . It resulted in six patients receiving transplants . That surprised and wowed her . \"I thought I was going to help this one person who\n",
            "\n",
            "==== Example 1 ====\n",
            "SOURCE  : On the 6th of April 1996, San Jose Clash and DC United strode out in front of 31,683 expectant fans at the Spartan Stadium in San Jose, California. The historic occasion was the first ever Major League Soccer match -- a brave new dawn for the world's favorite sport in a land its charms had yet to co...\n",
            "TOP     : On the 6th of April 1996, San Jose Clash and DC United strode out in front of 31,683 expectant fans . The historic occasion was a brave new dawn for the world's favorite sport in\n",
            "RERANKED: On the 6th of April 1996, San Jose Clash and DC United strode out in front of 31,683 expectant fans at the Spartan Stadium in San Jose, California . The historic occasion was the first\n",
            "\n",
            "==== Example 2 ====\n",
            "SOURCE  : French striker Bafetimbi Gomis, who has a history of fainting, said he is now \"feeling well\" after collapsing during Swansea's 3-2 loss at Tottenham in the Premier League on Wednesday. The worrying incident occurred in the first half at White Hart Lane -- after Tottenham scored in the seventh minute...\n",
            "TOP     : French striker Bafetimbi Gomis collapses during Swansea's 3-2 loss at Tottenham . The 29-year-old left the pitch conscious following about five minutes of treatment . The match was temporarily\n",
            "RERANKED: French striker Bafetimbi Gomis collapses during Swansea's 3-2 loss at Tottenham . The 29-year-old left the pitch conscious after about five minutes of treatment . The Welsh side's\n",
            "\n",
            "==== Example 3 ====\n",
            "SOURCE  : It was an act of frustration perhaps more commonly associated with golf's fictional anti-hero Happy Gilmore than the world's reigning No 1. player. But when Rory McIlroy pulled his second shot on the eighth hole of the WGC Cadillac Championship into a lake Friday, he might as well have been channeli...\n",
            "TOP     : Rory McIlroy pulled his second shot on the eighth hole of the WGC Cadillac Championship into a lake . The four-time major winner launched the 3-iron used to play the offending shot into the\n",
            "RERANKED: Rory McIlroy pulled his second shot on the eighth hole of the WGC Cadillac Championship into a lake . The four-time major winner launched the 3-iron used to play the offending shot into the\n",
            "\n",
            "==== Example 4 ====\n",
            "SOURCE  : A Pennsylvania community is pulling together to search for an eighth-grade student who has been missing since Wednesday. The search has drawn hundreds of volunteers on foot and online. The parents of Cayman Naib, 13, have been communicating through the Facebook group \"Find Cayman\" since a day after ...\n",
            "TOP     : Cayman Naib, 13, was last seen wearing a gray down winter jacket, black ski pants and hiking boots . He could be in the Radnor-Wayne area, roughly 20 miles from Philadelphia\n",
            "RERANKED: Cayman Naib, 13, was last seen wearing a gray down winter jacket, black ski pants and hiking boots . He could be in the Radnor-Wayne area, roughly 20 miles from Philadelphia\n"
          ]
        }
      ],
      "source": [
        "# Cell A3 — apply reranker and preview a few\n",
        "\n",
        "# Uses: beam_bundles (from A1), apply_reranker(...) & rerank_score(...) (from A2)\n",
        "top_out, rr_out = apply_reranker(beam_bundles)\n",
        "\n",
        "print(f\"Beams reranked for K={len(beam_bundles)} examples.\")\n",
        "for i in range(min(5, len(beam_bundles))):\n",
        "    print(f\"\\n==== Example {i} ====\")\n",
        "    print(\"SOURCE  :\", beam_bundles[i][\"src\"][:300].replace(\"\\n\",\" \") + (\"...\" if len(beam_bundles[i][\"src\"])>300 else \"\"))\n",
        "    print(\"TOP     :\", top_out[i])\n",
        "    print(\"RERANKED:\", rr_out[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0tI499qyMCm",
        "outputId": "3cc418c4-c3b3-4789-f570-69ee74455ff2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== top_beam: length stats vs references (K=50) ===\n",
            "Words (ref) : {'n': 50, 'mean': 32.28, 'median': 31.0, 'p25': 24.0, 'p75': 39.0, 'std': 10.25, 'min': 13, 'max': 59}\n",
            "Words (pred): {'n': 50, 'mean': 34.78, 'median': 35.0, 'p25': 32.25, 'p75': 37.75, 'std': 3.71, 'min': 22, 'max': 42}\n",
            "Δ Words mean=2.50 | median=4.00\n",
            "Chars (ref) : {'n': 50, 'mean': 185.78, 'median': 175.0, 'p25': 148.5, 'p75': 229.5, 'std': 57.17, 'min': 77, 'max': 312}\n",
            "Chars (pred): {'n': 50, 'mean': 195.1, 'median': 190.5, 'p25': 180.75, 'p75': 212.25, 'std': 22.89, 'min': 145, 'max': 246}\n",
            "Δ Chars mean=9.32 | median=15.00\n",
            "BPE (ref)  : {'n': 50, 'mean': 38.18, 'median': 37.0, 'p25': 28.0, 'p75': 48.0, 'std': 11.9, 'min': 16, 'max': 62}\n",
            "BPE (pred) : {'n': 50, 'mean': 41.92, 'median': 42.0, 'p25': 42.0, 'p75': 42.0, 'std': 0.44, 'min': 39, 'max': 42}\n",
            "Δ BPE mean=3.74 | median=5.00\n",
            "\n",
            "=== reranked: length stats vs references (K=50) ===\n",
            "Words (ref) : {'n': 50, 'mean': 32.28, 'median': 31.0, 'p25': 24.0, 'p75': 39.0, 'std': 10.25, 'min': 13, 'max': 59}\n",
            "Words (pred): {'n': 50, 'mean': 34.6, 'median': 35.0, 'p25': 33.0, 'p75': 37.0, 'std': 3.34, 'min': 22, 'max': 40}\n",
            "Δ Words mean=2.32 | median=4.00\n",
            "Chars (ref) : {'n': 50, 'mean': 185.78, 'median': 175.0, 'p25': 148.5, 'p75': 229.5, 'std': 57.17, 'min': 77, 'max': 312}\n",
            "Chars (pred): {'n': 50, 'mean': 197.42, 'median': 192.0, 'p25': 183.0, 'p75': 214.0, 'std': 22.67, 'min': 146, 'max': 246}\n",
            "Δ Chars mean=11.64 | median=16.00\n",
            "BPE (ref)  : {'n': 50, 'mean': 38.18, 'median': 37.0, 'p25': 28.0, 'p75': 48.0, 'std': 11.9, 'min': 16, 'max': 62}\n",
            "BPE (pred) : {'n': 50, 'mean': 41.9, 'median': 42.0, 'p25': 42.0, 'p75': 42.0, 'std': 0.46, 'min': 39, 'max': 42}\n",
            "Δ BPE mean=3.72 | median=5.00\n"
          ]
        }
      ],
      "source": [
        "# Cell A4 — length stats vs references (words/chars/BPE)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def _word_len_list(strs):\n",
        "    return np.array([len(str(s).split()) for s in strs], dtype=np.int32)\n",
        "\n",
        "def _char_len_list(strs):\n",
        "    return np.array([len(str(s)) for s in strs], dtype=np.int32)\n",
        "\n",
        "def _bpe_len_list(strs, tok):\n",
        "    return np.array([len(tok(str(s), add_special_tokens=False).input_ids) for s in strs], dtype=np.int32)\n",
        "\n",
        "def _stats(arr):\n",
        "    arr = np.array(arr)\n",
        "    return dict(\n",
        "        n=int(arr.size), mean=float(np.mean(arr)), median=float(np.median(arr)),\n",
        "        p25=float(np.percentile(arr,25)), p75=float(np.percentile(arr,75)),\n",
        "        std=float(np.std(arr)), min=int(np.min(arr)), max=int(np.max(arr))\n",
        "    )\n",
        "\n",
        "K = len(beam_bundles)\n",
        "refs = df_val[\"highlights\"].iloc[:K].astype(str).tolist()\n",
        "\n",
        "for name, preds in {\"top_beam\": top_out, \"reranked\": rr_out}.items():\n",
        "    preds = preds[:K]\n",
        "    ref_w, hyp_w = _word_len_list(refs), _word_len_list(preds)\n",
        "    ref_c, hyp_c = _char_len_list(refs), _char_len_list(preds)\n",
        "    ref_b, hyp_b = _bpe_len_list(refs, tok), _bpe_len_list(preds, tok)\n",
        "\n",
        "    print(f\"\\n=== {name}: length stats vs references (K={K}) ===\")\n",
        "    print(\"Words (ref) :\", {k:(round(v,2) if isinstance(v,float) else v) for k,v in _stats(ref_w).items()})\n",
        "    print(\"Words (pred):\", {k:(round(v,2) if isinstance(v,float) else v) for k,v in _stats(hyp_w).items()})\n",
        "    print(\"Δ Words mean={:.2f} | median={:.2f}\".format(float(np.mean(hyp_w-ref_w)), float(np.median(hyp_w-ref_w))))\n",
        "    print(\"Chars (ref) :\", {k:(round(v,2) if isinstance(v,float) else v) for k,v in _stats(ref_c).items()})\n",
        "    print(\"Chars (pred):\", {k:(round(v,2) if isinstance(v,float) else v) for k,v in _stats(hyp_c).items()})\n",
        "    print(\"Δ Chars mean={:.2f} | median={:.2f}\".format(float(np.mean(hyp_c-ref_c)), float(np.median(hyp_c-ref_c))))\n",
        "    print(\"BPE (ref)  :\", {k:(round(v,2) if isinstance(v,float) else v) for k,v in _stats(ref_b).items()})\n",
        "    print(\"BPE (pred) :\", {k:(round(v,2) if isinstance(v,float) else v) for k,v in _stats(hyp_b).items()})\n",
        "    print(\"Δ BPE mean={:.2f} | median={:.2f}\".format(float(np.mean(hyp_b-ref_b)), float(np.median(hyp_b-ref_b))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbLtI6NdyPPg",
        "outputId": "2d38017a-ef9c-44a9-efa8-6d7e866b5916"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating vs **reference** on K=50 examples\n",
            "\n",
            "=== Top-beam (entities vs ref + ROUGE) ===\n",
            "{'entP_ref': 0.3307, 'entR_ref': 0.349, 'entF1_ref': 0.3098, 'rouge1': 0.3413, 'rouge2': 0.135, 'rougeLsum': 0.2542}\n",
            "\n",
            "=== Reranked (entities vs ref + ROUGE) ===\n",
            "{'entP_ref': 0.3173, 'entR_ref': 0.39, 'entF1_ref': 0.3275, 'rouge1': 0.3525, 'rouge2': 0.1434, 'rougeLsum': 0.2595}\n",
            "\n",
            "=== Δ (reranked - top) ===\n",
            "{'entP_ref': -0.0134, 'entR_ref': 0.041, 'entF1_ref': 0.0177, 'rouge1': 0.0112, 'rouge2': 0.0085, 'rougeLsum': 0.0053}\n"
          ]
        }
      ],
      "source": [
        "# Cell A5 — evaluation vs reference (entities + ROUGE)\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# ---- spaCy NER optional ----\n",
        "try:\n",
        "    import spacy\n",
        "    try:\n",
        "        _nlp = spacy.load(\"en_core_web_sm\")\n",
        "    except Exception:\n",
        "        _nlp = None\n",
        "except Exception:\n",
        "    _nlp = None\n",
        "\n",
        "_WORD_RE = re.compile(r\"[A-Za-z0-9]+(?:[-_/][A-Za-z0-9]+)*\")\n",
        "_NUM_RE  = re.compile(r\"\\d[\\d,.\\-/]*\")\n",
        "\n",
        "def _words(txt: str): return _WORD_RE.findall(str(txt).lower())\n",
        "\n",
        "def _spans_simple(txt: str):\n",
        "    spans = set()\n",
        "    for m in _NUM_RE.finditer(str(txt)): spans.add(m.group(0))\n",
        "    toks = re.findall(r\"\\b[A-Z][a-z]+\\b\", str(txt))\n",
        "    for i in range(len(toks)-1):\n",
        "        if toks[i][0].isupper() and toks[i+1][0].isupper():\n",
        "            spans.add(f\"{toks[i]} {toks[i+1]}\")\n",
        "    return list(spans)\n",
        "\n",
        "def extract_entities(txt: str):\n",
        "    if _nlp is not None:\n",
        "        doc = _nlp(str(txt))\n",
        "        return [ent.text for ent in doc.ents]\n",
        "    return _spans_simple(txt)\n",
        "\n",
        "def entity_prf_against_reference(reference: str, hyp: str):\n",
        "    ref_ents = set(s.strip() for s in extract_entities(reference) if s.strip())\n",
        "    hyp_ents = set(s.strip() for s in extract_entities(hyp) if s.strip())\n",
        "    if not ref_ents and not hyp_ents: return 1.0, 1.0, 1.0\n",
        "    if not hyp_ents: return 0.0, 0.0, 0.0\n",
        "    tp = len(ref_ents & hyp_ents)\n",
        "    prec = tp / max(1, len(hyp_ents))\n",
        "    rec  = tp / max(1, len(ref_ents))\n",
        "    f1   = 0.0 if (prec+rec)==0 else 2*prec*rec/(prec+rec)\n",
        "    return prec, rec, f1\n",
        "\n",
        "# ---- ROUGE ----\n",
        "try:\n",
        "    from rouge_score import rouge_scorer\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"rouge-score\"])\n",
        "    from rouge_score import rouge_scorer\n",
        "\n",
        "_sc = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\"], use_stemmer=True)\n",
        "\n",
        "def rouge_f1_batch(refs, hyps):\n",
        "    r1=r2=rl=0.0\n",
        "    for ref, hyp in zip(refs, hyps):\n",
        "        sc = _sc.score(str(ref), str(hyp))\n",
        "        r1 += sc[\"rouge1\"].fmeasure\n",
        "        r2 += sc[\"rouge2\"].fmeasure\n",
        "        rl += sc[\"rougeLsum\"].fmeasure\n",
        "    n = len(hyps)\n",
        "    return {\"rouge1\": r1/n, \"rouge2\": r2/n, \"rougeLsum\": rl/n}\n",
        "\n",
        "def eval_entities_vs_ref_batch(refs, hyps):\n",
        "    n=len(hyps); p=r=f1=0.0\n",
        "    for ref, hyp in zip(refs, hyps):\n",
        "        pi,ri,fi = entity_prf_against_reference(ref, hyp)\n",
        "        p+=pi; r+=ri; f1+=fi\n",
        "    return {\"entP_ref\": p/n, \"entR_ref\": r/n, \"entF1_ref\": f1/n}\n",
        "\n",
        "# ---- run on the same K as beam_bundles ----\n",
        "K = len(beam_bundles)\n",
        "refs_eval = df_val[\"highlights\"].iloc[:K].astype(str).tolist()\n",
        "\n",
        "def _fmt(d): return {k: round(float(v), 4) for k,v in d.items()}\n",
        "\n",
        "# Top-beam\n",
        "top_rouge = rouge_f1_batch(refs_eval, top_out)\n",
        "top_ent   = eval_entities_vs_ref_batch(refs_eval, top_out)\n",
        "\n",
        "# Reranked\n",
        "rr_rouge  = rouge_f1_batch(refs_eval, rr_out)\n",
        "rr_ent    = eval_entities_vs_ref_batch(refs_eval, rr_out)\n",
        "\n",
        "print(f\"Evaluating vs **reference** on K={K} examples\")\n",
        "print(\"\\n=== Top-beam (entities vs ref + ROUGE) ===\")\n",
        "print(_fmt(top_ent | top_rouge))\n",
        "print(\"\\n=== Reranked (entities vs ref + ROUGE) ===\")\n",
        "print(_fmt(rr_ent | rr_rouge))\n",
        "print(\"\\n=== Δ (reranked - top) ===\")\n",
        "delta = {k:(rr_ent|rr_rouge)[k] - (top_ent|top_rouge)[k] for k in (top_ent|top_rouge)}\n",
        "print(_fmt(delta))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1miaElcVzI54"
      },
      "source": [
        "# Further Training 4k to 5k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4acc4dQ2PY-",
        "outputId": "ceb9bf2d-7aa2-4fba-f31b-58dc94099064"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: transformers 4.43.0\n",
            "Uninstalling transformers-4.43.0:\n",
            "  Successfully uninstalled transformers-4.43.0\n",
            "Found existing installation: rouge_score 0.1.2\n",
            "Uninstalling rouge_score-0.1.2:\n",
            "  Successfully uninstalled rouge_score-0.1.2\n",
            "Found existing installation: accelerate 0.34.2\n",
            "Uninstalling accelerate-0.34.2:\n",
            "  Successfully uninstalled accelerate-0.34.2\n",
            "Found existing installation: datasets 2.20.0\n",
            "Uninstalling datasets-2.20.0:\n",
            "  Successfully uninstalled datasets-2.20.0\n",
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "!pip install -q rouge-score==0.1.2\n",
        "\n",
        "!pip uninstall -y transformers rouge-score accelerate datasets\n",
        "\n",
        "!pip install -q \\\n",
        "  transformers==4.43.4 \\\n",
        "  accelerate==0.34.2 \\\n",
        "  datasets==2.20.0 \\\n",
        "  rouge-score==0.1.2\n",
        "import os, csv, math, time, random, json, re, gc\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Optional\n",
        "from torch import amp\n",
        "from rouge_score import rouge_scorer\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM,\n",
        "    get_linear_schedule_with_warmup, set_seed\n",
        ")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QSVCrm156XK"
      },
      "outputs": [],
      "source": [
        "TRAIN_CSV = \"/content/drive/MyDrive/cleaned_outputs/naa/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast_applied.csv\"\n",
        "VAL_CSV   = \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "\n",
        "RESUME_CKPT = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step4000\"\n",
        "\n",
        "\n",
        "RUN_DIR      = \"/content/drive/MyDrive/student_pgc_bartbase\"\n",
        "RESUME_STEP  = 4000\n",
        "TARGET_STEPS = 5000\n",
        "RESUME_CKPT  = os.path.join(RUN_DIR, f\"ckpt_step{RESUME_STEP}\")\n",
        "\n",
        "\n",
        "\n",
        "MODEL_NAME = \"facebook/bart-base\"\n",
        "\n",
        "# --- Lengths ---\n",
        "MAX_SRC_LEN = 400\n",
        "MAX_TGT_LEN = 100\n",
        "\n",
        "# --- Training ---\n",
        "SEED                 = 0\n",
        "BATCH_SIZE           = 48\n",
        "GRAD_ACCUM_STEPS     = 2\n",
        "NUM_WORKERS          = 4\n",
        "LR                   = 7e-6\n",
        "WARMUP_RATIO         = 0.10\n",
        "WEIGHT_DECAY         = 0.01\n",
        "MAX_STEPS            = 1000\n",
        "FP16                 = True\n",
        "FREEZE_ENCODER       = True\n",
        "LOG_EVERY            = 100\n",
        "SAVE_EVERY_STEPS     = 1000\n",
        "EPOCHS               = 99\n",
        "# --- Pointer/Coverage ---\n",
        "USE_POINTER          = True\n",
        "LAMBDA_COV           = 0.9\n",
        "EPS                  = 1e-8\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTN-53uY6t0Y"
      },
      "outputs": [],
      "source": [
        "def robust_read_csv(path: str) -> pd.DataFrame:\n",
        "    try:\n",
        "        df = pd.read_csv(path, engine=\"python\", quoting=csv.QUOTE_ALL, escapechar=\"\\\\\")\n",
        "        cols = {c.lower(): c for c in df.columns}\n",
        "        return df[[cols[\"id\"], cols[\"article\"], cols[\"highlights\"]]].rename(\n",
        "            columns={cols[\"id\"]:\"id\", cols[\"article\"]:\"article\", cols[\"highlights\"]:\"highlights\"}\n",
        "        )\n",
        "    except Exception:\n",
        "        rows = []\n",
        "        with open(path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "            reader = csv.reader(f, delimiter=\",\", quotechar='\"', escapechar=\"\\\\\")\n",
        "            header = next(reader)\n",
        "            header = [h.strip().lower() for h in header]\n",
        "            idx_id, idx_art, idx_sum = header.index(\"id\"), header.index(\"article\"), header.index(\"highlights\")\n",
        "            for row in reader:\n",
        "                rows.append([row[idx_id], row[idx_art], row[idx_sum]])\n",
        "        return pd.DataFrame(rows, columns=[\"id\",\"article\",\"highlights\"])\n",
        "\n",
        "# --- Dataset ---\n",
        "class CNNDMDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame):\n",
        "        self.ids  = df[\"id\"].astype(str).tolist()\n",
        "        self.srcs = df[\"article\"].astype(str).tolist()\n",
        "        self.tgts = df[\"highlights\"].astype(str).tolist()\n",
        "    def __len__(self): return len(self.ids)\n",
        "    def __getitem__(self, i):\n",
        "        return dict(id=self.ids[i], article=self.srcs[i], highlights=self.tgts[i])\n",
        "\n",
        "# --- Collator (no as_target_tokenizer; supports old/new HF) ---\n",
        "@dataclass\n",
        "class PGDataCollator:\n",
        "    tok: AutoTokenizer\n",
        "    max_src_len: int\n",
        "    max_tgt_len: int\n",
        "    def __call__(self, batch: List[Dict[str, str]]) -> Dict[str, torch.Tensor]:\n",
        "        src_texts = [b[\"article\"] for b in batch]\n",
        "        tgt_texts = [b[\"highlights\"] for b in batch]\n",
        "        try:\n",
        "            enc = self.tok(\n",
        "                src_texts,\n",
        "                text_target=tgt_texts,\n",
        "                padding=True, truncation=True,\n",
        "                max_length=self.max_src_len,\n",
        "                max_length_target=self.max_tgt_len,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "        except TypeError:\n",
        "            enc = self.tok(src_texts, padding=True, truncation=True,\n",
        "                           max_length=self.max_src_len, return_tensors=\"pt\")\n",
        "            tgt = self.tok(text_target=tgt_texts, padding=True, truncation=True,\n",
        "                           max_length=self.max_tgt_len, return_tensors=\"pt\")\n",
        "            enc[\"labels\"] = tgt[\"input_ids\"]\n",
        "        labels = enc[\"labels\"]\n",
        "        labels[labels == self.tok.pad_token_id] = -100\n",
        "        enc[\"labels\"] = labels\n",
        "        return enc\n",
        "\n",
        "# --- quick utils ---\n",
        "def _gpu_mem_mb():\n",
        "    if not torch.cuda.is_available(): return 0.0\n",
        "    return torch.cuda.max_memory_allocated() / (1024**2)\n",
        "\n",
        "class RunningMean:\n",
        "    def __init__(self, n=200):\n",
        "        self.buf, self.n = [], n\n",
        "    def add(self, x):\n",
        "        if x is None: return\n",
        "        self.buf.append(float(x))\n",
        "        if len(self.buf) > self.n: self.buf.pop(0)\n",
        "    @property\n",
        "    def mean(self):\n",
        "        return sum(self.buf)/len(self.buf) if self.buf else 0.0\n",
        "\n",
        "class StepLogger:\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "        if not os.path.exists(path):\n",
        "            with open(path, \"w\") as f:\n",
        "                f.write(\"epoch,step,loss,ce_loss,cov_loss,p_copy,p_gen,lr,grad_norm,toks_per_s,gpu_mem_mb\\n\")\n",
        "    def log(self, row: Dict):\n",
        "        with open(self.path, \"a\") as f:\n",
        "            f.write(\",\".join(str(row[k]) for k in [\"epoch\",\"step\",\"loss\",\"ce_loss\",\"cov_loss\",\"p_copy\",\"p_gen\",\"lr\",\"grad_norm\",\"toks_per_s\",\"gpu_mem_mb\"])+\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJ0eyA6P6uOw"
      },
      "outputs": [],
      "source": [
        "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
        "\n",
        "class CopyAwareBart(nn.Module):\n",
        "    \"\"\"\n",
        "    BART wrapper with pointer-generator + coverage.\n",
        "    - mixes vocab distribution with copy distribution built from cross-attn over source tokens\n",
        "    - computes CE over log(final_dist) + lambda_cov * coverage\n",
        "    \"\"\"\n",
        "    def __init__(self, base_lm: AutoModelForSeq2SeqLM, tokenizer: AutoTokenizer,\n",
        "                 lambda_cov: float = 1.0, eps: float = 1e-8, use_pointer: bool = True):\n",
        "        super().__init__()\n",
        "        self.base       = base_lm\n",
        "        self.tok        = tokenizer\n",
        "        self.lambda_cov = lambda_cov\n",
        "        self.eps        = eps\n",
        "        self.use_ptr    = use_pointer\n",
        "        hidden = self.base.config.d_model\n",
        "        self.p_gen_linear = nn.Linear(3*hidden, 1)\n",
        "        # caches for logging\n",
        "        self._last_ce_loss = None\n",
        "        self._last_cov_loss = None\n",
        "        self._last_p_copy_mean = None\n",
        "        self._last_p_gen_mean  = None\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        decoder_input_ids: Optional[torch.Tensor] = None,\n",
        "        decoder_attention_mask: Optional[torch.Tensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        past_key_values=None, use_cache: Optional[bool] = None, **kwargs\n",
        "    ) -> Seq2SeqLMOutput:\n",
        "\n",
        "        if labels is not None and decoder_input_ids is None:\n",
        "            decoder_input_ids = self.base.prepare_decoder_input_ids_from_labels(labels)\n",
        "        if decoder_attention_mask is None and decoder_input_ids is not None:\n",
        "            decoder_attention_mask = (decoder_input_ids != self.tok.pad_token_id).to(input_ids.dtype)\n",
        "\n",
        "        need_attn = self.use_ptr or bool(output_attentions)\n",
        "        need_hid  = self.use_ptr or bool(output_hidden_states)\n",
        "\n",
        "        base_out = self.base(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            output_attentions=need_attn,\n",
        "            output_hidden_states=need_hid,\n",
        "            labels=None,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        logits = base_out.logits  # [B,T,V]\n",
        "        final_logits = logits\n",
        "        cov_loss = None\n",
        "        copy_gate = None\n",
        "        gen_gate  = None\n",
        "        loss_ce   = None\n",
        "\n",
        "        if self.use_ptr:\n",
        "            # hidden states\n",
        "            dec_hid = base_out.decoder_hidden_states[-1]            # [B,T,H]\n",
        "            enc_out = base_out.encoder_last_hidden_state            # [B,S,H]\n",
        "\n",
        "            # cross-attn last layer: [B,heads,T,S] -> mean heads -> [B,T,S]\n",
        "            cross_atts = base_out.cross_attentions[-1].mean(dim=1)\n",
        "\n",
        "            # mask PAD in source attention\n",
        "            if attention_mask is not None:\n",
        "                src_pad_mask = (attention_mask == 0).unsqueeze(1)   # [B,1,S]\n",
        "                attn_clean   = cross_atts.masked_fill(src_pad_mask, 0.0)\n",
        "            else:\n",
        "                attn_clean   = cross_atts\n",
        "\n",
        "            # normalize per time step\n",
        "            denom     = attn_clean.sum(dim=-1, keepdim=True).clamp_min(self.eps)\n",
        "            attn_norm = attn_clean / denom                          # [B,T,S]\n",
        "\n",
        "            # context vectors\n",
        "            context = torch.bmm(attn_norm, enc_out)                 # [B,T,H]\n",
        "\n",
        "            # prev token embeddings\n",
        "            if decoder_input_ids is not None:\n",
        "                dec_emb = self.base.get_input_embeddings()(decoder_input_ids)  # [B,T,H]\n",
        "            else:\n",
        "                dec_emb = torch.zeros_like(dec_hid)\n",
        "\n",
        "            # generation/copy gates\n",
        "            p_gen  = torch.sigmoid(self.p_gen_linear(torch.cat([dec_hid, context, dec_emb], dim=-1)))  # [B,T,1]\n",
        "            p_copy = 1.0 - p_gen\n",
        "\n",
        "            # vocab distribution\n",
        "            vocab_dist = torch.softmax(logits, dim=-1)              # [B,T,V]\n",
        "\n",
        "            # copy distribution: scatter attention into vocab bins using source token ids\n",
        "            B, T, S = attn_norm.shape\n",
        "            V       = logits.size(-1)\n",
        "            copy_dist = torch.zeros(B, T, V, device=attn_norm.device, dtype=attn_norm.dtype)\n",
        "\n",
        "            batch_idx = torch.arange(B, device=input_ids.device)[:, None, None].expand(B, T, S)\n",
        "            time_idx  = torch.arange(T, device=input_ids.device)[None, :, None].expand(B, T, S)\n",
        "            vocab_idx = input_ids[:, None, :].expand(B, T, S)\n",
        "\n",
        "            copy_dist = copy_dist.index_put((batch_idx, time_idx, vocab_idx), attn_norm, accumulate=True)\n",
        "\n",
        "            # mix\n",
        "            final_dist   = p_gen * vocab_dist + p_copy * copy_dist   # [B,T,V]\n",
        "            final_logits = torch.log(final_dist + self.eps)\n",
        "\n",
        "            # gate means for logging\n",
        "            copy_gate = p_copy.mean().detach()\n",
        "            gen_gate  = p_gen.mean().detach()\n",
        "\n",
        "            # ---- coverage loss (fixed shape) ----\n",
        "            cov = torch.zeros_like(attn_norm[:, 0, :])               # [B,S]\n",
        "            step_losses = []\n",
        "            for t in range(T):\n",
        "                a_t = attn_norm[:, t, :]                             # [B,S]\n",
        "                step_losses.append(torch.min(a_t, cov).sum(dim=-1))  # [B]\n",
        "                cov = cov + a_t\n",
        "            cov_loss = torch.stack(step_losses, dim=1).mean()        # scalar\n",
        "\n",
        "        # loss over mixed distribution\n",
        "        if labels is not None:\n",
        "            V = final_logits.size(-1)\n",
        "            loss_fct = nn.NLLLoss(ignore_index=-100)\n",
        "            loss_ce  = loss_fct(final_logits.view(-1, V), labels.view(-1))\n",
        "            loss = loss_ce + (self.lambda_cov * cov_loss if cov_loss is not None else 0.0)\n",
        "        else:\n",
        "            loss = None\n",
        "\n",
        "        # expose scalars for logger\n",
        "        with torch.no_grad():\n",
        "            self._last_ce_loss     = loss_ce.detach() if loss_ce is not None else None\n",
        "            self._last_cov_loss    = cov_loss.detach() if cov_loss is not None else None\n",
        "            self._last_p_copy_mean = copy_gate if copy_gate is not None else None\n",
        "            self._last_p_gen_mean  = gen_gate  if gen_gate  is not None else None\n",
        "\n",
        "        return Seq2SeqLMOutput(\n",
        "            loss=loss,\n",
        "            logits=final_logits,\n",
        "            past_key_values=base_out.past_key_values,\n",
        "            decoder_hidden_states=base_out.decoder_hidden_states if need_hid else None,\n",
        "            decoder_attentions=base_out.decoder_attentions if need_attn else None,\n",
        "            cross_attentions=base_out.cross_attentions if need_attn else None,\n",
        "            encoder_last_hidden_state=base_out.encoder_last_hidden_state,\n",
        "            encoder_hidden_states=base_out.encoder_hidden_states if need_hid else None,\n",
        "            encoder_attentions=base_out.encoder_attentions if need_attn else None,\n",
        "        )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpkWThOX66EO",
        "outputId": "c2ab9bdc-3ee4-4470-8b7a-cb9d9b0f504f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        }
      ],
      "source": [
        "# ---- seed  ----\n",
        "import numpy as np\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# ---- tokenizer + base (force eager attention) ----\n",
        "tok = AutoTokenizer.from_pretrained(RESUME_CKPT, use_fast=True)\n",
        "cfg = AutoConfig.from_pretrained(RESUME_CKPT, attn_implementation=\"eager\")\n",
        "base = AutoModelForSeq2SeqLM.from_pretrained(RESUME_CKPT, config=cfg)\n",
        "\n",
        "base.config.use_cache = False\n",
        "base.gradient_checkpointing_enable()\n",
        "\n",
        "# wrap with pointer/coverage\n",
        "model = CopyAwareBart(base, tok, lambda_cov=LAMBDA_COV, eps=EPS, use_pointer=USE_POINTER).to(DEVICE)\n",
        "\n",
        "if FREEZE_ENCODER:\n",
        "    for p in model.base.model.encoder.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "\n",
        "# AMP scaler (new API)\n",
        "scaler = amp.GradScaler(\"cuda\", enabled=FP16)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmJTcIdy6_f-"
      },
      "outputs": [],
      "source": [
        "# ---- read data ----\n",
        "train_df = robust_read_csv(TRAIN_CSV)\n",
        "val_df   = robust_read_csv(VAL_CSV)\n",
        "\n",
        "# ---- dataset + collate ----\n",
        "collate   = PGDataCollator(tok=tok, max_src_len=MAX_SRC_LEN, max_tgt_len=MAX_TGT_LEN)\n",
        "train_ds  = CNNDMDataset(train_df)\n",
        "val_ds    = CNNDMDataset(val_df)\n",
        "\n",
        "# ---- loaders ----\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True,\n",
        "    collate_fn=collate,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "    num_workers=NUM_WORKERS, pin_memory=True,\n",
        "    persistent_workers=(NUM_WORKERS>0), prefetch_factor=(2 if NUM_WORKERS>0 else None),\n",
        "    collate_fn=collate,\n",
        ")\n",
        "\n",
        "# ---- logger + checkpoint helpers ----\n",
        "LOG_CSV = os.path.join(RUN_DIR, \"train_log.csv\")\n",
        "logger  = StepLogger(LOG_CSV)\n",
        "\n",
        "def save_checkpoint(tag):\n",
        "    \"\"\"Save base model, tokenizer, plus pointer head state.\"\"\"\n",
        "    ck = os.path.join(RUN_DIR, f\"ckpt_step{tag}\" if isinstance(tag,int) else f\"ckpt_{tag}\")\n",
        "    os.makedirs(ck, exist_ok=True)\n",
        "    model.base.save_pretrained(ck)\n",
        "    tok.save_pretrained(ck)\n",
        "    torch.save({\n",
        "        \"p_gen_linear\": model.p_gen_linear.state_dict(),\n",
        "        \"lambda_cov\": model.lambda_cov,\n",
        "        \"use_pointer\": model.use_ptr,\n",
        "    }, os.path.join(ck, \"pointer_head.pt\"))\n",
        "    print(f\"[checkpoint] saved → {ck}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdjH4mcD7Bwk",
        "outputId": "9c53a300-7966-4beb-f176-ad51216a6f5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[scheduler] total=1000, warmup=100\n"
          ]
        }
      ],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\"]\n",
        "params_decay, params_nodecay = [], []\n",
        "for n, p in model.named_parameters():\n",
        "    if not p.requires_grad:\n",
        "        continue\n",
        "    (params_nodecay if any(nd in n for nd in no_decay) else params_decay).append(p)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [{\"params\": params_decay, \"weight_decay\": WEIGHT_DECAY},\n",
        "     {\"params\": params_nodecay, \"weight_decay\": 0.0}],\n",
        "    lr=LR,\n",
        ")\n",
        "\n",
        "import math\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "num_update_steps_per_epoch = math.ceil(len(train_loader) / GRAD_ACCUM_STEPS)\n",
        "\n",
        "# >>> KEY FIX: respect MAX_STEPS if set\n",
        "t_total = int(MAX_STEPS) if MAX_STEPS else num_update_steps_per_epoch * EPOCHS\n",
        "warmup_steps = max(1, int(WARMUP_RATIO * t_total))  # e.g., 0.06 * 2000 = 120\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=t_total,\n",
        ")\n",
        "print(f\"[scheduler] total={t_total}, warmup={warmup_steps}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbiKft8r7FMI",
        "outputId": "e9f3234a-23d4-428c-9852-b71e5199614e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ep 1] step    100 | loss  2.7051 | ce  2.3852 | cov  0.3555 | p_copy 0.388 | p_gen 0.612 | lr 6.99e-06 | grad_norm 90605.20 | toks/s  16888.3 | gpu_mem 13856.3 MB\n",
            "[ep 1] step    200 | loss  2.6665 | ce  2.3464 | cov  0.3556 | p_copy 0.305 | p_gen 0.695 | lr 6.21e-06 | grad_norm 41910.84 | toks/s  16908.7 | gpu_mem 13856.3 MB\n",
            "[ep 1] step    300 | loss  2.6232 | ce  2.3032 | cov  0.3556 | p_copy 0.215 | p_gen 0.785 | lr 5.44e-06 | grad_norm 40995.21 | toks/s  16955.9 | gpu_mem 13856.3 MB\n",
            "[ep 1] step    400 | loss  2.6207 | ce  2.3004 | cov  0.3558 | p_copy 0.209 | p_gen 0.791 | lr 4.66e-06 | grad_norm 39502.64 | toks/s  17000.6 | gpu_mem 13856.3 MB\n",
            "[ep 1] step    500 | loss  2.6031 | ce  2.2843 | cov  0.3542 | p_copy 0.211 | p_gen 0.789 | lr 3.88e-06 | grad_norm 37819.66 | toks/s  17027.7 | gpu_mem 13856.3 MB\n",
            "[ep 1] step    600 | loss  2.5939 | ce  2.2756 | cov  0.3536 | p_copy 0.213 | p_gen 0.787 | lr 3.10e-06 | grad_norm 41142.63 | toks/s  17040.4 | gpu_mem 13856.3 MB\n",
            "[ep 1] step    700 | loss  2.6003 | ce  2.2814 | cov  0.3543 | p_copy 0.213 | p_gen 0.787 | lr 2.33e-06 | grad_norm 40517.85 | toks/s  17044.6 | gpu_mem 13856.3 MB\n",
            "[ep 1] step    800 | loss  2.5972 | ce  2.2798 | cov  0.3527 | p_copy 0.214 | p_gen 0.786 | lr 1.55e-06 | grad_norm 39297.14 | toks/s  17045.9 | gpu_mem 13856.3 MB\n",
            "[ep 1] step    900 | loss  2.6013 | ce  2.2840 | cov  0.3526 | p_copy 0.215 | p_gen 0.785 | lr 7.70e-07 | grad_norm 39529.25 | toks/s  17048.1 | gpu_mem 13856.3 MB\n",
            "[ep 1] step   1000 | loss  2.6055 | ce  2.2867 | cov  0.3542 | p_copy 0.215 | p_gen 0.785 | lr 0.00e+00 | grad_norm 19349.47 | toks/s  17050.6 | gpu_mem 13856.3 MB\n",
            "[checkpoint] saved → /content/drive/MyDrive/student_pgc_bartbase/ckpt_step1000\n",
            "[train] Reached MAX_STEPS=1000. Stopping early.\n",
            "[checkpoint] saved → /content/drive/MyDrive/student_pgc_bartbase/ckpt_step1000\n"
          ]
        }
      ],
      "source": [
        "global_step = 0\n",
        "\n",
        "best_val = float(\"inf\")\n",
        "\n",
        "# running averages\n",
        "m_loss = RunningMean(200); m_ce = RunningMean(200); m_cov = RunningMean(200)\n",
        "m_pcopy = RunningMean(200); m_pgen = RunningMean(200); m_toks = RunningMean(200)\n",
        "\n",
        "model.train()\n",
        "optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "start_time = time.time()\n",
        "tokens_seen = 0\n",
        "\n",
        "\n",
        "# running averages\n",
        "m_loss = RunningMean(200); m_ce = RunningMean(200); m_cov = RunningMean(200)\n",
        "m_pcopy = RunningMean(200); m_pgen = RunningMean(200); m_toks = RunningMean(200)\n",
        "\n",
        "model.train()\n",
        "optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "start_time = time.time()\n",
        "tokens_seen = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    for it, batch in enumerate(train_loader):\n",
        "        for k in batch:\n",
        "            batch[k] = batch[k].to(DEVICE)\n",
        "\n",
        "        # token count for throughput\n",
        "        with torch.no_grad():\n",
        "            toks_this_batch = int(batch[\"attention_mask\"].sum().item())\n",
        "            if \"labels\" in batch:\n",
        "                toks_this_batch += int((batch[\"labels\"] != -100).sum().item())\n",
        "\n",
        "        # forward pass with autocast\n",
        "        with amp.autocast(\"cuda\", enabled=FP16):\n",
        "            out = model(**batch)\n",
        "            loss = out.loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        took_step = False\n",
        "\n",
        "        if (it + 1) % GRAD_ACCUM_STEPS == 0:\n",
        "            # unscale once, then clip & log grad norm\n",
        "            # grad norm (scaled grads are fine for logging in AMP)\n",
        "            # grad norm (scaled grads are fine for logging in AMP)\n",
        "            total_norm = torch.norm(\n",
        "                torch.stack([p.grad.detach().norm(2) for p in model.parameters() if p.grad is not None])\n",
        "            ).item()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            scheduler.step()\n",
        "            global_step += 1\n",
        "            took_step = True\n",
        "\n",
        "            # update running stats\n",
        "            tokens_seen += toks_this_batch\n",
        "            m_loss.add(loss.item())\n",
        "            m_ce.add(getattr(model, \"_last_ce_loss\", None))\n",
        "            m_cov.add(getattr(model, \"_last_cov_loss\", None))\n",
        "            m_pcopy.add(getattr(model, \"_last_p_copy_mean\", None))\n",
        "            m_pgen.add(getattr(model, \"_last_p_gen_mean\", None))\n",
        "            elapsed = time.time() - start_time\n",
        "            m_toks.add(tokens_seen / max(elapsed, 1e-6))\n",
        "\n",
        "            if global_step % LOG_EVERY == 0:\n",
        "                lr = scheduler.get_last_lr()[0]\n",
        "                mem = _gpu_mem_mb()\n",
        "                print(\n",
        "                    f\"[ep {epoch+1}] step {global_step:>6} | \"\n",
        "                    f\"loss {m_loss.mean:7.4f} | ce {m_ce.mean:7.4f} | cov {m_cov.mean:7.4f} | \"\n",
        "                    f\"p_copy {m_pcopy.mean:5.3f} | p_gen {m_pgen.mean:5.3f} | \"\n",
        "                    f\"lr {lr:.2e} | grad_norm {total_norm:6.2f} | \"\n",
        "                    f\"toks/s {m_toks.mean:8.1f} | gpu_mem {mem:7.1f} MB\"\n",
        "                )\n",
        "                logger.log({\n",
        "                    \"epoch\": epoch+1,\n",
        "                    \"step\": global_step,\n",
        "                    \"loss\": round(m_loss.mean, 6),\n",
        "                    \"ce_loss\": round(m_ce.mean, 6),\n",
        "                    \"cov_loss\": round(m_cov.mean, 6),\n",
        "                    \"p_copy\": round(m_pcopy.mean, 6),\n",
        "                    \"p_gen\": round(m_pgen.mean, 6),\n",
        "                    \"lr\": lr,\n",
        "                    \"grad_norm\": total_norm,\n",
        "                    \"toks_per_s\": m_toks.mean,\n",
        "                    \"gpu_mem_mb\": mem,\n",
        "                })\n",
        "\n",
        "            if global_step % SAVE_EVERY_STEPS == 0:\n",
        "                save_checkpoint(global_step)\n",
        "\n",
        "            if global_step >= MAX_STEPS:\n",
        "                print(f\"[train] Reached MAX_STEPS={MAX_STEPS}. Stopping early.\")\n",
        "                save_checkpoint(f\"step{global_step}\")\n",
        "                break\n",
        "\n",
        "        if torch.cuda.is_available(): torch.cuda.synchronize()\n",
        "        del out\n",
        "        if took_step: torch.cuda.empty_cache()\n",
        "\n",
        "    if global_step >= MAX_STEPS:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "art2j2Y8KeHS"
      },
      "source": [
        "> Copy Aware Summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265,
          "referenced_widgets": [
            "e767ce1a23924be6a9f0d3fe5a326812"
          ]
        },
        "id": "T4OGLj6EKjr7",
        "outputId": "3da3f163-b16c-4e29-b831-97df5055ccfc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
            "  self.gen = func(*args, **kwds)\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[device] cuda\n",
            "[pointer_head] loaded=True\n",
            "[ptr] remaining: 100 / 100\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e767ce1a23924be6a9f0d3fe5a326812",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "PTR-BEAM (fp16):   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-4167544870.py:139: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16, enabled=USE_FP16_PTR):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[done] saved /content/drive/MyDrive/student_pgc_bartbase/ckpt_step5000/ptr_decode_ckpt5000_beam5_len18_44.csv | elapsed ~8.3 min\n",
            "[probe] p_copy_mean=0.291 | p_gen_mean=0.709\n"
          ]
        }
      ],
      "source": [
        "# ===== PTR-BEAM (beam=5, copy-aware) — decode 100 with your new lengths =====\n",
        "import os, gc, time, math, csv, pandas as pd, torch, torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ---------------- config ----------------\n",
        "CKPT_DIR = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step5000\"\n",
        "OUT_CSV  = os.path.join(CKPT_DIR, \"ptr_decode_ckpt5000_beam5_len18_44.csv\")\n",
        "VAL_CSV  = \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "SRC_COL, REF_COL = \"article\", \"highlights\"\n",
        "\n",
        "\n",
        "NUM_BEAMS      = 5\n",
        "MIN_NEW        = 18\n",
        "MAX_NEW        = 44\n",
        "NO_REPEAT      = 4\n",
        "LENGTH_PENALTY = 0.75\n",
        "MAX_SRC_LEN    = 400\n",
        "\n",
        "SAVE_EVERY = 50\n",
        "N_EXAMPLES = 100\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"[device]\", device)\n",
        "try:\n",
        "    torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True)\n",
        "except Exception:\n",
        "    pass\n",
        "if device == \"cuda\":\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "USE_FP16_PTR = (device == \"cuda\")\n",
        "\n",
        "tok  = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
        "base = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR, attn_implementation=\"eager\").to(device).eval()\n",
        "\n",
        "# ---------------- pointer wrapper (copy-aware) ----------------\n",
        "class CopyAwareBart(nn.Module):\n",
        "    def __init__(self, base_model, tokenizer, lambda_cov=1.0, eps=1e-8, use_pointer=True, gate_bias=-0.3):\n",
        "        super().__init__()\n",
        "        self.base   = base_model\n",
        "        self.tok    = tokenizer\n",
        "        self.lambda_cov = float(lambda_cov)\n",
        "        self.eps    = float(eps)\n",
        "        self.use_ptr= bool(use_pointer)\n",
        "        self.gate_bias = float(gate_bias)\n",
        "        H = self.base.config.d_model\n",
        "        self.p_gen_linear = nn.Linear(3*H, 1)\n",
        "        self._last_p_copy_mean = self._last_p_gen_mean = None\n",
        "\n",
        "    def _mix_pointer(self, out, input_ids, attention_mask, decoder_input_ids):\n",
        "        logits = out.logits  # (B,T,V)\n",
        "        B,T,V  = logits.shape\n",
        "        # mean over heads on last decoder layer's cross-attn\n",
        "        last_ca = out.cross_attentions[-1].mean(1)  # (B,T,S)\n",
        "        if attention_mask is not None:\n",
        "            last_ca = last_ca.masked_fill(attention_mask[:, None, :] == 0, 0.0)\n",
        "        attn = last_ca / last_ca.sum(-1, keepdim=True).clamp_min(self.eps)\n",
        "\n",
        "        dec_hid = out.decoder_hidden_states[-1]                    # (B,T,H)\n",
        "        ctx     = torch.bmm(attn, out.encoder_last_hidden_state)   # (B,T,H)\n",
        "        dec_inp = self.base.get_input_embeddings()(decoder_input_ids)\n",
        "        gate_in = torch.cat([dec_hid, ctx, dec_inp], dim=-1)\n",
        "        p_gen   = torch.sigmoid(self.p_gen_linear(gate_in) + self.gate_bias).squeeze(-1)\n",
        "        p_copy  = 1.0 - p_gen\n",
        "\n",
        "        copy_probs = torch.zeros_like(logits)\n",
        "        idx = input_ids.unsqueeze(1).expand(B, T, input_ids.size(1))\n",
        "        copy_probs.scatter_add_(2, idx, attn)\n",
        "\n",
        "        vocab_logp = F.log_softmax(logits, dim=-1)\n",
        "        logp_final = torch.logaddexp(\n",
        "            vocab_logp + (p_gen + self.eps).log().unsqueeze(-1),\n",
        "            (copy_probs + self.eps).log() + (p_copy + self.eps).log().unsqueeze(-1)\n",
        "        )\n",
        "\n",
        "        self._last_p_copy_mean = float(p_copy.detach().mean().cpu())\n",
        "        self._last_p_gen_mean  = float(p_gen.detach().mean().cpu())\n",
        "        return logp_final\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, decoder_input_ids=None,\n",
        "                output_attentions=True, output_hidden_states=True, use_cache=False, **kwargs):\n",
        "        out = self.base(\n",
        "            input_ids=input_ids, attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            output_attentions=True, output_hidden_states=True,\n",
        "            use_cache=use_cache, return_dict=True,\n",
        "        )\n",
        "        if not self.use_ptr:\n",
        "            return out\n",
        "        out.logits = self._mix_pointer(out, input_ids, attention_mask, decoder_input_ids)\n",
        "        return out\n",
        "\n",
        "def copyaware_generate(self, *args, **kwargs):\n",
        "    # delegate to HF generate; we use custom beam loop below for pointer logits\n",
        "    return self.base.generate(*args, **kwargs)\n",
        "CopyAwareBart.generate = copyaware_generate\n",
        "\n",
        "model_ptr = CopyAwareBart(base, tok, lambda_cov=1.0, use_pointer=True, gate_bias=-0.3).to(device).eval()\n",
        "\n",
        "# try to load pointer head (trained p_gen_linear)\n",
        "ph_path = os.path.join(CKPT_DIR, \"pointer_head.pt\")\n",
        "if os.path.exists(ph_path):\n",
        "    sd = torch.load(ph_path, map_location=\"cpu\")\n",
        "    if isinstance(sd, dict) and \"p_gen_linear\" in sd:\n",
        "        model_ptr.p_gen_linear.load_state_dict(sd[\"p_gen_linear\"])\n",
        "    elif isinstance(sd, dict) and any(k.startswith(\"p_gen_linear.\") for k in sd.keys()):\n",
        "        model_ptr.load_state_dict(sd, strict=False)\n",
        "    elif isinstance(sd, dict) and set(sd.keys()) == {\"weight\",\"bias\"}:\n",
        "        model_ptr.p_gen_linear.load_state_dict(sd)\n",
        "print(\"[pointer_head] loaded=True\")\n",
        "\n",
        "# ---------------- helpers ----------------\n",
        "def _ban_repeating_ngrams(logits: torch.Tensor, seq: torch.Tensor, ngram: int):\n",
        "    if ngram <= 0: return\n",
        "    toks = seq[0].tolist() if seq.dim()==2 else seq.tolist()\n",
        "    L = len(toks)\n",
        "    if ngram == 1:\n",
        "        for t in set(toks): logits[t] = -1e9\n",
        "        return\n",
        "    if L < ngram-1: return\n",
        "    prefix = tuple(toks[-(ngram - 1):])\n",
        "    banned = set()\n",
        "    for i in range(L - ngram + 1):\n",
        "        if tuple(toks[i:i + ngram - 1]) == prefix:\n",
        "            banned.add(toks[i + ngram - 1])\n",
        "    for t in banned: logits[t] = -1e9\n",
        "\n",
        "def generate_pointer_beam_one(text: str) -> str:\n",
        "    enc = tok([text], return_tensors=\"pt\", truncation=True, max_length=MAX_SRC_LEN, padding=True).to(device)\n",
        "    start_id = getattr(base.config, \"decoder_start_token_id\", None) or (tok.bos_token_id or tok.eos_token_id)\n",
        "    beams = [(torch.tensor([[start_id]], device=device, dtype=torch.long), 0.0, False)]\n",
        "    for step in range(MAX_NEW):\n",
        "        new_beams = []\n",
        "        for seq, score, done in beams:\n",
        "            if done:\n",
        "                new_beams.append((seq, score, True)); continue\n",
        "            with torch.cuda.amp.autocast(dtype=torch.float16, enabled=USE_FP16_PTR):\n",
        "                out = model_ptr(\n",
        "                    input_ids=enc.input_ids,\n",
        "                    attention_mask=enc.attention_mask,\n",
        "                    decoder_input_ids=seq,\n",
        "                    use_cache=False\n",
        "                )\n",
        "            # pointer-aware logits already mixed in model_ptr.forward()\n",
        "            step_logits = out.logits[:, -1, :].squeeze(0).float().clone()  # (V,)\n",
        "            # enforce min length before allowing EOS\n",
        "            if step + 1 < MIN_NEW and tok.eos_token_id is not None:\n",
        "                step_logits[tok.eos_token_id] = -1e9\n",
        "            # ngram repeat blocking\n",
        "            _ban_repeating_ngrams(step_logits, seq, NO_REPEAT)\n",
        "            # expand\n",
        "            topk_logp, topk_ids = torch.topk(step_logits, k=NUM_BEAMS)\n",
        "            for k in range(NUM_BEAMS):\n",
        "                tid  = topk_ids[k].view(1,1)\n",
        "                nseq = torch.cat([seq, tid], dim=1)\n",
        "                nfin = (tok.eos_token_id is not None and tid.item() == tok.eos_token_id) or (nseq.size(1) >= MAX_NEW)\n",
        "                raw  = score + float(topk_logp[k].item())\n",
        "                # Wu length normalization\n",
        "                lp   = ((5.0 + nseq.size(1))**LENGTH_PENALTY) / ((5.0 + 1.0)**LENGTH_PENALTY)\n",
        "                nsc  = raw / lp\n",
        "                new_beams.append((nseq, nsc, nfin))\n",
        "        # prune\n",
        "        new_beams.sort(key=lambda x: x[1], reverse=True)\n",
        "        beams = new_beams[:NUM_BEAMS]\n",
        "        # early stop if all beams finished\n",
        "        if all(b[2] for b in beams):\n",
        "            break\n",
        "    best_seq = max(beams, key=lambda x: x[1])[0]\n",
        "    return tok.batch_decode(best_seq, skip_special_tokens=True)[0]\n",
        "\n",
        "# ---------------- load data & prep output ----------------\n",
        "def robust_read_csv(path: str) -> pd.DataFrame:\n",
        "    required = (\"id\",\"article\",\"highlights\")\n",
        "    for enc in (\"utf-8\",\"utf-8-sig\",\"cp1252\"):\n",
        "        try:\n",
        "            df = pd.read_csv(path, engine=\"python\", quoting=csv.QUOTE_ALL, escapechar=\"\\\\\", encoding=enc)\n",
        "            cols = {c.lower(): c for c in df.columns}\n",
        "            assert all(k in cols for k in required)\n",
        "            out = df[[cols[\"id\"], cols[\"article\"], cols[\"highlights\"]]].copy()\n",
        "            out.columns = [\"id\",\"article\",\"highlights\"]; out[\"id\"] = out[\"id\"].astype(str)\n",
        "            return out\n",
        "        except Exception:\n",
        "            pass\n",
        "    # fallback\n",
        "    rows = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "        reader = csv.reader(f, delimiter=\",\", quotechar='\"', escapechar=\"\\\\\")\n",
        "        header = [h.strip().lower() for h in next(reader)]\n",
        "        i_id, i_art, i_sum = header.index(\"id\"), header.index(\"article\"), header.index(\"highlights\")\n",
        "        for row in reader: rows.append([row[i_id], row[i_art], row[i_sum]])\n",
        "    out = pd.DataFrame(rows, columns=[\"id\",\"article\",\"highlights\"]); out[\"id\"]=out[\"id\"].astype(str)\n",
        "    return out\n",
        "\n",
        "if os.path.exists(OUT_CSV):\n",
        "    df = pd.read_csv(OUT_CSV)\n",
        "    if \"ptr_pred\" not in df.columns:\n",
        "        df[\"ptr_pred\"] = \"\"\n",
        "else:\n",
        "    base_df = robust_read_csv(VAL_CSV).iloc[:N_EXAMPLES].copy()\n",
        "    df = base_df[[SRC_COL, REF_COL]].copy()\n",
        "    df[\"ptr_pred\"] = \"\"\n",
        "    df[\"len_ptr\"]  = 0\n",
        "\n",
        "# ---------------- run pointer decoding ----------------\n",
        "todo_idx = df.index[(df[\"ptr_pred\"].isna()) | (df[\"ptr_pred\"].astype(str) == \"\")].tolist()\n",
        "print(f\"[ptr] remaining: {len(todo_idx)} / {len(df)}\")\n",
        "\n",
        "t0 = time.time(); pending = []\n",
        "for j, idx in enumerate(tqdm(todo_idx, desc=\"PTR-BEAM (fp16)\")):\n",
        "    text = str(df.at[idx, SRC_COL])\n",
        "    pred = generate_pointer_beam_one(text)\n",
        "    df.at[idx, \"ptr_pred\"] = pred\n",
        "    df.at[idx, \"len_ptr\"]  = len(pred.split())\n",
        "    pending.append(idx)\n",
        "\n",
        "    if len(pending) >= SAVE_EVERY or j == len(todo_idx) - 1:\n",
        "        df.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
        "        pending.clear()\n",
        "        torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "mins = (time.time() - t0) / 60.0\n",
        "print(f\"[done] saved {OUT_CSV} | elapsed ~{mins:.1f} min\")\n",
        "print(f\"[probe] p_copy_mean={getattr(model_ptr,'_last_p_copy_mean',float('nan')):.3f} | p_gen_mean={getattr(model_ptr,'_last_p_gen_mean',float('nan')):.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1103
        },
        "id": "zYOQetuyPgIO",
        "outputId": "92635b37-5964-4f38-c6cb-b5633aa4cdf4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"display(show[cols])\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"len_pred\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 25,\n        \"max\": 37,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          25,\n          35,\n          28\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"len_ref\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 21,\n        \"max\": 46,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          45,\n          28,\n          21\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prediction\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Photographer Ignacio Evangelista spent several years criss-crossing Europe to capture these abandoned\\ncheckpoints. The result is a fascinating gallery of images that charts the unusual architecture of places\\nwhose fate has been intertwined with\",\n          \"I met Kelly Gissendaner in January 2010 in a nondescript classroom at Metro State Prison for Women in Atlanta.\\nShe arrived for class beaming with excitement about the journey she was about to begin\",\n          \"Melissa was sold into the sex trade by a family member when she was only 12 years old. Her life became a\\nprison: Chained to a bed to a warehouse, she endured regular beatings, rapes\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Spanish photographer Ignacio Evangelista's \\\"After Schengen\\\" project captures images of abandoned European\\ncheckpoints . Schengen agreement came into force 20 years ago, lifting border controls between participating\\nEuropean nations . Border checkpoints range from giant Soviet statements to small huts in deep, dark forests .\",\n          \"The execution of Kelly Gissendaner was postponed due to concerns over injection drugs . McBride: In her time\\non death row, Gissendaner has discovered hope through theology .\",\n          \"Sens. Cornyn and Klobuchar: Trafficking stealing kids' childhoods . Two bills introduced to combat problem\\npassed Judiciary Committee, they say .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-bc0b7719-8c21-4d60-9c3b-1ec60df43147\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>len_pred</th>\n",
              "      <th>len_ref</th>\n",
              "      <th>prediction</th>\n",
              "      <th>reference</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>34</td>\n",
              "      <td>39</td>\n",
              "      <td>Delta Air Lines plane skidded into fence at La...</td>\n",
              "      <td>Delta Air Lines Flight 1086 skidded into a fen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>34</td>\n",
              "      <td>28</td>\n",
              "      <td>I met Kelly Gissendaner in January 2010 in a n...</td>\n",
              "      <td>The execution of Kelly Gissendaner was postpon...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>25</td>\n",
              "      <td>46</td>\n",
              "      <td>French striker Bafetimbi Gomis collapses durin...</td>\n",
              "      <td>Bafetimbi Gomis collapses within 10 minutes of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>28</td>\n",
              "      <td>38</td>\n",
              "      <td>Racing is one of those. It's a crueler sport i...</td>\n",
              "      <td>Red Bull's No.1 driver Daniel Ricciardo says F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>37</td>\n",
              "      <td>29</td>\n",
              "      <td>We need to realize that instead of creating jo...</td>\n",
              "      <td>David Wheeler: Silicon Valley doesn't create j...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>35</td>\n",
              "      <td>21</td>\n",
              "      <td>Melissa was sold into the sex trade by a famil...</td>\n",
              "      <td>Sens. Cornyn and Klobuchar: Trafficking steali...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>35</td>\n",
              "      <td>27</td>\n",
              "      <td>Walter Mondale is released from the Mayo Clini...</td>\n",
              "      <td>Walter Mondale was released from the Mayo Clin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>35</td>\n",
              "      <td>44</td>\n",
              "      <td>Rock and Roll Hall of Fame announces an array ...</td>\n",
              "      <td>The Rock and Roll Hall of Fame announces the p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>34</td>\n",
              "      <td>45</td>\n",
              "      <td>Photographer Ignacio Evangelista spent several...</td>\n",
              "      <td>Spanish photographer Ignacio Evangelista's \"Af...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>Mar was crowned champion at the All England Cl...</td>\n",
              "      <td>Marion Bartoli asks whether she should make a ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bc0b7719-8c21-4d60-9c3b-1ec60df43147')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bc0b7719-8c21-4d60-9c3b-1ec60df43147 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bc0b7719-8c21-4d60-9c3b-1ec60df43147');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5f0e60b5-aeff-47bf-8e1a-d8951aaec13b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5f0e60b5-aeff-47bf-8e1a-d8951aaec13b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5f0e60b5-aeff-47bf-8e1a-d8951aaec13b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    len_pred  len_ref                                         prediction  \\\n",
              "26        34       39  Delta Air Lines plane skidded into fence at La...   \n",
              "86        34       28  I met Kelly Gissendaner in January 2010 in a n...   \n",
              "2         25       46  French striker Bafetimbi Gomis collapses durin...   \n",
              "55        28       38  Racing is one of those. It's a crueler sport i...   \n",
              "75        37       29  We need to realize that instead of creating jo...   \n",
              "93        35       21  Melissa was sold into the sex trade by a famil...   \n",
              "16        35       27  Walter Mondale is released from the Mayo Clini...   \n",
              "73        35       44  Rock and Roll Hall of Fame announces an array ...   \n",
              "54        34       45  Photographer Ignacio Evangelista spent several...   \n",
              "95        37       37  Mar was crowned champion at the All England Cl...   \n",
              "\n",
              "                                            reference  \n",
              "26  Delta Air Lines Flight 1086 skidded into a fen...  \n",
              "86  The execution of Kelly Gissendaner was postpon...  \n",
              "2   Bafetimbi Gomis collapses within 10 minutes of...  \n",
              "55  Red Bull's No.1 driver Daniel Ricciardo says F...  \n",
              "75  David Wheeler: Silicon Valley doesn't create j...  \n",
              "93  Sens. Cornyn and Klobuchar: Trafficking steali...  \n",
              "16  Walter Mondale was released from the Mayo Clin...  \n",
              "73  The Rock and Roll Hall of Fame announces the p...  \n",
              "54  Spanish photographer Ignacio Evangelista's \"Af...  \n",
              "95  Marion Bartoli asks whether she should make a ...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# === Cell 1: preview N samples side-by-side ===\n",
        "import pandas as pd, textwrap, random\n",
        "from IPython.display import display\n",
        "\n",
        "# paths\n",
        "OUT_CSV = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step5000/ptr_decode_ckpt5000_beam5_len18_44.csv\"\n",
        "\n",
        "\n",
        "N_SAMPLES = 10\n",
        "SEED = 0\n",
        "\n",
        "\n",
        "df = pd.read_csv(OUT_CSV)\n",
        "df = df.rename(columns={\"highlights\":\"reference\", \"ptr_pred\":\"prediction\"})\n",
        "df[\"prediction\"] = df[\"prediction\"].astype(str)\n",
        "df[\"reference\"]  = df[\"reference\"].astype(str)\n",
        "df[\"len_pred\"]   = df[\"prediction\"].apply(lambda s: len(s.split()))\n",
        "df[\"len_ref\"]    = df[\"reference\"].apply(lambda s: len(s.split()))\n",
        "\n",
        "random.seed(SEED)\n",
        "show = df.sample(n=min(N_SAMPLES, len(df)), random_state=SEED).copy()\n",
        "\n",
        "\n",
        "wrap = lambda s: textwrap.fill(str(s), width=110, break_long_words=False, replace_whitespace=False)\n",
        "show[\"prediction\"] = show[\"prediction\"].apply(wrap)\n",
        "show[\"reference\"]  = show[\"reference\"].apply(wrap)\n",
        "\n",
        "cols = [c for c in [\"id\",\"len_pred\",\"len_ref\",\"prediction\",\"reference\"] if c in show.columns]\n",
        "display(show[cols])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kcEMkpPPxe0",
        "outputId": "2e37df04-04a7-44c9-8552-715fffa5d446"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE (F1): {'rouge1': 0.308, 'rouge2': 0.1109, 'rougeL': 0.2261, 'rougeLsum': 0.2261}\n",
            "Lengths: {'pred_mean': 33.97, 'pred_median': 35.0, 'pred_min': 24, 'pred_max': 40, 'ref_mean': 33.95, 'ref_median': 33.0, 'ref_min': 13, 'ref_max': 59}\n"
          ]
        }
      ],
      "source": [
        "# === Cell 2: ROUGE-1/2/L/Lsum (F1) + length stats over the 100 decoded ===\n",
        "import numpy as np\n",
        "try:\n",
        "    from rouge_score import rouge_scorer\n",
        "except Exception:\n",
        "    %pip -q install rouge-score\n",
        "    from rouge_score import rouge_scorer\n",
        "\n",
        "OUT_CSV = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step5000/ptr_decode_ckpt5000_beam5_len18_44.csv\"\n",
        "df = pd.read_csv(OUT_CSV).rename(columns={\"highlights\":\"reference\", \"ptr_pred\":\"prediction\"})\n",
        "df[\"prediction\"] = df[\"prediction\"].astype(str)\n",
        "df[\"reference\"]  = df[\"reference\"].astype(str)\n",
        "\n",
        "# ROUGE\n",
        "scorer = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\",\"rougeLsum\"], use_stemmer=True)\n",
        "scores = {\"rouge1\":[], \"rouge2\":[], \"rougeL\":[], \"rougeLsum\":[]}\n",
        "for p, r in zip(df[\"prediction\"], df[\"reference\"]):\n",
        "    s = scorer.score(r, p)  # (reference, prediction)\n",
        "    for k in scores: scores[k].append(s[k].fmeasure)\n",
        "rouge_avg = {k: float(np.mean(v)) for k, v in scores.items()}\n",
        "\n",
        "# lengths (words)\n",
        "lens_pred = df[\"prediction\"].apply(lambda s: len(s.split()))\n",
        "lens_ref  = df[\"reference\"].apply(lambda s: len(s.split()))\n",
        "length_stats = {\n",
        "    \"pred_mean\":   float(lens_pred.mean()),\n",
        "    \"pred_median\": float(lens_pred.median()),\n",
        "    \"pred_min\":    int(lens_pred.min()),\n",
        "    \"pred_max\":    int(lens_pred.max()),\n",
        "    \"ref_mean\":    float(lens_ref.mean()),\n",
        "    \"ref_median\":  float(lens_ref.median()),\n",
        "    \"ref_min\":     int(lens_ref.min()),\n",
        "    \"ref_max\":     int(lens_ref.max()),\n",
        "}\n",
        "\n",
        "print(\"ROUGE (F1):\", {k: round(v, 4) for k, v in rouge_avg.items()})\n",
        "print(\"Lengths:\",   {k: (round(v,2) if isinstance(v,float) else v) for k,v in length_stats.items()})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IsX0aW0GQRyg",
        "outputId": "dc2ec1fc-a361-4de9-ea3b-9426bcbe1bf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.12/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.8.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.12/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.12/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Install a combo that plays nicely together (Py3.12)\n",
        "!pip -q install \"numpy==1.26.4\" \"spacy==3.7.4\" \\\n",
        "                \"thinc==8.2.5\" \"cymem==2.0.8\" \"preshed==3.0.9\" \"blis==0.7.11\" \"murmurhash==1.0.10\"\n",
        "!python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H29URJlCS9QG",
        "outputId": "33a5033e-bca6-4479-d430-f0253ebaeff7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== ENTITY (overall micro, spaCy) ===\n",
            "{'precision': 0.2699, 'recall': 0.2793, 'f1': 0.2745, 'tp': 105.0, 'fp': 284.0, 'fn': 271.0}\n",
            "\n",
            "=== ENTITY (by type) ===\n",
            "CARDINAL P=0.3208  R=0.3469  F1=0.3333  (tp=17 fp=36 fn=32)\n",
            "DATE     P=0.127  R=0.1176  F1=0.1221  (tp=8 fp=55 fn=60)\n",
            "GPE      P=0.3333  R=0.3167  F1=0.3248  (tp=19 fp=38 fn=41)\n",
            "LOC      P=0.0909  R=1.0  F1=0.1667  (tp=1 fp=10 fn=0)\n",
            "NORP     P=0.2121  R=0.2258  F1=0.2188  (tp=7 fp=26 fn=24)\n",
            "ORG      P=0.2111  R=0.2639  F1=0.2346  (tp=19 fp=71 fn=53)\n",
            "PERSON   P=0.4146  R=0.3579  F1=0.3842  (tp=34 fp=48 fn=61)\n",
            "\n",
            "=== NUMBERS exact-match ===\n",
            "{'precision': 0.2162, 'recall': 0.2319, 'f1': 0.2238, 'tp': 16.0, 'fp': 58.0, 'fn': 53.0}\n"
          ]
        }
      ],
      "source": [
        "# === spaCy entity metrics on your 100 summaries ===\n",
        "import re, pandas as pd\n",
        "from collections import defaultdict\n",
        "import spacy\n",
        "\n",
        "OUT_CSV = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step5000/ptr_decode_ckpt5000_beam5_len18_44.csv\"\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"attribute_ruler\",\"textcat\"])\n",
        "\n",
        "df = pd.read_csv(OUT_CSV).rename(columns={\"highlights\":\"reference\", \"ptr_pred\":\"prediction\"})\n",
        "df[\"prediction\"] = df[\"prediction\"].astype(str)\n",
        "df[\"reference\"]  = df[\"reference\"].astype(str)\n",
        "\n",
        "KEEP_TYPES = {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"NORP\",\"DATE\",\"CARDINAL\"}\n",
        "\n",
        "NUM_RE = re.compile(r\"(?<![\\w/])[$€£]?\\d[\\d,\\.:/-]*\")\n",
        "def norm(s: str) -> str:\n",
        "    s = re.sub(r\"\\s+\", \" \", s.strip().lower())\n",
        "    return s.strip(\" ,.;:!?\\\"'()[]{}\")\n",
        "\n",
        "def ents_for(texts):\n",
        "    out = []\n",
        "    for doc in nlp.pipe(texts, batch_size=64):\n",
        "        bucket = defaultdict(set)\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ in KEEP_TYPES:\n",
        "                bucket[ent.label_].add(norm(ent.text))\n",
        "        out.append(bucket)\n",
        "    return out\n",
        "\n",
        "pred_b = ents_for(df[\"prediction\"].tolist())\n",
        "ref_b  = ents_for(df[\"reference\"].tolist())\n",
        "\n",
        "pred_mentions, ref_mentions = defaultdict(set), defaultdict(set)\n",
        "for i, (pb, rb) in enumerate(zip(pred_b, ref_b)):\n",
        "    for t, spans in pb.items():\n",
        "        for s in spans:\n",
        "            if s: pred_mentions[t].add((i, s))\n",
        "    for t, spans in rb.items():\n",
        "        for s in spans:\n",
        "            if s: ref_mentions[t].add((i, s))\n",
        "\n",
        "def prf(tp, fp, fn):\n",
        "    p = tp/(tp+fp) if (tp+fp)>0 else 0.0\n",
        "    r = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
        "    f = 2*p*r/(p+r) if (p+r)>0 else 0.0\n",
        "    return p, r, f\n",
        "\n",
        "all_types = sorted(set(pred_mentions) | set(ref_mentions))\n",
        "by_type, TP, FP, FN = {}, 0, 0, 0\n",
        "for t in all_types:\n",
        "    P, R = pred_mentions.get(t, set()), ref_mentions.get(t, set())\n",
        "    tp, fp, fn = len(P & R), len(P - R), len(R - P)\n",
        "    P_, R_, F_ = prf(tp, fp, fn)\n",
        "    by_type[t] = dict(precision=P_, recall=R_, f1=F_, tp=tp, fp=fp, fn=fn)\n",
        "    TP += tp; FP += fp; FN += fn\n",
        "\n",
        "micro_p, micro_r, micro_f = prf(TP, FP, FN)\n",
        "overall = dict(precision=micro_p, recall=micro_r, f1=micro_f, tp=TP, fp=FP, fn=FN)\n",
        "\n",
        "# Numbers exact match (separate)\n",
        "num_pred, num_ref = set(), set()\n",
        "for i, (p, r) in enumerate(zip(df[\"prediction\"], df[\"reference\"])):\n",
        "    for m in NUM_RE.finditer(p): num_pred.add((i, norm(m.group(0))))\n",
        "    for m in NUM_RE.finditer(r): num_ref.add((i, norm(m.group(0))))\n",
        "tpn, fpn, fnn = len(num_pred & num_ref), len(num_pred - num_ref), len(num_ref - num_pred)\n",
        "pn, rn, fn_ = prf(tpn, fpn, fnn)\n",
        "numbers = dict(precision=pn, recall=rn, f1=fn_, tp=tpn, fp=fpn, fn=fnn)\n",
        "\n",
        "# Print nicely\n",
        "r4 = lambda x: round(float(x), 4)\n",
        "print(\"\\n=== ENTITY (overall micro, spaCy) ===\")\n",
        "print({k: (r4(v) if isinstance(v,(int,float)) else v) for k,v in overall.items()})\n",
        "\n",
        "print(\"\\n=== ENTITY (by type) ===\")\n",
        "for t in sorted(by_type):\n",
        "    v = by_type[t]\n",
        "    print(f\"{t:8s} P={r4(v['precision'])}  R={r4(v['recall'])}  F1={r4(v['f1'])}  (tp={v['tp']} fp={v['fp']} fn={v['fn']})\")\n",
        "\n",
        "print(\"\\n=== NUMBERS exact-match ===\")\n",
        "print({k: (r4(v) if isinstance(v,(int,float)) else v) for k,v in numbers.items()})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-syRkhOKHsGj"
      },
      "source": [
        "# Baseline Model No pointer finetuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354,
          "referenced_widgets": [
            "8cb4c767bb134e47bf8a96aead96a544",
            "469d43c5040f4737a3eedee03d1cbb58",
            "1306845bbd9841889001e7142940fcf2",
            "50e068e715074a8d99d1f5dd8858571b",
            "34c54f6c537441689a83317dbe072d04",
            "5921a6309fdd4027997dcd9e812fadec"
          ]
        },
        "id": "Md5k-AJrHrmr",
        "outputId": "3c9ae2b3-aa87-4e1c-e85f-0f96b5e0bd02"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8cb4c767bb134e47bf8a96aead96a544",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "469d43c5040f4737a3eedee03d1cbb58",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1306845bbd9841889001e7142940fcf2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50e068e715074a8d99d1f5dd8858571b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34c54f6c537441689a83317dbe072d04",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5921a6309fdd4027997dcd9e812fadec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.02G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[done] merged 'base_pred' into: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step5000/ptr_decode_ckpt5000_beam5_len18_44.csv\n"
          ]
        }
      ],
      "source": [
        "# Decode baseline (bart-large-cnn) on the SAME 100 examples as your pointer CSV, then merge as 'base_pred'\n",
        "import os, csv, torch, pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "PTR_CSV  = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step5000/ptr_decode_ckpt5000_beam5_len18_44.csv\"\n",
        "MODEL_ID = \"facebook/bart-large\"\n",
        "# decode settings (match your pointer run, but beam=5)\n",
        "DECODE = dict(\n",
        "    num_beams=5, do_sample=False,\n",
        "    min_new_tokens=18, max_new_tokens=44,\n",
        "    no_repeat_ngram_size=4, length_penalty=0.75,\n",
        "    early_stopping=True,\n",
        ")\n",
        "MAX_SRC_LEN = 400\n",
        "BATCH = 8\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "df = pd.read_csv(PTR_CSV)\n",
        "assert {\"article\",\"highlights\"}.issubset(df.columns), \"Pointer CSV must have article/highlights\"\n",
        "texts = df[\"article\"].astype(str).tolist()\n",
        "\n",
        "tok  = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
        "base = AutoModelForSeq2SeqLM.from_pretrained(MODEL_ID).to(DEVICE).eval()\n",
        "base.config.use_cache = True\n",
        "\n",
        "def decode_batch(batch):\n",
        "    enc = tok(batch, padding=True, truncation=True, max_length=MAX_SRC_LEN, return_tensors=\"pt\").to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        out_ids = base.generate(**enc, **DECODE)\n",
        "    return tok.batch_decode(out_ids, skip_special_tokens=True)\n",
        "\n",
        "preds = []\n",
        "for i in range(0, len(texts), BATCH):\n",
        "    preds += decode_batch(texts[i:i+BATCH])\n",
        "\n",
        "df[\"base_pred\"] = preds\n",
        "df[\"len_base\"]  = df[\"base_pred\"].apply(lambda s: len(str(s).split()))\n",
        "df.to_csv(PTR_CSV, index=False, encoding=\"utf-8\")\n",
        "print(\"[done] merged 'base_pred' into:\", PTR_CSV)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1wnVAWqVKDa",
        "outputId": "7f14af1d-43b5-4411-d756-d95539e97343"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== POINTER =====\n",
            "ROUGE (F1): {'rouge1': 0.308, 'rouge2': 0.1109, 'rougeL': 0.2261, 'rougeLsum': 0.2261}\n",
            "Lengths: {'pred_mean': 33.97, 'pred_median': 35.0, 'pred_min': 24, 'pred_max': 40, 'ref_mean': 33.95, 'ref_median': 33.0, 'ref_min': 13, 'ref_max': 59}\n",
            "ENTITY (overall micro, spaCy): {'precision': 0.2699, 'recall': 0.2793, 'f1': 0.2745, 'tp': 105.0, 'fp': 284.0, 'fn': 271.0}\n",
            "ENTITY (by type):\n",
            "  CARDINAL P=0.3208 R=0.3469 F1=0.3333 (tp=17 fp=36 fn=32)\n",
            "  DATE     P=0.127 R=0.1176 F1=0.1221 (tp=8 fp=55 fn=60)\n",
            "  GPE      P=0.3333 R=0.3167 F1=0.3248 (tp=19 fp=38 fn=41)\n",
            "  LOC      P=0.0909 R=1.0 F1=0.1667 (tp=1 fp=10 fn=0)\n",
            "  NORP     P=0.2121 R=0.2258 F1=0.2188 (tp=7 fp=26 fn=24)\n",
            "  ORG      P=0.2111 R=0.2639 F1=0.2346 (tp=19 fp=71 fn=53)\n",
            "  PERSON   P=0.4146 R=0.3579 F1=0.3842 (tp=34 fp=48 fn=61)\n",
            "NUMBERS exact-match: {'precision': 0.2162, 'recall': 0.2319, 'f1': 0.2238, 'tp': 16.0, 'fp': 58.0, 'fn': 53.0}\n",
            "\n",
            "===== BASELINE =====\n",
            "ROUGE (F1): {'rouge1': 0.3056, 'rouge2': 0.1053, 'rougeL': 0.2091, 'rougeLsum': 0.2091}\n",
            "Lengths: {'pred_mean': 34.06, 'pred_median': 35.0, 'pred_min': 26, 'pred_max': 41, 'ref_mean': 33.95, 'ref_median': 33.0, 'ref_min': 13, 'ref_max': 59}\n",
            "ENTITY (overall micro, spaCy): {'precision': 0.2673, 'recall': 0.2979, 'f1': 0.2818, 'tp': 112.0, 'fp': 307.0, 'fn': 264.0}\n",
            "ENTITY (by type):\n",
            "  CARDINAL P=0.2857 R=0.2857 F1=0.2857 (tp=14 fp=35 fn=35)\n",
            "  DATE     P=0.1375 R=0.1618 F1=0.1486 (tp=11 fp=69 fn=57)\n",
            "  GPE      P=0.234 R=0.3667 F1=0.2857 (tp=22 fp=72 fn=38)\n",
            "  LOC      P=0.1429 R=1.0 F1=0.25 (tp=1 fp=6 fn=0)\n",
            "  NORP     P=0.2821 R=0.3548 F1=0.3143 (tp=11 fp=28 fn=20)\n",
            "  ORG      P=0.2692 R=0.2917 F1=0.28 (tp=21 fp=57 fn=51)\n",
            "  PERSON   P=0.4444 R=0.3368 F1=0.3832 (tp=32 fp=40 fn=63)\n",
            "NUMBERS exact-match: {'precision': 0.2889, 'recall': 0.1884, 'f1': 0.2281, 'tp': 13.0, 'fp': 32.0, 'fn': 56.0}\n"
          ]
        }
      ],
      "source": [
        "# ===== ROUGE + Length + spaCy Entity Metrics (pointer + baseline present) =====\n",
        "import os, re, numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "CSV = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step5000/ptr_decode_ckpt5000_beam5_len18_44.csv\"\n",
        "REF_COL = \"highlights\"\n",
        "SYSTEM_COLS = [(\"ptr_pred\",\"POINTER\"), (\"base_pred\",\"BASELINE\")]\n",
        "# --- ROUGE import\n",
        "try:\n",
        "    from rouge_score import rouge_scorer\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"Please install rouge-score: pip install rouge-score\") from e\n",
        "\n",
        "# --- spaCy import (expects you've already installed & restarted once)\n",
        "try:\n",
        "    import spacy\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"attribute_ruler\",\"textcat\"])\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"spaCy model not loaded. Install & restart once:\\n\"\n",
        "                       \"pip install 'numpy==1.26.4' 'spacy==3.7.4' thinc cymem preshed blis murmurhash\\n\"\n",
        "                       \"python -m spacy download en_core_web_sm\") from e\n",
        "\n",
        "KEEP_TYPES = {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"NORP\",\"DATE\",\"CARDINAL\"}\n",
        "NUM_RE = re.compile(r\"(?<![\\w/])[$€£]?\\d[\\d,\\.:/-]*\")\n",
        "\n",
        "def norm(s: str) -> str:\n",
        "    s = re.sub(r\"\\s+\",\" \", str(s).strip().lower())\n",
        "    return s.strip(\" ,.;:!?\\\"'()[]{}\")\n",
        "\n",
        "def ents_for(texts):\n",
        "    \"\"\"List[Dict[type -> set(span)]] via spaCy.\"\"\"\n",
        "    out = []\n",
        "    for doc in nlp.pipe(texts, batch_size=64):\n",
        "        bucket = defaultdict(set)\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ in KEEP_TYPES:\n",
        "                bucket[ent.label_].add(norm(ent.text))\n",
        "        out.append(bucket)\n",
        "    return out\n",
        "\n",
        "def prf(tp, fp, fn):\n",
        "    p = tp/(tp+fp) if (tp+fp)>0 else 0.0\n",
        "    r = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
        "    f = 2*p*r/(p+r) if (p+r)>0 else 0.0\n",
        "    return p, r, f\n",
        "\n",
        "def rouge_and_lengths(refs, preds):\n",
        "    scorer = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\",\"rougeLsum\"], use_stemmer=True)\n",
        "    agg = {k: [] for k in [\"rouge1\",\"rouge2\",\"rougeL\",\"rougeLsum\"]}\n",
        "    for p, r in zip(preds, refs):\n",
        "        s = scorer.score(r, p)\n",
        "        for k in agg: agg[k].append(s[k].fmeasure)\n",
        "    rouge = {k: float(np.mean(v)) for k,v in agg.items()}\n",
        "    wlen = lambda s: len(str(s).split())\n",
        "    lp = np.array([wlen(x) for x in preds]); lr = np.array([wlen(x) for x in refs])\n",
        "    lengths = {\n",
        "        \"pred_mean\": float(lp.mean()), \"pred_median\": float(np.median(lp)),\n",
        "        \"pred_min\": int(lp.min()), \"pred_max\": int(lp.max()),\n",
        "        \"ref_mean\": float(lr.mean()), \"ref_median\": float(np.median(lr)),\n",
        "        \"ref_min\": int(lr.min()), \"ref_max\": int(lr.max()),\n",
        "    }\n",
        "    return rouge, lengths\n",
        "\n",
        "def entity_metrics_spacy(refs, preds):\n",
        "    pred_b = ents_for(preds); ref_b = ents_for(refs)\n",
        "    pred_mentions, ref_mentions = defaultdict(set), defaultdict(set)\n",
        "    for i, (pb, rb) in enumerate(zip(pred_b, ref_b)):\n",
        "        for t, spans in pb.items():\n",
        "            for s in spans:\n",
        "                if s: pred_mentions[t].add((i, s))\n",
        "        for t, spans in rb.items():\n",
        "            for s in spans:\n",
        "                if s: ref_mentions[t].add((i, s))\n",
        "    all_types = sorted(set(pred_mentions) | set(ref_mentions))\n",
        "    by_type = {}; TP=FP=FN=0\n",
        "    for t in all_types:\n",
        "        P, R = pred_mentions.get(t,set()), ref_mentions.get(t,set())\n",
        "        tp, fp, fn = len(P & R), len(P - R), len(R - P)\n",
        "        P_, R_, F_ = prf(tp, fp, fn)\n",
        "        by_type[t] = dict(precision=P_, recall=R_, f1=F_, tp=tp, fp=fp, fn=fn)\n",
        "        TP += tp; FP += fp; FN += fn\n",
        "    micro_p, micro_r, micro_f = prf(TP, FP, FN)\n",
        "    overall = dict(precision=micro_p, recall=micro_r, f1=micro_f, tp=TP, fp=FP, fn=FN)\n",
        "    # numbers exact-match (strict string match with digits/currency)\n",
        "    num_pred, num_ref = set(), set()\n",
        "    for i, (p, r) in enumerate(zip(preds, refs)):\n",
        "        for m in NUM_RE.finditer(p): num_pred.add((i, norm(m.group(0))))\n",
        "        for m in NUM_RE.finditer(r): num_ref.add((i, norm(m.group(0))))\n",
        "    tpn, fpn, fnn = len(num_pred & num_ref), len(num_pred - num_ref), len(num_ref - num_pred)\n",
        "    pn, rn, fn_ = prf(tpn, fpn, fnn)\n",
        "    numbers = dict(precision=pn, recall=rn, f1=fn_, tp=tpn, fp=fpn, fn=fnn)\n",
        "    return overall, by_type, numbers\n",
        "\n",
        "# --- load and run\n",
        "df = pd.read_csv(CSV)\n",
        "assert REF_COL in df.columns, f\"Missing {REF_COL} in CSV\"\n",
        "\n",
        "for col, label in SYSTEM_COLS:\n",
        "    if col not in df.columns:\n",
        "        print(f\"[skip] {label}: column '{col}' not found.\")\n",
        "        continue\n",
        "    refs  = df[REF_COL].astype(str).tolist()\n",
        "    preds = df[col].astype(str).tolist()\n",
        "\n",
        "    rouge, lengths = rouge_and_lengths(refs, preds)\n",
        "    overall, by_type, numbers = entity_metrics_spacy(refs, preds)\n",
        "\n",
        "    r4 = lambda x: round(float(x), 4)\n",
        "    print(f\"\\n===== {label} =====\")\n",
        "    print(\"ROUGE (F1):\", {k: r4(v) for k,v in rouge.items()})\n",
        "    print(\"Lengths:\",   {k: (r4(v) if isinstance(v,float) else v) for k,v in lengths.items()})\n",
        "    print(\"ENTITY (overall micro, spaCy):\", {k: r4(v) if isinstance(v,(int,float)) else v for k,v in overall.items()})\n",
        "    print(\"ENTITY (by type):\")\n",
        "    for t in sorted(by_type.keys()):\n",
        "        v = by_type[t]\n",
        "        print(f\"  {t:8s} P={r4(v['precision'])} R={r4(v['recall'])} F1={r4(v['f1'])} (tp={v['tp']} fp={v['fp']} fn={v['fn']})\")\n",
        "    print(\"NUMBERS exact-match:\", {k: r4(v) if isinstance(v,(int,float)) else v for k,v in numbers.items()})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "e8Y7DqOVVxhM",
        "outputId": "2668095b-eefa-4c62-c12f-2c7197685d69"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"display(show[cols]\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"len_reference\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 21,\n        \"max\": 46,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          28,\n          21,\n          39\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"len_pointer\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 25,\n        \"max\": 37,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          25,\n          35,\n          28\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"len_baseline\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 26,\n        \"max\": 40,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          34,\n          40,\n          26\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"The execution of Kelly Gissendaner was postponed due to concerns over injection drugs . McBride: In her time\\non death row, Gissendaner has discovered hope through theology .\",\n          \"Sens. Cornyn and Klobuchar: Trafficking stealing kids' childhoods . Two bills introduced to combat problem\\npassed Judiciary Committee, they say .\",\n          \"Delta Air Lines Flight 1086 skidded into a fence last week at a LaGuardia Airport beset by winter weather .\\nThe NTSB says the crew reported they did not sense any deceleration from the wheel brake upon landing .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pointer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"I met Kelly Gissendaner in January 2010 in a nondescript classroom at Metro State Prison for Women in Atlanta.\\nShe arrived for class beaming with excitement about the journey she was about to begin\",\n          \"Melissa was sold into the sex trade by a family member when she was only 12 years old. Her life became a\\nprison: Chained to a bed to a warehouse, she endured regular beatings, rapes\",\n          \"Delta Air Lines plane skidded into fence at LaGuardia Airport. Crew said they did not sense any deceleration\\nfrom the wheel brake upon landing. The runway appeared all white in the moments before landing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"baseline\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"I met Kelly Gissendaner in January 2010 in a nondescript classroom at Metro State Prison for Women in Atlanta.\\nShe arrived for class beaming with excitement about the journey she was about to begin\",\n          \"Melissa was sold into the sex trade by a family member when she was only 12 years old. Her life became a\\nprison: Chained to a bed in a warehouse, she endured regular beatings, rapes\",\n          \"The flight crew of the Delta Air Lines plane that skidded into a fence at LaGuardia Airport last week cited\\nbrake issues during the landing, according to an update on Monday from the NTSB. The\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-e47497d4-57ec-40e9-8fd8-b96a69df8767\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>len_reference</th>\n",
              "      <th>len_pointer</th>\n",
              "      <th>len_baseline</th>\n",
              "      <th>reference</th>\n",
              "      <th>pointer</th>\n",
              "      <th>baseline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>39</td>\n",
              "      <td>34</td>\n",
              "      <td>35</td>\n",
              "      <td>Delta Air Lines Flight 1086 skidded into a fence last week at a LaGuardia Airport beset by winter weather .\\nThe NTSB says the crew reported they did not sense any deceleration from the wheel brake upon landing .</td>\n",
              "      <td>Delta Air Lines plane skidded into fence at LaGuardia Airport. Crew said they did not sense any deceleration\\nfrom the wheel brake upon landing. The runway appeared all white in the moments before landing</td>\n",
              "      <td>The flight crew of the Delta Air Lines plane that skidded into a fence at LaGuardia Airport last week cited\\nbrake issues during the landing, according to an update on Monday from the NTSB. The</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>28</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "      <td>The execution of Kelly Gissendaner was postponed due to concerns over injection drugs . McBride: In her time\\non death row, Gissendaner has discovered hope through theology .</td>\n",
              "      <td>I met Kelly Gissendaner in January 2010 in a nondescript classroom at Metro State Prison for Women in Atlanta.\\nShe arrived for class beaming with excitement about the journey she was about to begin</td>\n",
              "      <td>I met Kelly Gissendaner in January 2010 in a nondescript classroom at Metro State Prison for Women in Atlanta.\\nShe arrived for class beaming with excitement about the journey she was about to begin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>46</td>\n",
              "      <td>25</td>\n",
              "      <td>26</td>\n",
              "      <td>Bafetimbi Gomis collapses within 10 minutes of kickoff at Tottenham . But he reportedly left the pitch\\nconscious and wearing an oxygen mask . Gomis later said that he was \"feeling well\" The incident came three\\nyears after Fabrice Muamba collapsed at White Hart Lane .</td>\n",
              "      <td>French striker Bafetimbi Gomis collapses during Swansea's 3-2 loss at Tottenham. Gomis spent the night in\\nhospital as a precaution, Swansea says. Gomomis had similar</td>\n",
              "      <td>French striker Bafetimbi Gomis, who has a history of fainting, said he is now \"feeling well\" after collapsing\\nduring Swansea's 3-2 loss at Tottenham in the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>38</td>\n",
              "      <td>28</td>\n",
              "      <td>33</td>\n",
              "      <td>Red Bull's No.1 driver Daniel Ricciardo says Formula One is a \"crueler sport\" Last year Ricciardo was\\ndisqualified from the Australian Grand Prix after finishing second . The 25-year-old is looking for redemption\\nin Melbourne on Sunday .</td>\n",
              "      <td>Racing is one of those. It's a crueler sport in that there's so many other variables,\" says Red Bull's No.1\\ndriver Daniel Ricciardo. Ricciardo experienced the full extent</td>\n",
              "      <td>When man relies on machine, there is always something that can go wrong. And there is no more unforgiving\\nenvironment than the high-stakes world of Formula One. \"Racing is one of those. It</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>29</td>\n",
              "      <td>37</td>\n",
              "      <td>40</td>\n",
              "      <td>David Wheeler: Silicon Valley doesn't create jobs; it's wiping out middle-class jobs . Young college graduates\\nare struggling with lack of jobs, yet many still idolize Silicon Valley .</td>\n",
              "      <td>We need to realize that instead of creating jobs, Silicon Valley is erasing them, leaving millennials\\nfinancially stranded. The commonly held belief is that with hard work and a good education, a young person in\\nAmerica can</td>\n",
              "      <td>We have no problem taking Wall Street executives to task for decisions that leave American families\\nfinancially devastated, yet we give Silicon Valley billionaires a pass when they do the same thing. Silicon\\nValley is tossing millennials aside like yesterday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>21</td>\n",
              "      <td>35</td>\n",
              "      <td>35</td>\n",
              "      <td>Sens. Cornyn and Klobuchar: Trafficking stealing kids' childhoods . Two bills introduced to combat problem\\npassed Judiciary Committee, they say .</td>\n",
              "      <td>Melissa was sold into the sex trade by a family member when she was only 12 years old. Her life became a\\nprison: Chained to a bed to a warehouse, she endured regular beatings, rapes</td>\n",
              "      <td>Melissa was sold into the sex trade by a family member when she was only 12 years old. Her life became a\\nprison: Chained to a bed in a warehouse, she endured regular beatings, rapes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>27</td>\n",
              "      <td>35</td>\n",
              "      <td>35</td>\n",
              "      <td>Walter Mondale was released from the Mayo Clinic on Saturday, hospital spokeswoman said . The former vice\\npresident, 87, was treated for cold and flu symptoms .</td>\n",
              "      <td>Walter Mondale is released from the Mayo Clinic. Mondale was diagnosed after he went to the hospital for a\\nroutine checkup following a fever. The 42nd vice president served under Carter between 1977 and 1981</td>\n",
              "      <td>Former Vice President Walter Mondale was released from the Mayo Clinic on Saturday after being admitted with\\ninfluenza, hospital spokeswoman Kelley Luckstein said. \"He's doing well. We treated him for flu and cold\\nsymptoms and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>44</td>\n",
              "      <td>35</td>\n",
              "      <td>33</td>\n",
              "      <td>The Rock and Roll Hall of Fame announces the presenters for its 2015 induction . Paul McCartney will introduce\\nRingo Starr; Stevie Wonder will induct Bill Withers . The ceremony will take place in April and will be\\nbroadcast on HBO in May .</td>\n",
              "      <td>Rock and Roll Hall of Fame announces an array of A-list presenters and performers for its 30th Annual\\nInduction Ceremony. Seven artists and musical groups will be honored with induction into the Rock and Roll</td>\n",
              "      <td>Paul McCartney, Patti Smith, Beck, and John Legend are ready to rock Cleveland. On Wednesday the Rock and Roll\\nHall of Fame announced an array of A-list presenters and performers for its 30th</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e47497d4-57ec-40e9-8fd8-b96a69df8767')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e47497d4-57ec-40e9-8fd8-b96a69df8767 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e47497d4-57ec-40e9-8fd8-b96a69df8767');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-dd98e6f9-330a-42d1-8339-58ebf6e39edc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dd98e6f9-330a-42d1-8339-58ebf6e39edc')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-dd98e6f9-330a-42d1-8339-58ebf6e39edc button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   len_reference  len_pointer  len_baseline  \\\n",
              "0             39           34            35   \n",
              "1             28           34            34   \n",
              "2             46           25            26   \n",
              "3             38           28            33   \n",
              "4             29           37            40   \n",
              "5             21           35            35   \n",
              "6             27           35            35   \n",
              "7             44           35            33   \n",
              "\n",
              "                                                                                                                                                                                                                                                                       reference  \\\n",
              "0                                                           Delta Air Lines Flight 1086 skidded into a fence last week at a LaGuardia Airport beset by winter weather .\\nThe NTSB says the crew reported they did not sense any deceleration from the wheel brake upon landing .   \n",
              "1                                                                                                 The execution of Kelly Gissendaner was postponed due to concerns over injection drugs . McBride: In her time\\non death row, Gissendaner has discovered hope through theology .   \n",
              "2  Bafetimbi Gomis collapses within 10 minutes of kickoff at Tottenham . But he reportedly left the pitch\\nconscious and wearing an oxygen mask . Gomis later said that he was \"feeling well\" The incident came three\\nyears after Fabrice Muamba collapsed at White Hart Lane .   \n",
              "3                                Red Bull's No.1 driver Daniel Ricciardo says Formula One is a \"crueler sport\" Last year Ricciardo was\\ndisqualified from the Australian Grand Prix after finishing second . The 25-year-old is looking for redemption\\nin Melbourne on Sunday .   \n",
              "4                                                                                      David Wheeler: Silicon Valley doesn't create jobs; it's wiping out middle-class jobs . Young college graduates\\nare struggling with lack of jobs, yet many still idolize Silicon Valley .   \n",
              "5                                                                                                                             Sens. Cornyn and Klobuchar: Trafficking stealing kids' childhoods . Two bills introduced to combat problem\\npassed Judiciary Committee, they say .   \n",
              "6                                                                                                              Walter Mondale was released from the Mayo Clinic on Saturday, hospital spokeswoman said . The former vice\\npresident, 87, was treated for cold and flu symptoms .   \n",
              "7                             The Rock and Roll Hall of Fame announces the presenters for its 2015 induction . Paul McCartney will introduce\\nRingo Starr; Stevie Wonder will induct Bill Withers . The ceremony will take place in April and will be\\nbroadcast on HBO in May .   \n",
              "\n",
              "                                                                                                                                                                                                                             pointer  \\\n",
              "0                       Delta Air Lines plane skidded into fence at LaGuardia Airport. Crew said they did not sense any deceleration\\nfrom the wheel brake upon landing. The runway appeared all white in the moments before landing   \n",
              "1                             I met Kelly Gissendaner in January 2010 in a nondescript classroom at Metro State Prison for Women in Atlanta.\\nShe arrived for class beaming with excitement about the journey she was about to begin   \n",
              "2                                                             French striker Bafetimbi Gomis collapses during Swansea's 3-2 loss at Tottenham. Gomis spent the night in\\nhospital as a precaution, Swansea says. Gomomis had similar   \n",
              "3                                                        Racing is one of those. It's a crueler sport in that there's so many other variables,\" says Red Bull's No.1\\ndriver Daniel Ricciardo. Ricciardo experienced the full extent   \n",
              "4  We need to realize that instead of creating jobs, Silicon Valley is erasing them, leaving millennials\\nfinancially stranded. The commonly held belief is that with hard work and a good education, a young person in\\nAmerica can   \n",
              "5                                             Melissa was sold into the sex trade by a family member when she was only 12 years old. Her life became a\\nprison: Chained to a bed to a warehouse, she endured regular beatings, rapes   \n",
              "6                   Walter Mondale is released from the Mayo Clinic. Mondale was diagnosed after he went to the hospital for a\\nroutine checkup following a fever. The 42nd vice president served under Carter between 1977 and 1981   \n",
              "7                  Rock and Roll Hall of Fame announces an array of A-list presenters and performers for its 30th Annual\\nInduction Ceremony. Seven artists and musical groups will be honored with induction into the Rock and Roll   \n",
              "\n",
              "                                                                                                                                                                                                                                                               baseline  \n",
              "0                                                                     The flight crew of the Delta Air Lines plane that skidded into a fence at LaGuardia Airport last week cited\\nbrake issues during the landing, according to an update on Monday from the NTSB. The  \n",
              "1                                                                I met Kelly Gissendaner in January 2010 in a nondescript classroom at Metro State Prison for Women in Atlanta.\\nShe arrived for class beaming with excitement about the journey she was about to begin  \n",
              "2                                                                                                          French striker Bafetimbi Gomis, who has a history of fainting, said he is now \"feeling well\" after collapsing\\nduring Swansea's 3-2 loss at Tottenham in the  \n",
              "3                                                                         When man relies on machine, there is always something that can go wrong. And there is no more unforgiving\\nenvironment than the high-stakes world of Formula One. \"Racing is one of those. It  \n",
              "4  We have no problem taking Wall Street executives to task for decisions that leave American families\\nfinancially devastated, yet we give Silicon Valley billionaires a pass when they do the same thing. Silicon\\nValley is tossing millennials aside like yesterday  \n",
              "5                                                                                Melissa was sold into the sex trade by a family member when she was only 12 years old. Her life became a\\nprison: Chained to a bed in a warehouse, she endured regular beatings, rapes  \n",
              "6                                  Former Vice President Walter Mondale was released from the Mayo Clinic on Saturday after being admitted with\\ninfluenza, hospital spokeswoman Kelley Luckstein said. \"He's doing well. We treated him for flu and cold\\nsymptoms and  \n",
              "7                                                                      Paul McCartney, Patti Smith, Beck, and John Legend are ready to rock Cleveland. On Wednesday the Rock and Roll\\nHall of Fame announced an array of A-list presenters and performers for its 30th  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# === Preview tri-compare: Reference vs POINTER vs BASELINE ===\n",
        "import pandas as pd, textwrap, random\n",
        "from IPython.display import display\n",
        "\n",
        "CSV = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step5000/ptr_decode_ckpt5000_beam5_len18_44.csv\"\n",
        "\n",
        "# how many examples to preview + formatting\n",
        "N_SAMPLES = 8\n",
        "SEED      = 0\n",
        "WRAP_COLS = 110  # characters per wrapped line for readability\n",
        "\n",
        "df = pd.read_csv(CSV)\n",
        "\n",
        "# make sure needed columns exist\n",
        "need = [\"highlights\", \"ptr_pred\", \"base_pred\"]\n",
        "missing = [c for c in need if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing columns in CSV: {missing}. \"\n",
        "                     f\"Make sure you've added 'base_pred' by running the baseline decode cell.\")\n",
        "\n",
        "# rename for clarity\n",
        "df = df.rename(columns={\n",
        "    \"highlights\": \"reference\",\n",
        "    \"ptr_pred\":   \"pointer\",\n",
        "    \"base_pred\":  \"baseline\",\n",
        "})\n",
        "\n",
        "# compute lengths\n",
        "for col in [\"reference\",\"pointer\",\"baseline\"]:\n",
        "    df[f\"len_{col}\"] = df[col].astype(str).apply(lambda s: len(s.split()))\n",
        "\n",
        "# sample N rows\n",
        "random.seed(SEED)\n",
        "show = df.sample(n=min(N_SAMPLES, len(df)), random_state=SEED).copy()\n",
        "\n",
        "# wrap text for readability in notebook output\n",
        "wrap = lambda s: textwrap.fill(str(s), width=WRAP_COLS, break_long_words=False, replace_whitespace=False)\n",
        "for col in [\"reference\",\"pointer\",\"baseline\"]:\n",
        "    show[col] = show[col].apply(wrap)\n",
        "\n",
        "# choose columns to display\n",
        "cols = []\n",
        "if \"id\" in show.columns:  # include id if present\n",
        "    cols.append(\"id\")\n",
        "cols += [\"len_reference\",\"len_pointer\",\"len_baseline\",\"reference\",\"pointer\",\"baseline\"]\n",
        "\n",
        "# nicer wide display\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "display(show[cols].reset_index(drop=True))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwOpQWShjFv7"
      },
      "source": [
        "# Train 5k to 6k finetune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2xcLsGFndMe",
        "outputId": "82f8ff4b-d0fa-45a6-b38d-9ff56afd6d66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ sanitized config.json\n",
            "✔️ repair step finished. Now reload tokenizer/model in the next cell.\n"
          ]
        }
      ],
      "source": [
        "# --- ONE-TIME REPAIR for ckpt_step5000 sidecars ---\n",
        "import os, json, shutil\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "CKPT_DIR = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step5000\"\n",
        "\n",
        "# A) ensure slow tokenizer sidecars exist (vocab.json + merges.txt).\n",
        "need_slow = any(not os.path.exists(os.path.join(CKPT_DIR, fn)) for fn in [\"vocab.json\",\"merges.txt\"])\n",
        "if need_slow:\n",
        "    print(\"[info] installing slow tokenizer files from facebook/bart-base …\")\n",
        "    base_tok = AutoTokenizer.from_pretrained(\"facebook/bart-base\", use_fast=False)\n",
        "    base_tok.save_pretrained(CKPT_DIR)\n",
        "\n",
        "# B) if a fast tokenizer.json exists and was corrupted, ignore it by keeping slow files.\n",
        "# (No need to delete tokenizer.json; we just won't use it.)\n",
        "\n",
        "# C) sanitize generation_config.json (or remove if sketchy)\n",
        "gcfg_path = os.path.join(CKPT_DIR, \"generation_config.json\")\n",
        "if os.path.exists(gcfg_path):\n",
        "    try:\n",
        "        with open(gcfg_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            G = json.load(f)\n",
        "        changed = False\n",
        "        if G.get(\"early_stopping\", None) is None:\n",
        "            G[\"early_stopping\"] = True; changed = True\n",
        "        # make sure IDs are ints\n",
        "        # we'll fill with tokenizer defaults after we load it below\n",
        "        if changed:\n",
        "            with open(gcfg_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(G, f)\n",
        "            print(\"✅ patched generation_config.json\")\n",
        "    except Exception as e:\n",
        "        os.remove(gcfg_path)\n",
        "        print(\"🗑️ removed malformed generation_config.json\")\n",
        "\n",
        "# D) sanitize config.json (this is what triggers the error you're seeing)\n",
        "cfg_path = os.path.join(CKPT_DIR, \"config.json\")\n",
        "with open(cfg_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    C = json.load(f)\n",
        "changed = False\n",
        "# If an old save injected generation fields into config with nulls, fix/remove them.\n",
        "if C.get(\"early_stopping\", None) is None:\n",
        "    # safest: drop it so HF doesn't try to read it into GenerationConfig\n",
        "    C.pop(\"early_stopping\", None); changed = True\n",
        "# (You can add more guards here if needed; typically early_stopping=null is the culprit.)\n",
        "\n",
        "if changed:\n",
        "    with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(C, f)\n",
        "    print(\"✅ sanitized config.json\")\n",
        "\n",
        "print(\"✔️ repair step finished. Now reload tokenizer/model in the next cell.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_WpUy-zjJmy",
        "outputId": "52b4d0c9-3c0c-4d39-ebdd-1020b7084bc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Found existing installation: transformers 4.55.4\n",
            "Uninstalling transformers-4.55.4:\n",
            "  Successfully uninstalled transformers-4.55.4\n",
            "Found existing installation: rouge_score 0.1.2\n",
            "Uninstalling rouge_score-0.1.2:\n",
            "  Successfully uninstalled rouge_score-0.1.2\n",
            "Found existing installation: accelerate 1.10.1\n",
            "Uninstalling accelerate-1.10.1:\n",
            "  Successfully uninstalled accelerate-1.10.1\n",
            "Found existing installation: datasets 4.0.0\n",
            "Uninstalling datasets-4.0.0:\n",
            "  Successfully uninstalled datasets-4.0.0\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mdevice: cpu\n"
          ]
        }
      ],
      "source": [
        "!pip install -q rouge-score==0.1.2\n",
        "\n",
        "!pip uninstall -y transformers rouge-score accelerate datasets\n",
        "\n",
        "!pip install -q \\\n",
        "  transformers==4.43.4 \\\n",
        "  accelerate==0.34.2 \\\n",
        "  datasets==2.20.0 \\\n",
        "  rouge-score==0.1.2\n",
        "import os, csv, math, time, random, json, re, gc\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Optional\n",
        "from torch import amp\n",
        "from rouge_score import rouge_scorer\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM,\n",
        "    get_linear_schedule_with_warmup, set_seed\n",
        ")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1BySBI6jkjE"
      },
      "outputs": [],
      "source": [
        "# --- Data paths (CSV with columns: id, article, highlights) ---\n",
        "TRAIN_CSV = \"/content/drive/MyDrive/cleaned_outputs/naa/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast_applied.csv\"\n",
        "VAL_CSV   = \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "\n",
        "RESUME_CKPT = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step5000\"\n",
        "\n",
        "\n",
        "RUN_DIR      = \"/content/drive/MyDrive/student_pgc_bartbase\"\n",
        "RESUME_STEP  = 5000\n",
        "TARGET_STEPS = 6000\n",
        "RESUME_CKPT  = os.path.join(RUN_DIR, f\"ckpt_step{RESUME_STEP}\")\n",
        "\n",
        "\n",
        "\n",
        "MODEL_NAME = \"facebook/bart-base\"\n",
        "\n",
        "# --- Lengths ---\n",
        "MAX_SRC_LEN = 400\n",
        "MAX_TGT_LEN = 100\n",
        "\n",
        "# --- Training ---\n",
        "SEED                 = 0\n",
        "BATCH_SIZE           = 56\n",
        "GRAD_ACCUM_STEPS     = 2\n",
        "NUM_WORKERS          = 4\n",
        "LR                   = 1e-5\n",
        "WARMUP_RATIO         = 0.03\n",
        "WEIGHT_DECAY         = 0.01\n",
        "MAX_STEPS            = 6000\n",
        "FP16                 = True\n",
        "FREEZE_ENCODER       = True\n",
        "LOG_EVERY            = 100\n",
        "SAVE_EVERY_STEPS     = 1000\n",
        "EPOCHS               = 99\n",
        "# --- Pointer/Coverage ---\n",
        "USE_POINTER          = True\n",
        "LAMBDA_COV           = 1.2\n",
        "AMBDA_GATE           = 0.03\n",
        "EPS                  = 1e-8\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqIYKfRQkn6x"
      },
      "outputs": [],
      "source": [
        "def robust_read_csv(path: str) -> pd.DataFrame:\n",
        "    try:\n",
        "        df = pd.read_csv(path, engine=\"python\", quoting=csv.QUOTE_ALL, escapechar=\"\\\\\")\n",
        "        cols = {c.lower(): c for c in df.columns}\n",
        "        return df[[cols[\"id\"], cols[\"article\"], cols[\"highlights\"]]].rename(\n",
        "            columns={cols[\"id\"]:\"id\", cols[\"article\"]:\"article\", cols[\"highlights\"]:\"highlights\"}\n",
        "        )\n",
        "    except Exception:\n",
        "        rows = []\n",
        "        with open(path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "            reader = csv.reader(f, delimiter=\",\", quotechar='\"', escapechar=\"\\\\\")\n",
        "            header = next(reader)\n",
        "            header = [h.strip().lower() for h in header]\n",
        "            idx_id, idx_art, idx_sum = header.index(\"id\"), header.index(\"article\"), header.index(\"highlights\")\n",
        "            for row in reader:\n",
        "                rows.append([row[idx_id], row[idx_art], row[idx_sum]])\n",
        "        return pd.DataFrame(rows, columns=[\"id\",\"article\",\"highlights\"])\n",
        "\n",
        "# --- Dataset ---\n",
        "class CNNDMDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame):\n",
        "        self.ids  = df[\"id\"].astype(str).tolist()\n",
        "        self.srcs = df[\"article\"].astype(str).tolist()\n",
        "        self.tgts = df[\"highlights\"].astype(str).tolist()\n",
        "    def __len__(self): return len(self.ids)\n",
        "    def __getitem__(self, i):\n",
        "        return dict(id=self.ids[i], article=self.srcs[i], highlights=self.tgts[i])\n",
        "\n",
        "# --- Collator (no as_target_tokenizer; supports old/new HF) ---\n",
        "@dataclass\n",
        "class PGDataCollator:\n",
        "    tok: AutoTokenizer\n",
        "    max_src_len: int\n",
        "    max_tgt_len: int\n",
        "    def __call__(self, batch: List[Dict[str, str]]) -> Dict[str, torch.Tensor]:\n",
        "        src_texts = [b[\"article\"] for b in batch]\n",
        "        tgt_texts = [b[\"highlights\"] for b in batch]\n",
        "        try:\n",
        "            enc = self.tok(\n",
        "                src_texts,\n",
        "                text_target=tgt_texts,\n",
        "                padding=True, truncation=True,\n",
        "                max_length=self.max_src_len,\n",
        "                max_length_target=self.max_tgt_len,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "        except TypeError:\n",
        "            enc = self.tok(src_texts, padding=True, truncation=True,\n",
        "                           max_length=self.max_src_len, return_tensors=\"pt\")\n",
        "            tgt = self.tok(text_target=tgt_texts, padding=True, truncation=True,\n",
        "                           max_length=self.max_tgt_len, return_tensors=\"pt\")\n",
        "            enc[\"labels\"] = tgt[\"input_ids\"]\n",
        "        labels = enc[\"labels\"]\n",
        "        labels[labels == self.tok.pad_token_id] = -100\n",
        "        enc[\"labels\"] = labels\n",
        "        return enc\n",
        "\n",
        "# --- quick utils ---\n",
        "def _gpu_mem_mb():\n",
        "    if not torch.cuda.is_available(): return 0.0\n",
        "    return torch.cuda.max_memory_allocated() / (1024**2)\n",
        "\n",
        "class RunningMean:\n",
        "    def __init__(self, n=200):\n",
        "        self.buf, self.n = [], n\n",
        "    def add(self, x):\n",
        "        if x is None: return\n",
        "        self.buf.append(float(x))\n",
        "        if len(self.buf) > self.n: self.buf.pop(0)\n",
        "    @property\n",
        "    def mean(self):\n",
        "        return sum(self.buf)/len(self.buf) if self.buf else 0.0\n",
        "\n",
        "class StepLogger:\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "        if not os.path.exists(path):\n",
        "            with open(path, \"w\") as f:\n",
        "                f.write(\"epoch,step,loss,ce_loss,cov_loss,p_copy,p_gen,lr,grad_norm,toks_per_s,gpu_mem_mb\\n\")\n",
        "    def log(self, row: Dict):\n",
        "        with open(self.path, \"a\") as f:\n",
        "            f.write(\",\".join(str(row[k]) for k in [\"epoch\",\"step\",\"loss\",\"ce_loss\",\"cov_loss\",\"p_copy\",\"p_gen\",\"lr\",\"grad_norm\",\"toks_per_s\",\"gpu_mem_mb\"])+\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed0NGe3Wkp82"
      },
      "outputs": [],
      "source": [
        "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
        "\n",
        "class CopyAwareBart(nn.Module):\n",
        "    \"\"\"\n",
        "    BART wrapper with pointer-generator + coverage.\n",
        "    - mixes vocab distribution with copy distribution built from cross-attn over source tokens\n",
        "    - computes CE over log(final_dist) + lambda_cov * coverage\n",
        "    \"\"\"\n",
        "    def __init__(self, base_lm: AutoModelForSeq2SeqLM, tokenizer: AutoTokenizer,\n",
        "                 lambda_cov: float = 1.0, eps: float = 1e-8, use_pointer: bool = True):\n",
        "        super().__init__()\n",
        "        self.base       = base_lm\n",
        "        self.tok        = tokenizer\n",
        "        self.lambda_cov = lambda_cov\n",
        "        self.eps        = eps\n",
        "        self.use_ptr    = use_pointer\n",
        "        hidden = self.base.config.d_model\n",
        "        self.p_gen_linear = nn.Linear(3*hidden, 1)\n",
        "        # caches for logging\n",
        "        self._last_ce_loss = None\n",
        "        self._last_cov_loss = None\n",
        "        self._last_p_copy_mean = None\n",
        "        self._last_p_gen_mean  = None\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        decoder_input_ids: Optional[torch.Tensor] = None,\n",
        "        decoder_attention_mask: Optional[torch.Tensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        past_key_values=None, use_cache: Optional[bool] = None, **kwargs\n",
        "    ) -> Seq2SeqLMOutput:\n",
        "\n",
        "        if labels is not None and decoder_input_ids is None:\n",
        "            decoder_input_ids = self.base.prepare_decoder_input_ids_from_labels(labels)\n",
        "        if decoder_attention_mask is None and decoder_input_ids is not None:\n",
        "            decoder_attention_mask = (decoder_input_ids != self.tok.pad_token_id).to(input_ids.dtype)\n",
        "\n",
        "        need_attn = self.use_ptr or bool(output_attentions)\n",
        "        need_hid  = self.use_ptr or bool(output_hidden_states)\n",
        "\n",
        "        base_out = self.base(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            output_attentions=need_attn,\n",
        "            output_hidden_states=need_hid,\n",
        "            labels=None,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        logits = base_out.logits  # [B,T,V]\n",
        "        final_logits = logits\n",
        "        cov_loss = None\n",
        "        copy_gate = None\n",
        "        gen_gate  = None\n",
        "        loss_ce   = None\n",
        "\n",
        "        if self.use_ptr:\n",
        "            # hidden states\n",
        "            dec_hid = base_out.decoder_hidden_states[-1]            # [B,T,H]\n",
        "            enc_out = base_out.encoder_last_hidden_state            # [B,S,H]\n",
        "\n",
        "            # cross-attn last layer: [B,heads,T,S] -> mean heads -> [B,T,S]\n",
        "            cross_atts = base_out.cross_attentions[-1].mean(dim=1)\n",
        "\n",
        "            # mask PAD in source attention\n",
        "            if attention_mask is not None:\n",
        "                src_pad_mask = (attention_mask == 0).unsqueeze(1)   # [B,1,S]\n",
        "                attn_clean   = cross_atts.masked_fill(src_pad_mask, 0.0)\n",
        "            else:\n",
        "                attn_clean   = cross_atts\n",
        "\n",
        "            # normalize per time step\n",
        "            denom     = attn_clean.sum(dim=-1, keepdim=True).clamp_min(self.eps)\n",
        "            attn_norm = attn_clean / denom                          # [B,T,S]\n",
        "\n",
        "            # context vectors\n",
        "            context = torch.bmm(attn_norm, enc_out)                 # [B,T,H]\n",
        "\n",
        "            # prev token embeddings\n",
        "            if decoder_input_ids is not None:\n",
        "                dec_emb = self.base.get_input_embeddings()(decoder_input_ids)  # [B,T,H]\n",
        "            else:\n",
        "                dec_emb = torch.zeros_like(dec_hid)\n",
        "\n",
        "            # generation/copy gates\n",
        "            p_gen  = torch.sigmoid(self.p_gen_linear(torch.cat([dec_hid, context, dec_emb], dim=-1)))  # [B,T,1]\n",
        "            p_copy = 1.0 - p_gen\n",
        "\n",
        "            # vocab distribution\n",
        "            vocab_dist = torch.softmax(logits, dim=-1)              # [B,T,V]\n",
        "\n",
        "            # copy distribution: scatter attention into vocab bins using source token ids\n",
        "            B, T, S = attn_norm.shape\n",
        "            V       = logits.size(-1)\n",
        "            copy_dist = torch.zeros(B, T, V, device=attn_norm.device, dtype=attn_norm.dtype)\n",
        "\n",
        "            batch_idx = torch.arange(B, device=input_ids.device)[:, None, None].expand(B, T, S)\n",
        "            time_idx  = torch.arange(T, device=input_ids.device)[None, :, None].expand(B, T, S)\n",
        "            vocab_idx = input_ids[:, None, :].expand(B, T, S)\n",
        "\n",
        "            copy_dist = copy_dist.index_put((batch_idx, time_idx, vocab_idx), attn_norm, accumulate=True)\n",
        "\n",
        "            # mix\n",
        "            final_dist   = p_gen * vocab_dist + p_copy * copy_dist   # [B,T,V]\n",
        "            final_logits = torch.log(final_dist + self.eps)\n",
        "\n",
        "            # gate means for logging\n",
        "            copy_gate = p_copy.mean().detach()\n",
        "            gen_gate  = p_gen.mean().detach()\n",
        "\n",
        "            # ---- coverage loss (fixed shape) ----\n",
        "            cov = torch.zeros_like(attn_norm[:, 0, :])               # [B,S]\n",
        "            step_losses = []\n",
        "            for t in range(T):\n",
        "                a_t = attn_norm[:, t, :]                             # [B,S]\n",
        "                step_losses.append(torch.min(a_t, cov).sum(dim=-1))  # [B]\n",
        "                cov = cov + a_t\n",
        "            cov_loss = torch.stack(step_losses, dim=1).mean()        # scalar\n",
        "\n",
        "        # loss over mixed distribution\n",
        "        if labels is not None:\n",
        "            V = final_logits.size(-1)\n",
        "            loss_fct = nn.NLLLoss(ignore_index=-100)\n",
        "            loss_ce  = loss_fct(final_logits.view(-1, V), labels.view(-1))\n",
        "            loss = loss_ce + (self.lambda_cov * cov_loss if cov_loss is not None else 0.0)\n",
        "        else:\n",
        "            loss = None\n",
        "\n",
        "        # expose scalars for logger\n",
        "        with torch.no_grad():\n",
        "            self._last_ce_loss     = loss_ce.detach() if loss_ce is not None else None\n",
        "            self._last_cov_loss    = cov_loss.detach() if cov_loss is not None else None\n",
        "            self._last_p_copy_mean = copy_gate if copy_gate is not None else None\n",
        "            self._last_p_gen_mean  = gen_gate  if gen_gate  is not None else None\n",
        "\n",
        "        return Seq2SeqLMOutput(\n",
        "            loss=loss,\n",
        "            logits=final_logits,\n",
        "            past_key_values=base_out.past_key_values,\n",
        "            decoder_hidden_states=base_out.decoder_hidden_states if need_hid else None,\n",
        "            decoder_attentions=base_out.decoder_attentions if need_attn else None,\n",
        "            cross_attentions=base_out.cross_attentions if need_attn else None,\n",
        "            encoder_last_hidden_state=base_out.encoder_last_hidden_state,\n",
        "            encoder_hidden_states=base_out.encoder_hidden_states if need_hid else None,\n",
        "            encoder_attentions=base_out.encoder_attentions if need_attn else None,\n",
        "        )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYXRSUYhkzYt",
        "outputId": "0d09df17-2d82-48e7-fa2c-21cc38bd5c25"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:606: UserWarning: `num_beams` is set to None - defaulting to 1.\n",
            "  warnings.warn(\"`num_beams` is set to None - defaulting to 1.\", UserWarning)\n"
          ]
        }
      ],
      "source": [
        "SEED = 0\n",
        "import numpy as np\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# ---- tokenizer + base (force eager attention) ----\n",
        "tok = AutoTokenizer.from_pretrained(RESUME_CKPT, use_fast=True)\n",
        "cfg = AutoConfig.from_pretrained(RESUME_CKPT, attn_implementation=\"eager\")\n",
        "base = AutoModelForSeq2SeqLM.from_pretrained(RESUME_CKPT, config=cfg)\n",
        "\n",
        "base.config.use_cache = False\n",
        "base.gradient_checkpointing_enable()\n",
        "\n",
        "# wrap with pointer/coverage\n",
        "model = CopyAwareBart(base, tok, lambda_cov=LAMBDA_COV, eps=EPS, use_pointer=USE_POINTER).to(DEVICE)\n",
        "\n",
        "if FREEZE_ENCODER:\n",
        "    for p in model.base.model.encoder.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "\n",
        "# AMP scaler (new API)\n",
        "scaler = amp.GradScaler(\"cuda\", enabled=FP16)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVsiVkLdnzIn"
      },
      "outputs": [],
      "source": [
        "# ---- read data ----\n",
        "train_df = robust_read_csv(TRAIN_CSV)\n",
        "val_df   = robust_read_csv(VAL_CSV)\n",
        "\n",
        "# ---- dataset + collate ----\n",
        "collate   = PGDataCollator(tok=tok, max_src_len=MAX_SRC_LEN, max_tgt_len=MAX_TGT_LEN)\n",
        "train_ds  = CNNDMDataset(train_df)\n",
        "val_ds    = CNNDMDataset(val_df)\n",
        "\n",
        "# ---- loaders ----\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    pin_memory=False,\n",
        "    persistent_workers=False,\n",
        "    collate_fn=collate,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "    num_workers=NUM_WORKERS, pin_memory=False,\n",
        "    persistent_workers=(NUM_WORKERS>0), prefetch_factor=(2 if NUM_WORKERS>0 else None),\n",
        "    collate_fn=collate,\n",
        ")\n",
        "\n",
        "# ---- logger + checkpoint helpers ----\n",
        "LOG_CSV = os.path.join(RUN_DIR, \"train_log.csv\")\n",
        "logger  = StepLogger(LOG_CSV)\n",
        "\n",
        "def save_checkpoint(tag):\n",
        "    \"\"\"Save base model, tokenizer, plus pointer head state.\"\"\"\n",
        "    ck = os.path.join(RUN_DIR, f\"ckpt_step{tag}\" if isinstance(tag,int) else f\"ckpt_{tag}\")\n",
        "    os.makedirs(ck, exist_ok=True)\n",
        "    model.base.save_pretrained(ck)\n",
        "    tok.save_pretrained(ck)\n",
        "    torch.save({\n",
        "        \"p_gen_linear\": model.p_gen_linear.state_dict(),\n",
        "        \"lambda_cov\": model.lambda_cov,\n",
        "        \"use_pointer\": model.use_ptr,\n",
        "    }, os.path.join(ck, \"pointer_head.pt\"))\n",
        "    print(f\"[checkpoint] saved → {ck}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dls5WTu5n_NW",
        "outputId": "f2c2e725-36c5-438f-e853-e26898f17d7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[scheduler] total=6000, warmup=180\n"
          ]
        }
      ],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\"]\n",
        "params_decay, params_nodecay = [], []\n",
        "for n, p in model.named_parameters():\n",
        "    if not p.requires_grad:\n",
        "        continue\n",
        "    (params_nodecay if any(nd in n for nd in no_decay) else params_decay).append(p)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [{\"params\": params_decay, \"weight_decay\": WEIGHT_DECAY},\n",
        "     {\"params\": params_nodecay, \"weight_decay\": 0.0}],\n",
        "    lr=LR,\n",
        ")\n",
        "\n",
        "import math\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "num_update_steps_per_epoch = math.ceil(len(train_loader) / GRAD_ACCUM_STEPS)\n",
        "\n",
        "# >>> KEY FIX: respect MAX_STEPS if set\n",
        "t_total = int(MAX_STEPS) if MAX_STEPS else num_update_steps_per_epoch * EPOCHS\n",
        "warmup_steps = max(1, int(WARMUP_RATIO * t_total))  # e.g., 0.06 * 2000 = 120\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=t_total,\n",
        ")\n",
        "print(f\"[scheduler] total={t_total}, warmup={warmup_steps}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhW7PYuhozLB",
        "outputId": "80b4e16f-14c0-4882-982c-0e04c1bf5a4c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:615: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `never` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import GenerationConfig\n",
        "\n",
        "# 0) (optional) if a stale generation_config.json sits on disk, remove it once:\n",
        "import os\n",
        "gcfg_path = os.path.join(RESUME_CKPT, \"generation_config.json\")\n",
        "if os.path.exists(gcfg_path):\n",
        "    os.remove(gcfg_path)\n",
        "\n",
        "# 1) scrub generation keys from the model CONFIG so they don't leak again\n",
        "BAD_KEYS = [\n",
        "    \"early_stopping\", \"num_beams\", \"no_repeat_ngram_size\", \"length_penalty\",\n",
        "    \"output_attentions\", \"output_hidden_states\", \"return_dict_in_generate\", \"use_cache\"\n",
        "]\n",
        "for k in BAD_KEYS:\n",
        "    if hasattr(base.config, k):\n",
        "        try:\n",
        "            delattr(base.config, k)        # best effort\n",
        "        except Exception:\n",
        "            base.config.__dict__.pop(k, None)\n",
        "\n",
        "# 2) attach a brand-new, VALID GenerationConfig (DON'T call from_model_config)\n",
        "gc = GenerationConfig(\n",
        "    # keep defaults minimal; set only what must be valid\n",
        "    early_stopping=\"never\",                      # avoids the num_beams=1 warning-on-save\n",
        "    pad_token_id=tok.pad_token_id or 1,\n",
        "    bos_token_id=tok.bos_token_id or 0,\n",
        "    eos_token_id=tok.eos_token_id or 2,\n",
        "    decoder_start_token_id=getattr(base.config, \"decoder_start_token_id\", tok.eos_token_id or 2),\n",
        ")\n",
        "base.generation_config = gc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxAU_nkiphB8"
      },
      "outputs": [],
      "source": [
        "from transformers import GenerationConfig\n",
        "\n",
        "# --- DO NOT scrub these from base.config ---\n",
        "# Ensure these attributes exist with sane values:\n",
        "if not hasattr(base.config, \"use_cache\"):                base.config.use_cache = False\n",
        "if not hasattr(base.config, \"output_attentions\"):        base.config.output_attentions = False\n",
        "if not hasattr(base.config, \"output_hidden_states\"):     base.config.output_hidden_states = False\n",
        "if not hasattr(base.config, \"return_dict_in_generate\"):  base.config.return_dict_in_generate = True\n",
        "\n",
        "# Only scrub the true generation-only keys that caused save warnings:\n",
        "for k in [\"early_stopping\", \"num_beams\", \"no_repeat_ngram_size\", \"length_penalty\"]:\n",
        "    if hasattr(base.config, k):\n",
        "        try: delattr(base.config, k)\n",
        "        except Exception: base.config.__dict__.pop(k, None)\n",
        "\n",
        "# Attach a clean, VALID GenerationConfig (don’t derive from model_config)\n",
        "base.generation_config = GenerationConfig(\n",
        "    early_stopping=\"never\",  # avoids save-time validation error when num_beams defaults to 1\n",
        "    pad_token_id=tok.pad_token_id or 1,\n",
        "    bos_token_id=tok.bos_token_id or 0,\n",
        "    eos_token_id=tok.eos_token_id or 2,\n",
        "    decoder_start_token_id=getattr(base.config, \"decoder_start_token_id\", tok.eos_token_id or 2),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFvLhBpXqFG2"
      },
      "outputs": [],
      "source": [
        "from transformers import GenerationConfig\n",
        "\n",
        "# 1) REMOVE generation-only fields from the *model* config\n",
        "#    (so HF stops warning that they're in the wrong place)\n",
        "for k in [\n",
        "    \"early_stopping\", \"num_beams\", \"no_repeat_ngram_size\", \"length_penalty\",\n",
        "    \"return_dict_in_generate\", \"forced_eos_token_id\"\n",
        "]:\n",
        "    if hasattr(base.config, k):\n",
        "        try:\n",
        "            delattr(base.config, k)\n",
        "        except Exception:\n",
        "            base.config.__dict__.pop(k, None)\n",
        "\n",
        "# 2) KEEP legit model flags (they're used in forward); ensure they exist\n",
        "if not hasattr(base.config, \"use_cache\"):               base.config.use_cache = False\n",
        "if not hasattr(base.config, \"output_attentions\"):       base.config.output_attentions = False\n",
        "if not hasattr(base.config, \"output_hidden_states\"):    base.config.output_hidden_states = False\n",
        "\n",
        "# 3) ATTACH a clean, valid GenerationConfig that saves without warnings\n",
        "#    (key: set num_beams > 1 so early_stopping is allowed)\n",
        "base.generation_config = GenerationConfig(\n",
        "    num_beams=2,                       # make save-pretrained happy\n",
        "    early_stopping=True,               # or \"never\" — both fine with beams>1\n",
        "    pad_token_id=tok.pad_token_id or 1,\n",
        "    bos_token_id=tok.bos_token_id or 0,\n",
        "    eos_token_id=tok.eos_token_id or 2,\n",
        "    decoder_start_token_id=getattr(base.config, \"decoder_start_token_id\", tok.eos_token_id or 2),\n",
        "    forced_eos_token_id=tok.eos_token_id or 2,\n",
        "    return_dict_in_generate=False      # keep default here; pass your own in generate() if you need it\n",
        ")\n",
        "\n",
        "# (Optional) if a stale on-disk generation_config.json is hanging around and keeps biting you:\n",
        "import os\n",
        "gcfg_path = os.path.join(RESUME_CKPT, \"generation_config.json\")\n",
        "if os.path.exists(gcfg_path):\n",
        "    try:\n",
        "        os.remove(gcfg_path)\n",
        "        print(\" removed stale generation_config.json on disk\")\n",
        "    except Exception:\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kwq4iXCnqwqA"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(tag):\n",
        "    import os\n",
        "    save_dir = os.path.join(RESUME_CKPT.rsplit(\"/\",1)[0], f\"ckpt_step{tag}\")\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # patch the generation config just before saving\n",
        "    gc = model.base.generation_config\n",
        "    gc.num_beams = 2           # safe default so HF validator is happy\n",
        "    gc.early_stopping = True   # valid since num_beams>1\n",
        "\n",
        "    # save model weights + config + gen_config.json\n",
        "    model.base.save_pretrained(save_dir)\n",
        "    tok.save_pretrained(save_dir)  # save tokenizer (use_fast=False is safest on Colab/Drive)\n",
        "\n",
        "    # pointer head too\n",
        "    torch.save({\n",
        "        \"p_gen_linear\": model.p_gen_linear.state_dict(),\n",
        "        \"lambda_cov\": float(model.lambda_cov),\n",
        "        \"use_pointer\": bool(model.use_ptr),\n",
        "    }, os.path.join(save_dir, \"pointer_head.pt\"))\n",
        "\n",
        "    print(f\"[checkpoint] saved → {save_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MmeJCJzoJk-",
        "outputId": "4c3460bc-2d14-490c-98a1-61779115d98c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ep 1] step   5100 | loss  2.5836 | ce  2.2065 | cov  0.3142 | p_copy 0.264 | p_gen 0.736 | lr 3.46e-06 | grad_norm 84386.43 | toks/s  16868.9 | gpu_mem 17529.6 MB\n",
            "[ep 1] step   5200 | loss  2.5963 | ce  2.2178 | cov  0.3154 | p_copy 0.260 | p_gen 0.740 | lr 4.29e-06 | grad_norm 85345.11 | toks/s  16892.1 | gpu_mem 17529.6 MB\n",
            "[ep 1] step   5300 | loss  2.6052 | ce  2.2263 | cov  0.3158 | p_copy 0.256 | p_gen 0.744 | lr 4.81e-06 | grad_norm 84651.76 | toks/s  16913.3 | gpu_mem 17529.6 MB\n",
            "[ep 1] step   5400 | loss  2.5869 | ce  2.2114 | cov  0.3130 | p_copy 0.256 | p_gen 0.744 | lr 3.56e-06 | grad_norm 84776.70 | toks/s  16914.6 | gpu_mem 17529.6 MB\n",
            "[ep 1] step   5500 | loss  2.5774 | ce  2.2041 | cov  0.3111 | p_copy 0.257 | p_gen 0.743 | lr 2.31e-06 | grad_norm 89099.02 | toks/s  16915.4 | gpu_mem 17529.6 MB\n",
            "[ep 1] step   5600 | loss  2.5816 | ce  2.2092 | cov  0.3104 | p_copy 0.256 | p_gen 0.744 | lr 1.06e-06 | grad_norm 85607.98 | toks/s  16916.4 | gpu_mem 17529.6 MB\n",
            "[ep 1] step   5700 | loss  2.5784 | ce  2.2064 | cov  0.3100 | p_copy 0.256 | p_gen 0.744 | lr 0.00e+00 | grad_norm 90928.34 | toks/s  16920.4 | gpu_mem 17529.6 MB\n",
            "[ep 1] step   5800 | loss  2.5703 | ce  2.1993 | cov  0.3092 | p_copy 0.255 | p_gen 0.745 | lr 0.00e+00 | grad_norm 84040.63 | toks/s  16920.0 | gpu_mem 17529.6 MB\n",
            "[ep 1] step   5900 | loss  2.5739 | ce  2.2041 | cov  0.3082 | p_copy 0.256 | p_gen 0.744 | lr 0.00e+00 | grad_norm 84343.59 | toks/s  16919.2 | gpu_mem 17529.6 MB\n",
            "[ep 1] step   6000 | loss  2.5878 | ce  2.2161 | cov  0.3097 | p_copy 0.256 | p_gen 0.744 | lr 0.00e+00 | grad_norm 82530.85 | toks/s  16918.2 | gpu_mem 17529.6 MB\n",
            "[checkpoint] saved → /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000\n",
            "[train] Reached MAX_STEPS=6000. Stopping early.\n",
            "[checkpoint] saved → /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000\n"
          ]
        }
      ],
      "source": [
        "RESUME_STEP = 5000\n",
        "MAX_STEPS   = 6000\n",
        "global_step = RESUME_STEP\n",
        "\n",
        "best_val = float(\"inf\")\n",
        "\n",
        "# running averages\n",
        "m_loss = RunningMean(200); m_ce = RunningMean(200); m_cov = RunningMean(200)\n",
        "m_pcopy = RunningMean(200); m_pgen = RunningMean(200); m_toks = RunningMean(200)\n",
        "\n",
        "model.train()\n",
        "optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "start_time = time.time()\n",
        "tokens_seen = 0\n",
        "\n",
        "\n",
        "# running averages\n",
        "m_loss = RunningMean(200); m_ce = RunningMean(200); m_cov = RunningMean(200)\n",
        "m_pcopy = RunningMean(200); m_pgen = RunningMean(200); m_toks = RunningMean(200)\n",
        "\n",
        "model.train()\n",
        "optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "start_time = time.time()\n",
        "tokens_seen = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    for it, batch in enumerate(train_loader):\n",
        "        for k in batch:\n",
        "            batch[k] = batch[k].to(DEVICE)\n",
        "\n",
        "        # token count for throughput\n",
        "        with torch.no_grad():\n",
        "            toks_this_batch = int(batch[\"attention_mask\"].sum().item())\n",
        "            if \"labels\" in batch:\n",
        "                toks_this_batch += int((batch[\"labels\"] != -100).sum().item())\n",
        "\n",
        "        # forward pass with autocast\n",
        "        with amp.autocast(\"cuda\", enabled=FP16):\n",
        "            out = model(**batch)\n",
        "            loss = out.loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        took_step = False\n",
        "\n",
        "        if (it + 1) % GRAD_ACCUM_STEPS == 0:\n",
        "            # unscale once, then clip & log grad norm\n",
        "            # grad norm (scaled grads are fine for logging in AMP)\n",
        "            # grad norm (scaled grads are fine for logging in AMP)\n",
        "            total_norm = torch.norm(\n",
        "                torch.stack([p.grad.detach().norm(2) for p in model.parameters() if p.grad is not None])\n",
        "            ).item()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            scheduler.step()\n",
        "            global_step += 1\n",
        "            took_step = True\n",
        "\n",
        "            # update running stats\n",
        "            tokens_seen += toks_this_batch\n",
        "            m_loss.add(loss.item())\n",
        "            m_ce.add(getattr(model, \"_last_ce_loss\", None))\n",
        "            m_cov.add(getattr(model, \"_last_cov_loss\", None))\n",
        "            m_pcopy.add(getattr(model, \"_last_p_copy_mean\", None))\n",
        "            m_pgen.add(getattr(model, \"_last_p_gen_mean\", None))\n",
        "            elapsed = time.time() - start_time\n",
        "            m_toks.add(tokens_seen / max(elapsed, 1e-6))\n",
        "\n",
        "            if global_step % LOG_EVERY == 0:\n",
        "                lr = scheduler.get_last_lr()[0]\n",
        "                mem = _gpu_mem_mb()\n",
        "                print(\n",
        "                    f\"[ep {epoch+1}] step {global_step:>6} | \"\n",
        "                    f\"loss {m_loss.mean:7.4f} | ce {m_ce.mean:7.4f} | cov {m_cov.mean:7.4f} | \"\n",
        "                    f\"p_copy {m_pcopy.mean:5.3f} | p_gen {m_pgen.mean:5.3f} | \"\n",
        "                    f\"lr {lr:.2e} | grad_norm {total_norm:6.2f} | \"\n",
        "                    f\"toks/s {m_toks.mean:8.1f} | gpu_mem {mem:7.1f} MB\"\n",
        "                )\n",
        "                logger.log({\n",
        "                    \"epoch\": epoch+1,\n",
        "                    \"step\": global_step,\n",
        "                    \"loss\": round(m_loss.mean, 6),\n",
        "                    \"ce_loss\": round(m_ce.mean, 6),\n",
        "                    \"cov_loss\": round(m_cov.mean, 6),\n",
        "                    \"p_copy\": round(m_pcopy.mean, 6),\n",
        "                    \"p_gen\": round(m_pgen.mean, 6),\n",
        "                    \"lr\": lr,\n",
        "                    \"grad_norm\": total_norm,\n",
        "                    \"toks_per_s\": m_toks.mean,\n",
        "                    \"gpu_mem_mb\": mem,\n",
        "                })\n",
        "\n",
        "            if global_step % SAVE_EVERY_STEPS == 0:\n",
        "                save_checkpoint(global_step)\n",
        "\n",
        "            if global_step >= MAX_STEPS:\n",
        "                print(f\"[train] Reached MAX_STEPS={MAX_STEPS}. Stopping early.\")\n",
        "                save_checkpoint(f\"step{global_step}\")\n",
        "                break\n",
        "\n",
        "        if torch.cuda.is_available(): torch.cuda.synchronize()\n",
        "        del out\n",
        "        if took_step: torch.cuda.empty_cache()\n",
        "\n",
        "    if global_step >= MAX_STEPS:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rg9M7_n4ygH",
        "outputId": "498a0137-58da-4bcf-e96f-7d1b0bd0eaf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Inspecting: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000 ===\n",
            "• files: config.json, generation_config.json, model.safetensors, tokenizer_config.json, special_tokens_map.json, vocab.json, merges.txt, tokenizer.json, pointer_head.pt\n",
            "• tokenizer(slow) OK\n",
            "• model load & generate() OK\n",
            "• file integrity:\n",
            "  - model.safetensors       557.91 MB  sha256:664ff84264ff\n",
            "  - config.json               0.00 MB  sha256:c85e6a802d70\n",
            "  - generation_config.json      0.00 MB  sha256:5897b3750113\n",
            "  - vocab.json                0.80 MB  sha256:ed19656ea170\n",
            "  - merges.txt                0.46 MB  sha256:1ce1664773c5\n",
            "  - tokenizer.json            2.11 MB  sha256:4dbf61e0b1d5\n",
            "  - pointer_head.pt           0.01 MB  sha256:81b28d775e93\n",
            "✅ /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000 looks good.\n"
          ]
        }
      ],
      "source": [
        "# === Checkpoint Doctor: validate & repair saved checkpoints ===\n",
        "import os, json, hashlib, traceback\n",
        "from typing import List\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, GenerationConfig\n",
        "\n",
        "def _sha256(path):\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(1<<20), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()[:12]\n",
        "\n",
        "def _exists_nonempty(p): return os.path.exists(p) and os.path.getsize(p) > 0\n",
        "\n",
        "def validate_and_repair_ckpt(ckpt_dir: str, try_fast: bool = False, repair: bool = True):\n",
        "    print(f\"\\n=== Inspecting: {ckpt_dir} ===\")\n",
        "    assert os.path.isdir(ckpt_dir), f\"Missing dir: {ckpt_dir}\"\n",
        "\n",
        "    # 1) Files present?\n",
        "    critical = [\"config.json\", \"pytorch_model.bin\", \"generation_config.json\"]\n",
        "    # allow safetensors alternative\n",
        "    if not _exists_nonempty(os.path.join(ckpt_dir, \"pytorch_model.bin\")) and not _exists_nonempty(os.path.join(ckpt_dir, \"model.safetensors\")):\n",
        "        raise FileNotFoundError(\"Neither pytorch_model.bin nor model.safetensors found.\")\n",
        "    print(\"• files:\", \", \".join([f for f in os.listdir(ckpt_dir) if f in {\"config.json\",\"pytorch_model.bin\",\"model.safetensors\",\"generation_config.json\",\"vocab.json\",\"merges.txt\",\"tokenizer.json\",\"tokenizer_config.json\",\"special_tokens_map.json\",\"pointer_head.pt\"}]))\n",
        "\n",
        "    # 2) Ensure slow tokenizer files exist (robust on Drive)\n",
        "    need_slow = not (_exists_nonempty(os.path.join(ckpt_dir,\"vocab.json\")) and _exists_nonempty(os.path.join(ckpt_dir,\"merges.txt\")))\n",
        "    if need_slow and repair:\n",
        "        print(\"• installing slow tokenizer (vocab/merges) into ckpt…\")\n",
        "        tok_base = AutoTokenizer.from_pretrained(\"facebook/bart-base\", use_fast=False)\n",
        "        tok_base.save_pretrained(ckpt_dir)\n",
        "\n",
        "    # 3) Sanitize generation_config.json\n",
        "    gcfg_path = os.path.join(ckpt_dir, \"generation_config.json\")\n",
        "    if os.path.exists(gcfg_path):\n",
        "        try:\n",
        "            G = json.load(open(gcfg_path,\"r\"))\n",
        "            changed = False\n",
        "            # must be boolean or \"never\"\n",
        "            if G.get(\"early_stopping\", None) not in (True, False, \"never\"):\n",
        "                G[\"early_stopping\"] = True; changed = True\n",
        "            # ensure IDs are ints\n",
        "            for k in [\"pad_token_id\",\"bos_token_id\",\"eos_token_id\",\"decoder_start_token_id\",\"forced_eos_token_id\"]:\n",
        "                if k in G and G[k] is not None:\n",
        "                    G[k] = int(G[k])\n",
        "            if changed and repair:\n",
        "                json.dump(G, open(gcfg_path,\"w\"))\n",
        "                print(\"• patched generation_config.json\")\n",
        "        except Exception as e:\n",
        "            if repair:\n",
        "                os.remove(gcfg_path)\n",
        "                print(\"• removed malformed generation_config.json (will use defaults)\")\n",
        "            else:\n",
        "                print(\"• warn: bad generation_config.json:\", e)\n",
        "\n",
        "    # 4) Try loading tokenizer (slow first; fast optional)\n",
        "    try:\n",
        "        tok = AutoTokenizer.from_pretrained(ckpt_dir, use_fast=False, local_files_only=True)\n",
        "        print(\"• tokenizer(slow) OK\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Tokenizer(slow) load failed: {e}\")\n",
        "\n",
        "    if try_fast:\n",
        "        try:\n",
        "            _ = AutoTokenizer.from_pretrained(ckpt_dir, use_fast=True, local_files_only=True)\n",
        "            print(\"• tokenizer(fast) OK\")\n",
        "        except Exception as e:\n",
        "            print(\"• tokenizer(fast) FAIL (not fatal):\", e)\n",
        "\n",
        "    # 5) Load model; attach clean GenerationConfig; quick smoke generate\n",
        "    try:\n",
        "        base = AutoModelForSeq2SeqLM.from_pretrained(ckpt_dir, attn_implementation=\"eager\")\n",
        "        # keep forward flags sane\n",
        "        if not hasattr(base.config, \"use_cache\"): base.config.use_cache = False\n",
        "        base.config.use_cache = False\n",
        "\n",
        "        # remove stray gen fields from model config (so HF won’t complain on next save)\n",
        "        for k in [\"early_stopping\",\"num_beams\",\"no_repeat_ngram_size\",\"length_penalty\",\"return_dict_in_generate\",\"forced_eos_token_id\"]:\n",
        "            if hasattr(base.config, k):\n",
        "                try: delattr(base.config, k)\n",
        "                except Exception: base.config.__dict__.pop(k, None)\n",
        "\n",
        "        # attach a clean, valid GenerationConfig (saves cleanly)\n",
        "        base.generation_config = GenerationConfig(\n",
        "            num_beams=2, early_stopping=True,\n",
        "            pad_token_id=tok.pad_token_id or 1,\n",
        "            bos_token_id=tok.bos_token_id or 0,\n",
        "            eos_token_id=tok.eos_token_id or 2,\n",
        "            decoder_start_token_id=getattr(base.config, \"decoder_start_token_id\", tok.eos_token_id or 2),\n",
        "            forced_eos_token_id=tok.eos_token_id or 2,\n",
        "        )\n",
        "        # tiny smoke generate\n",
        "        import torch\n",
        "        base.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        enc = tok(\"Smoke test: European Commission announced sanctions Tuesday.\", return_tensors=\"pt\").to(base.device)\n",
        "        _ = base.generate(**enc, num_beams=2, max_new_tokens=8)\n",
        "        print(\"• model load & generate() OK\")\n",
        "    except Exception as e:\n",
        "        print(\"• model load FAILED:\\n\", traceback.format_exc())\n",
        "        raise\n",
        "\n",
        "    # 6) Hash & sizes for critical files\n",
        "    report = []\n",
        "    for fn in [\"pytorch_model.bin\",\"model.safetensors\",\"config.json\",\"generation_config.json\",\"vocab.json\",\"merges.txt\",\"tokenizer.json\",\"pointer_head.pt\"]:\n",
        "        p = os.path.join(ckpt_dir, fn)\n",
        "        if os.path.exists(p):\n",
        "            report.append(f\"{fn:20s}  {os.path.getsize(p)/1e6:8.2f} MB  sha256:{_sha256(p)}\")\n",
        "    print(\"• file integrity:\")\n",
        "    for line in report: print(\"  -\", line)\n",
        "\n",
        "    print(f\"✅ {ckpt_dir} looks good.\")\n",
        "    return True\n",
        "\n",
        "# ---- run on your latest ckpt(s) ----\n",
        "CKPTS = [\n",
        "    \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000\",\n",
        "    # add more if you want:\n",
        "    # \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step5600\",\n",
        "    # \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step5000\",\n",
        "]\n",
        "for d in CKPTS:\n",
        "    try:\n",
        "        validate_and_repair_ckpt(d, try_fast=False, repair=True)\n",
        "    except Exception as e:\n",
        "        print(\"❌ Problem with\", d, \"→\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230,
          "referenced_widgets": [
            "4d3205e4f06746c3a340ede4e884d6fc"
          ]
        },
        "id": "LBa4a6-z5joT",
        "outputId": "5f5e88d8-c37a-4ab4-9358-53c44b93d4e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[device] cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
            "  self.gen = func(*args, **kwds)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[pointer_head] loaded=True\n",
            "[ptr] remaining: 100 / 100\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d3205e4f06746c3a340ede4e884d6fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "PTR-BEAM (fp16):   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1304452548.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16, enabled=USE_FP16_PTR):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[done] saved /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/ptr_decode_ckpt6000_beam5_len18_44.csv | elapsed ~9.4 min\n",
            "[probe] p_copy_mean=0.345 | p_gen_mean=0.655\n"
          ]
        }
      ],
      "source": [
        "# ===== PTR-BEAM (beam=5, copy-aware) — decode 100 with same lengths (ckpt_step6000) =====\n",
        "import os, gc, time, math, csv, pandas as pd, torch, torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ---------------- config ----------------\n",
        "CKPT_DIR = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000\"\n",
        "OUT_CSV  = os.path.join(CKPT_DIR, \"ptr_decode_ckpt6000_beam5_len18_44.csv\")\n",
        "VAL_CSV  = \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "SRC_COL, REF_COL = \"article\", \"highlights\"\n",
        "\n",
        "NUM_BEAMS      = 5\n",
        "MIN_NEW        = 18\n",
        "MAX_NEW        = 44\n",
        "NO_REPEAT      = 4\n",
        "LENGTH_PENALTY = 0.75\n",
        "MAX_SRC_LEN    = 400\n",
        "\n",
        "SAVE_EVERY = 50\n",
        "N_EXAMPLES = 100\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"[device]\", device)\n",
        "try:\n",
        "    torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True)\n",
        "except Exception:\n",
        "    pass\n",
        "if device == \"cuda\":\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "USE_FP16_PTR = (device == \"cuda\")\n",
        "\n",
        "# --- force slow tokenizer (robust on Drive) ---\n",
        "tok  = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=False, local_files_only=True)\n",
        "base = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR, attn_implementation=\"eager\").to(device).eval()\n",
        "\n",
        "# ---------------- pointer wrapper (copy-aware) ----------------\n",
        "class CopyAwareBart(nn.Module):\n",
        "    def __init__(self, base_model, tokenizer, lambda_cov=1.0, eps=1e-8, use_pointer=True, gate_bias=-0.3):\n",
        "        super().__init__()\n",
        "        self.base   = base_model\n",
        "        self.tok    = tokenizer\n",
        "        self.lambda_cov = float(lambda_cov)\n",
        "        self.eps    = float(eps)\n",
        "        self.use_ptr= bool(use_pointer)\n",
        "        self.gate_bias = float(gate_bias)\n",
        "        H = self.base.config.d_model\n",
        "        self.p_gen_linear = nn.Linear(3*H, 1)\n",
        "        self._last_p_copy_mean = self._last_p_gen_mean = None\n",
        "\n",
        "    def _mix_pointer(self, out, input_ids, attention_mask, decoder_input_ids):\n",
        "        logits = out.logits  # (B,T,V)\n",
        "        B,T,V  = logits.shape\n",
        "        last_ca = out.cross_attentions[-1].mean(1)  # (B,T,S)\n",
        "        if attention_mask is not None:\n",
        "            last_ca = last_ca.masked_fill(attention_mask[:, None, :] == 0, 0.0)\n",
        "        attn = last_ca / last_ca.sum(-1, keepdim=True).clamp_min(self.eps)\n",
        "\n",
        "        dec_hid = out.decoder_hidden_states[-1]                    # (B,T,H)\n",
        "        ctx     = torch.bmm(attn, out.encoder_last_hidden_state)   # (B,T,H)\n",
        "        dec_inp = self.base.get_input_embeddings()(decoder_input_ids)\n",
        "        gate_in = torch.cat([dec_hid, ctx, dec_inp], dim=-1)\n",
        "        p_gen   = torch.sigmoid(self.p_gen_linear(gate_in) + self.gate_bias).squeeze(-1)\n",
        "        p_copy  = 1.0 - p_gen\n",
        "\n",
        "        copy_probs = torch.zeros_like(logits)\n",
        "        idx = input_ids.unsqueeze(1).expand(B, T, input_ids.size(1))\n",
        "        copy_probs.scatter_add_(2, idx, attn)\n",
        "\n",
        "        vocab_logp = F.log_softmax(logits, dim=-1)\n",
        "        logp_final = torch.logaddexp(\n",
        "            vocab_logp + (p_gen + self.eps).log().unsqueeze(-1),\n",
        "            (copy_probs + self.eps).log() + (p_copy + self.eps).log().unsqueeze(-1)\n",
        "        )\n",
        "\n",
        "        self._last_p_copy_mean = float(p_copy.detach().mean().cpu())\n",
        "        self._last_p_gen_mean  = float(p_gen.detach().mean().cpu())\n",
        "        return logp_final\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, decoder_input_ids=None,\n",
        "                output_attentions=True, output_hidden_states=True, use_cache=False, **kwargs):\n",
        "        out = self.base(\n",
        "            input_ids=input_ids, attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            output_attentions=True, output_hidden_states=True,\n",
        "            use_cache=use_cache, return_dict=True,\n",
        "        )\n",
        "        if not self.use_ptr:\n",
        "            return out\n",
        "        out.logits = self._mix_pointer(out, input_ids, attention_mask, decoder_input_ids)\n",
        "        return out\n",
        "\n",
        "def copyaware_generate(self, *args, **kwargs):\n",
        "    return self.base.generate(*args, **kwargs)\n",
        "CopyAwareBart.generate = copyaware_generate\n",
        "\n",
        "model_ptr = CopyAwareBart(base, tok, lambda_cov=1.0, use_pointer=True, gate_bias=-0.3).to(device).eval()\n",
        "\n",
        "# try to load pointer head (trained p_gen_linear)\n",
        "ph_path = os.path.join(CKPT_DIR, \"pointer_head.pt\")\n",
        "if os.path.exists(ph_path):\n",
        "    sd = torch.load(ph_path, map_location=\"cpu\")\n",
        "    if isinstance(sd, dict) and \"p_gen_linear\" in sd:\n",
        "        model_ptr.p_gen_linear.load_state_dict(sd[\"p_gen_linear\"])\n",
        "    elif isinstance(sd, dict) and any(k.startswith(\"p_gen_linear.\") for k in sd.keys()):\n",
        "        model_ptr.load_state_dict(sd, strict=False)\n",
        "    elif isinstance(sd, dict) and set(sd.keys()) == {\"weight\",\"bias\"}:\n",
        "        model_ptr.p_gen_linear.load_state_dict(sd)\n",
        "print(\"[pointer_head] loaded=True\")\n",
        "\n",
        "# ---------------- helpers ----------------\n",
        "def _ban_repeating_ngrams(logits: torch.Tensor, seq: torch.Tensor, ngram: int):\n",
        "    if ngram <= 0: return\n",
        "    toks = seq[0].tolist() if seq.dim()==2 else seq.tolist()\n",
        "    L = len(toks)\n",
        "    if ngram == 1:\n",
        "        for t in set(toks): logits[t] = -1e9\n",
        "        return\n",
        "    if L < ngram-1: return\n",
        "    prefix = tuple(toks[-(ngram - 1):])\n",
        "    banned = set()\n",
        "    for i in range(L - ngram + 1):\n",
        "        if tuple(toks[i:i + ngram - 1]) == prefix:\n",
        "            banned.add(toks[i + ngram - 1])\n",
        "    for t in banned: logits[t] = -1e9\n",
        "\n",
        "def generate_pointer_beam_one(text: str) -> str:\n",
        "    enc = tok([text], return_tensors=\"pt\", truncation=True, max_length=MAX_SRC_LEN, padding=True).to(device)\n",
        "    start_id = int(getattr(base.config, \"decoder_start_token_id\", tok.eos_token_id or 2))\n",
        "    beams = [(torch.tensor([[start_id]], device=device, dtype=torch.long), 0.0, False)]\n",
        "    for step in range(MAX_NEW):\n",
        "        new_beams = []\n",
        "        for seq, score, done in beams:\n",
        "            if done:\n",
        "                new_beams.append((seq, score, True)); continue\n",
        "            with torch.cuda.amp.autocast(dtype=torch.float16, enabled=USE_FP16_PTR):\n",
        "                out = model_ptr(\n",
        "                    input_ids=enc.input_ids,\n",
        "                    attention_mask=enc.attention_mask,\n",
        "                    decoder_input_ids=seq,\n",
        "                    use_cache=False\n",
        "                )\n",
        "            step_logits = out.logits[:, -1, :].squeeze(0).float().clone()  # (V,)\n",
        "            if step + 1 < MIN_NEW and tok.eos_token_id is not None:\n",
        "                step_logits[tok.eos_token_id] = -1e9\n",
        "            _ban_repeating_ngrams(step_logits, seq, NO_REPEAT)\n",
        "            topk_logp, topk_ids = torch.topk(step_logits, k=NUM_BEAMS)\n",
        "            for k in range(NUM_BEAMS):\n",
        "                tid  = topk_ids[k].view(1,1)\n",
        "                nseq = torch.cat([seq, tid], dim=1)\n",
        "                nfin = (tok.eos_token_id is not None and tid.item() == tok.eos_token_id) or (nseq.size(1) >= MAX_NEW)\n",
        "                raw  = score + float(topk_logp[k].item())\n",
        "                lp   = ((5.0 + nseq.size(1))**LENGTH_PENALTY) / ((5.0 + 1.0)**LENGTH_PENALTY)\n",
        "                nsc  = raw / lp\n",
        "                new_beams.append((nseq, nsc, nfin))\n",
        "        new_beams.sort(key=lambda x: x[1], reverse=True)\n",
        "        beams = new_beams[:NUM_BEAMS]\n",
        "        if all(b[2] for b in beams): break\n",
        "    best_seq = max(beams, key=lambda x: x[1])[0]\n",
        "    return tok.batch_decode(best_seq, skip_special_tokens=True)[0]\n",
        "\n",
        "# ---------------- load data & prep output ----------------\n",
        "def robust_read_csv(path: str) -> pd.DataFrame:\n",
        "    required = (\"id\",\"article\",\"highlights\")\n",
        "    for enc in (\"utf-8\",\"utf-8-sig\",\"cp1252\"):\n",
        "        try:\n",
        "            df = pd.read_csv(path, engine=\"python\", quoting=csv.QUOTE_ALL, escapechar=\"\\\\\", encoding=enc)\n",
        "            cols = {c.lower(): c for c in df.columns}\n",
        "            assert all(k in cols for k in required)\n",
        "            out = df[[cols[\"id\"], cols[\"article\"], cols[\"highlights\"]]].copy()\n",
        "            out.columns = [\"id\",\"article\",\"highlights\"]; out[\"id\"] = out[\"id\"].astype(str)\n",
        "            return out\n",
        "        except Exception:\n",
        "            pass\n",
        "    rows = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "        reader = csv.reader(f, delimiter=\",\", quotechar='\"', escapechar=\"\\\\\")\n",
        "        header = [h.strip().lower() for h in next(reader)]\n",
        "        i_id, i_art, i_sum = header.index(\"id\"), header.index(\"article\"), header.index(\"highlights\")\n",
        "        for row in reader: rows.append([row[i_id], row[i_art], row[i_sum]])\n",
        "    out = pd.DataFrame(rows, columns=[\"id\",\"article\",\"highlights\"]); out[\"id\"]=out[\"id\"].astype(str)\n",
        "    return out\n",
        "\n",
        "if os.path.exists(OUT_CSV):\n",
        "    df = pd.read_csv(OUT_CSV)\n",
        "    if \"ptr_pred\" not in df.columns:\n",
        "        df[\"ptr_pred\"] = \"\"\n",
        "    if \"len_ptr\" not in df.columns:\n",
        "        df[\"len_ptr\"] = 0\n",
        "else:\n",
        "    base_df = robust_read_csv(VAL_CSV).iloc[:N_EXAMPLES].copy()\n",
        "    df = base_df[[SRC_COL, REF_COL]].copy()\n",
        "    df[\"ptr_pred\"] = \"\"\n",
        "    df[\"len_ptr\"]  = 0\n",
        "\n",
        "# ---------------- run pointer decoding ----------------\n",
        "todo_idx = df.index[(df[\"ptr_pred\"].isna()) | (df[\"ptr_pred\"].astype(str) == \"\")].tolist()\n",
        "print(f\"[ptr] remaining: {len(todo_idx)} / {len(df)}\")\n",
        "\n",
        "t0 = time.time(); pending = []\n",
        "for j, idx in enumerate(tqdm(todo_idx, desc=\"PTR-BEAM (fp16)\")):\n",
        "    text = str(df.at[idx, SRC_COL])\n",
        "    pred = generate_pointer_beam_one(text)\n",
        "    df.at[idx, \"ptr_pred\"] = pred\n",
        "    df.at[idx, \"len_ptr\"]  = len(pred.split())\n",
        "    pending.append(idx)\n",
        "    if len(pending) >= SAVE_EVERY or j == len(todo_idx) - 1:\n",
        "        df.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
        "        pending.clear()\n",
        "        torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "mins = (time.time() - t0) / 60.0\n",
        "print(f\"[done] saved {OUT_CSV} | elapsed ~{mins:.1f} min\")\n",
        "print(f\"[probe] p_copy_mean={getattr(model_ptr,'_last_p_copy_mean',float('nan')):.3f} | p_gen_mean={getattr(model_ptr,'_last_p_gen_mean',float('nan')):.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLbUN1OBV6mc",
        "outputId": "db7e068f-6610-4db9-c13c-099f4f3dc9f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "[FILE] /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/ptr_decode_ckpt6000_beam5_len18_44.csv\n",
            "[COLUMNS] reference='highlights'  generated='ptr_pred'  (source='article')\n",
            "\n",
            "=== ROUGE (mean over rows) ===\n",
            "{'rouge1': {'p': 0.3049, 'r': 0.3392, 'f': 0.3129}, 'rouge2': {'p': 0.1093, 'r': 0.1235, 'f': 0.1126}, 'rougeLsum': {'p': 0.2242, 'r': 0.2508, 'f': 0.2305}}\n"
          ]
        }
      ],
      "source": [
        "# --- ROUGE for ptr_decode_ckpt6000_beam5_len18_44.csv ---\n",
        "\n",
        "!pip -q install rouge-score pandas\n",
        "\n",
        "import pandas as pd\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "PATH = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/ptr_decode_ckpt6000_beam5_len18_44.csv\"\n",
        "\n",
        "def _read_csv(path: str) -> pd.DataFrame:\n",
        "    try:\n",
        "        return pd.read_csv(path)\n",
        "    except Exception:\n",
        "        return pd.read_csv(path, engine=\"python\")\n",
        "\n",
        "def _find_col(cols, cands):\n",
        "    cl = {c.lower(): c for c in cols}\n",
        "    for cand in cands:\n",
        "        if cand in cl:\n",
        "            return cl[cand]\n",
        "    return None\n",
        "\n",
        "df = _read_csv(PATH)\n",
        "cols_lower = [c.lower() for c in df.columns]\n",
        "\n",
        "src_col = _find_col(df.columns, [\"article\",\"source\",\"src\",\"document\",\"input\"])\n",
        "ref_col = _find_col(df.columns, [\"reference\",\"highlights\",\"target\",\"summary\",\"tgt\",\"ref\"])\n",
        "pred_col = _find_col(df.columns, [\"gen\",\"prediction\",\"output\",\"decoded\",\"summary_pred\",\"system\",\"generated\",\"hypothesis\",\"ptr\",\"summary\"])\n",
        "\n",
        "if pred_col is None:\n",
        "    # fallback: use last non-source/non-ref text-like column\n",
        "    text_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
        "    candidates = [c for c in text_cols if c not in {src_col, ref_col}]\n",
        "    pred_col = candidates[-1] if candidates else text_cols[-1]\n",
        "\n",
        "assert ref_col is not None and pred_col is not None, f\"Could not detect columns. Found -> ref:{ref_col} pred:{pred_col}\"\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\"], use_stemmer=True)\n",
        "\n",
        "tot = {\"r1_p\":0,\"r1_r\":0,\"r1_f\":0,\"r2_p\":0,\"r2_r\":0,\"r2_f\":0,\"rl_p\":0,\"rl_r\":0,\"rl_f\":0}\n",
        "n = 0\n",
        "\n",
        "for hyp, ref in zip(df[pred_col].astype(str), df[ref_col].astype(str)):\n",
        "    scores = scorer.score(ref, hyp)\n",
        "    tot[\"r1_p\"] += scores[\"rouge1\"].precision\n",
        "    tot[\"r1_r\"] += scores[\"rouge1\"].recall\n",
        "    tot[\"r1_f\"] += scores[\"rouge1\"].fmeasure\n",
        "    tot[\"r2_p\"] += scores[\"rouge2\"].precision\n",
        "    tot[\"r2_r\"] += scores[\"rouge2\"].recall\n",
        "    tot[\"r2_f\"] += scores[\"rouge2\"].fmeasure\n",
        "    tot[\"rl_p\"] += scores[\"rougeLsum\"].precision\n",
        "    tot[\"rl_r\"] += scores[\"rougeLsum\"].recall\n",
        "    tot[\"rl_f\"] += scores[\"rougeLsum\"].fmeasure\n",
        "    n += 1\n",
        "\n",
        "for k in tot: tot[k] = tot[k] / max(n,1)\n",
        "\n",
        "print(f\"[FILE] {PATH}\")\n",
        "print(f\"[COLUMNS] reference='{ref_col}'  generated='{pred_col}'  (source='{src_col}')\")\n",
        "print(\"\\n=== ROUGE (mean over rows) ===\")\n",
        "print({\n",
        "    \"rouge1\": {\"p\": round(tot[\"r1_p\"],4), \"r\": round(tot[\"r1_r\"],4), \"f\": round(tot[\"r1_f\"],4)},\n",
        "    \"rouge2\": {\"p\": round(tot[\"r2_p\"],4), \"r\": round(tot[\"r2_r\"],4), \"f\": round(tot[\"r2_f\"],4)},\n",
        "    \"rougeLsum\": {\"p\": round(tot[\"rl_p\"],4), \"r\": round(tot[\"rl_r\"],4), \"f\": round(tot[\"rl_f\"],4)},\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48lzBci5WZON",
        "outputId": "908c53fd-bbe8-4705-fb32-8ce02adb5e2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[FILE] /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/ptr_decode_ckpt6000_beam5_len18_44.csv\n",
            "[COLUMNS] reference='highlights'  generated='ptr_pred'  (source='article')\n",
            "\n",
            "=== ENTITY (overall) ===\n",
            "Precision: 0.2867\n",
            "Recall:    0.2907\n",
            "F1:        0.2887\n",
            "\n",
            "=== ENTITY (by type) ===\n",
            "TYPE             P      R     F1   TP   FP   FN\n",
            "CARDINAL     0.309  0.347  0.327    17   38   32\n",
            "DATE         0.141  0.152  0.146    10   61   56\n",
            "EVENT        0.500  0.333  0.400     1    1    2\n",
            "FAC          0.400  0.400  0.400     2    3    3\n",
            "GPE          0.359  0.348  0.354    23   41   43\n",
            "LOC          0.111  1.000  0.200     1    8    0\n",
            "MONEY        0.000  0.000  0.000     0    3    8\n",
            "NORP         0.212  0.233  0.222     7   26   23\n",
            "ORDINAL      0.400  0.308  0.348     4    6    9\n",
            "ORG          0.260  0.292  0.275    19   54   46\n",
            "PERCENT      0.500  0.250  0.333     1    1    3\n",
            "PERSON       0.411  0.359  0.383    37   53   66\n",
            "PRODUCT      0.250  1.000  0.400     1    3    0\n",
            "QUANTITY     0.250  0.333  0.286     1    3    2\n",
            "TIME         0.143  0.125  0.133     1    6    7\n",
            "WORK_OF_ART   0.000  0.000  0.000     0    4    5\n"
          ]
        }
      ],
      "source": [
        "# --- ENTITY SCORES for ptr_decode_ckpt6000_beam5_len18_44.csv ---\n",
        "\n",
        "# If needed:\n",
        "!pip -q install spacy pandas\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import pandas as pd, re\n",
        "import spacy\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "PATH = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/ptr_decode_ckpt6000_beam5_len18_44.csv\"\n",
        "\n",
        "def _read_csv(path: str) -> pd.DataFrame:\n",
        "    try:\n",
        "        return pd.read_csv(path)\n",
        "    except Exception:\n",
        "        return pd.read_csv(path, engine=\"python\")\n",
        "\n",
        "def _find_col(cols, cands):\n",
        "    cl = {c.lower(): c for c in cols}\n",
        "    for cand in cands:\n",
        "        if cand in cl:\n",
        "            return cl[cand]\n",
        "    return None\n",
        "\n",
        "def _norm(s: str) -> str:\n",
        "    s = s.strip().lower()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s\n",
        "\n",
        "df = _read_csv(PATH)\n",
        "src_col = _find_col(df.columns, [\"article\",\"source\",\"src\",\"document\",\"input\"])\n",
        "ref_col = _find_col(df.columns, [\"reference\",\"highlights\",\"target\",\"summary\",\"tgt\",\"ref\"])\n",
        "pred_col = _find_col(df.columns, [\"gen\",\"prediction\",\"output\",\"decoded\",\"summary_pred\",\"system\",\"generated\",\"hypothesis\",\"ptr\",\"summary\"])\n",
        "if pred_col is None:\n",
        "    text_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
        "    candidates = [c for c in text_cols if c not in {src_col, ref_col}]\n",
        "    pred_col = candidates[-1] if candidates else text_cols[-1]\n",
        "assert ref_col is not None and pred_col is not None\n",
        "\n",
        "# Load spaCy model, with graceful download if needed\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"], check=True)\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Extract entity sets per row: set of (label, text_norm)\n",
        "def ents(text: str):\n",
        "    doc = nlp(text if isinstance(text, str) else str(text))\n",
        "    return {(ent.label_, _norm(ent.text)) for ent in doc.ents if ent.text.strip()}\n",
        "\n",
        "TP=0; FP=0; FN=0\n",
        "by_type = defaultdict(lambda: {\"TP\":0,\"FP\":0,\"FN\":0})\n",
        "\n",
        "for hyp, ref in zip(df[pred_col].astype(str), df[ref_col].astype(str)):\n",
        "    H = ents(hyp)\n",
        "    R = ents(ref)\n",
        "    # micro on sets (avoid double-counting repeats within a single doc)\n",
        "    tp = len(H & R)\n",
        "    fp = len(H - R)\n",
        "    fn = len(R - H)\n",
        "    TP += tp; FP += fp; FN += fn\n",
        "    # by type\n",
        "    Ht = defaultdict(set); Rt = defaultdict(set)\n",
        "    for lab, txt in H: Ht[lab].add(txt)\n",
        "    for lab, txt in R: Rt[lab].add(txt)\n",
        "    all_labs = set(Ht.keys()) | set(Rt.keys())\n",
        "    for lab in all_labs:\n",
        "        tp_t = len(Ht[lab] & Rt[lab])\n",
        "        fp_t = len(Ht[lab] - Rt[lab])\n",
        "        fn_t = len(Rt[lab] - Ht[lab])\n",
        "        by_type[lab][\"TP\"] += tp_t\n",
        "        by_type[lab][\"FP\"] += fp_t\n",
        "        by_type[lab][\"FN\"] += fn_t\n",
        "\n",
        "def _prf(tp, fp, fn):\n",
        "    p = tp / (tp+fp) if (tp+fp)>0 else 0.0\n",
        "    r = tp / (tp+fn) if (tp+fn)>0 else 0.0\n",
        "    f = 2*p*r/(p+r) if (p+r)>0 else 0.0\n",
        "    return p, r, f\n",
        "\n",
        "P,R,F = _prf(TP,FP,FN)\n",
        "print(f\"[FILE] {PATH}\")\n",
        "print(f\"[COLUMNS] reference='{ref_col}'  generated='{pred_col}'  (source='{src_col}')\")\n",
        "print(\"\\n=== ENTITY (overall) ===\")\n",
        "print(f\"Precision: {P:.4f}\")\n",
        "print(f\"Recall:    {R:.4f}\")\n",
        "print(f\"F1:        {F:.4f}\")\n",
        "\n",
        "print(\"\\n=== ENTITY (by type) ===\")\n",
        "rows = []\n",
        "for lab in sorted(by_type.keys()):\n",
        "    p,r,f = _prf(by_type[lab][\"TP\"], by_type[lab][\"FP\"], by_type[lab][\"FN\"])\n",
        "    rows.append((lab, p, r, f, by_type[lab][\"TP\"], by_type[lab][\"FP\"], by_type[lab][\"FN\"]))\n",
        "# Pretty print\n",
        "header = f\"{'TYPE':10s}  {'P':>6s} {'R':>6s} {'F1':>6s}   TP   FP   FN\"\n",
        "print(header)\n",
        "for lab,p,r,f,tp,fp,fn in rows:\n",
        "    print(f\"{lab:10s}  {p:6.3f} {r:6.3f} {f:6.3f}   {tp:3d}  {fp:3d}  {fn:3d}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP3yAGvClZfX"
      },
      "source": [
        "# PTG + CopyNext Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrJBRD8XoOo5",
        "outputId": "fbd0bd85-9dcd-4ab5-b67b-f8a980035c39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Usage:   \n",
            "  pip3 <command> [options]\n",
            "\n",
            "no such option: -U\n",
            "Device: cuda\n",
            "Checkpoint: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000\n",
            "Outputs -> /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/decodes_spanbias\n"
          ]
        }
      ],
      "source": [
        "!pip -qU install transformers accelerate sentencepiece pandas tqdm\n",
        "\n",
        "import os, gc, math, random, json, time, csv, pandas as pd, torch\n",
        "import torch.nn.functional as F\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "\n",
        "SEED = 0\n",
        "random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "CKPT_DIR = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000\"\n",
        "VAL_CSV  = \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "OUT_DIR  = os.path.join(CKPT_DIR, \"decodes_spanbias\")\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- Decode args (keep consistent with your project) ---\n",
        "DECODE_ARGS = dict(\n",
        "    num_beams=5,\n",
        "    num_return_sequences=5,\n",
        "    max_new_tokens=44,\n",
        "    min_new_tokens=18,\n",
        "    no_repeat_ngram_size=4,\n",
        "    length_penalty=0.75,\n",
        "    early_stopping=True,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True\n",
        ")\n",
        "\n",
        "\n",
        "MAX_SRC_LEN = 512\n",
        "\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"Checkpoint: {CKPT_DIR}\")\n",
        "print(f\"Outputs -> {OUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "wFZuZHrqpbR4",
        "outputId": "3113320f-5330-45ce-80a0-62788209906b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'total': 13368, 'using': 1000}\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"work_df\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"article\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"He is the protege of Total Football mastermind Rinus Michels, but 'Little General' Dick Advocaat has been pitched straight into battle at Sunderland. In nine games there will be little time to roll out the revered brand of football which won fame and acclaim with the Ajax and Holland teams of the 1970s. Instead, the 67-year-old is tasked with saving a failing side from relegation. Then, successful or otherwise, he will be gone. But who is the man who, at 67, is making his Premier League debut as Sunderland's oldest-ever manager? Dick Advocaat (left) in action during his playing days for Roda against PSV Eindhoven . Advocaat on a cycle ride when assistant coach with Holland in 1987 - seen here with Adri van Tiggelen, Ronald Koeman, Rene van der Gijp and Marco van Basten . Advocaat has held many managerial positions both in his native Holland and abroad - see here at PSV Eindhoven in 1996 . Advocaat is best remembered in Great Britain for his two league titles with Rangers in the late 1990s . Advocaat certainly doesn't want for experience. This is the 18th job of a 28-year managerial career which has taken in nearly 900 matches and yielded 10 major trophies. He is best known on these shores for three-and-a-half years at Rangers, a stay which produced two SPL titles - including a record 21-point winning margin - two Scottish Cups and a League Cup. Advocaat, though, has since been accused of reckless spending - a claim he defends - and it was at Ibrox that he splashed out a club record \\u00a312million for Chelsea striker Tore Andre Flo. That name alone is enough to cause alarm among followers of Sunderland. For they paid Rangers \\u00a36.75m for the Norwegian after a relatively successful two years north of the border. VIDEO Sunderland appoint Advocaat as manager . There will be a few familiure faces in the Premier League for Advocaat. He has worked with and against Ronald Koeman in Holland . Manchester United manager Louis van Gaal is another manager that Advocaat knows well . Former England, Manchester City and Leicester boss Sven Goran Eriksson is someone else that Advovaat knows from his time in the game . Successful, however, is not a word used to reflect on his time at the Stadium of Light and, after four league goals in 29 appearances, he left on a free transfer. That, of course, is not Advocaat's fault and there will be no scope for new additions during his time on Wearside, should the Flo connection cast doubt over his judgment. Rather, it will be his ability to make a snap judgement on those already in residence which will determine his legacy with the Black Cats. He is certainly well remembered at the likes of PSV and Zenit St Petersburg, where in 2008 his domestic champions beat former club Rangers in the final of the UEFA Cup, a triumph which won him honorary citizenship of the Russian city. Working with Premier League players will not be new to Advocaat as he has come across many in his time in international football - including Manchester United's Marouane Fellaini while he was coach of Belgium between 2009-10 . Former Arsenal star Andrey Arshavin is another player Advocaat has worked with during his time as Russia manager between 2010-12 . Advocaat signed Brazil international Giovane Elber for German side Borussia Monchengladbach in 2005 . During his first spell in Eindhoven in the mid-Nineties he lifted the Dutch Cup and Eredivisie title, working with the likes of Brazilian striker Ronaldo. His second stint in 2012 saw the club finish second in the league and lose in the cup final, but a win ratio of 65 per cent equalled that of his initial tenure and still stands as the best of his career. Were he to win six of nine matches with Sunderland then it would perhaps eclipse everything he has achieved. But there have been rocky times, too. Advocaat has nine games left in the Premier League season and his aim is to ensure Sunderland don't go down . Despite his years of managerial experience, Advocaat has never been in a relegation battle before . He was hounded from his second spell as Holland national-team boss after both the media and supporters were critical of his handling of the team, despite them reaching the semi-finals of Euro 2004. Ten years earlier he took Holland to the last eight of the World Cup but was on the brink of losing his job before the finals in the USA after a fallout with star player Ruud Gullit, who retired from international football in protest. Subsequent posts at South Korea and Russia saw his teams fail to emerge from the group stages of World Cup 2006 and Euro 2012 respectively, while his latest job was an unhappy five-month period in charge of Serbia, where he was sacked in November. Despite his origin as a pupil of Michels, Advocaat has been criticised for playing defensive football with too much emphasis on structure and discipline. However, on the evidence of Sunderland's shambolic 4-0 defeat to Aston Villa on Saturday, structure and discipline is probably the best place to start.\",\n          \"Stoke City manager Mark Hughes accused Joel Ward of making 'the best save of the match' after being denied a clear penalty during in Saturday's 2-1 home defeat against Crystal Palace. Hughes was left feeling aggrieved after two big decisions failed to go his side's way having taken a 14th minute lead through Mame Biram Diouf. Asmir Begovic was controversially penalised for a challenge on Yanick Bolasie to allow Palace to level from the spot after 41 minutes. And then after Wilfried Zaha had put the visitors ahead, Diouf's goalbound shot in the second half was blocked two-handed by Ward with the Palace getting way with it. Stoke City boss Mark Hughes has accused Joel Ward (right) of making 'the best save of the match' Marko Arnautovic shares words with Crystal Palace's Ward during the Barclays Premier League clash . 'That decision left us scratching our heads,' said Hughes. 'Referee Andre Marriner missed a lot of things. I know Julian Speroni did well in the Palace goal but arguably that was the best save of the match. He has got two hands on it for heaven's sake. How he doesn't give it, I've no idea.' It rubbed salt into the wound for Hughes who had earlier seen Palace awarded their penalty. 'When you see the replay, the lad Bolasie has got his foot up high above Amsir. That has why Asmir has missed the ball and the referee has deemed it a penalty. It was a key moment, that is one you want them (the officials) to get.' Even so, Hughes didn't have any excuses for Palace's winning goal from Wilfried Zaha on the stroke of half-time from Glenn Murray's flick-on. Asmir Begovic was adjudged to have fouled Yannick Bolasie during a colision inside the penalty area . Begovic was shown a yellow card by Andre Marriner after he was adjudged to have brought down Bolasie . 'It was a mistake from us. It was just a long ball, if you don't clear the first ball, you have to deal with the second ball. We didn't and that is what cost us. We are better than that.' Stoke slipped to ninth while Palace are now 11 points above the bottom three and Alan Pardew is above his former club Newcastle United in the table. 'It was a cracking game particularly if you were a neutral or a Palace fan,' said Pardew. 'I think Stoke were unhappy about our penalty. I haven't seen it back but it was a turning point.' Zaha was all smiles after his winner after being told to stop his 'little sulks' by Pardew earlier last week. 'He had a tough game, Erik Pieters was a player we tried to sign, he is an aggressive defender. Wilf struggled but he has his moments and we showed a lot of character to have a go. 'It was a tough day for the officials. All of it was fair, but it was a tough physical encounter. It was almost like a Six Nations game at times, in a tough way.' Crystal Palace winger Wilfried Zaha (far right) celebrates with Glenn Murray after scoring against Stoke City .\",\n          \"An ancient tomb belonging to Amenhotep, guard of the temple of Egyptian deity Amun, has been discovered in the southern city of Luxor by an American research team, the Egypt's antiquities ministry said on Tuesday. The ministry says the tomb, found near the southern city of Luxor, dates back to the New Kingdom of the 18th Dynasty (1543-1292 BC) - the most famous of ancient Egypt dynasties. The tombs were found earlier this month near the Sheikh Abd el-Qurna dig site, situated at the feet of the Theban mountains, between the famed valleys of the Kings and Queens over the town of al-Qurna. A team working near the historic city of Luxor has found not one, but two ancient tombs . Both tombs feature astonishing murals which are believed to date back to the 18th Dynasty of the Egyptian New Kingdom . This means the tombs were likely created sometime between 1543-1292BC . The tombs were found earlier this month near the Sheikh Abd el-Qurna dig site, situated at the feet of the Theban mountains . Incredible photographs distributed by the ministry show a tomb with bright green and brown paintings with hieroglyphics with murals that depict both celebrations and everyday activity, and despite their age are still remarkably vibrant and colorful. Antiquities Minister Mamdouh al-Damaty said in a statement that the tombs do sadly appear to have been looted at some point and the sarcophogi containing the bejeweled mummies were missing. 'The tomb contains many stunning scenes with bright colours painted on plaster,' Antiquities Minister Mamdouh Eldamaty said in a statement. 'Many of scenes represent the tomb owner and his wife in front of an offering table and a view of a goddess nursing a royal child as well as scenes of the daily life,' he added. The tomb was discovered by a team of American archaeologists alongside an Egyptian inspectors' team in the city of Luxor, 700 kilometres (435 miles) south of Cairo. However, the temple guard's final resting place had been vandalised for unknown reasons. The T-shaped tomb 'was deliberately damaged in ancient times,' said Sultan Eid, the ministry's general director for the Upper Egypt region. 'The name and titles of the tomb owner, some hieroglyphic texts and scenes in addition to the names of the god Amun were deliberately erased,' Eid added. The first tomb was discovered on March 2, and the second was discovered on March 10. The second tomb is believed to be that of Sa-mut and his wife, Ta-Khaeet. The first tomb meanwhile is believed to be that of Sa-Mut's father, Amenhotep. Luxor, a city of some 500,000 people on the banks of the Nile in southern Egypt, is an open-air museum of intricate temples and pharaonic tombs. Eldamaty said in a separate statement that a royal rest house belonging to King Thutmosis II, also from the 18th Dynasty of the New Kingdom period, had been discovered in the Suez Canal province of Ismailia. The murals depict both celebrations and everyday activity, and despite their age are still remarkably vibrant and colorful . Antiquities Minister Mamdouh al-Damaty said in a statement that the tombs do sadly appear to have been looted at some point . Luxor, a city of some 500,000 people on the banks of the Nile in southern Egypt, is an open-air museum of intricate temples and pharaonic tombs .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"highlights\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"Dick Advocaat has had 18 managerial jobs over a 28 year career . Advocaat is best known on these shores for his time at Rangers where he won two league titles . Advocaat has a lot of experience at international level having managed Holland (twice), Russia, Belgium, UAE, South Korea, and Serbia . At 67-years-old Advocaat is Sunderland's oldest manager - so can he be successful at the Stadium of Light . CLICK HERE for all the latest Sunderland news .\",\n          \"Stoke boss Mark Hughes felt his side were denied a clear penalty . Hughes has accused Joel Ward of blocking shot with his hand . Yannick Bolasie won penalty when he was adjudged to have been fouled . The Eagles claimed all three points thanks to a Wilfried Zaha winner .\",\n          \"A team working near the historic city of Luxor has found not one, but two ancient tombs . Both tombs feature astonishing murals which are believed to date back to the 18th Dynasty of the Egyptian New Kingdom . This means the tombs were likely created sometime between 1543-1292BC .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"acfaf04a58b2318e16c79fb8f28778b40f54b27b\",\n          \"3a60fd11705ae43bfeb09415f5057c0998b6698b\",\n          \"9dff9b3a31b7a8628b5b8d02b280770599908900\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "work_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-c74b63ab-bf52-4990-abc4-2ffcc6724137\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article</th>\n",
              "      <th>highlights</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>These days the Cheltenham Festival is a marath...</td>\n",
              "      <td>Willie Mullins looks to have chance of establi...</td>\n",
              "      <td>126be4c62168774d45fcf210b93500b57fdf4a42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A baby girl was found alive after being strapp...</td>\n",
              "      <td>Lynn Jennifer Groesbeck, 25, was driving home ...</td>\n",
              "      <td>d26bc2a98769fcb6d4fed3d0034d4cd8f840ef98</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c74b63ab-bf52-4990-abc4-2ffcc6724137')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c74b63ab-bf52-4990-abc4-2ffcc6724137 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c74b63ab-bf52-4990-abc4-2ffcc6724137');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8ef39fbd-b364-4de5-ac2c-958c6ce4d64d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8ef39fbd-b364-4de5-ac2c-958c6ce4d64d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8ef39fbd-b364-4de5-ac2c-958c6ce4d64d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                             article  \\\n",
              "0  These days the Cheltenham Festival is a marath...   \n",
              "1  A baby girl was found alive after being strapp...   \n",
              "\n",
              "                                          highlights  \\\n",
              "0  Willie Mullins looks to have chance of establi...   \n",
              "1  Lynn Jennifer Groesbeck, 25, was driving home ...   \n",
              "\n",
              "                                         id  \n",
              "0  126be4c62168774d45fcf210b93500b57fdf4a42  \n",
              "1  d26bc2a98769fcb6d4fed3d0034d4cd8f840ef98  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ==== Safe CSV read with mild fallbacks ====\n",
        "def _strict_read(path, required=(\"article\",\"highlights\"), nrows=None):\n",
        "    tried = []\n",
        "    for engine in (\"c\",\"python\"):\n",
        "        for enc in (\"utf-8\",\"utf-8-sig\"):\n",
        "            try:\n",
        "                df = pd.read_csv(path, engine=engine, encoding=enc)\n",
        "                tried.append((engine,enc,\"OK\",len(df)))\n",
        "                break\n",
        "            except Exception as e:\n",
        "                tried.append((engine,enc,str(e),None))\n",
        "                continue\n",
        "        else:\n",
        "            continue\n",
        "        break\n",
        "    missing = [c for c in required if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing required columns {missing}. Tried={tried}\")\n",
        "    if nrows is not None:\n",
        "        # Deterministic sample\n",
        "        df = df.sample(n=nrows, random_state=SEED).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "N_EXAMPLES = 1000\n",
        "val_df = _strict_read(VAL_CSV, nrows=None)\n",
        "work_df = val_df.sample(n=N_EXAMPLES, random_state=SEED).reset_index(drop=True)\n",
        "print({\"total\": len(val_df), \"using\": len(work_df)})\n",
        "work_df.head(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QITQRkFWphD7",
        "outputId": "f46adac9-3665-48ac-a000-840fff68e9b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded. top attn_impl: None | inner: None\n",
            "Loaded. attn_implementation: eager\n"
          ]
        }
      ],
      "source": [
        "# ==== Load tokenizer & model (PGC checkpoint) ====\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR).to(DEVICE).eval()\n",
        "print(\"Loaded. top attn_impl:\", getattr(model.config, \"attn_implementation\", None),\n",
        "      \"| inner:\", getattr(model.model.config, \"attn_implementation\", None))\n",
        "\n",
        "\n",
        "if getattr(model.config, \"attn_implementation\", None) != \"eager\":\n",
        "    model.config.attn_implementation = \"eager\"\n",
        "\n",
        "_ = model.to(DEVICE).eval()\n",
        "print(\"Loaded. attn_implementation:\", getattr(model.config, \"attn_implementation\", None))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsE__IqrppFV"
      },
      "outputs": [],
      "source": [
        "from contextlib import contextmanager\n",
        "import torch\n",
        "\n",
        "@contextmanager\n",
        "def _force_eager_attn(m):\n",
        "    top_prev   = getattr(m.config, \"attn_implementation\", None)\n",
        "    inner_prev = getattr(getattr(m, \"model\", m).config, \"attn_implementation\", None)\n",
        "    if top_prev   != \"eager\": m.config.attn_implementation = \"eager\"\n",
        "    if inner_prev != \"eager\": m.model.config.attn_implementation = \"eager\"\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        if top_prev   != \"eager\": m.config.attn_implementation = top_prev\n",
        "        if inner_prev != \"eager\": m.model.config.attn_implementation = inner_prev\n",
        "\n",
        "import math\n",
        "import torch\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def cross_attn_align(enc, cand_ids):\n",
        "    \"\"\"\n",
        "    Alignment proxy via hidden states (works with SDPA/eager).\n",
        "    1) Teacher-forced decoder to get decoder hidden states.\n",
        "    2) Dot-product with encoder hidden states to approximate cross-attn.\n",
        "    Returns: (align_idx[T_tgt], align_peak[T_tgt]) with peaks in [0,1].\n",
        "    \"\"\"\n",
        "    # Teacher forcing: predict token t from tokens up to t-1\n",
        "    dec_inp = cand_ids[:, :-1].contiguous()\n",
        "\n",
        "    with autocast_if_cuda():\n",
        "        # Encoder forward (no attentions needed)\n",
        "        enc_out = model.model.encoder(\n",
        "            input_ids=enc[\"input_ids\"],\n",
        "            attention_mask=enc.get(\"attention_mask\", None),\n",
        "            output_hidden_states=False,\n",
        "            return_dict=True\n",
        "        )\n",
        "        # Decoder forward to get hidden states\n",
        "        dec_out = model.model.decoder(\n",
        "            input_ids=dec_inp,\n",
        "            encoder_hidden_states=enc_out.last_hidden_state,\n",
        "            encoder_attention_mask=enc.get(\"attention_mask\", None),\n",
        "            output_hidden_states=True,\n",
        "            use_cache=False,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "    # Hidden states\n",
        "    enc_h = enc_out.last_hidden_state           # (B, S, H)\n",
        "    dec_h = dec_out.last_hidden_state           # (B, T, H)\n",
        "\n",
        "    # Dot-product attention proxy: (B, T, S)\n",
        "    attn_logits = torch.bmm(dec_h, enc_h.transpose(1, 2)) / math.sqrt(dec_h.size(-1))\n",
        "\n",
        "    # Mask encoder pads\n",
        "    if enc.get(\"attention_mask\", None) is not None:\n",
        "        enc_mask = (enc[\"attention_mask\"] == 0).unsqueeze(1)  # (B, 1, S)\n",
        "        attn_logits = attn_logits.masked_fill(enc_mask, -1e9)\n",
        "\n",
        "    attn = attn_logits.softmax(dim=-1)         # (B, T, S)\n",
        "    align_peak, align_idx = attn.max(dim=-1)   # (B, T), (B, T)\n",
        "\n",
        "    # We use B=1 in this pipeline\n",
        "    return align_idx[0].tolist(), align_peak[0].tolist()\n",
        "\n",
        "\n",
        "\n",
        "def span_contiguity(align_idx, align_peak, peak_thr=0.30):\n",
        "    \"\"\"\n",
        "    Counts number of contiguous copy transitions i_t == i_{t-1}+1 when both steps have strong cross-attn peak.\n",
        "    \"\"\"\n",
        "    cont = 0\n",
        "    copied_prev = False\n",
        "    prev_i = None\n",
        "    for t,(i,pk) in enumerate(zip(align_idx, align_peak)):\n",
        "        copied = (pk >= peak_thr)\n",
        "        if t>0 and copied and copied_prev and (i == prev_i + 1):\n",
        "            cont += 1\n",
        "        copied_prev, prev_i = copied, i\n",
        "    return cont\n",
        "\n",
        "def avg_copied_span_len(align_idx, align_peak, peak_thr=0.30):\n",
        "    \"\"\"\n",
        "    Average length over copied runs (length >= 2). If none, return 1.0 (neutral).\n",
        "    \"\"\"\n",
        "    Ls, run = [], 0\n",
        "    for pk in align_peak:\n",
        "        if pk >= peak_thr:\n",
        "            run += 1\n",
        "        else:\n",
        "            if run >= 2: Ls.append(run)\n",
        "            run = 0\n",
        "    if run >= 2: Ls.append(run)\n",
        "    return (sum(Ls)/len(Ls)) if Ls else 1.0\n",
        "\n",
        "def pick_best_for_single(enc, seqs, base_scores, gamma=0.05, eps=0.02, peak_thr=0.30):\n",
        "    \"\"\"\n",
        "    Reranks K candidates for a single example.\n",
        "    score = base/len + gamma*contiguity + eps*(avg_span_len - 1).\n",
        "    Returns: (best_idx, info_list_per_candidate)\n",
        "    \"\"\"\n",
        "    infos = []\n",
        "    best_idx, best_score = None, None\n",
        "    for k, (seq_ids, base) in enumerate(zip(seqs, base_scores)):\n",
        "        # Teacher-force this candidate to get cross-attn alignments\n",
        "        cand = seq_ids.unsqueeze(0).to(DEVICE)\n",
        "        ai, ap = cross_attn_align(enc, cand)\n",
        "        cont = span_contiguity(ai, ap, peak_thr=peak_thr)\n",
        "        avl  = avg_copied_span_len(ai, ap, peak_thr=peak_thr)\n",
        "        tgt_len = cand.size(1) - 1  # subtract BOS\n",
        "        # sequences_scores are approx length-normalized already; apply only a gentle norm\n",
        "        tgt_len = max(int(cand.size(1) - 1), 1)\n",
        "        final = (base.item() / tgt_len**0.5) + gamma*cont + eps*(avl - 1.0)\n",
        "\n",
        "        infos.append({\n",
        "            \"k\": k, \"base_score\": float(base.item()), \"len\": int(tgt_len),\n",
        "            \"contiguity\": int(cont), \"avg_span_len\": float(avl), \"final_score\": float(final)\n",
        "        })\n",
        "        if (best_score is None) or (final > best_score):\n",
        "            best_score, best_idx = final, k\n",
        "    return best_idx, infos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbupob8eA9WH"
      },
      "outputs": [],
      "source": [
        "# --- Helpers you need once ---\n",
        "\n",
        "import torch\n",
        "from contextlib import contextmanager\n",
        "\n",
        "# if you don't already have this in your notebook:\n",
        "@contextmanager\n",
        "def autocast_if_cuda():\n",
        "    if torch.cuda.is_available():\n",
        "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "            yield\n",
        "    else:\n",
        "        yield\n",
        "\n",
        "# device + tiny guard\n",
        "DEVICE = next(model.parameters()).device\n",
        "\n",
        "def _to_device(batch):\n",
        "    return {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "\n",
        "# --- The missing function: returns (enc_inputs, generate_output) ---\n",
        "@torch.no_grad()\n",
        "def generate_candidates(texts, max_src_len=512, **gen_kwargs):\n",
        "    \"\"\"\n",
        "    Tokenize `texts` (list[str]), truncate/pad to `max_src_len`,\n",
        "    and call model.generate with your decode args.\n",
        "    Returns:\n",
        "        enc  : dict with encoder input_ids/attention_mask (on device)\n",
        "        gen  : HF GenerateDecoderOnlyOutput/BeamSearchEncoderDecoderOutput\n",
        "               with .sequences and .sequences_scores (because you set\n",
        "               return_dict_in_generate=True, output_scores=True)\n",
        "    \"\"\"\n",
        "    # 1) Tokenize\n",
        "    tok_batch = tok(\n",
        "        texts,\n",
        "        max_length=max_src_len,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    enc = _to_device(tok_batch)\n",
        "\n",
        "    # 2) Generate K candidates (K = num_return_sequences)\n",
        "    model.eval()\n",
        "    with _force_eager_attn(model):           # you defined this earlier\n",
        "        with autocast_if_cuda():\n",
        "            gen = model.generate(\n",
        "                **enc,\n",
        "                **gen_kwargs,\n",
        "            )\n",
        "    return enc, gen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158,
          "referenced_widgets": [
            "491519cb542d42969c0734e911295ecc"
          ]
        },
        "id": "CWSVzgImp6aG",
        "outputId": "d0461542-b5ec-4d9c-9289-7bee2411a229"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "491519cb542d42969c0734e911295ecc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "PGC+SpanBias (rerank):   0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-991526474.py:10: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[saved] /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/decodes_spanbias/spanbias_details_K5_20250826_201347.csv  (5000 rows)\n",
            "[saved] /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/decodes_spanbias/spanbias_best_20250826_201347.csv  (1000 rows)\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "# ==== RUN ====\n",
        "GAMMA = 0.05   # weight for contiguity count\n",
        "EPS   = 0.02   # weight for (avg_span_len - 1)\n",
        "PEAK  = 0.25   # cross-attn peak threshold\n",
        "\n",
        "# Output files\n",
        "STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "DETAILS_CSV = os.path.join(OUT_DIR, f\"spanbias_details_K{DECODE_ARGS['num_return_sequences']}_{STAMP}.csv\")\n",
        "BEST_CSV    = os.path.join(OUT_DIR, f\"spanbias_best_{STAMP}.csv\")\n",
        "\n",
        "details_rows = []\n",
        "best_rows    = []\n",
        "\n",
        "pbar = tqdm(range(len(work_df)), desc=\"PGC+SpanBias (rerank)\", leave=True)\n",
        "for i in pbar:\n",
        "    art = str(work_df.loc[i, \"article\"])\n",
        "    ref = str(work_df.loc[i, \"highlights\"])\n",
        "\n",
        "    # 1) Generate K candidates\n",
        "    enc, gen = generate_candidates([art], max_src_len=MAX_SRC_LEN, **DECODE_ARGS)\n",
        "    seqs = gen.sequences    # shape: [K, T]\n",
        "    base = gen.sequences_scores\n",
        "\n",
        "    # 2) Rerank with span-continuation bias\n",
        "    best_k, infos = pick_best_for_single(enc, seqs, base, gamma=GAMMA, eps=EPS, peak_thr=PEAK)\n",
        "\n",
        "    # 3) Save all candidates (details)\n",
        "    for info in infos:\n",
        "        k = info[\"k\"]\n",
        "        summ = tok.decode(seqs[k], skip_special_tokens=True)\n",
        "        row = {\n",
        "            \"row_id\": i,\n",
        "            \"candidate_k\": k,\n",
        "            \"article\": art,\n",
        "            \"reference\": ref,\n",
        "            \"summary\": summ,\n",
        "            **{k2: info[k2] for k2 in (\"base_score\",\"len\",\"contiguity\",\"avg_span_len\",\"final_score\")}\n",
        "        }\n",
        "        details_rows.append(row)\n",
        "\n",
        "    # 4) Save best\n",
        "    best_sum = tok.decode(seqs[best_k], skip_special_tokens=True)\n",
        "    best_rows.append({\n",
        "        \"row_id\": i, \"article\": art, \"reference\": ref, \"summary\": best_sum,\n",
        "        \"best_k\": best_k, **{k2: infos[best_k][k2] for k2 in (\"base_score\",\"len\",\"contiguity\",\"avg_span_len\",\"final_score\")}\n",
        "    })\n",
        "\n",
        "\n",
        "def _safe_to_csv(df: pd.DataFrame, path: str):\n",
        "    tmp = path + \".tmp\"\n",
        "    df.to_csv(\n",
        "        tmp,\n",
        "        index=False,\n",
        "        quoting=csv.QUOTE_ALL,\n",
        "        escapechar=\"\\\\\",\n",
        "        lineterminator=\"\\n\",   # <-- correct spelling\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "    # strict re-open to verify integrity before moving into place\n",
        "    _ = pd.read_csv(tmp, engine=\"python\", encoding=\"utf-8\")\n",
        "    os.replace(tmp, path)\n",
        "    print(f\"[saved] {path}  ({len(df)} rows)\")\n",
        "\n",
        "\n",
        "details_df = pd.DataFrame(details_rows)\n",
        "best_df    = pd.DataFrame(best_rows)\n",
        "\n",
        "_safe_to_csv(details_df, DETAILS_CSV)\n",
        "_safe_to_csv(best_df,    BEST_CSV)\n",
        "\n",
        "print(\"Done.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "q2Z8b37GqTb2",
        "outputId": "b88d5576-23b0-4727-a494-db792f762841"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"display(sample5[cols])\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"row_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 270,\n        \"min\": 298,\n        \"max\": 993,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          859,\n          672,\n          298\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Manchester United captain scored in win at Old Trafford on Sunday . READ our player ratings for Man United's 3-0 win over Tottenham here . CLICK HERE for all the latest Manchester United news .\",\n          \"Manchester United are clear at the top of social media and money tables . They have almost 25 million more followers than the next club Chelsea . Manchester City, the Premier League champions, lag behind in fifth . United can also boast the highest income, ahead of City and Chelsea . Sportsmail's HOW BIG IS YOUR CLUB? Study finally settles great debate .\",\n          \"The contest to win a trip to Ireland using a picture of Eilean Donan Castle . Eagle-eyed Scots spotted the gaffe and pointed out the mistake . One Facebook user writes: 'Sounds nice, but it's a shame they can't show a picture of Ireland'\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"top_beam_summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Rooney took part in a 'Chevrolet FC hosts Man Utd in Google+ Hangout' on Monday. He was asked who the best singer is at the club. He replied: 'Probably me\",\n          \"Manchester United are the most popular English football club worldwide and also the wealthiest. They are comfortably ahead of all their rivals in both these categories. Chelsea, Arsenal, Liverpool and Manchester City all make the top 20.\",\n          \"WomenFreebies.co.uk used a photo of Scottish Castle Eilean Donan to advertise a competition for Ireland. Unsurprisingly, the use of the Scottish castle advertising a holiday to Ireland didn't go\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"spanbias_summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Rooney took part in a 'Chevrolet FC hosts Man Utd in Google+ Hangout' on Monday. He was asked who the best singer is at the club. He replied: 'Probably me\",\n          \"Manchester United are the most popular English football club worldwide and also the wealthiest. They are comfortably ahead of all their rivals in both these categories. Chelsea, Arsenal, Liverpool and Manchester City all make the top 20.\",\n          \"WomenFreebies.co.uk used a photo of Scottish Castle Eilean Donan to advertise a competition for Ireland. Unsurprisingly, the use of the Scottish castle advertising a holiday to Ireland didn't go\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_k\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contiguity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_span_len\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"final_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02671607394548918,\n        \"min\": -0.11245196797401773,\n        \"max\": -0.04706554047556105,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          -0.11245196797401773\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-447e52ee-9c4a-448e-a583-e5db7e5ab91f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>row_id</th>\n",
              "      <th>reference</th>\n",
              "      <th>top_beam_summary</th>\n",
              "      <th>spanbias_summary</th>\n",
              "      <th>best_k</th>\n",
              "      <th>contiguity</th>\n",
              "      <th>avg_span_len</th>\n",
              "      <th>final_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>993</td>\n",
              "      <td>Barcelona's Dani Alves is well-known for being...</td>\n",
              "      <td>Dani Alves has released a charity single with ...</td>\n",
              "      <td>Dani Alves has released a charity single with ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.110358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>859</td>\n",
              "      <td>Manchester United captain scored in win at Old...</td>\n",
              "      <td>Rooney took part in a 'Chevrolet FC hosts Man ...</td>\n",
              "      <td>Rooney took part in a 'Chevrolet FC hosts Man ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.112452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>298</td>\n",
              "      <td>The contest to win a trip to Ireland using a p...</td>\n",
              "      <td>WomenFreebies.co.uk used a photo of Scottish C...</td>\n",
              "      <td>WomenFreebies.co.uk used a photo of Scottish C...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.047066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>553</td>\n",
              "      <td>German engineering firm Festo created the Bion...</td>\n",
              "      <td>German engineers have crafted machines that mi...</td>\n",
              "      <td>German engineers have crafted machines that mi...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.086075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>672</td>\n",
              "      <td>Manchester United are clear at the top of soci...</td>\n",
              "      <td>Manchester United are the most popular English...</td>\n",
              "      <td>Manchester United are the most popular English...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.099098</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-447e52ee-9c4a-448e-a583-e5db7e5ab91f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-447e52ee-9c4a-448e-a583-e5db7e5ab91f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-447e52ee-9c4a-448e-a583-e5db7e5ab91f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-53ce7a65-210d-4d29-806b-3bcf40c83e65\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-53ce7a65-210d-4d29-806b-3bcf40c83e65')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-53ce7a65-210d-4d29-806b-3bcf40c83e65 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   row_id                                          reference  \\\n",
              "0     993  Barcelona's Dani Alves is well-known for being...   \n",
              "1     859  Manchester United captain scored in win at Old...   \n",
              "2     298  The contest to win a trip to Ireland using a p...   \n",
              "3     553  German engineering firm Festo created the Bion...   \n",
              "4     672  Manchester United are clear at the top of soci...   \n",
              "\n",
              "                                    top_beam_summary  \\\n",
              "0  Dani Alves has released a charity single with ...   \n",
              "1  Rooney took part in a 'Chevrolet FC hosts Man ...   \n",
              "2  WomenFreebies.co.uk used a photo of Scottish C...   \n",
              "3  German engineers have crafted machines that mi...   \n",
              "4  Manchester United are the most popular English...   \n",
              "\n",
              "                                    spanbias_summary  best_k  contiguity  \\\n",
              "0  Dani Alves has released a charity single with ...       0           0   \n",
              "1  Rooney took part in a 'Chevrolet FC hosts Man ...       0           0   \n",
              "2  WomenFreebies.co.uk used a photo of Scottish C...       0           0   \n",
              "3  German engineers have crafted machines that mi...       0           0   \n",
              "4  Manchester United are the most popular English...       0           0   \n",
              "\n",
              "   avg_span_len  final_score  \n",
              "0           1.0    -0.110358  \n",
              "1           1.0    -0.112452  \n",
              "2           1.0    -0.047066  \n",
              "3           1.0    -0.086075  \n",
              "4           1.0    -0.099098  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# === Display 5 example triples: reference | top-beam (k=0) | span-bias best ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random, csv, os\n",
        "\n",
        "SEED = 0\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "\n",
        "def _strict_read(path):\n",
        "    return pd.read_csv(path, engine=\"python\", encoding=\"utf-8\")\n",
        "\n",
        "# If the in-memory DataFrames don't exist, load them from disk.\n",
        "if \"details_df\" not in globals():\n",
        "    assert \"DETAILS_CSV\" in globals(), \"DETAILS_CSV path is not defined.\"\n",
        "    details_df = _strict_read(DETAILS_CSV)\n",
        "\n",
        "if \"best_df\" not in globals():\n",
        "    assert \"BEST_CSV\" in globals(), \"BEST_CSV path is not defined.\"\n",
        "    best_df = _strict_read(BEST_CSV)\n",
        "\n",
        "# Top-beam k=0 per item\n",
        "top_df = (\n",
        "    details_df[details_df[\"candidate_k\"]==0]\n",
        "    .loc[:, [\"row_id\", \"summary\"]]\n",
        "    .rename(columns={\"summary\":\"top_beam_summary\"})\n",
        ")\n",
        "\n",
        "# Best (span-bias reranked) per item\n",
        "best_view = best_df.loc[:, [\"row_id\",\"article\",\"reference\",\"summary\",\"best_k\",\"contiguity\",\"avg_span_len\",\"final_score\"]]\n",
        "best_view = best_view.rename(columns={\"summary\":\"spanbias_summary\"})\n",
        "\n",
        "# Merge and pick 5 deterministic samples\n",
        "pair_df = best_view.merge(top_df, on=\"row_id\", how=\"left\")\n",
        "sample5 = pair_df.sample(n=5, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "# Show compact columns\n",
        "cols = [\"row_id\",\"reference\",\"top_beam_summary\",\"spanbias_summary\",\"best_k\",\"contiguity\",\"avg_span_len\",\"final_score\"]\n",
        "display(sample5[cols])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLApvy_ttzWj",
        "outputId": "005f6183-9e84-499e-c806-aeb17d63025c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "=== ROUGE (mean F1) ===\n",
            "Top beam: {'r1_f': np.float64(0.3735), 'r2_f': np.float64(0.1696), 'rl_f': np.float64(0.2696), 'len_ref_mean': np.float64(58.278), 'len_hyp_mean': np.float64(32.974), 'len_hyp_median': 33.0}\n",
            "Best {'r1_f': np.float64(0.3735), 'r2_f': np.float64(0.1696), 'rl_f': np.float64(0.2696), 'len_ref_mean': np.float64(58.278), 'len_hyp_mean': np.float64(32.974), 'len_hyp_median': 33.0}\n",
            "\n",
            "=== Δ (SPANBIAS - TOP) ===\n",
            "{'Δ_r1_f': np.float64(0.0), 'Δ_r2_f': np.float64(0.0), 'Δ_rl_f': np.float64(0.0), 'Δ_len_hyp_mean': np.float64(0.0)}\n"
          ]
        }
      ],
      "source": [
        "# Install ROUGE (correct flag order)\n",
        "!pip install -q -U rouge-score\n",
        "\n",
        "from rouge_score import rouge_scorer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def _strict_read(path):\n",
        "    return pd.read_csv(path, engine=\"python\", encoding=\"utf-8\")\n",
        "\n",
        "# Ensure we have the frames (load from disk if needed)\n",
        "if \"details_df\" not in globals():\n",
        "    details_df = _strict_read(DETAILS_CSV)\n",
        "if \"best_df\" not in globals():\n",
        "    best_df = _strict_read(BEST_CSV)\n",
        "\n",
        "# Align: top-beam (k=0) vs span-bias best\n",
        "eval_df = best_df.merge(\n",
        "    details_df[details_df[\"candidate_k\"]==0][[\"row_id\",\"summary\"]].rename(columns={\"summary\":\"top_beam_summary\"}),\n",
        "    on=\"row_id\", how=\"left\"\n",
        ").rename(columns={\"summary\":\"spanbias_summary\"})\n",
        "\n",
        "refs  = eval_df[\"reference\"].astype(str).tolist()\n",
        "tops  = eval_df[\"top_beam_summary\"].astype(str).tolist()\n",
        "bests = eval_df[\"spanbias_summary\"].astype(str).tolist()\n",
        "\n",
        "def _rouge_df(refs, hyps):\n",
        "    scorer = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\"], use_stemmer=True)\n",
        "    rows = []\n",
        "    for r, h in zip(refs, hyps):\n",
        "        s = scorer.score(r, h)\n",
        "        rows.append({\n",
        "            \"r1_f\": s[\"rouge1\"].fmeasure,\n",
        "            \"r2_f\": s[\"rouge2\"].fmeasure,\n",
        "            \"rl_f\": s[\"rougeLsum\"].fmeasure,\n",
        "            \"len_ref\": len(r.split()),\n",
        "            \"len_hyp\": len(h.split())\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "rouge_top  = _rouge_df(refs, tops)\n",
        "rouge_best = _rouge_df(refs, bests)\n",
        "\n",
        "def _agg(df):\n",
        "    return {\n",
        "        \"r1_f\": df[\"r1_f\"].mean(),\n",
        "        \"r2_f\": df[\"r2_f\"].mean(),\n",
        "        \"rl_f\": df[\"rl_f\"].mean(),\n",
        "        \"len_ref_mean\": df[\"len_ref\"].mean(),\n",
        "        \"len_hyp_mean\": df[\"len_hyp\"].mean(),\n",
        "        \"len_hyp_median\": df[\"len_hyp\"].median(),\n",
        "    }\n",
        "\n",
        "rep_top  = _agg(rouge_top)\n",
        "rep_best = _agg(rouge_best)\n",
        "delta = {\n",
        "    \"Δ_r1_f\": rep_best[\"r1_f\"] - rep_top[\"r1_f\"],\n",
        "    \"Δ_r2_f\": rep_best[\"r2_f\"] - rep_top[\"r2_f\"],\n",
        "    \"Δ_rl_f\": rep_best[\"rl_f\"] - rep_top[\"rl_f\"],\n",
        "    \"Δ_len_hyp_mean\": rep_best[\"len_hyp_mean\"] - rep_top[\"len_hyp_mean\"],\n",
        "}\n",
        "\n",
        "print(\"=== ROUGE (mean F1) ===\")\n",
        "print(\"Top beam:\" , {k: round(v,4) for k,v in rep_top.items()})\n",
        "print(\"Best\" , {k: round(v,4) for k,v in rep_best.items()})\n",
        "print(\"\\n=== Δ (SPANBIAS - TOP) ===\")\n",
        "print({k: round(v,4) for k,v in delta.items()})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oGXHcoRuFXZ",
        "outputId": "bfa8454b-0c0a-4568-94c0-4b3cb8da53a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Install spaCy (correct flag order) + model\n",
        "!pip install -q -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "# Ensure frames\n",
        "def _strict_read(path):\n",
        "    return pd.read_csv(path, engine=\"python\", encoding=\"utf-8\")\n",
        "\n",
        "if \"details_df\" not in globals():\n",
        "    details_df = _strict_read(DETAILS_CSV)\n",
        "if \"best_df\" not in globals():\n",
        "    best_df = _strict_read(BEST_CSV)\n",
        "\n",
        "# Align: top-beam vs span-bias best\n",
        "eval_df = best_df.merge(\n",
        "    details_df[details_df[\"candidate_k\"]==0][[\"row_id\",\"summary\"]].rename(columns={\"summary\":\"top_beam_summary\"}),\n",
        "    on=\"row_id\", how=\"left\"\n",
        ").rename(columns={\"summary\":\"spanbias_summary\"})\n",
        "\n",
        "refs  = eval_df[\"reference\"].astype(str).tolist()\n",
        "tops  = eval_df[\"top_beam_summary\"].astype(str).tolist()\n",
        "bests = eval_df[\"spanbias_summary\"].astype(str).tolist()\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"lemmatizer\",\"textcat\"])\n",
        "nlp.max_length = 2_000_000\n",
        "\n",
        "def _ents(text):\n",
        "    doc = nlp(text)\n",
        "    return [(ent.text.strip().lower(), ent.label_) for ent in doc.ents]\n",
        "\n",
        "def _score_sets(ref_list, hyp_list):\n",
        "    ref_set, hyp_set = set(ref_list), set(hyp_list)\n",
        "    tp = len(ref_set & hyp_set)\n",
        "    fp = len(hyp_set - ref_set)\n",
        "    fn = len(ref_set - hyp_set)\n",
        "    p = tp / (tp+fp) if tp+fp else 0.0\n",
        "    r = tp / (tp+fn) if tp+fn else 0.0\n",
        "    f = 2*p*r / (p+r) if (p+r) else 0.0\n",
        "    return tp, fp, fn, p, r, f\n",
        "\n",
        "def entity_eval(refs, hyps):\n",
        "    # overall (ignore type)\n",
        "    overall_refs = [t for r in refs for (t,_) in _ents(r)]\n",
        "    overall_hyps = [t for h in hyps for (t,_) in _ents(h)]\n",
        "    tp, fp, fn, p, r, f = _score_sets([(t,) for t in overall_refs], [(t,) for t in overall_hyps])\n",
        "    overall = {\"TP\": tp, \"FP\": fp, \"FN\": fn, \"entP\": p, \"entR\": r, \"entF1\": f}\n",
        "\n",
        "    # by type\n",
        "    by_type_refs, by_type_hyps = defaultdict(list), defaultdict(list)\n",
        "    for r in refs:\n",
        "        for t,lab in _ents(r): by_type_refs[lab].append(t)\n",
        "    for h in hyps:\n",
        "        for t,lab in _ents(h): by_type_hyps[lab].append(t)\n",
        "\n",
        "    by_type = {}\n",
        "    for lab in sorted(set(list(by_type_refs.keys()) + list(by_type_hyps.keys()))):\n",
        "        tp, fp, fn, p, r, f = _score_sets(\n",
        "            [(t,) for t in by_type_refs[lab]],\n",
        "            [(t,) for t in by_type_hyps[lab]]\n",
        "        )\n",
        "        by_type[lab] = {\"TP\": tp, \"FP\": fp, \"FN\": fn, \"P\": p, \"R\": r, \"F1\": f}\n",
        "\n",
        "    return overall, by_type\n",
        "\n",
        "overall_top,  bytype_top  = entity_eval(refs, tops)\n",
        "overall_best, bytype_best = entity_eval(refs, bests)\n",
        "\n",
        "def _round(d):\n",
        "    out = {}\n",
        "    for k,v in d.items():\n",
        "        out[k] = round(v,4) if isinstance(v, float) else ( _round(v) if isinstance(v, dict) else v )\n",
        "    return out\n",
        "\n",
        "print(\"=== ENTITY (Overall) ===\")\n",
        "print(\"TOP     :\", _round(overall_top))\n",
        "print(\"SPANBIAS:\", _round(overall_best))\n",
        "print(\"\\n=== ENTITY (By type) — TOP ===\")\n",
        "print(_round(bytype_top))\n",
        "print(\"\\n=== ENTITY (By type) — SPANBIAS ===\")\n",
        "print(_round(bytype_best))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKYi1jsjvmQd",
        "outputId": "84525837-0ebe-4750-a07f-fd9477eaabb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ENTITY vs REFERENCE (Overall) ===\n",
            "{'TP': 239, 'FP': 153, 'FN': 306, 'entP': 0.6097, 'entR': 0.4385, 'entF1': 0.5101}\n",
            "\n",
            "=== ENTITY vs SOURCE (Overall) ===\n",
            "{'TP': 381, 'FP': 11, 'FN': 2510, 'entP': 0.9719, 'entR': 0.1318, 'entF1': 0.2321}\n",
            "\n",
            "=== ENTITY vs REFERENCE (By type) ===\n",
            "{'CARDINAL': {'TP': 19, 'FP': 16, 'FN': 25, 'P': 0.5429, 'R': 0.4318, 'F1': 0.481}, 'DATE': {'TP': 31, 'FP': 34, 'FN': 58, 'P': 0.4769, 'R': 0.3483, 'F1': 0.4026}, 'EVENT': {'TP': 1, 'FP': 1, 'FN': 1, 'P': 0.5, 'R': 0.5, 'F1': 0.5}, 'FAC': {'TP': 5, 'FP': 2, 'FN': 6, 'P': 0.7143, 'R': 0.4545, 'F1': 0.5556}, 'GPE': {'TP': 34, 'FP': 18, 'FN': 38, 'P': 0.6538, 'R': 0.4722, 'F1': 0.5484}, 'LAW': {'TP': 1, 'FP': 0, 'FN': 0, 'P': 1.0, 'R': 1.0, 'F1': 1.0}, 'LOC': {'TP': 1, 'FP': 5, 'FN': 7, 'P': 0.1667, 'R': 0.125, 'F1': 0.1429}, 'MONEY': {'TP': 6, 'FP': 5, 'FN': 8, 'P': 0.5455, 'R': 0.4286, 'F1': 0.48}, 'NORP': {'TP': 11, 'FP': 6, 'FN': 12, 'P': 0.6471, 'R': 0.4783, 'F1': 0.55}, 'ORDINAL': {'TP': 4, 'FP': 2, 'FN': 0, 'P': 0.6667, 'R': 1.0, 'F1': 0.8}, 'ORG': {'TP': 31, 'FP': 43, 'FN': 66, 'P': 0.4189, 'R': 0.3196, 'F1': 0.3626}, 'PERCENT': {'TP': 0, 'FP': 0, 'FN': 4, 'P': 0.0, 'R': 0.0, 'F1': 0.0}, 'PERSON': {'TP': 83, 'FP': 26, 'FN': 79, 'P': 0.7615, 'R': 0.5123, 'F1': 0.6125}, 'PRODUCT': {'TP': 2, 'FP': 2, 'FN': 3, 'P': 0.5, 'R': 0.4, 'F1': 0.4444}, 'QUANTITY': {'TP': 0, 'FP': 1, 'FN': 5, 'P': 0.0, 'R': 0.0, 'F1': 0.0}, 'TIME': {'TP': 2, 'FP': 6, 'FN': 8, 'P': 0.25, 'R': 0.2, 'F1': 0.2222}, 'WORK_OF_ART': {'TP': 0, 'FP': 1, 'FN': 0, 'P': 0.0, 'R': 0.0, 'F1': 0.0}}\n",
            "\n",
            "=== ENTITY vs SOURCE (By type) ===\n",
            "{'CARDINAL': {'TP': 33, 'FP': 2, 'FN': 203, 'P': 0.9429, 'R': 0.1398, 'F1': 0.2435}, 'DATE': {'TP': 62, 'FP': 3, 'FN': 428, 'P': 0.9538, 'R': 0.1265, 'F1': 0.2234}, 'EVENT': {'TP': 2, 'FP': 0, 'FN': 17, 'P': 1.0, 'R': 0.1053, 'F1': 0.1905}, 'FAC': {'TP': 7, 'FP': 0, 'FN': 67, 'P': 1.0, 'R': 0.0946, 'F1': 0.1728}, 'GPE': {'TP': 50, 'FP': 2, 'FN': 272, 'P': 0.9615, 'R': 0.1553, 'F1': 0.2674}, 'LANGUAGE': {'TP': 0, 'FP': 0, 'FN': 2, 'P': 0.0, 'R': 0.0, 'F1': 0.0}, 'LAW': {'TP': 1, 'FP': 0, 'FN': 13, 'P': 1.0, 'R': 0.0714, 'F1': 0.1333}, 'LOC': {'TP': 6, 'FP': 0, 'FN': 40, 'P': 1.0, 'R': 0.1304, 'F1': 0.2308}, 'MONEY': {'TP': 11, 'FP': 0, 'FN': 85, 'P': 1.0, 'R': 0.1146, 'F1': 0.2056}, 'NORP': {'TP': 17, 'FP': 0, 'FN': 90, 'P': 1.0, 'R': 0.1589, 'F1': 0.2742}, 'ORDINAL': {'TP': 6, 'FP': 0, 'FN': 10, 'P': 1.0, 'R': 0.375, 'F1': 0.5455}, 'ORG': {'TP': 67, 'FP': 7, 'FN': 534, 'P': 0.9054, 'R': 0.1115, 'F1': 0.1985}, 'PERCENT': {'TP': 0, 'FP': 0, 'FN': 8, 'P': 0.0, 'R': 0.0, 'F1': 0.0}, 'PERSON': {'TP': 106, 'FP': 3, 'FN': 709, 'P': 0.9725, 'R': 0.1301, 'F1': 0.2294}, 'PRODUCT': {'TP': 3, 'FP': 1, 'FN': 20, 'P': 0.75, 'R': 0.1304, 'F1': 0.2222}, 'QUANTITY': {'TP': 1, 'FP': 0, 'FN': 40, 'P': 1.0, 'R': 0.0244, 'F1': 0.0476}, 'TIME': {'TP': 8, 'FP': 0, 'FN': 80, 'P': 1.0, 'R': 0.0909, 'F1': 0.1667}, 'WORK_OF_ART': {'TP': 1, 'FP': 0, 'FN': 27, 'P': 1.0, 'R': 0.0357, 'F1': 0.069}}\n",
            "\n",
            "Unsupported entities in summaries (total across 100 docs): 19\n"
          ]
        }
      ],
      "source": [
        "# === Entity metrics vs REFERENCE and vs SOURCE (overall + by type + unsupported counts) ===\n",
        "import re, pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "# Ensure eval_df with columns: row_id, article, reference, top_beam_summary, spanbias_summary\n",
        "def _strict_read(path): return pd.read_csv(path, engine=\"python\", encoding=\"utf-8\")\n",
        "if \"details_df\" not in globals(): details_df = _strict_read(DETAILS_CSV)\n",
        "if \"best_df\" not in globals(): best_df = _strict_read(BEST_CSV)\n",
        "\n",
        "eval_df = best_df.merge(\n",
        "    details_df[details_df[\"candidate_k\"]==0][[\"row_id\",\"summary\"]]\n",
        "      .rename(columns={\"summary\":\"top_beam_summary\"}),\n",
        "    on=\"row_id\", how=\"left\"\n",
        ").rename(columns={\"summary\":\"spanbias_summary\"})\n",
        "\n",
        "# Use span-bias winners by default; set USE_TOP=True to evaluate top-beam instead.\n",
        "USE_TOP = False\n",
        "hyps = eval_df[\"top_beam_summary\" if USE_TOP else \"spanbias_summary\"].astype(str).tolist()\n",
        "refs = eval_df[\"reference\"].astype(str).tolist()\n",
        "srcs = eval_df[\"article\"].astype(str).tolist()\n",
        "\n",
        "# spaCy is already installed/loaded in your previous cell:\n",
        "# nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"lemmatizer\",\"textcat\"])\n",
        "\n",
        "def _normalize_text(t: str) -> str:\n",
        "    t = t.lower().strip()\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    # light numeric normalization\n",
        "    t = re.sub(r\"(?<=\\d),(?=\\d{3}\\b)\", \"\", t)   # 12,345 -> 12345\n",
        "    t = t.replace(\"%\", \" percent\")\n",
        "    return t\n",
        "\n",
        "def _ents(text):\n",
        "    doc = nlp(text)\n",
        "    return [( _normalize_text(ent.text), ent.label_) for ent in doc.ents]\n",
        "\n",
        "def _score_sets(ref_list, hyp_list):\n",
        "    ref_set, hyp_set = set(ref_list), set(hyp_list)\n",
        "    tp = len(ref_set & hyp_set); fp = len(hyp_set - ref_set); fn = len(ref_set - hyp_set)\n",
        "    p = tp / (tp+fp) if tp+fp else 0.0\n",
        "    r = tp / (tp+fn) if tp+fn else 0.0\n",
        "    f = 2*p*r / (p+r) if (p+r) else 0.0\n",
        "    return tp, fp, fn, p, r, f\n",
        "\n",
        "def entity_eval_pair(left_texts, right_texts):\n",
        "    # overall (ignore type)\n",
        "    L = [t for txt in left_texts  for (t,_) in _ents(txt)]\n",
        "    R = [t for txt in right_texts for (t,_) in _ents(txt)]\n",
        "    tp, fp, fn, p, r, f = _score_sets([(t,) for t in L], [(t,) for t in R])\n",
        "    overall = {\"TP\": tp, \"FP\": fp, \"FN\": fn, \"entP\": p, \"entR\": r, \"entF1\": f}\n",
        "\n",
        "    # by type\n",
        "    L_by, R_by = defaultdict(list), defaultdict(list)\n",
        "    for txt in left_texts:\n",
        "        for t,lab in _ents(txt): L_by[lab].append(t)\n",
        "    for txt in right_texts:\n",
        "        for t,lab in _ents(txt): R_by[lab].append(t)\n",
        "\n",
        "    by_type = {}\n",
        "    for lab in sorted(set(list(L_by.keys()) + list(R_by.keys()))):\n",
        "        tp, fp, fn, p, r, f = _score_sets([(t,) for t in L_by[lab]], [(t,) for t in R_by[lab]])\n",
        "        by_type[lab] = {\"TP\": tp, \"FP\": fp, \"FN\": fn, \"P\": p, \"R\": r, \"F1\": f}\n",
        "    return overall, by_type\n",
        "\n",
        "# 1) Reference ↔ Hypothesis (faithfulness to gold)\n",
        "overall_ref, bytype_ref = entity_eval_pair(refs, hyps)\n",
        "\n",
        "# 2) Source ↔ Hypothesis (supported vs unsupported in article)\n",
        "overall_src, bytype_src = entity_eval_pair(srcs, hyps)\n",
        "\n",
        "def _round(d):\n",
        "    out = {}\n",
        "    for k,v in d.items():\n",
        "        if isinstance(v, dict): out[k] = _round(v)\n",
        "        else: out[k] = round(v,4) if isinstance(v, float) else v\n",
        "    return out\n",
        "\n",
        "print(\"=== ENTITY vs REFERENCE (Overall) ===\")\n",
        "print(_round(overall_ref))\n",
        "print(\"\\n=== ENTITY vs SOURCE (Overall) ===\")\n",
        "print(_round(overall_src))\n",
        "\n",
        "print(\"\\n=== ENTITY vs REFERENCE (By type) ===\")\n",
        "print(_round(bytype_ref))\n",
        "print(\"\\n=== ENTITY vs SOURCE (By type) ===\")\n",
        "print(_round(bytype_src))\n",
        "\n",
        "# Extra: Unsupported-in-source count (hallucinated entities)\n",
        "unsupported_total = 0\n",
        "for s, h in zip(srcs, hyps):\n",
        "    s_set = set((t,lab) for t,lab in _ents(s))\n",
        "    h_set = set((t,lab) for t,lab in _ents(h))\n",
        "    unsupported_total += len(h_set - s_set)\n",
        "print(f\"\\nUnsupported entities in summaries (total across {len(hyps)} docs): {unsupported_total}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RleAsk5ukcY",
        "outputId": "7e3bf8da-b4f7-4484-b822-f483bd2b177d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best_k counts: {0: 100} | changed: 0\n",
            "Mean contiguity (top vs best): 0.0 vs 0.0\n",
            "Mean avg_span_len (top vs best): 1.0 vs 1.0\n"
          ]
        }
      ],
      "source": [
        "# How often did the reranker actually change the winner?\n",
        "vc = best_df[\"best_k\"].value_counts().sort_index()\n",
        "print(\"best_k counts:\", vc.to_dict(), \"| changed:\", int((best_df[\"best_k\"]!=0).sum()))\n",
        "\n",
        "# Compare contiguity of top-beam vs chosen best (per item)\n",
        "top_stats = (\n",
        "    details_df[details_df[\"candidate_k\"]==0]\n",
        "    .set_index(\"row_id\")[[\"contiguity\",\"avg_span_len\",\"final_score\"]]\n",
        "    .rename(columns={\"contiguity\":\"top_cont\",\"avg_span_len\":\"top_avl\",\"final_score\":\"top_final\"})\n",
        ")\n",
        "best_stats = best_df.set_index(\"row_id\")[[\"contiguity\",\"avg_span_len\",\"final_score\"]] \\\n",
        "    .rename(columns={\"contiguity\":\"best_cont\",\"avg_span_len\":\"best_avl\",\"final_score\":\"best_final\"})\n",
        "\n",
        "cmp = top_stats.join(best_stats, how=\"inner\")\n",
        "print(\"Mean contiguity (top vs best):\", float(cmp[\"top_cont\"].mean()), \"vs\", float(cmp[\"best_cont\"].mean()))\n",
        "print(\"Mean avg_span_len (top vs best):\", float(cmp[\"top_avl\"].mean()), \"vs\", float(cmp[\"best_avl\"].mean()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        },
        "id": "9dSJyfMcy7Q_",
        "outputId": "8b24b0e9-d4b2-4189-c76e-1341de42b842"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ENTITY vs REFERENCE (Overall) ===\n",
            "TOP : {'TP': 239, 'FP': 153, 'FN': 306, 'entP': 0.6097, 'entR': 0.4385, 'entF1': 0.5101}\n",
            "BEST: {'TP': 239, 'FP': 153, 'FN': 306, 'entP': 0.6097, 'entR': 0.4385, 'entF1': 0.5101}\n",
            "\n",
            "=== ENTITY vs REFERENCE (By type) — TOP vs BEST ===\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"print({k: round(v,4) for k,v in delta\",\n  \"rows\": 17,\n  \"fields\": [\n    {\n      \"column\": \"TYPE\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 17,\n        \"samples\": [\n          \"CARDINAL\",\n          \"DATE\",\n          \"LAW\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TP_TOP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 21,\n        \"min\": 0,\n        \"max\": 83,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          6,\n          19,\n          83\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FP_TOP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13,\n        \"min\": 0,\n        \"max\": 43,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          43,\n          34,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FN_TOP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 25,\n        \"min\": 0,\n        \"max\": 79,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          66,\n          79,\n          25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"P_TOP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2901193844021498,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.6667,\n          0.0,\n          0.5429\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"R_TOP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.29181881768135026,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.3196,\n          0.5123,\n          0.4318\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1_TOP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.27755283850424717,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.8,\n          0.0,\n          0.481\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TP_BEST\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 21,\n        \"min\": 0,\n        \"max\": 83,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          6,\n          19,\n          83\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FP_BEST\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13,\n        \"min\": 0,\n        \"max\": 43,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          43,\n          34,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FN_BEST\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 25,\n        \"min\": 0,\n        \"max\": 79,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          66,\n          79,\n          25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"P_BEST\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2901193844021498,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.6667,\n          0.0,\n          0.5429\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"R_BEST\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.29181881768135026,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.3196,\n          0.5123,\n          0.4318\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1_BEST\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.27755283850424717,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.8,\n          0.0,\n          0.481\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-ffe11920-54fe-483e-99b4-7d8ec8f5feb3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TYPE</th>\n",
              "      <th>TP_TOP</th>\n",
              "      <th>FP_TOP</th>\n",
              "      <th>FN_TOP</th>\n",
              "      <th>P_TOP</th>\n",
              "      <th>R_TOP</th>\n",
              "      <th>F1_TOP</th>\n",
              "      <th>TP_BEST</th>\n",
              "      <th>FP_BEST</th>\n",
              "      <th>FN_BEST</th>\n",
              "      <th>P_BEST</th>\n",
              "      <th>R_BEST</th>\n",
              "      <th>F1_BEST</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CARDINAL</td>\n",
              "      <td>19</td>\n",
              "      <td>16</td>\n",
              "      <td>25</td>\n",
              "      <td>0.5429</td>\n",
              "      <td>0.4318</td>\n",
              "      <td>0.4810</td>\n",
              "      <td>19</td>\n",
              "      <td>16</td>\n",
              "      <td>25</td>\n",
              "      <td>0.5429</td>\n",
              "      <td>0.4318</td>\n",
              "      <td>0.4810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DATE</td>\n",
              "      <td>31</td>\n",
              "      <td>34</td>\n",
              "      <td>58</td>\n",
              "      <td>0.4769</td>\n",
              "      <td>0.3483</td>\n",
              "      <td>0.4026</td>\n",
              "      <td>31</td>\n",
              "      <td>34</td>\n",
              "      <td>58</td>\n",
              "      <td>0.4769</td>\n",
              "      <td>0.3483</td>\n",
              "      <td>0.4026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>EVENT</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5000</td>\n",
              "      <td>0.5000</td>\n",
              "      <td>0.5000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5000</td>\n",
              "      <td>0.5000</td>\n",
              "      <td>0.5000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>FAC</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>0.7143</td>\n",
              "      <td>0.4545</td>\n",
              "      <td>0.5556</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>0.7143</td>\n",
              "      <td>0.4545</td>\n",
              "      <td>0.5556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>GPE</td>\n",
              "      <td>34</td>\n",
              "      <td>18</td>\n",
              "      <td>38</td>\n",
              "      <td>0.6538</td>\n",
              "      <td>0.4722</td>\n",
              "      <td>0.5484</td>\n",
              "      <td>34</td>\n",
              "      <td>18</td>\n",
              "      <td>38</td>\n",
              "      <td>0.6538</td>\n",
              "      <td>0.4722</td>\n",
              "      <td>0.5484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LAW</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LOC</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>0.1667</td>\n",
              "      <td>0.1250</td>\n",
              "      <td>0.1429</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>0.1667</td>\n",
              "      <td>0.1250</td>\n",
              "      <td>0.1429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>MONEY</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>0.5455</td>\n",
              "      <td>0.4286</td>\n",
              "      <td>0.4800</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>0.5455</td>\n",
              "      <td>0.4286</td>\n",
              "      <td>0.4800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>NORP</td>\n",
              "      <td>11</td>\n",
              "      <td>6</td>\n",
              "      <td>12</td>\n",
              "      <td>0.6471</td>\n",
              "      <td>0.4783</td>\n",
              "      <td>0.5500</td>\n",
              "      <td>11</td>\n",
              "      <td>6</td>\n",
              "      <td>12</td>\n",
              "      <td>0.6471</td>\n",
              "      <td>0.4783</td>\n",
              "      <td>0.5500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ORDINAL</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.6667</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.8000</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.6667</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.8000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>ORG</td>\n",
              "      <td>31</td>\n",
              "      <td>43</td>\n",
              "      <td>66</td>\n",
              "      <td>0.4189</td>\n",
              "      <td>0.3196</td>\n",
              "      <td>0.3626</td>\n",
              "      <td>31</td>\n",
              "      <td>43</td>\n",
              "      <td>66</td>\n",
              "      <td>0.4189</td>\n",
              "      <td>0.3196</td>\n",
              "      <td>0.3626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>PERCENT</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>PERSON</td>\n",
              "      <td>83</td>\n",
              "      <td>26</td>\n",
              "      <td>79</td>\n",
              "      <td>0.7615</td>\n",
              "      <td>0.5123</td>\n",
              "      <td>0.6125</td>\n",
              "      <td>83</td>\n",
              "      <td>26</td>\n",
              "      <td>79</td>\n",
              "      <td>0.7615</td>\n",
              "      <td>0.5123</td>\n",
              "      <td>0.6125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>PRODUCT</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.5000</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.4444</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.5000</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.4444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>QUANTITY</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>TIME</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>0.2500</td>\n",
              "      <td>0.2000</td>\n",
              "      <td>0.2222</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>0.2500</td>\n",
              "      <td>0.2000</td>\n",
              "      <td>0.2222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>WORK_OF_ART</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ffe11920-54fe-483e-99b4-7d8ec8f5feb3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ffe11920-54fe-483e-99b4-7d8ec8f5feb3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ffe11920-54fe-483e-99b4-7d8ec8f5feb3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-9bd06688-e0dc-4c53-b1d9-401234d88c77\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9bd06688-e0dc-4c53-b1d9-401234d88c77')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-9bd06688-e0dc-4c53-b1d9-401234d88c77 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "           TYPE  TP_TOP  FP_TOP  FN_TOP   P_TOP   R_TOP  F1_TOP  TP_BEST  \\\n",
              "0      CARDINAL      19      16      25  0.5429  0.4318  0.4810       19   \n",
              "1          DATE      31      34      58  0.4769  0.3483  0.4026       31   \n",
              "2         EVENT       1       1       1  0.5000  0.5000  0.5000        1   \n",
              "3           FAC       5       2       6  0.7143  0.4545  0.5556        5   \n",
              "4           GPE      34      18      38  0.6538  0.4722  0.5484       34   \n",
              "5           LAW       1       0       0  1.0000  1.0000  1.0000        1   \n",
              "6           LOC       1       5       7  0.1667  0.1250  0.1429        1   \n",
              "7         MONEY       6       5       8  0.5455  0.4286  0.4800        6   \n",
              "8          NORP      11       6      12  0.6471  0.4783  0.5500       11   \n",
              "9       ORDINAL       4       2       0  0.6667  1.0000  0.8000        4   \n",
              "10          ORG      31      43      66  0.4189  0.3196  0.3626       31   \n",
              "11      PERCENT       0       0       4  0.0000  0.0000  0.0000        0   \n",
              "12       PERSON      83      26      79  0.7615  0.5123  0.6125       83   \n",
              "13      PRODUCT       2       2       3  0.5000  0.4000  0.4444        2   \n",
              "14     QUANTITY       0       1       5  0.0000  0.0000  0.0000        0   \n",
              "15         TIME       2       6       8  0.2500  0.2000  0.2222        2   \n",
              "16  WORK_OF_ART       0       1       0  0.0000  0.0000  0.0000        0   \n",
              "\n",
              "    FP_BEST  FN_BEST  P_BEST  R_BEST  F1_BEST  \n",
              "0        16       25  0.5429  0.4318   0.4810  \n",
              "1        34       58  0.4769  0.3483   0.4026  \n",
              "2         1        1  0.5000  0.5000   0.5000  \n",
              "3         2        6  0.7143  0.4545   0.5556  \n",
              "4        18       38  0.6538  0.4722   0.5484  \n",
              "5         0        0  1.0000  1.0000   1.0000  \n",
              "6         5        7  0.1667  0.1250   0.1429  \n",
              "7         5        8  0.5455  0.4286   0.4800  \n",
              "8         6       12  0.6471  0.4783   0.5500  \n",
              "9         2        0  0.6667  1.0000   0.8000  \n",
              "10       43       66  0.4189  0.3196   0.3626  \n",
              "11        0        4  0.0000  0.0000   0.0000  \n",
              "12       26       79  0.7615  0.5123   0.6125  \n",
              "13        2        3  0.5000  0.4000   0.4444  \n",
              "14        1        5  0.0000  0.0000   0.0000  \n",
              "15        6        8  0.2500  0.2000   0.2222  \n",
              "16        1        0  0.0000  0.0000   0.0000  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Δ (BEST - TOP) ===\n",
            "{'Δ_entP': 0.0, 'Δ_entR': 0.0, 'Δ_entF1': 0.0}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd, re\n",
        "from collections import defaultdict\n",
        "\n",
        "# -------- config --------\n",
        "BEST_PATH    = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/decodes_spanbias/spanbias_best_20250826_000803.csv\"\n",
        "DETAILS_PATH = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/decodes_spanbias/spanbias_details_K5_20250826_000803.csv\"\n",
        "\n",
        "# -------- io --------\n",
        "def _read(path):\n",
        "    return pd.read_csv(path, engine=\"python\", encoding=\"utf-8\")\n",
        "details_df = _read(DETAILS_PATH)\n",
        "best_df    = _read(BEST_PATH)\n",
        "\n",
        "# Align rows: add top-beam (k=0) summary next to Best summary\n",
        "top_k0 = (\n",
        "    details_df[details_df[\"candidate_k\"]==0][[\"row_id\",\"summary\"]]\n",
        "    .rename(columns={\"summary\":\"top_beam_summary\"})\n",
        ")\n",
        "eval_df = best_df.merge(top_k0, on=\"row_id\", how=\"left\") \\\n",
        "                 .rename(columns={\"summary\":\"best_summary\"})\n",
        "\n",
        "refs = eval_df[\"reference\"].astype(str).tolist()\n",
        "tops = eval_df[\"top_beam_summary\"].astype(str).tolist()\n",
        "bests = eval_df[\"best_summary\"].astype(str).tolist()\n",
        "\n",
        "# -------- spaCy setup --------\n",
        "try:\n",
        "    import spacy\n",
        "except ImportError:\n",
        "    !pip install -q -U spacy\n",
        "    import spacy\n",
        "\n",
        "# Try to load; if model missing, download and load\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"lemmatizer\",\"textcat\"])\n",
        "except OSError:\n",
        "    import sys, subprocess, pkgutil\n",
        "    subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"], check=True)\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"lemmatizer\",\"textcat\"])\n",
        "\n",
        "nlp.max_length = 2_000_000\n",
        "\n",
        "# -------- entity helpers (normalization designed for exact string matching) --------\n",
        "def _normalize_text(t: str) -> str:\n",
        "    t = t.lower().strip()\n",
        "    t = re.sub(r\"(?<=\\d),(?=\\d{3}\\b)\", \"\", t)  # 12,345 -> 12345\n",
        "    t = t.replace(\"%\", \" percent\")\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    return t\n",
        "\n",
        "def _ents_list(text: str):\n",
        "    doc = nlp(text)\n",
        "    return [(_normalize_text(ent.text), ent.label_) for ent in doc.ents]\n",
        "\n",
        "def _score_sets(ref_list, hyp_list):\n",
        "    ref_set, hyp_set = set(ref_list), set(hyp_list)\n",
        "    tp = len(ref_set & hyp_set)\n",
        "    fp = len(hyp_set - ref_set)\n",
        "    fn = len(ref_set - hyp_set)\n",
        "    p = tp / (tp + fp) if (tp+fp) else 0.0\n",
        "    r = tp / (tp + fn) if (tp+fn) else 0.0\n",
        "    f = 2*p*r / (p+r) if (p+r) else 0.0\n",
        "    return tp, fp, fn, p, r, f\n",
        "\n",
        "def evaluate_vs_reference(ref_texts, hyp_texts):\n",
        "    # Overall (ignore type)\n",
        "    ref_over = [t for r in ref_texts for (t,_) in _ents_list(r)]\n",
        "    hyp_over = [t for h in hyp_texts for (t,_) in _ents_list(h)]\n",
        "    tp, fp, fn, p, r, f = _score_sets([(t,) for t in ref_over], [(t,) for t in hyp_over])\n",
        "    overall = {\"TP\": tp, \"FP\": fp, \"FN\": fn, \"entP\": p, \"entR\": r, \"entF1\": f}\n",
        "\n",
        "    # By type\n",
        "    ref_by, hyp_by = defaultdict(list), defaultdict(list)\n",
        "    for r in ref_texts:\n",
        "        for t,lab in _ents_list(r): ref_by[lab].append(t)\n",
        "    for h in hyp_texts:\n",
        "        for t,lab in _ents_list(h): hyp_by[lab].append(t)\n",
        "\n",
        "    rows = []\n",
        "    all_labels = sorted(set(list(ref_by.keys()) + list(hyp_by.keys())))\n",
        "    for lab in all_labels:\n",
        "        tp, fp, fn, p, r, f = _score_sets([(t,) for t in ref_by[lab]], [(t,) for t in hyp_by[lab]])\n",
        "        rows.append({\"TYPE\": lab, \"TP\": tp, \"FP\": fp, \"FN\": fn, \"P\": p, \"R\": r, \"F1\": f})\n",
        "    by_type_df = pd.DataFrame(rows).sort_values(\"TYPE\").reset_index(drop=True)\n",
        "    return overall, by_type_df\n",
        "\n",
        "# -------- compute --------\n",
        "overall_top,  bytype_top  = evaluate_vs_reference(refs, tops)\n",
        "overall_best, bytype_best = evaluate_vs_reference(refs, bests)\n",
        "\n",
        "# Merge by-type tables for a compact comparison\n",
        "cmp_bytype = bytype_top.merge(\n",
        "    bytype_best, on=\"TYPE\", suffixes=(\"_TOP\",\"_BEST\"), how=\"outer\"\n",
        ").fillna(0.0)\n",
        "\n",
        "# Round for display\n",
        "def _round_dict(d):\n",
        "    return {k:(round(v,4) if isinstance(v, float) else v) for k,v in d.items()}\n",
        "overall_top  = _round_dict(overall_top)\n",
        "overall_best = _round_dict(overall_best)\n",
        "for c in [\"P_TOP\",\"R_TOP\",\"F1_TOP\",\"P_BEST\",\"R_BEST\",\"F1_BEST\"]:\n",
        "    if c in cmp_bytype.columns: cmp_bytype[c] = cmp_bytype[c].astype(float).round(4)\n",
        "\n",
        "# -------- show --------\n",
        "print(\"=== ENTITY vs REFERENCE (Overall) ===\")\n",
        "print(\"TOP :\", overall_top)\n",
        "print(\"BEST:\", overall_best)\n",
        "\n",
        "print(\"\\n=== ENTITY vs REFERENCE (By type) — TOP vs BEST ===\")\n",
        "display(cmp_bytype[[\"TYPE\",\"TP_TOP\",\"FP_TOP\",\"FN_TOP\",\"P_TOP\",\"R_TOP\",\"F1_TOP\",\n",
        "                    \"TP_BEST\",\"FP_BEST\",\"FN_BEST\",\"P_BEST\",\"R_BEST\",\"F1_BEST\"]])\n",
        "\n",
        "# Optional delta summary (BEST - TOP)\n",
        "delta = {\n",
        "    \"Δ_entP\": overall_best[\"entP\"] - overall_top[\"entP\"],\n",
        "    \"Δ_entR\": overall_best[\"entR\"] - overall_top[\"entR\"],\n",
        "    \"Δ_entF1\": overall_best[\"entF1\"] - overall_top[\"entF1\"],\n",
        "}\n",
        "print(\"\\n=== Δ (BEST - TOP) ===\")\n",
        "print({k: round(v,4) for k,v in delta.items()})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "IP5l0sACvSO5",
        "outputId": "bdc2b394-8fb6-4777-927b-8530bbe3f661"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"gamma\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06454972243679027,\n        \"min\": 0.1,\n        \"max\": 0.25,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.15,\n          0.25,\n          0.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.016329931618554526,\n        \"min\": 0.03,\n        \"max\": 0.07,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.03,\n          0.05,\n          0.07\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"changed\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-75be3c55-1947-4693-8802-1fe931100910\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gamma</th>\n",
              "      <th>eps</th>\n",
              "      <th>changed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.10</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.15</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.20</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-75be3c55-1947-4693-8802-1fe931100910')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-75be3c55-1947-4693-8802-1fe931100910 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-75be3c55-1947-4693-8802-1fe931100910');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-1da352fd-8a8e-40b9-8577-3feb5da4dbbe\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1da352fd-8a8e-40b9-8577-3feb5da4dbbe')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-1da352fd-8a8e-40b9-8577-3feb5da4dbbe button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   gamma   eps  changed\n",
              "0   0.10  0.03        0\n",
              "1   0.15  0.05        0\n",
              "2   0.20  0.05        0\n",
              "3   0.25  0.07        0"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def rerank_offline(details_df, gamma=0.15, eps=0.05):\n",
        "    dd = details_df.copy()\n",
        "    # Gentle length norm\n",
        "    dd[\"len\"] = dd[\"len\"].clip(lower=1)\n",
        "    dd[\"new_score\"] = (dd[\"base_score\"] / (dd[\"len\"]**0.5)) + gamma*dd[\"contiguity\"] + eps*(dd[\"avg_span_len\"] - 1.0)\n",
        "    # pick argmax per row_id\n",
        "    idx = dd.sort_values([\"row_id\",\"new_score\"], ascending=[True, False]) \\\n",
        "            .groupby(\"row_id\", as_index=False).head(1).index\n",
        "    best_new = dd.loc[idx, [\"row_id\",\"article\",\"reference\",\"summary\",\"candidate_k\",\"base_score\",\"contiguity\",\"avg_span_len\",\"new_score\"]]\n",
        "    best_new = best_new.rename(columns={\"candidate_k\":\"best_k\"})\n",
        "    return best_new.reset_index(drop=True)\n",
        "\n",
        "# Sweep a few settings\n",
        "grid = [(0.10,0.03),(0.15,0.05),(0.20,0.05),(0.25,0.07)]\n",
        "results = []\n",
        "for (g,e) in grid:\n",
        "    best_new = rerank_offline(details_df, gamma=g, eps=e)\n",
        "    changed = int((best_new[\"best_k\"]!=0).sum())\n",
        "    results.append({\"gamma\":g,\"eps\":e,\"changed\":changed})\n",
        "pd.DataFrame(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL78exPW0TdR"
      },
      "source": [
        "#Exp Decode 1k sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BO_PNOgBxL4E"
      },
      "outputs": [],
      "source": [
        "# === CopyNext2: config + logits processor + helpers ===\n",
        "!pip install -q -U transformers pandas tqdm rouge-score spacy\n",
        "import torch, os, time, csv, re, math, pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from pathlib import Path\n",
        "from transformers.generation.logits_process import LogitsProcessor, LogitsProcessorList\n",
        "\n",
        "# ---- reuse model/tok if already loaded; else load from your ckpt ----\n",
        "try:\n",
        "    tok, model\n",
        "except NameError:\n",
        "    CKPT_DIR = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000\"\n",
        "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "    tok = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR)\n",
        "    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\").eval()\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# push a bit longer source to help recall\n",
        "MAX_SRC_LEN_CN2 = 800 if 'MAX_SRC_LEN' not in globals() else max(MAX_SRC_LEN, 800)\n",
        "\n",
        "# Diverse beams so candidates differ\n",
        "CN2_ARGS = dict(\n",
        "    num_beams=10,\n",
        "    num_return_sequences=10,\n",
        "    num_beam_groups=5,\n",
        "    diversity_penalty=0.3,\n",
        "    max_new_tokens=100, min_new_tokens=55,\n",
        "    no_repeat_ngram_size=3,\n",
        "    length_penalty=2.0,\n",
        "    early_stopping=True,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True\n",
        ")\n",
        "\n",
        "# Output paths\n",
        "if 'CKPT_DIR' not in globals():\n",
        "    CKPT_DIR = \"/content\"\n",
        "OUT_DIR = os.path.join(CKPT_DIR, \"decodes_copynext2\")\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "DETAILS_CN2 = os.path.join(OUT_DIR, f\"copynext2_details_{STAMP}.csv\")\n",
        "BEST_CN2    = os.path.join(OUT_DIR, f\"copynext2_best_{STAMP}.csv\")\n",
        "\n",
        "# ---- CopyNext-style logits processor (online) ----\n",
        "class SpanContinuationProcessor(LogitsProcessor):\n",
        "    \"\"\"\n",
        "    If hypothesis suffix matches a contiguous span of the source, bump the next source token's logit.\n",
        "    Training-free CopyNext; works for shared vocab (BART/PGC).\n",
        "    \"\"\"\n",
        "    def __init__(self, src_input_ids, tokenizer, num_beams, gamma=0.8, max_span=8, skip_special=True):\n",
        "        self.src_ids = [ids.tolist() if torch.is_tensor(ids) else list(ids) for ids in src_input_ids]\n",
        "        self.num_beams = int(num_beams)\n",
        "        self.gamma = float(gamma)\n",
        "        self.max_span = int(max_span)\n",
        "        self.skip_ids = set(tokenizer.all_special_ids) if skip_special else set()\n",
        "\n",
        "    @staticmethod\n",
        "    def _find_suffix_in_src(src_ids, hyp_ids, max_span):\n",
        "        if not hyp_ids: return 0, -1\n",
        "        max_check = min(len(hyp_ids), max_span)\n",
        "        for L in range(max_check, 0, -1):              # prefer longer spans\n",
        "            suffix = hyp_ids[-L:]\n",
        "            for end in range(L-1, len(src_ids)):\n",
        "                if src_ids[end-L+1:end+1] == suffix:\n",
        "                    return L, end\n",
        "        return 0, -1\n",
        "\n",
        "    def __call__(self, input_ids, scores):\n",
        "        B_beams = input_ids.size(0)\n",
        "        for row in range(B_beams):\n",
        "            sample_idx = row // self.num_beams\n",
        "            src = self.src_ids[sample_idx]\n",
        "            hyp = input_ids[row].tolist()\n",
        "            L, end = self._find_suffix_in_src(src, hyp, self.max_span)\n",
        "            if L >= 1 and end + 1 < len(src):\n",
        "                next_id = src[end + 1]\n",
        "                if next_id not in self.skip_ids:\n",
        "                    scores[row, next_id] += self.gamma  # small nudge\n",
        "        return scores\n",
        "\n",
        "# ---- lexical/LCS span stats for reranking (offline) ----\n",
        "from difflib import SequenceMatcher\n",
        "_tok_re = re.compile(r\"[A-Za-z0-9%]+(?:'[A-Za-z0-9]+)?\")\n",
        "def _norm_tokens(text: str):\n",
        "    t = text.lower()\n",
        "    t = re.sub(r\"(?<=\\d),(?=\\d{3}\\b)\", \"\", t)  # 12,345 -> 12345\n",
        "    t = t.replace(\"%\", \" percent \")\n",
        "    return _tok_re.findall(t)\n",
        "\n",
        "def lcs_span_stats(src_text: str, hyp_text: str, min_len: int = 2):\n",
        "    a = _norm_tokens(src_text); b = _norm_tokens(hyp_text)\n",
        "    sm = SequenceMatcher(a=a, b=b, autojunk=False)\n",
        "    runs = [blk.size for blk in sm.get_matching_blocks() if blk.size >= min_len]\n",
        "    if not runs:\n",
        "        return 0, 1.0\n",
        "    cont = sum(L-1 for L in runs)             # contiguous transitions inside each span\n",
        "    avl  = sum(runs)/len(runs)\n",
        "    return cont, avl\n",
        "\n",
        "# ---- unsupported-entity penalty (vs source) ----\n",
        "import spacy\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"lemmatizer\",\"textcat\"])\n",
        "except OSError:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"], check=True)\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"lemmatizer\",\"textcat\"])\n",
        "nlp.max_length = 2_000_000\n",
        "\n",
        "def _ents_norm(txt):\n",
        "    doc = nlp(txt)\n",
        "    def norm(t):\n",
        "        t = t.lower().strip()\n",
        "        t = re.sub(r\"(?<=\\d),(?=\\d{3}\\b)\",\"\",t).replace(\"%\",\" percent\")\n",
        "        t = re.sub(r\"\\s+\",\" \",t)\n",
        "        return t\n",
        "    return set((norm(ent.text), ent.label_) for ent in doc.ents)\n",
        "\n",
        "def unsupported_count(src_text, hyp_text):\n",
        "    s = _ents_norm(src_text); h = _ents_norm(hyp_text)\n",
        "    return len(h - s)\n",
        "\n",
        "# ---- safe saver ----\n",
        "def save_csv(df, path):\n",
        "    tmp = path + \".tmp\"\n",
        "    df.to_csv(tmp, index=False, quoting=csv.QUOTE_ALL, escapechar=\"\\\\\", lineterminator=\"\\n\", encoding=\"utf-8\")\n",
        "    _ = pd.read_csv(tmp, engine=\"python\", encoding=\"utf-8\")\n",
        "    os.replace(tmp, path)\n",
        "    print(f\"[saved] {path} ({len(df)} rows)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133,
          "referenced_widgets": [
            "f6112e325b2e4584812044550cb13e2a"
          ]
        },
        "id": "kt13q_Q11gJG",
        "outputId": "48485ff3-4420-445a-e4b7-1c6e370a7a4c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6112e325b2e4584812044550cb13e2a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "CopyNext2 generate:   0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Group Beam Search is scheduled to be moved to a `custom_generate` repository in v4.55.0. To prevent loss of backward compatibility, add `trust_remote_code=True` to your `generate` call.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[saved] /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/decodes_copynext2/copynext2_details_20250826_161305.csv (1000 rows)\n"
          ]
        }
      ],
      "source": [
        "# === Generate another 1000 (CopyNext2) ===\n",
        "# Uses work_df if available; otherwise sample 100 from your validation CSV (set VAL_CSV)\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "try:\n",
        "    work_df\n",
        "    cn2_df = work_df.copy()\n",
        "except NameError:\n",
        "    # FALLBACK: load your validation CSV and sample deterministically\n",
        "    VAL_CSV = \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "    val_df = pd.read_csv(VAL_CSV, engine=\"python\", encoding=\"utf-8\")\n",
        "    cn2_df = val_df.sample(n=1000, random_state=1).reset_index(drop=True)\n",
        "\n",
        "N = min(100, len(cn2_df))\n",
        "articles = cn2_df.loc[:N-1, \"article\"].astype(str).tolist()\n",
        "references = cn2_df.loc[:N-1, \"highlights\"].astype(str).tolist()\n",
        "\n",
        "details_rows = []\n",
        "BATCH = 4  # batch size for generation\n",
        "K = CN2_ARGS[\"num_return_sequences\"]\n",
        "\n",
        "for start in tqdm(range(0, N, BATCH), desc=\"CopyNext2 generate\"):\n",
        "    batch_arts = articles[start:start+BATCH]\n",
        "    batch_refs = references[start:start+BATCH]\n",
        "    enc = tok(batch_arts, return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_SRC_LEN_CN2).to(DEVICE)\n",
        "\n",
        "    proc = SpanContinuationProcessor(\n",
        "        src_input_ids=enc[\"input_ids\"], tokenizer=tok,\n",
        "        num_beams=CN2_ARGS[\"num_beams\"], gamma=0.8, max_span=8\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **enc, **CN2_ARGS, logits_processor=LogitsProcessorList([proc])\n",
        "        )\n",
        "\n",
        "    seqs = out.sequences.view(len(batch_arts), K, -1)            # [B, K, T]\n",
        "    base = out.sequences_scores.view(len(batch_arts), K)         # [B, K]\n",
        "\n",
        "    for bi in range(len(batch_arts)):\n",
        "        art = batch_arts[bi]; ref = batch_refs[bi]\n",
        "        for k in range(K):\n",
        "            ids = seqs[bi, k]\n",
        "            summ = tok.decode(ids, skip_special_tokens=True)\n",
        "            # token length (defensive)\n",
        "            tgt_len = max(int(ids.size(0) - 1), 1)\n",
        "            details_rows.append({\n",
        "                \"row_id\": start + bi,\n",
        "                \"candidate_k\": k,\n",
        "                \"article\": art,\n",
        "                \"reference\": ref,\n",
        "                \"summary\": summ,\n",
        "                \"base_score\": float(base[bi, k].item()),\n",
        "                \"len\": tgt_len\n",
        "            })\n",
        "\n",
        "details_cn2 = pd.DataFrame(details_rows)\n",
        "save_csv(details_cn2, DETAILS_CN2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537,
          "referenced_widgets": [
            "f68c702a65d34f9dbebbfa8e6a1f15ee"
          ]
        },
        "id": "-h-Of6yX3wpl",
        "outputId": "00e926b2-d969-4785-9c52-f3b5efe64096"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f68c702a65d34f9dbebbfa8e6a1f15ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "entities(vs source):   0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[saved] /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/decodes_copynext2/copynext2_details_20250826_161305.csv (1000 rows)\n",
            "[saved] /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/decodes_copynext2/copynext2_best_20250826_161305.csv (100 rows)\n",
            "{'changed': 63, 'gamma': 0.3, 'eps': 0.08, 'delta': 0.05}\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"best_cn2\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"row_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          83,\n          53,\n          70\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"article\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"FA chairman Greg Dyke has called for a public debate between the four candidates for the FIFA presidency and offered to host the event at Wembley. Dyke made his wish for a TV inquisition known during an awkward weekend in Sepp Blatter's company at the rules-deciding IFAB meeting in Belfast. The FA have made clear their strong opposition to the FIFA president serving a fifth term. Meanwhile, in a separate move, Sky and the BBC have made a joint approach to Blatter and rivals Prince Ali of Jordan, Luis Figo and Michael van Praag to stage such an event. FA chairman Greg Dyke has called for a public debate between the four candidates for the FIFA presidency . The TV networks - through leading sports news broadcasters Paul Kelso and Richard Conway - have written to the contestants outlining their ambitious proposals for a live, hour-long debate in the UK with an audience of fans representing all 209 FIFA member nations. This fans' congress would provide questions, with others drawn from football supporters via the Sky and BBC websites, and Facebook, to ensure maximum interaction. All four challengers have been promised equal time and emphasis to present their manifestos and visions for FIFA's future. Blatter has yet to respond to the Sky-BBC letter but it is highly unlikely he'll agree to such public exposure. His three opponents will all be in favour. Sepp Blatter has yet to respond to the Sky-BBC letter but it is unlikely he'll agree to such public exposure . The official photo of the IFAB summit shows Greg Dyke seated between, of all people, Sepp Blatter and Thailand's Worawi Makudi, whose presence at Belfast's Cullodon Hotel as a representative of the Asian Football Confederation was bizarre in the extreme. Makudi has an acrimonious history with the FA and is under investigation by FIFA's ethics committee for breaching World Cup bid rules. It's understood Makudi played no part in the rules debate. FIFA said Makudi was selected by the AFC to attend and is innocent of any code violations until found guilty. The home nations are in disagreement as to whose turn it is to take the FIFA British vice-presidency . The 2011 gentlemen's agreement between the four home nations, which declares it is Wales's turn to take the FIFA British vice-presidency, continues to cause ructions. The three other countries believe the agreement was nullified by a new statute that has all UEFA associations involved in the vote at the Congress later this month. But Welsh president Trefor Lloyd-Hughes says he has a signed contract in his possession that declares no change can be made in the rotation pledge unless there is unanimous agreement. Meanwhile, England's David Gill, persuaded by UEFA president Michel Platini to stand for that FIFA place against Lloyd-Hughes, has written to all UEFA countries outlining his plans. To spice up their battle, Wales are still annoyed that a UEFA ExCo, including Gill, voted for Hampden Park over the Millennium Stadium as one of 13 venues to stage Euro 2020. It's believed Gill rated the Cardiff venue higher. General secretary Alex Horne left the FA at the end of January but has not yet been replaced . Greg Dyke and David Gill, who are on the nominations panel choosing the FA's next chief executive, were coy over the weekend about how far they have progressed in finding general secretary Alex Horne's replacement. However, it's understood the selection has been made after final interviews last week and an announcement is imminent. Zimbabwe's Sean Williams celebrates taking the wicket of Umar Akmal during their World Cup match . BBC ON STICKY WICKET . BBC Sport's live text reporting of Pakistan's World Cup victory over Zimbabwe seems to have relied heavily on rival website Cricinfo. Umar Akmal's dismissal, bowled by Sean Williams, was described in the following way on Cricinfo: 'Akmal made it look an even better delivery than it actually was, because he had moved inside the line and was trying to push that inside-out through cover.' The licence-fee-funded BBC, who had just changed commentators, wrote: 'Akmal made it look an even better delivery than it actually was, because he had moved inside the line and was trying to push that inside out through cover.' Exactly the same. A BBC spokeswoman said: 'The line should have been credited. This was a simple human error.' The post-11pm peak viewing audience of two million on ITV for the Carl Frampton fight on Saturday has left the network intent on showing more live boxing. Television network ITV are determined to show more live boxing after seeing recent viewing figures .\",\n          \"Following Sunday's El Clasico spectacle that hugely intensified the run-in for the La Liga title between Barcelona and Real Madrid, Rio Ferdinand gave his stance on the age-old debate of who is better: Cristano Ronaldo or Lionel Messi? Choosing to stay somewhat impartial to the discussion the former England international says we should stand back and appreciate the sheer brilliance we are witnessing in the presence of two of the world's greatest ever players. 'It's inevitable we do all this \\\"who's better? Blah, blah, blah\\\" but we should just be enjoying being around these two players and seeing it,' the QPR defender said. Rio Ferdinand (right) speaks to the camera alongside Queen's Park Rangers team-mate Bobby Zamora . Lionel Messi is La Liga's top scorer this season with 32 goals but Cristiano Ronaldo is not far behind . The Real Madrid striker's goal in El Clasico on Sunday took him to just one goal behind the Barcelona forward . 'When Ronaldo, the Brazilian one - who should have been the best if he weren't injured - was scoring 30 goals a season [everybody said]: \\\"Oh he's a genius, he's the best ever.\\\"' Messi and Ronaldo are this season's top scorers in La Liga, with 32 and 31 goals respectively with 10 games to go. Speaking from his car, kitted out with an X-Box and television screen, the former Manchester United star posted an image on Instagram of himself and Hoops team-mate Bobby Zamora. The two were taking each other on in an in-car game of FIFA whilst travelling to training at the Imperial College Sports Ground, London. The 2008 Champions League winner wrote: 'En route to training.. Winning at FIFA #shock' after beating the former Fulham striker 2-1 with Paris Saint Germain. The former England international raises his finger in victory after beating Zamora in an in-car game of FIFA .\",\n          \"It is a measure of Arsenal's progress that fixtures like these no longer end in boos and angry recrimination. As the final whistle approached, the crowd at the Emirates were unusually vocal, anticipating trips to Wembley and singing praise of their team. Earlier it had the feel of one of the afternoons with which we have become so familiar in recent years, one of those days when Arsenal break world records for possessions stats but come up against an inspired goalkeeper and end the day with fans furious at their manager, the board of directors and life in general. But with Olivier Giroud in exceptional form, Aaron Ramsey also outstanding and Mesut Ozil thriving against the kind of opposition he relishes, Arsenal eventually cantered to victory in some style. Olivier Giroud leaps in the air after opening the scoring in Arsenal's 3-0 Premier League victory over West Ham . The France international gives Arsenal the lead on the stroke of half time with his sixth goal in his last seven games . Aaron Ramsey is chased by Santi Cazorla after doubling Arsenal's lead in the closing stages of the second half . Giroud gives substitute Mathieu Flamini a pat on the back after the Frenchman seals victory with a late strike . ARSENAL: Ospina 6.5, Chambers 7, Mertesacker 6.5, Koscielny 6.5, Monreal 7, Ramsey 7, Coquelin 7, Walcott 6 (Cazorla 6.5), Ozil 7 (Flamini 6.5), Sanchez 6 (Welbeck 6.5), Giroud 8.5 . Subs: Szczesny, Gibbs, Akpom, Bellerin . Scorers: Giroud 45, Ramsey 81, Flamini 84 . Booked: Sanchez . WEST HAM: Arian 8, O'Brien 6.5, Kouyate 6.5, Cresswell 7, Downing 6, Noble 6 (Nene 6), Song 6.5, Nolan 6, Jarvis 6.5 (Amalfitano 6), Sakho 6.5 . Subs: Demel, J\\u00e4\\u00e4skel\\u00e4inen, Poyet, Cullen, Onariase . Booked: Sakho . Man of the Match: Olivier Giroud . Referee: Chris Foy/ Anthony Taylor . CLICK HERE for all the stats, including Olivier Giroud's heat map from our superb Match Zone . Better measures of their real progress remain, of course. On Tuesday night they visit Monaco where you might anticipate another glorious comeback which ultimately ends in failure. Then there will be the league table at the end of the season which will likely tell the tale of an opportunity missed to challenge for the title. Arsene Wenger conceded that, even with Chelsea still to come to the Emirates, Arsenal are not yet in the title race. 'Not at the moment,' he said. 'But we can just keep going. We won eight of the last nine. We are stronger today than we were at the start of the season. 'We suffered a lot from the World Cup, where the players came back. What is for sure is that they understand each other much better than six or seven months ago and that makes everyone more dangerous.' At least Arsenal are not going backwards these days; the problem may be that they aren't moving forwards fast enough but they are at least pointing in the right direction. West Ham defender James Collins (left) was fortunate not to concede an early penalty after brining down Theo Walcott . The Arsenal winger crashes to the turf under Collins' challenge but was not awarded a spot kick . West Ham forward Diafra Sakho (centre) tries to evade Arsenal defenders Laurent Koscielny and Per Mertesacker (left) Arsenal playmaker Mesut Ozil (left) uses his silky skills to bring the ball down in front of Stewart Downing (centre) and Mark Noble . Arsenal's leading scorer Alexis Sanchez (centre) uses his trickery to escape the attentions of Joey O'Brien . Hammers keeper Adrian threatened to spoil Arsenal's afternoon with a succession of fine first half saves . West Ham manager Sam Allardyce looked as though he understood as much and had settled for one of those afternoons when he aims to 'out-tactic' the opposition. You could hardly blame him given the resources at his disposal, with Enner Valencia and James Tomkins the latest additions to his injury list and Carl Jenkinson ineligible. Kevin Nolan almost shocked Arsenal with a clean strike on 23 minutes but at half-time, Arsenal had 74 per cent of the possession. At times it looked as if West Ham goalkeeper Adrian was has having one of those inspired afternoon. It was later revealed he was playing with a dislocated finger sustained in the warm up, a fact which made his performance truly heroic. 'He's been a brave lad,' said assistant manager Neil McDonald. Theo Walcott was having a less satisfactory time. Thrust back in for his first start in more than a month, he did little to help his ongoing contract talks. Three times he was presented with the opportunity to open the scoring; three times he spurned the chances. Gunners defender Calum Chambers closes down West Ham winger Matt Jarvis as Arsenal take control . Song and Koscielny compete for the ball in an aerial duel as the sun shines at the Emirates Stadium . West Ham manager Sam Allardyce lets his feelings of frustration be known from the sidelines . He could plead mitigation for the first effort on six minutes. A delightful back heel from Giroud set him up with just Adrian to beat, but James Collins came through the man and the ball to prevent him scoring. It might have been a penalty but it was Walcott's own hesitation in front of goal which had presented Collins with the chance to launch his saving tackle. Adrian would then make excellent saves from Alexis Sanchez's header on 16 minutes and from an improvised touch of the knee from Ramsey on 32 minutes, before Walcott received his next chance. Played in by a delightful Ramsey pass, he took a touch, looked up and struck his shot directly at the goalkeeper.Adrian denied Sanchez from a half volley and on 44 minutes he pushed away a strike from Ozil, which landed at Walcott's feet, yards from goal but he skewed the ball high and wide, appearing to lose his balance. Ozil reacts after West Ham keeper Adrian pulls off one of a succession of first half saves . Frenchman Giroud gets his shot away despite being surrounded by four West Ham players . The Gunners centre forward watches as the ball sails towards the left corner of West Ham's goal . Adrian, who had previously kept West Ham in the game with a series of fine saves, dives in vain towards Giroud's shot . Giroud was on hand to provide deliverance just before half-time. Ramsey and Ozil exchanged a delicate one-two, but such is the Frenchman's confidence these days, he nicked it off Ramsey's foot, took a touch and drove a fine finish into the far corner of the net. 'It was fantastic because of the combination and the finishing. It was the kind of goal we loved to score,' said Wenger. It was his 14th goal of the season in 24 games. That Monaco game aside, he is having an excellent season and since that dismal night, he has responded incredibly well. After being criticised for missing chances against Monaco, Giroud holds his hands to his ears to soak up the applause . Giroud strikes a pose midway through his celebration as Arsenal take the lead at the Emirates . Walcott, who had several chances to open the first half scoring himself, leaps on Giroud's back in celebration . 'It's one of his strengths,' said Wenger, who called his performance 'outstanding'. Having set out to defend in numbers, a change of strategy was necessary for West Ham and they were equal to the challenge. They came out in the second-half with an entirely different mindset, pushing Arsenal back into their half and searching for an equaliser. In terms of clear-cut chances there was still little - Matt Jarvis' curling cross which Ospina spilled and almost let in Sakho was one unsettling moment. But Arsenal were no longer able to play as they wished. Still, their time would come. On 80 minutes, Giroud dummied a throw-in to let in Ramsey, who played in the Frenchman. He returned the ball to Ramsey, who drove it past Adrian to cap a fine a performance of growing authority. Three minutes later Santi Cazorla, on as a substitute, exchanged passes with Giroud and planted a cross almost on the foot of Mathieu Flamini, who had the simplest task of converting from close range for the third goal. Ramsey (right) gets some air as he battles for the ball with former Arsenal team-mate Song . West Ham manager Sam Allardyce looks on bemused as referee Chris Foy is forced to hand over to assistant Anthony Taylor . Former Manchester United forward Danny Welbeck (right) was brought on in the second half to replace Sanchez . Arsenal manager Arsene Wenger watches impatiently from the touchline with his side only leading by one goal . Wales midfielder Ramsey meets Giroud's pass with a left footed shot to score his first goal since December . Ramsey wheels away to celebrate giving Arsenal a 2-0 lead on 81minutes of the London derby . Aaron Ramsey is swamped by team-mates Giroud and Danny Welbeck after doubling Arsenal's lead . Substitute Mathieu Flamini is in the right place at the right time as he meets Cazorla's cross to seal a 3-0 victory . Arsenal team-mates embrace Flamini as they keep alive their hopes of playing Champions League football next season .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Greg Dyke wants a public debate between the four candidates for the FIFA presidency and has offered to host it at Wembley Stadium . BBC and Sky are also keen to host a debate between the candidates . The home nations are in disagreement as to whose turn it is to take the FIFA British vice-presidency . BBC appear to have plagiarised Cricinfo in an online live text report .\",\n          \"Rio Ferdinand played FIFA 15 and talked Lionel Messi vs Cristiano Ronaldo . The QPR defender said we should appreciate the talents of the magical duo . Ferdinand spoke from his car complete with an X-Box and multiple screens .\",\n          \"Arsenal kept up their chase for a top four finish in the Premier League with a 3-0 win over West Ham at the Emirates . Olivier Giroud netted his sixth goal in seven games to open the scoring for Gunners on the stroke of half time . Referee Chris Foy was replaced by assistant Anthony Taylor midway through the second half . Wales midfielder Aaron Ramsey was assisted by Giroud to double the lead with his first goal since December . Substitute Mathieu Flamini sealed victory after coming on to meet a cross from Santi Cazorla .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"FA chairman Greg Dyke has called for a public debate between the four candidates for the FIFA presidency. The TV networks have written to the contestants outlining their ambitious proposals for a live, hour-long debate. The FA have made a joint approach to Blatter and rivals Prince Ali of Jordan, Luis Figo and Michael van Praag to stage such an event.\",\n          \"The former England international says we should stand back and appreciate the sheer brilliance we are witnessing in the presence of two of the world's greatest ever players. The former Manchester United star posted an image on Instagram of himself and Hoops team-mate Bobby Zamora. The two were taking each other on in an in-car game of FIFA. The pair were taking part in the game while travelling to training at the Imperial College Sports Ground.\",\n          \"Olivier Giroud opened the scoring in Arsenal's 3-0 Premier League victory over West Ham. The France international gave Arsenal the lead on the stroke of half time with his sixth goal in his last seven games. Aaron Ramsey was chased by Santi Cazorla after doubling Arsenal's lead in the closing stages of the game.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_k\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 9,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          3,\n          0,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"base_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0042039008407752775,\n        \"min\": -0.014469360001385212,\n        \"max\": 0.007983306422829628,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          -0.002708881162106991,\n          -0.003835257375612855,\n          -0.006206172052770853\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"len\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 83,\n        \"max\": 100,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          97,\n          91,\n          100\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contiguity_lcs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11,\n        \"min\": 21,\n        \"max\": 85,\n        \"num_unique_values\": 39,\n        \"samples\": [\n          56,\n          85,\n          72\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_span_len_lcs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9.33794491276796,\n        \"min\": 5.2,\n        \"max\": 57.0,\n        \"num_unique_values\": 78,\n        \"samples\": [\n          11.666666666666666,\n          33.5,\n          11.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"unsupported\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"new_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.9858935605405876,\n        \"min\": 6.634553063999862,\n        \"max\": 28.5598134902562,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          13.439716032095328,\n          21.55961647426244,\n          15.039379382794722\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "best_cn2"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-c80c9a03-695a-46a9-826c-c4644cd4531a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>row_id</th>\n",
              "      <th>article</th>\n",
              "      <th>reference</th>\n",
              "      <th>summary</th>\n",
              "      <th>best_k</th>\n",
              "      <th>base_score</th>\n",
              "      <th>len</th>\n",
              "      <th>contiguity_lcs</th>\n",
              "      <th>avg_span_len_lcs</th>\n",
              "      <th>unsupported</th>\n",
              "      <th>new_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>These days the Cheltenham Festival is a marath...</td>\n",
              "      <td>Willie Mullins looks to have chance of establi...</td>\n",
              "      <td>The Cheltenham Festival is a marathon not a sp...</td>\n",
              "      <td>4</td>\n",
              "      <td>0.003095</td>\n",
              "      <td>100</td>\n",
              "      <td>65</td>\n",
              "      <td>33.500000</td>\n",
              "      <td>0</td>\n",
              "      <td>22.100309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>A baby girl was found alive after being strapp...</td>\n",
              "      <td>Lynn Jennifer Groesbeck, 25, was driving home ...</td>\n",
              "      <td>The baby girl was found alive after being stra...</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.005904</td>\n",
              "      <td>100</td>\n",
              "      <td>48</td>\n",
              "      <td>10.600000</td>\n",
              "      <td>0</td>\n",
              "      <td>15.167410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>A 36-year-old woman and her three children wer...</td>\n",
              "      <td>40-year-old Domingo Villa Arellano, who is an ...</td>\n",
              "      <td>Domingo Villa Arellano, who is an ex-cop, alle...</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.003815</td>\n",
              "      <td>100</td>\n",
              "      <td>64</td>\n",
              "      <td>22.333333</td>\n",
              "      <td>1</td>\n",
              "      <td>20.856285</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c80c9a03-695a-46a9-826c-c4644cd4531a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c80c9a03-695a-46a9-826c-c4644cd4531a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c80c9a03-695a-46a9-826c-c4644cd4531a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-daee8572-1a26-4959-8741-0a93e9177730\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-daee8572-1a26-4959-8741-0a93e9177730')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-daee8572-1a26-4959-8741-0a93e9177730 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   row_id                                            article  \\\n",
              "0       0  These days the Cheltenham Festival is a marath...   \n",
              "1       1  A baby girl was found alive after being strapp...   \n",
              "2       2  A 36-year-old woman and her three children wer...   \n",
              "\n",
              "                                           reference  \\\n",
              "0  Willie Mullins looks to have chance of establi...   \n",
              "1  Lynn Jennifer Groesbeck, 25, was driving home ...   \n",
              "2  40-year-old Domingo Villa Arellano, who is an ...   \n",
              "\n",
              "                                             summary  best_k  base_score  len  \\\n",
              "0  The Cheltenham Festival is a marathon not a sp...       4    0.003095  100   \n",
              "1  The baby girl was found alive after being stra...       0   -0.005904  100   \n",
              "2  Domingo Villa Arellano, who is an ex-cop, alle...       1   -0.003815  100   \n",
              "\n",
              "   contiguity_lcs  avg_span_len_lcs  unsupported  new_score  \n",
              "0              65         33.500000            0  22.100309  \n",
              "1              48         10.600000            0  15.167410  \n",
              "2              64         22.333333            1  20.856285  "
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# === Offline rerank for CopyNext2: LCS contiguity + unsupported penalty ===\n",
        "dd = details_cn2.copy()\n",
        "\n",
        "# LCS span stats\n",
        "contigs, avlens = [], []\n",
        "for i,row in dd.iterrows():\n",
        "    c,a = lcs_span_stats(row[\"article\"], row[\"summary\"])\n",
        "    contigs.append(c); avlens.append(a)\n",
        "dd[\"contiguity_lcs\"] = contigs\n",
        "dd[\"avg_span_len_lcs\"] = avlens\n",
        "\n",
        "# Unsupported entities vs source\n",
        "unsupp = []\n",
        "for i,row in tqdm(list(dd.iterrows()), total=len(dd), desc=\"entities(vs source)\"):\n",
        "    unsupp.append(unsupported_count(row[\"article\"], row[\"summary\"]))\n",
        "dd[\"unsupported\"] = unsupp\n",
        "\n",
        "# Scoring: base / sqrt(len) + γ*contig + ε*(avg_len-1) − δ*unsupported\n",
        "GAMMA, EPS, DELTA = 0.30, 0.08, 0.05\n",
        "dd[\"len\"] = dd[\"len\"].clip(lower=1)\n",
        "dd[\"new_score\"] = (dd[\"base_score\"] / (dd[\"len\"]**0.5)) + \\\n",
        "                  (GAMMA * dd[\"contiguity_lcs\"]) + \\\n",
        "                  (EPS   * (dd[\"avg_span_len_lcs\"] - 1.0)) - \\\n",
        "                  (DELTA * dd[\"unsupported\"])\n",
        "\n",
        "best_cn2 = (\n",
        "    dd.sort_values([\"row_id\",\"new_score\"], ascending=[True, False])\n",
        "      .groupby(\"row_id\", as_index=False).head(1)\n",
        "      .loc[:, [\"row_id\",\"article\",\"reference\",\"summary\",\"candidate_k\",\n",
        "               \"base_score\",\"len\",\"contiguity_lcs\",\"avg_span_len_lcs\",\"unsupported\",\"new_score\"]]\n",
        "      .rename(columns={\"candidate_k\":\"best_k\"})\n",
        "      .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "save_csv(dd,        DETAILS_CN2)   # overwrite with added columns\n",
        "save_csv(best_cn2,  BEST_CN2)\n",
        "\n",
        "changed = int((best_cn2[\"best_k\"] != 0).sum())\n",
        "print({\"changed\": changed, \"gamma\":GAMMA, \"eps\":EPS, \"delta\":DELTA})\n",
        "best_cn2.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGQ8xWAgK8zd",
        "outputId": "f631320e-b00e-4cdd-ee5c-f00b2f589443"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE TOP : {'r1_f': 0.4373, 'r2_f': 0.2025, 'rl_f': 0.2964}\n",
            "ROUGE CN2 : {'r1_f': 0.4204, 'r2_f': 0.1884, 'rl_f': 0.2848}\n",
            "Δ ROUGE   : {'r1': -0.0168, 'r2': -0.0141, 'rl': -0.0116}\n",
            "ENT TOP : {'TP': 272, 'FP': 263, 'FN': 278, 'entP': 0.5084, 'entR': 0.4945, 'entF1': 0.5014}\n",
            "ENT CN2 : {'TP': 270, 'FP': 286, 'FN': 280, 'entP': 0.4856, 'entR': 0.4909, 'entF1': 0.4882}\n",
            "Δ ENT   : {'entP': -0.0228, 'entR': -0.0036, 'entF1': -0.0131}\n"
          ]
        }
      ],
      "source": [
        "#1000\n",
        "# === ROUGE & spaCy entity vs reference: top-beam (k=0) vs CopyNext2 best ===\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Build eval frame aligning TOP(k=0) and BEST per row\n",
        "top_k0 = details_cn2[details_cn2[\"candidate_k\"]==0][[\"row_id\",\"summary\"]].rename(columns={\"summary\":\"top_beam_summary\"})\n",
        "ev = best_cn2.merge(top_k0, on=\"row_id\", how=\"left\").rename(columns={\"summary\":\"cn2_summary\"})\n",
        "\n",
        "refs  = ev[\"reference\"].astype(str).tolist()\n",
        "tops  = ev[\"top_beam_summary\"].astype(str).tolist()\n",
        "bests = ev[\"cn2_summary\"].astype(str).tolist()\n",
        "\n",
        "# ROUGE\n",
        "sc = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\"], use_stemmer=True)\n",
        "def _mean(refs, hyps):\n",
        "    r1=r2=rl=0.0\n",
        "    for r,h in zip(refs, hyps):\n",
        "        s=sc.score(r,h); r1+=s[\"rouge1\"].fmeasure; r2+=s[\"rouge2\"].fmeasure; rl+=s[\"rougeLsum\"].fmeasure\n",
        "    n=len(refs); return r1/n, r2/n, rl/n\n",
        "r1t,r2t,rlt = _mean(refs, tops)\n",
        "r1b,r2b,rlb = _mean(refs, bests)\n",
        "print(\"ROUGE TOP :\", {\"r1_f\":round(r1t,4), \"r2_f\":round(r2t,4), \"rl_f\":round(rlt,4)})\n",
        "print(\"ROUGE CN2 :\", {\"r1_f\":round(r1b,4), \"r2_f\":round(r2b,4), \"rl_f\":round(rlb,4)})\n",
        "print(\"Δ ROUGE   :\", {\"r1\":round(r1b-r1t,4), \"r2\":round(r2b-r2t,4), \"rl\":round(rlb-rlt,4)})\n",
        "\n",
        "# Entity vs references (overall)\n",
        "def ents_norm_list(text):\n",
        "    doc = nlp(text)\n",
        "    def norm(t):\n",
        "        t=t.lower().strip()\n",
        "        t=re.sub(r\"(?<=\\d),(?=\\d{3}\\b)\",\"\",t).replace(\"%\",\" percent\")\n",
        "        t=re.sub(r\"\\s+\",\" \",t); return t\n",
        "    return [(norm(ent.text), ent.label_) for ent in doc.ents]\n",
        "\n",
        "def entPRF(refs, hyps):\n",
        "    ref_all = [t for r in refs for (t,_) in ents_norm_list(r)]\n",
        "    hyp_all = [t for h in hyps for (t,_) in ents_norm_list(h)]\n",
        "    ref_set, hyp_set = set((t,) for t in ref_all), set((t,) for t in hyp_all)\n",
        "    tp = len(ref_set & hyp_set); fp = len(hyp_set - ref_set); fn = len(ref_set - hyp_set)\n",
        "    p = tp/(tp+fp) if tp+fp else 0.0; r = tp/(tp+fn) if tp+fn else 0.0; f = 2*p*r/(p+r) if p+r else 0.0\n",
        "    return dict(TP=tp, FP=fp, FN=fn, entP=p, entR=r, entF1=f)\n",
        "\n",
        "ent_top  = entPRF(refs, tops)\n",
        "ent_cn2  = entPRF(refs, bests)\n",
        "print(\"ENT TOP :\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in ent_top.items()})\n",
        "print(\"ENT CN2 :\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in ent_cn2.items()})\n",
        "print(\"Δ ENT   :\", {k:round(ent_cn2[k]-ent_top[k],4) for k in [\"entP\",\"entR\",\"entF1\"]})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv61z45y5t9r"
      },
      "source": [
        "# CopyNext Bias smarter Gate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjkTbEXV6DRY"
      },
      "outputs": [],
      "source": [
        "# === CopyNext3: setup + gated online bias + helpers ===\n",
        "!pip install -q -U transformers pandas tqdm rouge-score spacy\n",
        "\n",
        "import os, re, csv, math, time, torch, pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "from difflib import SequenceMatcher\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers.generation.logits_process import LogitsProcessor, LogitsProcessorList\n",
        "\n",
        "# ---- Paths ----\n",
        "BASE_DIR   = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000\"\n",
        "OUT_PARENT = os.path.join(BASE_DIR, \"decodes_spanbias\")\n",
        "OUT_DIR    = os.path.join(OUT_PARENT, \"decodes_copynext3\")\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "DETAILS_CN3 = os.path.join(OUT_DIR, f\"copynext3_details_{STAMP}.csv\")\n",
        "BEST_CN3    = os.path.join(OUT_DIR, f\"copynext3_best_{STAMP}.csv\")\n",
        "\n",
        "# ---- Model ----\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "try:\n",
        "    tok, model\n",
        "except NameError:\n",
        "    tok = AutoTokenizer.from_pretrained(BASE_DIR, use_fast=True)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(BASE_DIR).to(DEVICE).eval()\n",
        "\n",
        "# ---- Decode args (diverse beams so reranker has real choices) ----\n",
        "DECODE_ARGS_CN3 = dict(\n",
        "    num_beams=10,\n",
        "    num_return_sequences=10,\n",
        "    num_beam_groups=5,\n",
        "    diversity_penalty=0.3,\n",
        "    max_new_tokens=44,\n",
        "    min_new_tokens=18,\n",
        "    no_repeat_ngram_size=4,\n",
        "    length_penalty=0.75,\n",
        "    early_stopping=True,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,\n",
        ")\n",
        "MAX_SRC_LEN_CN3 = 800  # bump to 1024 if your GPU allows\n",
        "\n",
        "# ---- Gated CopyNext logits processor (online) ----\n",
        "_wordish = re.compile(r\"^[A-Za-z0-9]+$\")\n",
        "\n",
        "class SpanContinuationProcessorV2(LogitsProcessor):\n",
        "    \"\"\"\n",
        "    Training-free CopyNext-style bias with guards:\n",
        "      • only continue if hyp suffix matches a contiguous source slice\n",
        "      • avoid continuing across punctuation / specials\n",
        "      • cap span length\n",
        "    \"\"\"\n",
        "    def __init__(self, src_input_ids, tokenizer, num_beams, gamma=0.6, max_span=5):\n",
        "        self.src_ids = [ids.tolist() if torch.is_tensor(ids) else list(ids) for ids in src_input_ids]\n",
        "        self.num_beams = int(num_beams)\n",
        "        self.gamma = float(gamma)\n",
        "        self.max_span = int(max_span)\n",
        "        self.tok = tokenizer\n",
        "        self.skip_ids = set(tokenizer.all_special_ids)\n",
        "\n",
        "    def _is_wordish_id(self, tid):\n",
        "        if tid in self.skip_ids: return False\n",
        "        tok = self.tok.convert_ids_to_tokens(int(tid)).replace(\"Ġ\",\"\")\n",
        "        return bool(_wordish.match(tok)) or any(ch.isdigit() for ch in tok)\n",
        "\n",
        "    @staticmethod\n",
        "    def _find_suffix(src, hyp, max_span):\n",
        "        if not hyp: return 0, -1\n",
        "        max_check = min(len(hyp), max_span)\n",
        "        for L in range(max_check, 0, -1):\n",
        "            suf = hyp[-L:]\n",
        "            for end in range(L-1, len(src)):\n",
        "                if src[end-L+1:end+1] == suf:\n",
        "                    return L, end\n",
        "        return 0, -1\n",
        "\n",
        "    def __call__(self, input_ids, scores):\n",
        "        rows = input_ids.size(0)\n",
        "        for row in range(rows):\n",
        "            sample = row // self.num_beams\n",
        "            src = self.src_ids[sample]\n",
        "            hyp = input_ids[row].tolist()\n",
        "            if not hyp or not self._is_wordish_id(hyp[-1]):  # don't continue over punctuation\n",
        "                continue\n",
        "            L, end = self._find_suffix(src, hyp, self.max_span)\n",
        "            if L >= 1 and end + 1 < len(src):\n",
        "                next_id = src[end + 1]\n",
        "                if self._is_wordish_id(next_id):\n",
        "                    scores[row, next_id] += self.gamma\n",
        "        return scores\n",
        "\n",
        "# ---- LCS (contiguous) helpers for rerank ----\n",
        "_tok_re = re.compile(r\"[A-Za-z0-9%]+(?:'[A-Za-z0-9]+)?\")\n",
        "def _norm_tokens(t):\n",
        "    t = t.lower()\n",
        "    t = re.sub(r\"(?<=\\d),(?=\\d{3}\\b)\", \"\", t)\n",
        "    t = t.replace(\"%\", \" percent \")\n",
        "    return _tok_re.findall(t)\n",
        "\n",
        "def lcs_blocks(src, hyp, min_len=2):\n",
        "    a=_norm_tokens(src); b=_norm_tokens(hyp)\n",
        "    return [blk.size for blk in SequenceMatcher(a=a,b=b,autojunk=False).get_matching_blocks() if blk.size>=min_len]\n",
        "\n",
        "def bounded_span_reward(runs, L_lo=2, L_hi=5, L_pen=7, pen_w=0.5):\n",
        "    # reward spans in [2..5]; clip longer spans and lightly penalize >7 to avoid long quotes\n",
        "    reward = 0.0\n",
        "    for L in runs:\n",
        "        if L <= L_hi: reward += (L - 1)\n",
        "        else:         reward += (L_hi - 1) - pen_w*max(0, L - L_pen)\n",
        "    return reward\n",
        "\n",
        "# ---- spaCy unsupported-entity penalty (vs source) ----\n",
        "import spacy\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"lemmatizer\",\"textcat\"])\n",
        "except OSError:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"], check=True)\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"lemmatizer\",\"textcat\"])\n",
        "nlp.max_length = 2_000_000\n",
        "\n",
        "def ents_norm(txt):\n",
        "    doc = nlp(txt)\n",
        "    def norm(u):\n",
        "        u=u.lower().strip()\n",
        "        u=re.sub(r\"(?<=\\d),(?=\\d{3}\\b)\",\"\",u).replace(\"%\",\" percent\")\n",
        "        u=re.sub(r\"\\s+\",\" \",u)\n",
        "        return u\n",
        "    return set((norm(ent.text), ent.label_) for ent in doc.ents)\n",
        "\n",
        "def unsupported(src, hyp):\n",
        "    S=ents_norm(src); H=ents_norm(hyp); return len(H - S)\n",
        "\n",
        "# ---- safe saver ----\n",
        "def save_csv(df, path):\n",
        "    tmp = path + \".tmp\"\n",
        "    df.to_csv(tmp, index=False, quoting=csv.QUOTE_ALL, escapechar=\"\\\\\",\n",
        "              lineterminator=\"\\n\", encoding=\"utf-8\")\n",
        "    _ = pd.read_csv(tmp, engine=\"python\", encoding=\"utf-8\")\n",
        "    os.replace(tmp, path)\n",
        "    print(f\"[saved] {path} ({len(df)} rows)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "803e3ffc4b4246c7b08bc7496f7f1381"
          ]
        },
        "id": "gAgjEeTU6ETZ",
        "outputId": "1dbda1f2-aeee-4526-8dbc-4f672fc0a148"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "803e3ffc4b4246c7b08bc7496f7f1381",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "CopyNext3 generate:   0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[saved] /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/decodes_spanbias/decodes_copynext3/copynext3_details_20250826_011005.csv (1000 rows)\n"
          ]
        }
      ],
      "source": [
        "# === Generate 100 with CopyNext3 online bias ===\n",
        "import numpy as np\n",
        "\n",
        "# Use your existing work_df if present, else sample deterministically from VAL_CSV\n",
        "try:\n",
        "    work_df\n",
        "    cn3_df = work_df.copy()\n",
        "except NameError:\n",
        "    VAL_CSV = \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "    assert os.path.exists(VAL_CSV), f\"VAL_CSV not found: {VAL_CSV}\"\n",
        "    val_df = pd.read_csv(VAL_CSV, engine=\"python\", encoding=\"utf-8\")\n",
        "    cn3_df = val_df.sample(n=100, random_state=3).reset_index(drop=True)\n",
        "\n",
        "N = min(100, len(cn3_df))\n",
        "articles   = cn3_df.loc[:N-1, \"article\"].astype(str).tolist()\n",
        "references = cn3_df.loc[:N-1, \"highlights\"].astype(str).tolist()\n",
        "\n",
        "details_rows = []\n",
        "BATCH = 4\n",
        "K = DECODE_ARGS_CN3[\"num_return_sequences\"]\n",
        "\n",
        "for start in tqdm(range(0, N, BATCH), desc=\"CopyNext3 generate\"):\n",
        "    batch_arts = articles[start:start+BATCH]\n",
        "    batch_refs = references[start:start+BATCH]\n",
        "    enc = tok(batch_arts, return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_SRC_LEN_CN3).to(DEVICE)\n",
        "\n",
        "    proc = SpanContinuationProcessorV2(\n",
        "        src_input_ids=enc[\"input_ids\"], tokenizer=tok,\n",
        "        num_beams=DECODE_ARGS_CN3[\"num_beams\"], gamma=0.6, max_span=5\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**enc, **DECODE_ARGS_CN3, logits_processor=LogitsProcessorList([proc]))\n",
        "\n",
        "    seqs = out.sequences.view(len(batch_arts), K, -1)\n",
        "    base = out.sequences_scores.view(len(batch_arts), K)\n",
        "\n",
        "    for bi in range(len(batch_arts)):\n",
        "        art = batch_arts[bi]; ref = batch_refs[bi]\n",
        "        for k in range(K):\n",
        "            ids = seqs[bi, k]\n",
        "            summ = tok.decode(ids, skip_special_tokens=True)\n",
        "            tgt_len = max(int(ids.size(0) - 1), 1)\n",
        "            details_rows.append({\n",
        "                \"row_id\": start + bi,\n",
        "                \"candidate_k\": k,\n",
        "                \"article\": art,\n",
        "                \"reference\": ref,\n",
        "                \"summary\": summ,\n",
        "                \"base_score\": float(base[bi, k].item()),\n",
        "                \"len\": tgt_len\n",
        "            })\n",
        "\n",
        "details_cn3 = pd.DataFrame(details_rows)\n",
        "save_csv(details_cn3, DETAILS_CN3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509,
          "referenced_widgets": [
            "f683c639ebfe4084a704081c160e215a"
          ]
        },
        "id": "mmn4Uu4J7TaD",
        "outputId": "25734fe2-3299-458e-8042-891736684cdd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f683c639ebfe4084a704081c160e215a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "entities(vs source):   0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[saved] /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/decodes_spanbias/decodes_copynext3/copynext3_details_20250826_011005.csv (1000 rows)\n",
            "[saved] /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/decodes_spanbias/decodes_copynext3/copynext3_best_20250826_011005.csv (100 rows)\n",
            "{'changed': 86, 'gamma': 0.18, 'delta': 0.03}\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"best_cn3\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"row_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          83,\n          53,\n          70\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"article\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"FA chairman Greg Dyke has called for a public debate between the four candidates for the FIFA presidency and offered to host the event at Wembley. Dyke made his wish for a TV inquisition known during an awkward weekend in Sepp Blatter's company at the rules-deciding IFAB meeting in Belfast. The FA have made clear their strong opposition to the FIFA president serving a fifth term. Meanwhile, in a separate move, Sky and the BBC have made a joint approach to Blatter and rivals Prince Ali of Jordan, Luis Figo and Michael van Praag to stage such an event. FA chairman Greg Dyke has called for a public debate between the four candidates for the FIFA presidency . The TV networks - through leading sports news broadcasters Paul Kelso and Richard Conway - have written to the contestants outlining their ambitious proposals for a live, hour-long debate in the UK with an audience of fans representing all 209 FIFA member nations. This fans' congress would provide questions, with others drawn from football supporters via the Sky and BBC websites, and Facebook, to ensure maximum interaction. All four challengers have been promised equal time and emphasis to present their manifestos and visions for FIFA's future. Blatter has yet to respond to the Sky-BBC letter but it is highly unlikely he'll agree to such public exposure. His three opponents will all be in favour. Sepp Blatter has yet to respond to the Sky-BBC letter but it is unlikely he'll agree to such public exposure . The official photo of the IFAB summit shows Greg Dyke seated between, of all people, Sepp Blatter and Thailand's Worawi Makudi, whose presence at Belfast's Cullodon Hotel as a representative of the Asian Football Confederation was bizarre in the extreme. Makudi has an acrimonious history with the FA and is under investigation by FIFA's ethics committee for breaching World Cup bid rules. It's understood Makudi played no part in the rules debate. FIFA said Makudi was selected by the AFC to attend and is innocent of any code violations until found guilty. The home nations are in disagreement as to whose turn it is to take the FIFA British vice-presidency . The 2011 gentlemen's agreement between the four home nations, which declares it is Wales's turn to take the FIFA British vice-presidency, continues to cause ructions. The three other countries believe the agreement was nullified by a new statute that has all UEFA associations involved in the vote at the Congress later this month. But Welsh president Trefor Lloyd-Hughes says he has a signed contract in his possession that declares no change can be made in the rotation pledge unless there is unanimous agreement. Meanwhile, England's David Gill, persuaded by UEFA president Michel Platini to stand for that FIFA place against Lloyd-Hughes, has written to all UEFA countries outlining his plans. To spice up their battle, Wales are still annoyed that a UEFA ExCo, including Gill, voted for Hampden Park over the Millennium Stadium as one of 13 venues to stage Euro 2020. It's believed Gill rated the Cardiff venue higher. General secretary Alex Horne left the FA at the end of January but has not yet been replaced . Greg Dyke and David Gill, who are on the nominations panel choosing the FA's next chief executive, were coy over the weekend about how far they have progressed in finding general secretary Alex Horne's replacement. However, it's understood the selection has been made after final interviews last week and an announcement is imminent. Zimbabwe's Sean Williams celebrates taking the wicket of Umar Akmal during their World Cup match . BBC ON STICKY WICKET . BBC Sport's live text reporting of Pakistan's World Cup victory over Zimbabwe seems to have relied heavily on rival website Cricinfo. Umar Akmal's dismissal, bowled by Sean Williams, was described in the following way on Cricinfo: 'Akmal made it look an even better delivery than it actually was, because he had moved inside the line and was trying to push that inside-out through cover.' The licence-fee-funded BBC, who had just changed commentators, wrote: 'Akmal made it look an even better delivery than it actually was, because he had moved inside the line and was trying to push that inside out through cover.' Exactly the same. A BBC spokeswoman said: 'The line should have been credited. This was a simple human error.' The post-11pm peak viewing audience of two million on ITV for the Carl Frampton fight on Saturday has left the network intent on showing more live boxing. Television network ITV are determined to show more live boxing after seeing recent viewing figures .\",\n          \"Following Sunday's El Clasico spectacle that hugely intensified the run-in for the La Liga title between Barcelona and Real Madrid, Rio Ferdinand gave his stance on the age-old debate of who is better: Cristano Ronaldo or Lionel Messi? Choosing to stay somewhat impartial to the discussion the former England international says we should stand back and appreciate the sheer brilliance we are witnessing in the presence of two of the world's greatest ever players. 'It's inevitable we do all this \\\"who's better? Blah, blah, blah\\\" but we should just be enjoying being around these two players and seeing it,' the QPR defender said. Rio Ferdinand (right) speaks to the camera alongside Queen's Park Rangers team-mate Bobby Zamora . Lionel Messi is La Liga's top scorer this season with 32 goals but Cristiano Ronaldo is not far behind . The Real Madrid striker's goal in El Clasico on Sunday took him to just one goal behind the Barcelona forward . 'When Ronaldo, the Brazilian one - who should have been the best if he weren't injured - was scoring 30 goals a season [everybody said]: \\\"Oh he's a genius, he's the best ever.\\\"' Messi and Ronaldo are this season's top scorers in La Liga, with 32 and 31 goals respectively with 10 games to go. Speaking from his car, kitted out with an X-Box and television screen, the former Manchester United star posted an image on Instagram of himself and Hoops team-mate Bobby Zamora. The two were taking each other on in an in-car game of FIFA whilst travelling to training at the Imperial College Sports Ground, London. The 2008 Champions League winner wrote: 'En route to training.. Winning at FIFA #shock' after beating the former Fulham striker 2-1 with Paris Saint Germain. The former England international raises his finger in victory after beating Zamora in an in-car game of FIFA .\",\n          \"It is a measure of Arsenal's progress that fixtures like these no longer end in boos and angry recrimination. As the final whistle approached, the crowd at the Emirates were unusually vocal, anticipating trips to Wembley and singing praise of their team. Earlier it had the feel of one of the afternoons with which we have become so familiar in recent years, one of those days when Arsenal break world records for possessions stats but come up against an inspired goalkeeper and end the day with fans furious at their manager, the board of directors and life in general. But with Olivier Giroud in exceptional form, Aaron Ramsey also outstanding and Mesut Ozil thriving against the kind of opposition he relishes, Arsenal eventually cantered to victory in some style. Olivier Giroud leaps in the air after opening the scoring in Arsenal's 3-0 Premier League victory over West Ham . The France international gives Arsenal the lead on the stroke of half time with his sixth goal in his last seven games . Aaron Ramsey is chased by Santi Cazorla after doubling Arsenal's lead in the closing stages of the second half . Giroud gives substitute Mathieu Flamini a pat on the back after the Frenchman seals victory with a late strike . ARSENAL: Ospina 6.5, Chambers 7, Mertesacker 6.5, Koscielny 6.5, Monreal 7, Ramsey 7, Coquelin 7, Walcott 6 (Cazorla 6.5), Ozil 7 (Flamini 6.5), Sanchez 6 (Welbeck 6.5), Giroud 8.5 . Subs: Szczesny, Gibbs, Akpom, Bellerin . Scorers: Giroud 45, Ramsey 81, Flamini 84 . Booked: Sanchez . WEST HAM: Arian 8, O'Brien 6.5, Kouyate 6.5, Cresswell 7, Downing 6, Noble 6 (Nene 6), Song 6.5, Nolan 6, Jarvis 6.5 (Amalfitano 6), Sakho 6.5 . Subs: Demel, J\\u00e4\\u00e4skel\\u00e4inen, Poyet, Cullen, Onariase . Booked: Sakho . Man of the Match: Olivier Giroud . Referee: Chris Foy/ Anthony Taylor . CLICK HERE for all the stats, including Olivier Giroud's heat map from our superb Match Zone . Better measures of their real progress remain, of course. On Tuesday night they visit Monaco where you might anticipate another glorious comeback which ultimately ends in failure. Then there will be the league table at the end of the season which will likely tell the tale of an opportunity missed to challenge for the title. Arsene Wenger conceded that, even with Chelsea still to come to the Emirates, Arsenal are not yet in the title race. 'Not at the moment,' he said. 'But we can just keep going. We won eight of the last nine. We are stronger today than we were at the start of the season. 'We suffered a lot from the World Cup, where the players came back. What is for sure is that they understand each other much better than six or seven months ago and that makes everyone more dangerous.' At least Arsenal are not going backwards these days; the problem may be that they aren't moving forwards fast enough but they are at least pointing in the right direction. West Ham defender James Collins (left) was fortunate not to concede an early penalty after brining down Theo Walcott . The Arsenal winger crashes to the turf under Collins' challenge but was not awarded a spot kick . West Ham forward Diafra Sakho (centre) tries to evade Arsenal defenders Laurent Koscielny and Per Mertesacker (left) Arsenal playmaker Mesut Ozil (left) uses his silky skills to bring the ball down in front of Stewart Downing (centre) and Mark Noble . Arsenal's leading scorer Alexis Sanchez (centre) uses his trickery to escape the attentions of Joey O'Brien . Hammers keeper Adrian threatened to spoil Arsenal's afternoon with a succession of fine first half saves . West Ham manager Sam Allardyce looked as though he understood as much and had settled for one of those afternoons when he aims to 'out-tactic' the opposition. You could hardly blame him given the resources at his disposal, with Enner Valencia and James Tomkins the latest additions to his injury list and Carl Jenkinson ineligible. Kevin Nolan almost shocked Arsenal with a clean strike on 23 minutes but at half-time, Arsenal had 74 per cent of the possession. At times it looked as if West Ham goalkeeper Adrian was has having one of those inspired afternoon. It was later revealed he was playing with a dislocated finger sustained in the warm up, a fact which made his performance truly heroic. 'He's been a brave lad,' said assistant manager Neil McDonald. Theo Walcott was having a less satisfactory time. Thrust back in for his first start in more than a month, he did little to help his ongoing contract talks. Three times he was presented with the opportunity to open the scoring; three times he spurned the chances. Gunners defender Calum Chambers closes down West Ham winger Matt Jarvis as Arsenal take control . Song and Koscielny compete for the ball in an aerial duel as the sun shines at the Emirates Stadium . West Ham manager Sam Allardyce lets his feelings of frustration be known from the sidelines . He could plead mitigation for the first effort on six minutes. A delightful back heel from Giroud set him up with just Adrian to beat, but James Collins came through the man and the ball to prevent him scoring. It might have been a penalty but it was Walcott's own hesitation in front of goal which had presented Collins with the chance to launch his saving tackle. Adrian would then make excellent saves from Alexis Sanchez's header on 16 minutes and from an improvised touch of the knee from Ramsey on 32 minutes, before Walcott received his next chance. Played in by a delightful Ramsey pass, he took a touch, looked up and struck his shot directly at the goalkeeper.Adrian denied Sanchez from a half volley and on 44 minutes he pushed away a strike from Ozil, which landed at Walcott's feet, yards from goal but he skewed the ball high and wide, appearing to lose his balance. Ozil reacts after West Ham keeper Adrian pulls off one of a succession of first half saves . Frenchman Giroud gets his shot away despite being surrounded by four West Ham players . The Gunners centre forward watches as the ball sails towards the left corner of West Ham's goal . Adrian, who had previously kept West Ham in the game with a series of fine saves, dives in vain towards Giroud's shot . Giroud was on hand to provide deliverance just before half-time. Ramsey and Ozil exchanged a delicate one-two, but such is the Frenchman's confidence these days, he nicked it off Ramsey's foot, took a touch and drove a fine finish into the far corner of the net. 'It was fantastic because of the combination and the finishing. It was the kind of goal we loved to score,' said Wenger. It was his 14th goal of the season in 24 games. That Monaco game aside, he is having an excellent season and since that dismal night, he has responded incredibly well. After being criticised for missing chances against Monaco, Giroud holds his hands to his ears to soak up the applause . Giroud strikes a pose midway through his celebration as Arsenal take the lead at the Emirates . Walcott, who had several chances to open the first half scoring himself, leaps on Giroud's back in celebration . 'It's one of his strengths,' said Wenger, who called his performance 'outstanding'. Having set out to defend in numbers, a change of strategy was necessary for West Ham and they were equal to the challenge. They came out in the second-half with an entirely different mindset, pushing Arsenal back into their half and searching for an equaliser. In terms of clear-cut chances there was still little - Matt Jarvis' curling cross which Ospina spilled and almost let in Sakho was one unsettling moment. But Arsenal were no longer able to play as they wished. Still, their time would come. On 80 minutes, Giroud dummied a throw-in to let in Ramsey, who played in the Frenchman. He returned the ball to Ramsey, who drove it past Adrian to cap a fine a performance of growing authority. Three minutes later Santi Cazorla, on as a substitute, exchanged passes with Giroud and planted a cross almost on the foot of Mathieu Flamini, who had the simplest task of converting from close range for the third goal. Ramsey (right) gets some air as he battles for the ball with former Arsenal team-mate Song . West Ham manager Sam Allardyce looks on bemused as referee Chris Foy is forced to hand over to assistant Anthony Taylor . Former Manchester United forward Danny Welbeck (right) was brought on in the second half to replace Sanchez . Arsenal manager Arsene Wenger watches impatiently from the touchline with his side only leading by one goal . Wales midfielder Ramsey meets Giroud's pass with a left footed shot to score his first goal since December . Ramsey wheels away to celebrate giving Arsenal a 2-0 lead on 81minutes of the London derby . Aaron Ramsey is swamped by team-mates Giroud and Danny Welbeck after doubling Arsenal's lead . Substitute Mathieu Flamini is in the right place at the right time as he meets Cazorla's cross to seal a 3-0 victory . Arsenal team-mates embrace Flamini as they keep alive their hopes of playing Champions League football next season .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Greg Dyke wants a public debate between the four candidates for the FIFA presidency and has offered to host it at Wembley Stadium . BBC and Sky are also keen to host a debate between the candidates . The home nations are in disagreement as to whose turn it is to take the FIFA British vice-presidency . BBC appear to have plagiarised Cricinfo in an online live text report .\",\n          \"Rio Ferdinand played FIFA 15 and talked Lionel Messi vs Cristiano Ronaldo . The QPR defender said we should appreciate the talents of the magical duo . Ferdinand spoke from his car complete with an X-Box and multiple screens .\",\n          \"Arsenal kept up their chase for a top four finish in the Premier League with a 3-0 win over West Ham at the Emirates . Olivier Giroud netted his sixth goal in seven games to open the scoring for Gunners on the stroke of half time . Referee Chris Foy was replaced by assistant Anthony Taylor midway through the second half . Wales midfielder Aaron Ramsey was assisted by Giroud to double the lead with his first goal since December . Substitute Mathieu Flamini sealed victory after coming on to meet a cross from Santi Cazorla .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"FA chairman Greg Dyke calls for a public debate between the four candidates. The TV networks have written to the contestants outlining their ambitious proposals for a live, hour-long debate. The FA have made clear their\",\n          \"Rio Ferdinand says we should stand back and appreciate the sheer brilliance of two of the world's greatest ever players. The QPR defender said we should just be enjoying being around these two players and seeing it.\",\n          \"Olivier Giroud scores twice to give Arsenal the lead in the second half. Aaron Ramsey scores twice to make it 3-0 to keep Arsenal in the Premier League. Arsene Wenger admits his side are\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_k\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 9,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          8,\n          3,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"base_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5383722499043025,\n        \"min\": -2.6652462482452393,\n        \"max\": 0.4445358216762543,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          -1.5903594493865967,\n          -1.292244553565979,\n          -1.691623568534851\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"len\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 44,\n        \"max\": 44,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          44\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contiguity_bounded\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.410327068198554,\n        \"min\": 0.0,\n        \"max\": 19.0,\n        \"num_unique_values\": 33,\n        \"samples\": [\n          5.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"unsupported\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"new_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7710257022532989,\n        \"min\": -0.05284755306019973,\n        \"max\": 3.2884812658244624,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          1.4702442920267478\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "best_cn3"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-8aba633c-d4d9-4767-89d1-15df9fc8489c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>row_id</th>\n",
              "      <th>article</th>\n",
              "      <th>reference</th>\n",
              "      <th>summary</th>\n",
              "      <th>best_k</th>\n",
              "      <th>base_score</th>\n",
              "      <th>len</th>\n",
              "      <th>contiguity_bounded</th>\n",
              "      <th>unsupported</th>\n",
              "      <th>new_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>These days the Cheltenham Festival is a marath...</td>\n",
              "      <td>Willie Mullins looks to have chance of establi...</td>\n",
              "      <td>DOUVAN is the first of the Mullins battalion t...</td>\n",
              "      <td>5</td>\n",
              "      <td>-0.142327</td>\n",
              "      <td>44</td>\n",
              "      <td>9.5</td>\n",
              "      <td>1</td>\n",
              "      <td>1.658543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>A baby girl was found alive after being strapp...</td>\n",
              "      <td>Lynn Jennifer Groesbeck, 25, was driving home ...</td>\n",
              "      <td>Lily Groesbeck, 18 months old, was found dead ...</td>\n",
              "      <td>3</td>\n",
              "      <td>-1.266686</td>\n",
              "      <td>44</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.969040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>A 36-year-old woman and her three children wer...</td>\n",
              "      <td>40-year-old Domingo Villa Arellano, who is an ...</td>\n",
              "      <td>Domingo Villa Arellano, 40, allegedly carried ...</td>\n",
              "      <td>5</td>\n",
              "      <td>-0.909547</td>\n",
              "      <td>44</td>\n",
              "      <td>11.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.782881</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8aba633c-d4d9-4767-89d1-15df9fc8489c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8aba633c-d4d9-4767-89d1-15df9fc8489c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8aba633c-d4d9-4767-89d1-15df9fc8489c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3f853759-1d90-4788-9f0b-6d5708fd8005\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3f853759-1d90-4788-9f0b-6d5708fd8005')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3f853759-1d90-4788-9f0b-6d5708fd8005 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   row_id                                            article  \\\n",
              "0       0  These days the Cheltenham Festival is a marath...   \n",
              "1       1  A baby girl was found alive after being strapp...   \n",
              "2       2  A 36-year-old woman and her three children wer...   \n",
              "\n",
              "                                           reference  \\\n",
              "0  Willie Mullins looks to have chance of establi...   \n",
              "1  Lynn Jennifer Groesbeck, 25, was driving home ...   \n",
              "2  40-year-old Domingo Villa Arellano, who is an ...   \n",
              "\n",
              "                                             summary  best_k  base_score  len  \\\n",
              "0  DOUVAN is the first of the Mullins battalion t...       5   -0.142327   44   \n",
              "1  Lily Groesbeck, 18 months old, was found dead ...       3   -1.266686   44   \n",
              "2  Domingo Villa Arellano, 40, allegedly carried ...       5   -0.909547   44   \n",
              "\n",
              "   contiguity_bounded  unsupported  new_score  \n",
              "0                 9.5            1   1.658543  \n",
              "1                12.0            0   1.969040  \n",
              "2                11.0            2   1.782881  "
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# === Offline rerank: bounded span reward + unsupported penalty ===\n",
        "dd = details_cn3.copy()\n",
        "\n",
        "# contiguous LCS blocks\n",
        "runs_list, rew_list = [], []\n",
        "for _,r in dd.iterrows():\n",
        "    runs = lcs_blocks(r[\"article\"], r[\"summary\"], min_len=2)\n",
        "    rew  = bounded_span_reward(runs, L_lo=2, L_hi=5, L_pen=7, pen_w=0.5)\n",
        "    runs_list.append(runs); rew_list.append(rew)\n",
        "dd[\"span_runs\"] = runs_list\n",
        "dd[\"contiguity_bounded\"] = rew_list\n",
        "\n",
        "# unsupported entities vs source\n",
        "uns_list = []\n",
        "for _,r in tqdm(list(dd.iterrows()), total=len(dd), desc=\"entities(vs source)\"):\n",
        "    uns_list.append(unsupported(r[\"article\"], r[\"summary\"]))\n",
        "dd[\"unsupported\"] = uns_list\n",
        "\n",
        "# score: base / sqrt(len) + γ*bounded_contig − δ*unsupported\n",
        "GAMMA, DELTA = 0.18, 0.03\n",
        "dd[\"len\"] = dd[\"len\"].clip(lower=1)\n",
        "dd[\"new_score\"] = (dd[\"base_score\"] / (dd[\"len\"]**0.5)) + GAMMA*dd[\"contiguity_bounded\"] - DELTA*dd[\"unsupported\"]\n",
        "\n",
        "best_cn3 = (\n",
        "    dd.sort_values([\"row_id\",\"new_score\"], ascending=[True, False])\n",
        "      .groupby(\"row_id\", as_index=False).head(1)\n",
        "      .loc[:, [\"row_id\",\"article\",\"reference\",\"summary\",\"candidate_k\",\"base_score\",\"len\",\n",
        "               \"contiguity_bounded\",\"unsupported\",\"new_score\"]]\n",
        "      .rename(columns={\"candidate_k\":\"best_k\"})\n",
        "      .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "save_csv(dd,       DETAILS_CN3)  # overwrite with new columns\n",
        "save_csv(best_cn3, BEST_CN3)\n",
        "\n",
        "changed = int((best_cn3[\"best_k\"] != 0).sum())\n",
        "print({\"changed\": changed, \"gamma\":GAMMA, \"delta\":DELTA})\n",
        "best_cn3.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocAHWGQk7YcN",
        "outputId": "261f0052-1d66-4c7b-826c-388bef9efef6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE TOP: {'r1_f': 0.3914, 'r2_f': 0.1763, 'rl_f': 0.2702}\n",
            "ROUGE CN3: {'r1_f': 0.3853, 'r2_f': 0.1725, 'rl_f': 0.2764}\n",
            "Δ ROUGE  : {'r1': -0.0061, 'r2': -0.0038, 'rl': 0.0062}\n",
            "ENT TOP  : {'TP': 220, 'FP': 151, 'FN': 325, 'entP': 0.593, 'entR': 0.4037, 'entF1': 0.4803}\n",
            "ENT CN3  : {'TP': 203, 'FP': 162, 'FN': 342, 'entP': 0.5562, 'entR': 0.3725, 'entF1': 0.4462}\n",
            "Δ ENT    : {'entP': -0.0368, 'entR': -0.0312, 'entF1': -0.0342}\n",
            "Unsupported entities across 100 docs (CN3 best): 20\n"
          ]
        }
      ],
      "source": [
        "# === ROUGE & spaCy entity vs reference: top-beam (k=0) vs CopyNext3 best ===\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Align TOP(k=0) vs BEST\n",
        "top_k0 = details_cn3[details_cn3[\"candidate_k\"]==0][[\"row_id\",\"summary\"]].rename(columns={\"summary\":\"top_beam_summary\"})\n",
        "ev = best_cn3.merge(top_k0, on=\"row_id\", how=\"left\").rename(columns={\"summary\":\"cn3_summary\"})\n",
        "\n",
        "refs  = ev[\"reference\"].astype(str).tolist()\n",
        "tops  = ev[\"top_beam_summary\"].astype(str).tolist()\n",
        "bests = ev[\"cn3_summary\"].astype(str).tolist()\n",
        "\n",
        "# ROUGE\n",
        "sc = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\"], use_stemmer=True)\n",
        "def _mean(refs, hyps):\n",
        "    r1=r2=rl=0.0\n",
        "    for r,h in zip(refs, hyps):\n",
        "        s=sc.score(r,h); r1+=s[\"rouge1\"].fmeasure; r2+=s[\"rouge2\"].fmeasure; rl+=s[\"rougeLsum\"].fmeasure\n",
        "    n=len(refs); return r1/n, r2/n, rl/n\n",
        "r1t,r2t,rlt = _mean(refs, tops)\n",
        "r1b,r2b,rlb = _mean(refs, bests)\n",
        "print(\"ROUGE TOP:\", {\"r1_f\":round(r1t,4), \"r2_f\":round(r2t,4), \"rl_f\":round(rlt,4)})\n",
        "print(\"ROUGE CN3:\", {\"r1_f\":round(r1b,4), \"r2_f\":round(r2b,4), \"rl_f\":round(rlb,4)})\n",
        "print(\"Δ ROUGE  :\", {\"r1\":round(r1b-r1t,4), \"r2\":round(r2b-r2t,4), \"rl\":round(rlb-rlt,4)})\n",
        "\n",
        "# Entity vs references (overall)\n",
        "def ents_norm_list(text):\n",
        "    doc = nlp(text)\n",
        "    def norm(u):\n",
        "        u=u.lower().strip()\n",
        "        u=re.sub(r\"(?<=\\d),(?=\\d{3}\\b)\",\"\",u).replace(\"%\",\" percent\")\n",
        "        u=re.sub(r\"\\s+\",\" \",u); return u\n",
        "    return [(norm(ent.text), ent.label_) for ent in doc.ents]\n",
        "\n",
        "def entPRF(refs, hyps):\n",
        "    ref_all = [t for r in refs for (t,_) in ents_norm_list(r)]\n",
        "    hyp_all = [t for h in hyps for (t,_) in ents_norm_list(h)]\n",
        "    ref_set, hyp_set = set((t,) for t in ref_all), set((t,) for t in hyp_all)\n",
        "    tp = len(ref_set & hyp_set); fp = len(hyp_set - ref_set); fn = len(ref_set - hyp_set)\n",
        "    p = tp/(tp+fp) if tp+fp else 0.0; r = tp/(tp+fn) if tp+fn else 0.0; f = 2*p*r/(p+r) if p+r else 0.0\n",
        "    return dict(TP=tp, FP=fp, FN=fn, entP=p, entR=r, entF1=f)\n",
        "\n",
        "ent_top = entPRF(refs, tops)\n",
        "ent_cn3 = entPRF(refs, bests)\n",
        "print(\"ENT TOP  :\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in ent_top.items()})\n",
        "print(\"ENT CN3  :\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in ent_cn3.items()})\n",
        "print(\"Δ ENT    :\", {k:round(ent_cn3[k]-ent_top[k],4) for k in [\"entP\",\"entR\",\"entF1\"]})\n",
        "\n",
        "# Unsupported entities total (hallucinations proxy)\n",
        "uns_total = int(best_cn3[\"unsupported\"].sum())\n",
        "print(f\"Unsupported entities across {len(best_cn3)} docs (CN3 best): {uns_total}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mh7Qa2DXJCf6"
      },
      "source": [
        "# Hybrid exp-1 1k sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVDFV8ldJoC4"
      },
      "outputs": [],
      "source": [
        "# === EntityAware CopyNext logits processor ===\n",
        "from transformers.generation.logits_process import LogitsProcessor\n",
        "\n",
        "class EntityAwareSpanProcessor(LogitsProcessor):\n",
        "    \"\"\"\n",
        "    CopyNext-style processor with entity boost:\n",
        "    - Normal span continuation gets +gamma\n",
        "    - If the next token continues an entity span, boost extra (+gamma_entity)\n",
        "    \"\"\"\n",
        "    def __init__(self, src_input_ids, tokenizer, num_beams,\n",
        "                 gamma=0.5, gamma_entity=1.5, max_span=8, skip_special=True):\n",
        "        self.src_ids = [ids.tolist() if torch.is_tensor(ids) else list(ids) for ids in src_input_ids]\n",
        "        self.num_beams = int(num_beams)\n",
        "        self.gamma = float(gamma)\n",
        "        self.gamma_entity = float(gamma_entity)\n",
        "        self.max_span = int(max_span)\n",
        "        self.skip_ids = set(tokenizer.all_special_ids) if skip_special else set()\n",
        "\n",
        "        # Extract entity token sets from source text\n",
        "        self.entity_token_sets = []\n",
        "        for ids in self.src_ids:\n",
        "            text = tokenizer.decode(ids, skip_special_tokens=True)\n",
        "            ents = [ent.text for ent in nlp(text).ents]\n",
        "            tok_sets = []\n",
        "            for e in ents:\n",
        "                tok_ids = tokenizer(e, add_special_tokens=False)[\"input_ids\"]\n",
        "                if tok_ids:\n",
        "                    tok_sets.append(set(tok_ids))\n",
        "            self.entity_token_sets.append(tok_sets)\n",
        "\n",
        "    @staticmethod\n",
        "    def _find_suffix_in_src(src_ids, hyp_ids, max_span):\n",
        "        if not hyp_ids: return 0, -1\n",
        "        max_check = min(len(hyp_ids), max_span)\n",
        "        for L in range(max_check, 0, -1):\n",
        "            suffix = hyp_ids[-L:]\n",
        "            for end in range(L-1, len(src_ids)):\n",
        "                if src_ids[end-L+1:end+1] == suffix:\n",
        "                    return L, end\n",
        "        return 0, -1\n",
        "\n",
        "    def __call__(self, input_ids, scores):\n",
        "        B_beams = input_ids.size(0)\n",
        "        for row in range(B_beams):\n",
        "            sample_idx = row // self.num_beams\n",
        "            src = self.src_ids[sample_idx]\n",
        "            hyp = input_ids[row].tolist()\n",
        "            L, end = self._find_suffix_in_src(src, hyp, self.max_span)\n",
        "            if L >= 1 and end + 1 < len(src):\n",
        "                next_id = src[end + 1]\n",
        "                if next_id not in self.skip_ids:\n",
        "                    boost = self.gamma\n",
        "                    # Extra boost if token is part of any entity span\n",
        "                    for tokset in self.entity_token_sets[sample_idx]:\n",
        "                        if next_id in tokset:\n",
        "                            boost = self.gamma_entity\n",
        "                            break\n",
        "                    scores[row, next_id] += boost\n",
        "        return scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "427fcd10db5d43a689c295629b86d202"
          ]
        },
        "id": "-f9Cn37UN_FU",
        "outputId": "83de2b4c-32aa-41d6-88c4-ce81942c58f6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "427fcd10db5d43a689c295629b86d202",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "EntityAware CopyNext generate:   0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[saved] /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/decodes_copynext2/copynext2_details_20250826_161305.csv (1000 rows)\n",
            "[done] Generated 1000 candidates\n"
          ]
        }
      ],
      "source": [
        "# === Generate summaries with EntityAware CopyNext ===\n",
        "details_rows = []\n",
        "BATCH = 4\n",
        "K = CN2_ARGS[\"num_return_sequences\"]\n",
        "\n",
        "for start in tqdm(range(0, N, BATCH), desc=\"EntityAware CopyNext generate\"):\n",
        "    batch_arts = articles[start:start+BATCH]\n",
        "    batch_refs = references[start:start+BATCH]\n",
        "    enc = tok(batch_arts, return_tensors=\"pt\", truncation=True, padding=True,\n",
        "              max_length=MAX_SRC_LEN_CN2).to(DEVICE)\n",
        "\n",
        "    proc = EntityAwareSpanProcessor(\n",
        "        src_input_ids=enc[\"input_ids\"], tokenizer=tok,\n",
        "        num_beams=CN2_ARGS[\"num_beams\"],\n",
        "        gamma=0.5, gamma_entity=1.5, max_span=8\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **enc, **CN2_ARGS, logits_processor=LogitsProcessorList([proc])\n",
        "        )\n",
        "\n",
        "    seqs = out.sequences.view(len(batch_arts), K, -1)\n",
        "    base = out.sequences_scores.view(len(batch_arts), K)\n",
        "\n",
        "    for bi in range(len(batch_arts)):\n",
        "        art = batch_arts[bi]; ref = batch_refs[bi]\n",
        "        for k in range(K):\n",
        "            ids = seqs[bi, k]\n",
        "            summ = tok.decode(ids, skip_special_tokens=True)\n",
        "            tgt_len = max(int(ids.size(0) - 1), 1)\n",
        "            details_rows.append({\n",
        "                \"row_id\": start + bi,\n",
        "                \"candidate_k\": k,\n",
        "                \"article\": art,\n",
        "                \"reference\": ref,\n",
        "                \"summary\": summ,\n",
        "                \"base_score\": float(base[bi, k].item()),\n",
        "                \"len\": tgt_len\n",
        "            })\n",
        "\n",
        "details_cn2 = pd.DataFrame(details_rows)\n",
        "save_csv(details_cn2, DETAILS_CN2)\n",
        "print(f\"[done] Generated {len(details_cn2)} candidates\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "f0e3153ef65f4917a2e2dadabc56fdb8"
          ]
        },
        "id": "kST1m58tOf1d",
        "outputId": "3be4a348-179d-42c1-876e-a05f57e4d4ac"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0e3153ef65f4917a2e2dadabc56fdb8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "entities(vs source):   0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[saved] /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/decodes_copynext2/copynext2_details_20250826_161305.csv (1000 rows)\n",
            "[saved] /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/decodes_copynext2/copynext2_best_20250826_161305.csv (100 rows)\n",
            "{'changed': 65}\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"best_cn2\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"row_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          83,\n          53,\n          70\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"article\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"FA chairman Greg Dyke has called for a public debate between the four candidates for the FIFA presidency and offered to host the event at Wembley. Dyke made his wish for a TV inquisition known during an awkward weekend in Sepp Blatter's company at the rules-deciding IFAB meeting in Belfast. The FA have made clear their strong opposition to the FIFA president serving a fifth term. Meanwhile, in a separate move, Sky and the BBC have made a joint approach to Blatter and rivals Prince Ali of Jordan, Luis Figo and Michael van Praag to stage such an event. FA chairman Greg Dyke has called for a public debate between the four candidates for the FIFA presidency . The TV networks - through leading sports news broadcasters Paul Kelso and Richard Conway - have written to the contestants outlining their ambitious proposals for a live, hour-long debate in the UK with an audience of fans representing all 209 FIFA member nations. This fans' congress would provide questions, with others drawn from football supporters via the Sky and BBC websites, and Facebook, to ensure maximum interaction. All four challengers have been promised equal time and emphasis to present their manifestos and visions for FIFA's future. Blatter has yet to respond to the Sky-BBC letter but it is highly unlikely he'll agree to such public exposure. His three opponents will all be in favour. Sepp Blatter has yet to respond to the Sky-BBC letter but it is unlikely he'll agree to such public exposure . The official photo of the IFAB summit shows Greg Dyke seated between, of all people, Sepp Blatter and Thailand's Worawi Makudi, whose presence at Belfast's Cullodon Hotel as a representative of the Asian Football Confederation was bizarre in the extreme. Makudi has an acrimonious history with the FA and is under investigation by FIFA's ethics committee for breaching World Cup bid rules. It's understood Makudi played no part in the rules debate. FIFA said Makudi was selected by the AFC to attend and is innocent of any code violations until found guilty. The home nations are in disagreement as to whose turn it is to take the FIFA British vice-presidency . The 2011 gentlemen's agreement between the four home nations, which declares it is Wales's turn to take the FIFA British vice-presidency, continues to cause ructions. The three other countries believe the agreement was nullified by a new statute that has all UEFA associations involved in the vote at the Congress later this month. But Welsh president Trefor Lloyd-Hughes says he has a signed contract in his possession that declares no change can be made in the rotation pledge unless there is unanimous agreement. Meanwhile, England's David Gill, persuaded by UEFA president Michel Platini to stand for that FIFA place against Lloyd-Hughes, has written to all UEFA countries outlining his plans. To spice up their battle, Wales are still annoyed that a UEFA ExCo, including Gill, voted for Hampden Park over the Millennium Stadium as one of 13 venues to stage Euro 2020. It's believed Gill rated the Cardiff venue higher. General secretary Alex Horne left the FA at the end of January but has not yet been replaced . Greg Dyke and David Gill, who are on the nominations panel choosing the FA's next chief executive, were coy over the weekend about how far they have progressed in finding general secretary Alex Horne's replacement. However, it's understood the selection has been made after final interviews last week and an announcement is imminent. Zimbabwe's Sean Williams celebrates taking the wicket of Umar Akmal during their World Cup match . BBC ON STICKY WICKET . BBC Sport's live text reporting of Pakistan's World Cup victory over Zimbabwe seems to have relied heavily on rival website Cricinfo. Umar Akmal's dismissal, bowled by Sean Williams, was described in the following way on Cricinfo: 'Akmal made it look an even better delivery than it actually was, because he had moved inside the line and was trying to push that inside-out through cover.' The licence-fee-funded BBC, who had just changed commentators, wrote: 'Akmal made it look an even better delivery than it actually was, because he had moved inside the line and was trying to push that inside out through cover.' Exactly the same. A BBC spokeswoman said: 'The line should have been credited. This was a simple human error.' The post-11pm peak viewing audience of two million on ITV for the Carl Frampton fight on Saturday has left the network intent on showing more live boxing. Television network ITV are determined to show more live boxing after seeing recent viewing figures .\",\n          \"Following Sunday's El Clasico spectacle that hugely intensified the run-in for the La Liga title between Barcelona and Real Madrid, Rio Ferdinand gave his stance on the age-old debate of who is better: Cristano Ronaldo or Lionel Messi? Choosing to stay somewhat impartial to the discussion the former England international says we should stand back and appreciate the sheer brilliance we are witnessing in the presence of two of the world's greatest ever players. 'It's inevitable we do all this \\\"who's better? Blah, blah, blah\\\" but we should just be enjoying being around these two players and seeing it,' the QPR defender said. Rio Ferdinand (right) speaks to the camera alongside Queen's Park Rangers team-mate Bobby Zamora . Lionel Messi is La Liga's top scorer this season with 32 goals but Cristiano Ronaldo is not far behind . The Real Madrid striker's goal in El Clasico on Sunday took him to just one goal behind the Barcelona forward . 'When Ronaldo, the Brazilian one - who should have been the best if he weren't injured - was scoring 30 goals a season [everybody said]: \\\"Oh he's a genius, he's the best ever.\\\"' Messi and Ronaldo are this season's top scorers in La Liga, with 32 and 31 goals respectively with 10 games to go. Speaking from his car, kitted out with an X-Box and television screen, the former Manchester United star posted an image on Instagram of himself and Hoops team-mate Bobby Zamora. The two were taking each other on in an in-car game of FIFA whilst travelling to training at the Imperial College Sports Ground, London. The 2008 Champions League winner wrote: 'En route to training.. Winning at FIFA #shock' after beating the former Fulham striker 2-1 with Paris Saint Germain. The former England international raises his finger in victory after beating Zamora in an in-car game of FIFA .\",\n          \"It is a measure of Arsenal's progress that fixtures like these no longer end in boos and angry recrimination. As the final whistle approached, the crowd at the Emirates were unusually vocal, anticipating trips to Wembley and singing praise of their team. Earlier it had the feel of one of the afternoons with which we have become so familiar in recent years, one of those days when Arsenal break world records for possessions stats but come up against an inspired goalkeeper and end the day with fans furious at their manager, the board of directors and life in general. But with Olivier Giroud in exceptional form, Aaron Ramsey also outstanding and Mesut Ozil thriving against the kind of opposition he relishes, Arsenal eventually cantered to victory in some style. Olivier Giroud leaps in the air after opening the scoring in Arsenal's 3-0 Premier League victory over West Ham . The France international gives Arsenal the lead on the stroke of half time with his sixth goal in his last seven games . Aaron Ramsey is chased by Santi Cazorla after doubling Arsenal's lead in the closing stages of the second half . Giroud gives substitute Mathieu Flamini a pat on the back after the Frenchman seals victory with a late strike . ARSENAL: Ospina 6.5, Chambers 7, Mertesacker 6.5, Koscielny 6.5, Monreal 7, Ramsey 7, Coquelin 7, Walcott 6 (Cazorla 6.5), Ozil 7 (Flamini 6.5), Sanchez 6 (Welbeck 6.5), Giroud 8.5 . Subs: Szczesny, Gibbs, Akpom, Bellerin . Scorers: Giroud 45, Ramsey 81, Flamini 84 . Booked: Sanchez . WEST HAM: Arian 8, O'Brien 6.5, Kouyate 6.5, Cresswell 7, Downing 6, Noble 6 (Nene 6), Song 6.5, Nolan 6, Jarvis 6.5 (Amalfitano 6), Sakho 6.5 . Subs: Demel, J\\u00e4\\u00e4skel\\u00e4inen, Poyet, Cullen, Onariase . Booked: Sakho . Man of the Match: Olivier Giroud . Referee: Chris Foy/ Anthony Taylor . CLICK HERE for all the stats, including Olivier Giroud's heat map from our superb Match Zone . Better measures of their real progress remain, of course. On Tuesday night they visit Monaco where you might anticipate another glorious comeback which ultimately ends in failure. Then there will be the league table at the end of the season which will likely tell the tale of an opportunity missed to challenge for the title. Arsene Wenger conceded that, even with Chelsea still to come to the Emirates, Arsenal are not yet in the title race. 'Not at the moment,' he said. 'But we can just keep going. We won eight of the last nine. We are stronger today than we were at the start of the season. 'We suffered a lot from the World Cup, where the players came back. What is for sure is that they understand each other much better than six or seven months ago and that makes everyone more dangerous.' At least Arsenal are not going backwards these days; the problem may be that they aren't moving forwards fast enough but they are at least pointing in the right direction. West Ham defender James Collins (left) was fortunate not to concede an early penalty after brining down Theo Walcott . The Arsenal winger crashes to the turf under Collins' challenge but was not awarded a spot kick . West Ham forward Diafra Sakho (centre) tries to evade Arsenal defenders Laurent Koscielny and Per Mertesacker (left) Arsenal playmaker Mesut Ozil (left) uses his silky skills to bring the ball down in front of Stewart Downing (centre) and Mark Noble . Arsenal's leading scorer Alexis Sanchez (centre) uses his trickery to escape the attentions of Joey O'Brien . Hammers keeper Adrian threatened to spoil Arsenal's afternoon with a succession of fine first half saves . West Ham manager Sam Allardyce looked as though he understood as much and had settled for one of those afternoons when he aims to 'out-tactic' the opposition. You could hardly blame him given the resources at his disposal, with Enner Valencia and James Tomkins the latest additions to his injury list and Carl Jenkinson ineligible. Kevin Nolan almost shocked Arsenal with a clean strike on 23 minutes but at half-time, Arsenal had 74 per cent of the possession. At times it looked as if West Ham goalkeeper Adrian was has having one of those inspired afternoon. It was later revealed he was playing with a dislocated finger sustained in the warm up, a fact which made his performance truly heroic. 'He's been a brave lad,' said assistant manager Neil McDonald. Theo Walcott was having a less satisfactory time. Thrust back in for his first start in more than a month, he did little to help his ongoing contract talks. Three times he was presented with the opportunity to open the scoring; three times he spurned the chances. Gunners defender Calum Chambers closes down West Ham winger Matt Jarvis as Arsenal take control . Song and Koscielny compete for the ball in an aerial duel as the sun shines at the Emirates Stadium . West Ham manager Sam Allardyce lets his feelings of frustration be known from the sidelines . He could plead mitigation for the first effort on six minutes. A delightful back heel from Giroud set him up with just Adrian to beat, but James Collins came through the man and the ball to prevent him scoring. It might have been a penalty but it was Walcott's own hesitation in front of goal which had presented Collins with the chance to launch his saving tackle. Adrian would then make excellent saves from Alexis Sanchez's header on 16 minutes and from an improvised touch of the knee from Ramsey on 32 minutes, before Walcott received his next chance. Played in by a delightful Ramsey pass, he took a touch, looked up and struck his shot directly at the goalkeeper.Adrian denied Sanchez from a half volley and on 44 minutes he pushed away a strike from Ozil, which landed at Walcott's feet, yards from goal but he skewed the ball high and wide, appearing to lose his balance. Ozil reacts after West Ham keeper Adrian pulls off one of a succession of first half saves . Frenchman Giroud gets his shot away despite being surrounded by four West Ham players . The Gunners centre forward watches as the ball sails towards the left corner of West Ham's goal . Adrian, who had previously kept West Ham in the game with a series of fine saves, dives in vain towards Giroud's shot . Giroud was on hand to provide deliverance just before half-time. Ramsey and Ozil exchanged a delicate one-two, but such is the Frenchman's confidence these days, he nicked it off Ramsey's foot, took a touch and drove a fine finish into the far corner of the net. 'It was fantastic because of the combination and the finishing. It was the kind of goal we loved to score,' said Wenger. It was his 14th goal of the season in 24 games. That Monaco game aside, he is having an excellent season and since that dismal night, he has responded incredibly well. After being criticised for missing chances against Monaco, Giroud holds his hands to his ears to soak up the applause . Giroud strikes a pose midway through his celebration as Arsenal take the lead at the Emirates . Walcott, who had several chances to open the first half scoring himself, leaps on Giroud's back in celebration . 'It's one of his strengths,' said Wenger, who called his performance 'outstanding'. Having set out to defend in numbers, a change of strategy was necessary for West Ham and they were equal to the challenge. They came out in the second-half with an entirely different mindset, pushing Arsenal back into their half and searching for an equaliser. In terms of clear-cut chances there was still little - Matt Jarvis' curling cross which Ospina spilled and almost let in Sakho was one unsettling moment. But Arsenal were no longer able to play as they wished. Still, their time would come. On 80 minutes, Giroud dummied a throw-in to let in Ramsey, who played in the Frenchman. He returned the ball to Ramsey, who drove it past Adrian to cap a fine a performance of growing authority. Three minutes later Santi Cazorla, on as a substitute, exchanged passes with Giroud and planted a cross almost on the foot of Mathieu Flamini, who had the simplest task of converting from close range for the third goal. Ramsey (right) gets some air as he battles for the ball with former Arsenal team-mate Song . West Ham manager Sam Allardyce looks on bemused as referee Chris Foy is forced to hand over to assistant Anthony Taylor . Former Manchester United forward Danny Welbeck (right) was brought on in the second half to replace Sanchez . Arsenal manager Arsene Wenger watches impatiently from the touchline with his side only leading by one goal . Wales midfielder Ramsey meets Giroud's pass with a left footed shot to score his first goal since December . Ramsey wheels away to celebrate giving Arsenal a 2-0 lead on 81minutes of the London derby . Aaron Ramsey is swamped by team-mates Giroud and Danny Welbeck after doubling Arsenal's lead . Substitute Mathieu Flamini is in the right place at the right time as he meets Cazorla's cross to seal a 3-0 victory . Arsenal team-mates embrace Flamini as they keep alive their hopes of playing Champions League football next season .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Greg Dyke wants a public debate between the four candidates for the FIFA presidency and has offered to host it at Wembley Stadium . BBC and Sky are also keen to host a debate between the candidates . The home nations are in disagreement as to whose turn it is to take the FIFA British vice-presidency . BBC appear to have plagiarised Cricinfo in an online live text report .\",\n          \"Rio Ferdinand played FIFA 15 and talked Lionel Messi vs Cristiano Ronaldo . The QPR defender said we should appreciate the talents of the magical duo . Ferdinand spoke from his car complete with an X-Box and multiple screens .\",\n          \"Arsenal kept up their chase for a top four finish in the Premier League with a 3-0 win over West Ham at the Emirates . Olivier Giroud netted his sixth goal in seven games to open the scoring for Gunners on the stroke of half time . Referee Chris Foy was replaced by assistant Anthony Taylor midway through the second half . Wales midfielder Aaron Ramsey was assisted by Giroud to double the lead with his first goal since December . Substitute Mathieu Flamini sealed victory after coming on to meet a cross from Santi Cazorla .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"FA chairman Greg Dyke has called for a public debate between the four candidates for the FIFA presidency. The TV networks have written to the contestants outlining their ambitious proposals for a live, hour-long debate. The FA have made a joint approach to Blatter and rivals Prince Ali of Jordan, Luis Figo and Michael van Praag to stage the event.\",\n          \"The former England international says we should stand back and appreciate the sheer brilliance we are witnessing in the presence of two of the world's greatest ever players. The former Manchester United star posted an image on Instagram of himself and Hoops team-mate Bobby Zamora. The two were taking each other on in an in-car game of FIFA. The pair were taking part in the game while travelling to training at the Imperial College Sports Ground.\",\n          \"Olivier Giroud opened the scoring in Arsenal's 3-0 Premier League victory over West Ham. The France international gave Arsenal the lead with a late strike. Aaron Ramsey and Mesut Ozil also scored for the Gunners. Arsenal's leading scorer Alexis Sanchez scored twice in the second half.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_k\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 9,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          7,\n          1,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"base_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0035904688052274115,\n        \"min\": -0.014469360001385212,\n        \"max\": 0.006110723130404949,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          -0.0029996251687407494,\n          -0.003873131237924099,\n          -0.009579298086464405\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"len\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 80,\n        \"max\": 100,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          97,\n          82,\n          100\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contiguity_lcs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12,\n        \"min\": 21,\n        \"max\": 84,\n        \"num_unique_values\": 43,\n        \"samples\": [\n          56,\n          58,\n          57\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_span_len_lcs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.647167259589095,\n        \"min\": 5.0,\n        \"max\": 43.0,\n        \"num_unique_values\": 88,\n        \"samples\": [\n          10.8,\n          17.0,\n          24.666666666666668\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"unsupported\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0,\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"len_reward\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.13217350897585306,\n        \"min\": 0.11025052530448522,\n        \"max\": 0.5460744266397094,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          0.14938177525041801,\n          0.4867522559599717,\n          0.11025052530448522\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"new_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 38.39417288734636,\n        \"min\": 64.0975182919524,\n        \"max\": 260.6328296084713,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          119.2089858153219,\n          202.68290912569364,\n          85.03187205214701\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "best_cn2"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-8917ecf7-c761-4b3c-a51e-13c4152eb8db\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>row_id</th>\n",
              "      <th>article</th>\n",
              "      <th>reference</th>\n",
              "      <th>summary</th>\n",
              "      <th>best_k</th>\n",
              "      <th>base_score</th>\n",
              "      <th>len</th>\n",
              "      <th>contiguity_lcs</th>\n",
              "      <th>avg_span_len_lcs</th>\n",
              "      <th>unsupported</th>\n",
              "      <th>len_reward</th>\n",
              "      <th>new_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>These days the Cheltenham Festival is a marath...</td>\n",
              "      <td>Willie Mullins looks to have chance of establi...</td>\n",
              "      <td>The Cheltenham Festival is a marathon not a sp...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.006111</td>\n",
              "      <td>100</td>\n",
              "      <td>64</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.110251</td>\n",
              "      <td>192.433843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>A baby girl was found alive after being strapp...</td>\n",
              "      <td>Lynn Jennifer Groesbeck, 25, was driving home ...</td>\n",
              "      <td>The baby girl was found alive after being stra...</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.005965</td>\n",
              "      <td>100</td>\n",
              "      <td>48</td>\n",
              "      <td>10.600000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.110251</td>\n",
              "      <td>146.152326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>A 36-year-old woman and her three children wer...</td>\n",
              "      <td>40-year-old Domingo Villa Arellano, who is an ...</td>\n",
              "      <td>Domingo Villa Arellano, who is an ex-cop, alle...</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.003852</td>\n",
              "      <td>100</td>\n",
              "      <td>64</td>\n",
              "      <td>22.333333</td>\n",
              "      <td>1</td>\n",
              "      <td>0.110251</td>\n",
              "      <td>193.499258</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8917ecf7-c761-4b3c-a51e-13c4152eb8db')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8917ecf7-c761-4b3c-a51e-13c4152eb8db button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8917ecf7-c761-4b3c-a51e-13c4152eb8db');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-23e270f4-fb31-4da6-9e3d-cb56635594db\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-23e270f4-fb31-4da6-9e3d-cb56635594db')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-23e270f4-fb31-4da6-9e3d-cb56635594db button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   row_id                                            article  \\\n",
              "0       0  These days the Cheltenham Festival is a marath...   \n",
              "1       1  A baby girl was found alive after being strapp...   \n",
              "2       2  A 36-year-old woman and her three children wer...   \n",
              "\n",
              "                                           reference  \\\n",
              "0  Willie Mullins looks to have chance of establi...   \n",
              "1  Lynn Jennifer Groesbeck, 25, was driving home ...   \n",
              "2  40-year-old Domingo Villa Arellano, who is an ...   \n",
              "\n",
              "                                             summary  best_k  base_score  len  \\\n",
              "0  The Cheltenham Festival is a marathon not a sp...       0    0.006111  100   \n",
              "1  The baby girl was found alive after being stra...       0   -0.005965  100   \n",
              "2  Domingo Villa Arellano, who is an ex-cop, alle...       1   -0.003852  100   \n",
              "\n",
              "   contiguity_lcs  avg_span_len_lcs  unsupported  len_reward   new_score  \n",
              "0              64         17.000000            1    0.110251  192.433843  \n",
              "1              48         10.600000            0    0.110251  146.152326  \n",
              "2              64         22.333333            1    0.110251  193.499258  "
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# === Offline rerank for EntityAware CopyNext (entity-dominant) ===\n",
        "dd = details_cn2.copy()\n",
        "\n",
        "# LCS span stats\n",
        "contigs, avlens = [], []\n",
        "for i,row in dd.iterrows():\n",
        "    c,a = lcs_span_stats(row[\"article\"], row[\"summary\"])\n",
        "    contigs.append(c); avlens.append(a)\n",
        "dd[\"contiguity_lcs\"] = contigs\n",
        "dd[\"avg_span_len_lcs\"] = avlens\n",
        "\n",
        "# Unsupported entities vs source\n",
        "unsupp = []\n",
        "for i,row in tqdm(list(dd.iterrows()), total=len(dd), desc=\"entities(vs source)\"):\n",
        "    unsupp.append(unsupported_count(row[\"article\"], row[\"summary\"]))\n",
        "dd[\"unsupported\"] = unsupp\n",
        "\n",
        "# Length reward (Gaussian)\n",
        "TARGET_LEN = 58\n",
        "LEN_SIGMA = 20.0\n",
        "def length_reward(l, target=TARGET_LEN, sigma=LEN_SIGMA):\n",
        "    return math.exp(-((l - target)**2) / (2 * sigma**2))\n",
        "\n",
        "dd[\"len\"] = dd[\"len\"].clip(lower=1)\n",
        "dd[\"len_reward\"] = dd[\"len\"].apply(lambda l: length_reward(l))\n",
        "\n",
        "# Entity-dominant scoring\n",
        "dd[\"entity_score\"] = -dd[\"unsupported\"] + dd[\"contiguity_lcs\"]\n",
        "dd[\"new_score\"] = (3.0 * dd[\"entity_score\"]) \\\n",
        "                  + (0.5 * (dd[\"base_score\"] / (dd[\"len\"]**0.3))) \\\n",
        "                  + (0.2 * dd[\"avg_span_len_lcs\"]) \\\n",
        "                  + (0.3 * dd[\"len_reward\"])\n",
        "\n",
        "best_cn2 = (\n",
        "    dd.sort_values([\"row_id\",\"new_score\"], ascending=[True, False])\n",
        "      .groupby(\"row_id\", as_index=False).head(1)\n",
        "      .loc[:, [\"row_id\",\"article\",\"reference\",\"summary\",\"candidate_k\",\n",
        "               \"base_score\",\"len\",\"contiguity_lcs\",\"avg_span_len_lcs\",\n",
        "               \"unsupported\",\"len_reward\",\"new_score\"]]\n",
        "      .rename(columns={\"candidate_k\":\"best_k\"})\n",
        "      .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "save_csv(dd,       DETAILS_CN2)\n",
        "save_csv(best_cn2, BEST_CN2)\n",
        "\n",
        "print({\"changed\": int((best_cn2[\"best_k\"] != 0).sum())})\n",
        "best_cn2.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5Yr-Q3hQqBO",
        "outputId": "91292a9f-8e44-4bfe-cae6-31759da14c1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE TOP : {'r1_f': 0.4263, 'r2_f': 0.1929, 'rl_f': 0.2856}\n",
            "ROUGE CN2 : {'r1_f': 0.4242, 'r2_f': 0.19, 'rl_f': 0.2866}\n",
            "Δ ROUGE   : {'r1': -0.002, 'r2': -0.0029, 'rl': 0.001}\n",
            "ENT TOP : {'TP': 272, 'FP': 287, 'FN': 278, 'entP': 0.4866, 'entR': 0.4945, 'entF1': 0.4905}\n",
            "ENT CN2 : {'TP': 274, 'FP': 292, 'FN': 276, 'entP': 0.4841, 'entR': 0.4982, 'entF1': 0.491}\n",
            "Δ ENT   : {'entP': -0.0025, 'entR': 0.0036, 'entF1': 0.0005}\n"
          ]
        }
      ],
      "source": [
        "# === ROUGE & Entity eval ===\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Align top-beam (k=0) with best reranked\n",
        "top_k0 = details_cn2[details_cn2[\"candidate_k\"]==0][[\"row_id\",\"summary\"]]\\\n",
        "           .rename(columns={\"summary\":\"top_beam_summary\"})\n",
        "ev = best_cn2.merge(top_k0, on=\"row_id\", how=\"left\")\\\n",
        "             .rename(columns={\"summary\":\"cn2_summary\"})\n",
        "\n",
        "refs  = ev[\"reference\"].astype(str).tolist()\n",
        "tops  = ev[\"top_beam_summary\"].astype(str).tolist()\n",
        "bests = ev[\"cn2_summary\"].astype(str).tolist()\n",
        "\n",
        "# ROUGE\n",
        "sc = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\"], use_stemmer=True)\n",
        "def _mean(refs, hyps):\n",
        "    r1=r2=rl=0.0\n",
        "    for r,h in zip(refs, hyps):\n",
        "        s=sc.score(r,h); r1+=s[\"rouge1\"].fmeasure; r2+=s[\"rouge2\"].fmeasure; rl+=s[\"rougeLsum\"].fmeasure\n",
        "    n=len(refs); return r1/n, r2/n, rl/n\n",
        "r1t,r2t,rlt = _mean(refs, tops)\n",
        "r1b,r2b,rlb = _mean(refs, bests)\n",
        "print(\"ROUGE TOP :\", {\"r1_f\":round(r1t,4), \"r2_f\":round(r2t,4), \"rl_f\":round(rlt,4)})\n",
        "print(\"ROUGE CN2 :\", {\"r1_f\":round(r1b,4), \"r2_f\":round(r2b,4), \"rl_f\":round(rlb,4)})\n",
        "print(\"Δ ROUGE   :\", {\"r1\":round(r1b-r1t,4), \"r2\":round(r2b-r2t,4), \"rl\":round(rlb-rlt,4)})\n",
        "\n",
        "# Entity metrics\n",
        "def ents_norm_list(text):\n",
        "    doc = nlp(text)\n",
        "    def norm(t):\n",
        "        t=t.lower().strip()\n",
        "        t=re.sub(r\"(?<=\\d),(?=\\d{3}\\b)\",\"\",t).replace(\"%\",\" percent\")\n",
        "        t=re.sub(r\"\\s+\",\" \",t); return t\n",
        "    return [(norm(ent.text), ent.label_) for ent in doc.ents]\n",
        "\n",
        "def entPRF(refs, hyps):\n",
        "    ref_all = [t for r in refs for (t,_) in ents_norm_list(r)]\n",
        "    hyp_all = [t for h in hyps for (t,_) in ents_norm_list(h)]\n",
        "    ref_set, hyp_set = set((t,) for t in ref_all), set((t,) for t in hyp_all)\n",
        "    tp = len(ref_set & hyp_set); fp = len(hyp_set - ref_set); fn = len(ref_set - hyp_set)\n",
        "    p = tp/(tp+fp) if tp+fp else 0.0\n",
        "    r = tp/(tp+fn) if tp+fn else 0.0\n",
        "    f = 2*p*r/(p+r) if p+r else 0.0\n",
        "    return dict(TP=tp, FP=fp, FN=fn, entP=p, entR=r, entF1=f)\n",
        "\n",
        "ent_top  = entPRF(refs, tops)\n",
        "ent_cn2  = entPRF(refs, bests)\n",
        "print(\"ENT TOP :\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in ent_top.items()})\n",
        "print(\"ENT CN2 :\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in ent_cn2.items()})\n",
        "print(\"Δ ENT   :\", {k:round(ent_cn2[k]-ent_top[k],4) for k in [\"entP\",\"entR\",\"entF1\"]})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CazFR00ZSYcF"
      },
      "source": [
        "# Hyb exp-2 1k sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDKzKuF3SApm"
      },
      "outputs": [],
      "source": [
        "# === Stronger EntityAware CopyNext processor ===\n",
        "class EntityAwareSpanProcessor(LogitsProcessor):\n",
        "    def __init__(self, src_input_ids, tokenizer, num_beams,\n",
        "                 gamma=0.5, gamma_entity=2.5, max_span=8, skip_special=True):\n",
        "        self.src_ids = [ids.tolist() if torch.is_tensor(ids) else list(ids) for ids in src_input_ids]\n",
        "        self.num_beams = int(num_beams)\n",
        "        self.gamma = float(gamma)\n",
        "        self.gamma_entity = float(gamma_entity)\n",
        "        self.max_span = int(max_span)\n",
        "        self.skip_ids = set(tokenizer.all_special_ids) if skip_special else set()\n",
        "\n",
        "        # Precompute entity token sets\n",
        "        self.entity_token_sets = []\n",
        "        for ids in self.src_ids:\n",
        "            text = tokenizer.decode(ids, skip_special_tokens=True)\n",
        "            ents = [ent.text for ent in nlp(text).ents]\n",
        "            tok_sets = []\n",
        "            for e in ents:\n",
        "                tok_ids = tokenizer(e, add_special_tokens=False)[\"input_ids\"]\n",
        "                if tok_ids:\n",
        "                    tok_sets.append(set(tok_ids))\n",
        "            self.entity_token_sets.append(tok_sets)\n",
        "\n",
        "    @staticmethod\n",
        "    def _find_suffix_in_src(src_ids, hyp_ids, max_span):\n",
        "        if not hyp_ids: return 0, -1\n",
        "        max_check = min(len(hyp_ids), max_span)\n",
        "        for L in range(max_check, 0, -1):\n",
        "            suffix = hyp_ids[-L:]\n",
        "            for end in range(L-1, len(src_ids)):\n",
        "                if src_ids[end-L+1:end+1] == suffix:\n",
        "                    return L, end\n",
        "        return 0, -1\n",
        "\n",
        "    def __call__(self, input_ids, scores):\n",
        "        B_beams = input_ids.size(0)\n",
        "        for row in range(B_beams):\n",
        "            sample_idx = row // self.num_beams\n",
        "            src = self.src_ids[sample_idx]\n",
        "            hyp = input_ids[row].tolist()\n",
        "            L, end = self._find_suffix_in_src(src, hyp, self.max_span)\n",
        "            if L >= 1 and end + 1 < len(src):\n",
        "                next_id = src[end + 1]\n",
        "                if next_id not in self.skip_ids:\n",
        "                    boost = self.gamma\n",
        "                    for tokset in self.entity_token_sets[sample_idx]:\n",
        "                        if next_id in tokset:\n",
        "                            boost = self.gamma_entity\n",
        "                            break\n",
        "                    scores[row, next_id] += boost\n",
        "        return scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537,
          "referenced_widgets": [
            "383af00b12ac43318375bde5e89712c0"
          ]
        },
        "id": "93taszsnSkxA",
        "outputId": "45bcc3eb-77dd-455d-a0ed-9206426a4f00"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "383af00b12ac43318375bde5e89712c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "entities(vs source):   0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[saved] /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/decodes_copynext2/copynext2_details_20250826_161305.csv (1000 rows)\n",
            "[saved] /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/decodes_copynext2/copynext2_best_20250826_161305.csv (100 rows)\n",
            "{'changed': 83}\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"best_cn2\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"row_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          83,\n          53,\n          70\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"article\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"FA chairman Greg Dyke has called for a public debate between the four candidates for the FIFA presidency and offered to host the event at Wembley. Dyke made his wish for a TV inquisition known during an awkward weekend in Sepp Blatter's company at the rules-deciding IFAB meeting in Belfast. The FA have made clear their strong opposition to the FIFA president serving a fifth term. Meanwhile, in a separate move, Sky and the BBC have made a joint approach to Blatter and rivals Prince Ali of Jordan, Luis Figo and Michael van Praag to stage such an event. FA chairman Greg Dyke has called for a public debate between the four candidates for the FIFA presidency . The TV networks - through leading sports news broadcasters Paul Kelso and Richard Conway - have written to the contestants outlining their ambitious proposals for a live, hour-long debate in the UK with an audience of fans representing all 209 FIFA member nations. This fans' congress would provide questions, with others drawn from football supporters via the Sky and BBC websites, and Facebook, to ensure maximum interaction. All four challengers have been promised equal time and emphasis to present their manifestos and visions for FIFA's future. Blatter has yet to respond to the Sky-BBC letter but it is highly unlikely he'll agree to such public exposure. His three opponents will all be in favour. Sepp Blatter has yet to respond to the Sky-BBC letter but it is unlikely he'll agree to such public exposure . The official photo of the IFAB summit shows Greg Dyke seated between, of all people, Sepp Blatter and Thailand's Worawi Makudi, whose presence at Belfast's Cullodon Hotel as a representative of the Asian Football Confederation was bizarre in the extreme. Makudi has an acrimonious history with the FA and is under investigation by FIFA's ethics committee for breaching World Cup bid rules. It's understood Makudi played no part in the rules debate. FIFA said Makudi was selected by the AFC to attend and is innocent of any code violations until found guilty. The home nations are in disagreement as to whose turn it is to take the FIFA British vice-presidency . The 2011 gentlemen's agreement between the four home nations, which declares it is Wales's turn to take the FIFA British vice-presidency, continues to cause ructions. The three other countries believe the agreement was nullified by a new statute that has all UEFA associations involved in the vote at the Congress later this month. But Welsh president Trefor Lloyd-Hughes says he has a signed contract in his possession that declares no change can be made in the rotation pledge unless there is unanimous agreement. Meanwhile, England's David Gill, persuaded by UEFA president Michel Platini to stand for that FIFA place against Lloyd-Hughes, has written to all UEFA countries outlining his plans. To spice up their battle, Wales are still annoyed that a UEFA ExCo, including Gill, voted for Hampden Park over the Millennium Stadium as one of 13 venues to stage Euro 2020. It's believed Gill rated the Cardiff venue higher. General secretary Alex Horne left the FA at the end of January but has not yet been replaced . Greg Dyke and David Gill, who are on the nominations panel choosing the FA's next chief executive, were coy over the weekend about how far they have progressed in finding general secretary Alex Horne's replacement. However, it's understood the selection has been made after final interviews last week and an announcement is imminent. Zimbabwe's Sean Williams celebrates taking the wicket of Umar Akmal during their World Cup match . BBC ON STICKY WICKET . BBC Sport's live text reporting of Pakistan's World Cup victory over Zimbabwe seems to have relied heavily on rival website Cricinfo. Umar Akmal's dismissal, bowled by Sean Williams, was described in the following way on Cricinfo: 'Akmal made it look an even better delivery than it actually was, because he had moved inside the line and was trying to push that inside-out through cover.' The licence-fee-funded BBC, who had just changed commentators, wrote: 'Akmal made it look an even better delivery than it actually was, because he had moved inside the line and was trying to push that inside out through cover.' Exactly the same. A BBC spokeswoman said: 'The line should have been credited. This was a simple human error.' The post-11pm peak viewing audience of two million on ITV for the Carl Frampton fight on Saturday has left the network intent on showing more live boxing. Television network ITV are determined to show more live boxing after seeing recent viewing figures .\",\n          \"Following Sunday's El Clasico spectacle that hugely intensified the run-in for the La Liga title between Barcelona and Real Madrid, Rio Ferdinand gave his stance on the age-old debate of who is better: Cristano Ronaldo or Lionel Messi? Choosing to stay somewhat impartial to the discussion the former England international says we should stand back and appreciate the sheer brilliance we are witnessing in the presence of two of the world's greatest ever players. 'It's inevitable we do all this \\\"who's better? Blah, blah, blah\\\" but we should just be enjoying being around these two players and seeing it,' the QPR defender said. Rio Ferdinand (right) speaks to the camera alongside Queen's Park Rangers team-mate Bobby Zamora . Lionel Messi is La Liga's top scorer this season with 32 goals but Cristiano Ronaldo is not far behind . The Real Madrid striker's goal in El Clasico on Sunday took him to just one goal behind the Barcelona forward . 'When Ronaldo, the Brazilian one - who should have been the best if he weren't injured - was scoring 30 goals a season [everybody said]: \\\"Oh he's a genius, he's the best ever.\\\"' Messi and Ronaldo are this season's top scorers in La Liga, with 32 and 31 goals respectively with 10 games to go. Speaking from his car, kitted out with an X-Box and television screen, the former Manchester United star posted an image on Instagram of himself and Hoops team-mate Bobby Zamora. The two were taking each other on in an in-car game of FIFA whilst travelling to training at the Imperial College Sports Ground, London. The 2008 Champions League winner wrote: 'En route to training.. Winning at FIFA #shock' after beating the former Fulham striker 2-1 with Paris Saint Germain. The former England international raises his finger in victory after beating Zamora in an in-car game of FIFA .\",\n          \"It is a measure of Arsenal's progress that fixtures like these no longer end in boos and angry recrimination. As the final whistle approached, the crowd at the Emirates were unusually vocal, anticipating trips to Wembley and singing praise of their team. Earlier it had the feel of one of the afternoons with which we have become so familiar in recent years, one of those days when Arsenal break world records for possessions stats but come up against an inspired goalkeeper and end the day with fans furious at their manager, the board of directors and life in general. But with Olivier Giroud in exceptional form, Aaron Ramsey also outstanding and Mesut Ozil thriving against the kind of opposition he relishes, Arsenal eventually cantered to victory in some style. Olivier Giroud leaps in the air after opening the scoring in Arsenal's 3-0 Premier League victory over West Ham . The France international gives Arsenal the lead on the stroke of half time with his sixth goal in his last seven games . Aaron Ramsey is chased by Santi Cazorla after doubling Arsenal's lead in the closing stages of the second half . Giroud gives substitute Mathieu Flamini a pat on the back after the Frenchman seals victory with a late strike . ARSENAL: Ospina 6.5, Chambers 7, Mertesacker 6.5, Koscielny 6.5, Monreal 7, Ramsey 7, Coquelin 7, Walcott 6 (Cazorla 6.5), Ozil 7 (Flamini 6.5), Sanchez 6 (Welbeck 6.5), Giroud 8.5 . Subs: Szczesny, Gibbs, Akpom, Bellerin . Scorers: Giroud 45, Ramsey 81, Flamini 84 . Booked: Sanchez . WEST HAM: Arian 8, O'Brien 6.5, Kouyate 6.5, Cresswell 7, Downing 6, Noble 6 (Nene 6), Song 6.5, Nolan 6, Jarvis 6.5 (Amalfitano 6), Sakho 6.5 . Subs: Demel, J\\u00e4\\u00e4skel\\u00e4inen, Poyet, Cullen, Onariase . Booked: Sakho . Man of the Match: Olivier Giroud . Referee: Chris Foy/ Anthony Taylor . CLICK HERE for all the stats, including Olivier Giroud's heat map from our superb Match Zone . Better measures of their real progress remain, of course. On Tuesday night they visit Monaco where you might anticipate another glorious comeback which ultimately ends in failure. Then there will be the league table at the end of the season which will likely tell the tale of an opportunity missed to challenge for the title. Arsene Wenger conceded that, even with Chelsea still to come to the Emirates, Arsenal are not yet in the title race. 'Not at the moment,' he said. 'But we can just keep going. We won eight of the last nine. We are stronger today than we were at the start of the season. 'We suffered a lot from the World Cup, where the players came back. What is for sure is that they understand each other much better than six or seven months ago and that makes everyone more dangerous.' At least Arsenal are not going backwards these days; the problem may be that they aren't moving forwards fast enough but they are at least pointing in the right direction. West Ham defender James Collins (left) was fortunate not to concede an early penalty after brining down Theo Walcott . The Arsenal winger crashes to the turf under Collins' challenge but was not awarded a spot kick . West Ham forward Diafra Sakho (centre) tries to evade Arsenal defenders Laurent Koscielny and Per Mertesacker (left) Arsenal playmaker Mesut Ozil (left) uses his silky skills to bring the ball down in front of Stewart Downing (centre) and Mark Noble . Arsenal's leading scorer Alexis Sanchez (centre) uses his trickery to escape the attentions of Joey O'Brien . Hammers keeper Adrian threatened to spoil Arsenal's afternoon with a succession of fine first half saves . West Ham manager Sam Allardyce looked as though he understood as much and had settled for one of those afternoons when he aims to 'out-tactic' the opposition. You could hardly blame him given the resources at his disposal, with Enner Valencia and James Tomkins the latest additions to his injury list and Carl Jenkinson ineligible. Kevin Nolan almost shocked Arsenal with a clean strike on 23 minutes but at half-time, Arsenal had 74 per cent of the possession. At times it looked as if West Ham goalkeeper Adrian was has having one of those inspired afternoon. It was later revealed he was playing with a dislocated finger sustained in the warm up, a fact which made his performance truly heroic. 'He's been a brave lad,' said assistant manager Neil McDonald. Theo Walcott was having a less satisfactory time. Thrust back in for his first start in more than a month, he did little to help his ongoing contract talks. Three times he was presented with the opportunity to open the scoring; three times he spurned the chances. Gunners defender Calum Chambers closes down West Ham winger Matt Jarvis as Arsenal take control . Song and Koscielny compete for the ball in an aerial duel as the sun shines at the Emirates Stadium . West Ham manager Sam Allardyce lets his feelings of frustration be known from the sidelines . He could plead mitigation for the first effort on six minutes. A delightful back heel from Giroud set him up with just Adrian to beat, but James Collins came through the man and the ball to prevent him scoring. It might have been a penalty but it was Walcott's own hesitation in front of goal which had presented Collins with the chance to launch his saving tackle. Adrian would then make excellent saves from Alexis Sanchez's header on 16 minutes and from an improvised touch of the knee from Ramsey on 32 minutes, before Walcott received his next chance. Played in by a delightful Ramsey pass, he took a touch, looked up and struck his shot directly at the goalkeeper.Adrian denied Sanchez from a half volley and on 44 minutes he pushed away a strike from Ozil, which landed at Walcott's feet, yards from goal but he skewed the ball high and wide, appearing to lose his balance. Ozil reacts after West Ham keeper Adrian pulls off one of a succession of first half saves . Frenchman Giroud gets his shot away despite being surrounded by four West Ham players . The Gunners centre forward watches as the ball sails towards the left corner of West Ham's goal . Adrian, who had previously kept West Ham in the game with a series of fine saves, dives in vain towards Giroud's shot . Giroud was on hand to provide deliverance just before half-time. Ramsey and Ozil exchanged a delicate one-two, but such is the Frenchman's confidence these days, he nicked it off Ramsey's foot, took a touch and drove a fine finish into the far corner of the net. 'It was fantastic because of the combination and the finishing. It was the kind of goal we loved to score,' said Wenger. It was his 14th goal of the season in 24 games. That Monaco game aside, he is having an excellent season and since that dismal night, he has responded incredibly well. After being criticised for missing chances against Monaco, Giroud holds his hands to his ears to soak up the applause . Giroud strikes a pose midway through his celebration as Arsenal take the lead at the Emirates . Walcott, who had several chances to open the first half scoring himself, leaps on Giroud's back in celebration . 'It's one of his strengths,' said Wenger, who called his performance 'outstanding'. Having set out to defend in numbers, a change of strategy was necessary for West Ham and they were equal to the challenge. They came out in the second-half with an entirely different mindset, pushing Arsenal back into their half and searching for an equaliser. In terms of clear-cut chances there was still little - Matt Jarvis' curling cross which Ospina spilled and almost let in Sakho was one unsettling moment. But Arsenal were no longer able to play as they wished. Still, their time would come. On 80 minutes, Giroud dummied a throw-in to let in Ramsey, who played in the Frenchman. He returned the ball to Ramsey, who drove it past Adrian to cap a fine a performance of growing authority. Three minutes later Santi Cazorla, on as a substitute, exchanged passes with Giroud and planted a cross almost on the foot of Mathieu Flamini, who had the simplest task of converting from close range for the third goal. Ramsey (right) gets some air as he battles for the ball with former Arsenal team-mate Song . West Ham manager Sam Allardyce looks on bemused as referee Chris Foy is forced to hand over to assistant Anthony Taylor . Former Manchester United forward Danny Welbeck (right) was brought on in the second half to replace Sanchez . Arsenal manager Arsene Wenger watches impatiently from the touchline with his side only leading by one goal . Wales midfielder Ramsey meets Giroud's pass with a left footed shot to score his first goal since December . Ramsey wheels away to celebrate giving Arsenal a 2-0 lead on 81minutes of the London derby . Aaron Ramsey is swamped by team-mates Giroud and Danny Welbeck after doubling Arsenal's lead . Substitute Mathieu Flamini is in the right place at the right time as he meets Cazorla's cross to seal a 3-0 victory . Arsenal team-mates embrace Flamini as they keep alive their hopes of playing Champions League football next season .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Greg Dyke wants a public debate between the four candidates for the FIFA presidency and has offered to host it at Wembley Stadium . BBC and Sky are also keen to host a debate between the candidates . The home nations are in disagreement as to whose turn it is to take the FIFA British vice-presidency . BBC appear to have plagiarised Cricinfo in an online live text report .\",\n          \"Rio Ferdinand played FIFA 15 and talked Lionel Messi vs Cristiano Ronaldo . The QPR defender said we should appreciate the talents of the magical duo . Ferdinand spoke from his car complete with an X-Box and multiple screens .\",\n          \"Arsenal kept up their chase for a top four finish in the Premier League with a 3-0 win over West Ham at the Emirates . Olivier Giroud netted his sixth goal in seven games to open the scoring for Gunners on the stroke of half time . Referee Chris Foy was replaced by assistant Anthony Taylor midway through the second half . Wales midfielder Aaron Ramsey was assisted by Giroud to double the lead with his first goal since December . Substitute Mathieu Flamini sealed victory after coming on to meet a cross from Santi Cazorla .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"FA chairman Greg Dyke has called for a public debate between the four candidates for the FIFA presidency. The TV networks have written to the contestants outlining their ambitious proposals for a live, hour-long debate. The FA have made a joint approach to Blatter and rivals Prince Ali of Jordan, Luis Figo and Michael van Praag to stage the event.\",\n          \"Former England international says we should stand back and appreciate the sheer brilliance we are witnessing in the presence of two of the world's greatest ever players. The former Manchester United star posted an image on Instagram of himself and Hoops team-mate Bobby Zamora. The pair were taking each other on in an in-car game of FIFA. The two were taking part in the game while travelling to training at the Imperial College Sports Ground in London.\",\n          \"Olivier Giroud opened the scoring in Arsenal's 3-0 Premier League victory over West Ham. The France international gave Arsenal the lead with a late strike. Aaron Ramsey and Mesut Ozil also scored for the Gunners. Arsenal's leading scorer Alexis Sanchez scored twice in the second half.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_k\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 9,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          5,\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"base_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004034534440989368,\n        \"min\": -0.014947611838579178,\n        \"max\": 0.006623337976634502,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          -0.0029996251687407494,\n          -0.0057356529869139194,\n          -0.009579298086464405\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"len\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 80,\n        \"max\": 100,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          97,\n          82,\n          100\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contiguity_lcs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13,\n        \"min\": 11,\n        \"max\": 82,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          50,\n          51,\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_span_len_lcs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.30829763852731,\n        \"min\": 3.0,\n        \"max\": 57.0,\n        \"num_unique_values\": 83,\n        \"samples\": [\n          12.6,\n          9.0,\n          14.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"unsupported\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          0,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"supported\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 2,\n        \"max\": 14,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          14,\n          10,\n          12\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"len_reward\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.13217350897585306,\n        \"min\": 0.11025052530448522,\n        \"max\": 0.5460744266397094,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          0.14938177525041801,\n          0.4867522559599717,\n          0.11025052530448522\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"new_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14.771057858810574,\n        \"min\": 20.34533061971487,\n        \"max\": 80.65891310989511,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          52.859062574202525,\n          52.68281639130444,\n          55.63211267323588\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "best_cn2"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-8865a72b-1314-4dcc-84e4-bf1185718501\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>row_id</th>\n",
              "      <th>article</th>\n",
              "      <th>reference</th>\n",
              "      <th>summary</th>\n",
              "      <th>best_k</th>\n",
              "      <th>base_score</th>\n",
              "      <th>len</th>\n",
              "      <th>contiguity_lcs</th>\n",
              "      <th>avg_span_len_lcs</th>\n",
              "      <th>unsupported</th>\n",
              "      <th>supported</th>\n",
              "      <th>len_reward</th>\n",
              "      <th>new_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>These days the Cheltenham Festival is a marath...</td>\n",
              "      <td>Willie Mullins looks to have chance of establi...</td>\n",
              "      <td>DOUVAN is the first of the Mullins battalion t...</td>\n",
              "      <td>9</td>\n",
              "      <td>0.002351</td>\n",
              "      <td>100</td>\n",
              "      <td>40</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>0.110251</td>\n",
              "      <td>63.033311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>A baby girl was found alive after being strapp...</td>\n",
              "      <td>Lynn Jennifer Groesbeck, 25, was driving home ...</td>\n",
              "      <td>Lynn Jennifer Groesbeck, 25, from Springville,...</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.006587</td>\n",
              "      <td>100</td>\n",
              "      <td>34</td>\n",
              "      <td>6.666667</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0.110251</td>\n",
              "      <td>46.832413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>A 36-year-old woman and her three children wer...</td>\n",
              "      <td>40-year-old Domingo Villa Arellano, who is an ...</td>\n",
              "      <td>A 36-year-old woman and her three children wer...</td>\n",
              "      <td>8</td>\n",
              "      <td>-0.005785</td>\n",
              "      <td>100</td>\n",
              "      <td>57</td>\n",
              "      <td>15.250000</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>0.110251</td>\n",
              "      <td>46.432494</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8865a72b-1314-4dcc-84e4-bf1185718501')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8865a72b-1314-4dcc-84e4-bf1185718501 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8865a72b-1314-4dcc-84e4-bf1185718501');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-869a25d4-9c70-40a2-bffd-ff58c81adef5\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-869a25d4-9c70-40a2-bffd-ff58c81adef5')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-869a25d4-9c70-40a2-bffd-ff58c81adef5 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   row_id                                            article  \\\n",
              "0       0  These days the Cheltenham Festival is a marath...   \n",
              "1       1  A baby girl was found alive after being strapp...   \n",
              "2       2  A 36-year-old woman and her three children wer...   \n",
              "\n",
              "                                           reference  \\\n",
              "0  Willie Mullins looks to have chance of establi...   \n",
              "1  Lynn Jennifer Groesbeck, 25, was driving home ...   \n",
              "2  40-year-old Domingo Villa Arellano, who is an ...   \n",
              "\n",
              "                                             summary  best_k  base_score  len  \\\n",
              "0  DOUVAN is the first of the Mullins battalion t...       9    0.002351  100   \n",
              "1  Lynn Jennifer Groesbeck, 25, from Springville,...       1   -0.006587  100   \n",
              "2  A 36-year-old woman and her three children wer...       8   -0.005785  100   \n",
              "\n",
              "   contiguity_lcs  avg_span_len_lcs  unsupported  supported  len_reward  \\\n",
              "0              40          9.000000            1         12    0.110251   \n",
              "1              34          6.666667            0          8    0.110251   \n",
              "2              57         15.250000            1          8    0.110251   \n",
              "\n",
              "   new_score  \n",
              "0  63.033311  \n",
              "1  46.832413  \n",
              "2  46.432494  "
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# === Offline rerank with stronger entity weighting ===\n",
        "dd = details_cn2.copy()\n",
        "\n",
        "# LCS stats\n",
        "contigs, avlens = [], []\n",
        "for i,row in dd.iterrows():\n",
        "    c,a = lcs_span_stats(row[\"article\"], row[\"summary\"])\n",
        "    contigs.append(c); avlens.append(a)\n",
        "dd[\"contiguity_lcs\"] = contigs\n",
        "dd[\"avg_span_len_lcs\"] = avlens\n",
        "\n",
        "# Unsupported entities vs source\n",
        "unsupp = []\n",
        "supp   = []\n",
        "for i,row in tqdm(list(dd.iterrows()), total=len(dd), desc=\"entities(vs source)\"):\n",
        "    u = unsupported_count(row[\"article\"], row[\"summary\"])\n",
        "    unsupp.append(u)\n",
        "    # reward = # of overlapping entities\n",
        "    src_ents = _ents_norm(row[\"article\"])\n",
        "    hyp_ents = _ents_norm(row[\"summary\"])\n",
        "    supp.append(len(hyp_ents & src_ents))\n",
        "dd[\"unsupported\"] = unsupp\n",
        "dd[\"supported\"]   = supp\n",
        "\n",
        "# Length reward\n",
        "TARGET_LEN = 58\n",
        "LEN_SIGMA = 20.0\n",
        "def length_reward(l, target=TARGET_LEN, sigma=LEN_SIGMA):\n",
        "    return math.exp(-((l - target)**2) / (2 * sigma**2))\n",
        "\n",
        "dd[\"len\"] = dd[\"len\"].clip(lower=1)\n",
        "dd[\"len_reward\"] = dd[\"len\"].apply(lambda l: length_reward(l))\n",
        "\n",
        "# === Entity-heavy scoring ===\n",
        "# entity_score = supported - unsupported\n",
        "dd[\"entity_score\"] = dd[\"supported\"] - dd[\"unsupported\"]\n",
        "\n",
        "dd[\"new_score\"] = (5.0 * dd[\"entity_score\"]) \\\n",
        "                  + (0.4 * (dd[\"base_score\"] / (dd[\"len\"]**0.3))) \\\n",
        "                  + (0.2 * dd[\"contiguity_lcs\"]) \\\n",
        "                  + (0.3 * dd[\"len_reward\"])\n",
        "\n",
        "best_cn2 = (\n",
        "    dd.sort_values([\"row_id\",\"new_score\"], ascending=[True, False])\n",
        "      .groupby(\"row_id\", as_index=False).head(1)\n",
        "      .loc[:, [\"row_id\",\"article\",\"reference\",\"summary\",\"candidate_k\",\n",
        "               \"base_score\",\"len\",\"contiguity_lcs\",\"avg_span_len_lcs\",\n",
        "               \"unsupported\",\"supported\",\"len_reward\",\"new_score\"]]\n",
        "      .rename(columns={\"candidate_k\":\"best_k\"})\n",
        "      .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "save_csv(dd,       DETAILS_CN2)\n",
        "save_csv(best_cn2, BEST_CN2)\n",
        "\n",
        "print({\"changed\": int((best_cn2['best_k'] != 0).sum())})\n",
        "best_cn2.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkTlFJqBTlUf",
        "outputId": "70d5c266-8528-4b2f-df62-5bd3e8cc8c0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE TOP : {'r1_f': 0.4263, 'r2_f': 0.1929, 'rl_f': 0.2856}\n",
            "ROUGE CN2 : {'r1_f': 0.4352, 'r2_f': 0.2022, 'rl_f': 0.2914}\n",
            "Δ ROUGE   : {'r1': 0.0089, 'r2': 0.0093, 'rl': 0.0058}\n",
            "ENT TOP : {'TP': 272, 'FP': 287, 'FN': 278, 'entP': 0.4866, 'entR': 0.4945, 'entF1': 0.4905}\n",
            "ENT CN2 : {'TP': 304, 'FP': 341, 'FN': 246, 'entP': 0.4713, 'entR': 0.5527, 'entF1': 0.5088}\n",
            "Δ ENT   : {'entP': -0.0153, 'entR': 0.0582, 'entF1': 0.0183}\n"
          ]
        }
      ],
      "source": [
        "# === ROUGE & Entity eval: top-beam vs entity-aware CN2 ===\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Align top-beam (k=0) with best reranked\n",
        "top_k0 = details_cn2[details_cn2[\"candidate_k\"]==0][[\"row_id\",\"summary\"]]\\\n",
        "           .rename(columns={\"summary\":\"top_beam_summary\"})\n",
        "ev = best_cn2.merge(top_k0, on=\"row_id\", how=\"left\")\\\n",
        "             .rename(columns={\"summary\":\"cn2_summary\"})\n",
        "\n",
        "refs  = ev[\"reference\"].astype(str).tolist()\n",
        "tops  = ev[\"top_beam_summary\"].astype(str).tolist()\n",
        "bests = ev[\"cn2_summary\"].astype(str).tolist()\n",
        "\n",
        "# ---- ROUGE ----\n",
        "sc = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\"], use_stemmer=True)\n",
        "def _mean(refs, hyps):\n",
        "    r1=r2=rl=0.0\n",
        "    for r,h in zip(refs, hyps):\n",
        "        s=sc.score(r,h); r1+=s[\"rouge1\"].fmeasure; r2+=s[\"rouge2\"].fmeasure; rl+=s[\"rougeLsum\"].fmeasure\n",
        "    n=len(refs); return r1/n, r2/n, rl/n\n",
        "\n",
        "r1t,r2t,rlt = _mean(refs, tops)\n",
        "r1b,r2b,rlb = _mean(refs, bests)\n",
        "\n",
        "print(\"ROUGE TOP :\", {\"r1_f\":round(r1t,4), \"r2_f\":round(r2t,4), \"rl_f\":round(rlt,4)})\n",
        "print(\"ROUGE CN2 :\", {\"r1_f\":round(r1b,4), \"r2_f\":round(r2b,4), \"rl_f\":round(rlb,4)})\n",
        "print(\"Δ ROUGE   :\", {\"r1\":round(r1b-r1t,4), \"r2\":round(r2b-r2t,4), \"rl\":round(rlb-rlt,4)})\n",
        "\n",
        "# ---- Entity metrics ----\n",
        "def ents_norm_list(text):\n",
        "    doc = nlp(text)\n",
        "    def norm(t):\n",
        "        t=t.lower().strip()\n",
        "        t=re.sub(r\"(?<=\\d),(?=\\d{3}\\b)\",\"\",t).replace(\"%\",\" percent\")\n",
        "        t=re.sub(r\"\\s+\",\" \",t); return t\n",
        "    return [(norm(ent.text), ent.label_) for ent in doc.ents]\n",
        "\n",
        "def entPRF(refs, hyps):\n",
        "    ref_all = [t for r in refs for (t,_) in ents_norm_list(r)]\n",
        "    hyp_all = [t for h in hyps for (t,_) in ents_norm_list(h)]\n",
        "    ref_set, hyp_set = set((t,) for t in ref_all), set((t,) for t in hyp_all)\n",
        "    tp = len(ref_set & hyp_set); fp = len(hyp_set - ref_set); fn = len(ref_set - hyp_set)\n",
        "    p = tp/(tp+fp) if tp+fp else 0.0\n",
        "    r = tp/(tp+fn) if tp+fn else 0.0\n",
        "    f = 2*p*r/(p+r) if p+r else 0.0\n",
        "    return dict(TP=tp, FP=fp, FN=fn, entP=p, entR=r, entF1=f)\n",
        "\n",
        "ent_top  = entPRF(refs, tops)\n",
        "ent_cn2  = entPRF(refs, bests)\n",
        "\n",
        "print(\"ENT TOP :\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in ent_top.items()})\n",
        "print(\"ENT CN2 :\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in ent_cn2.items()})\n",
        "print(\"Δ ENT   :\", {k:round(ent_cn2[k]-ent_top[k],4) for k in [\"entP\",\"entR\",\"entF1\"]})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAMsiXolV6Lr"
      },
      "source": [
        "# Hybrid exp-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhZuu8xQVAih"
      },
      "outputs": [],
      "source": [
        "# === Entity-type weighted scoring helpers ===\n",
        "\n",
        "def weighted_entity_score(src_text, hyp_text):\n",
        "    \"\"\"Reward/penalize entities by type.\"\"\"\n",
        "    src_ents = _ents_norm(src_text)\n",
        "    hyp_ents = _ents_norm(hyp_text)\n",
        "\n",
        "    overlap = hyp_ents & src_ents\n",
        "    unsupported = hyp_ents - src_ents\n",
        "\n",
        "    score = 0\n",
        "    for ent, label in overlap:\n",
        "        if label in [\"PERSON\", \"ORG\", \"GPE\"]:\n",
        "            score += 2   # strong reward\n",
        "        else:\n",
        "            score += 1   # softer reward\n",
        "    for ent, label in unsupported:\n",
        "        if label in [\"PERSON\", \"ORG\", \"GPE\"]:\n",
        "            score -= 2   # strong penalty\n",
        "        else:\n",
        "            score -= 1   # softer penalty\n",
        "    return score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537,
          "referenced_widgets": [
            "12040fd6495a411289d0e8157ed00bf0"
          ]
        },
        "id": "zM_lI_9sWU1n",
        "outputId": "8a74b77d-56ce-4eaa-bde2-4c7d270a1703"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12040fd6495a411289d0e8157ed00bf0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "weighted entity scores:   0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[saved] /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/decodes_copynext2/copynext2_details_20250826_161305.csv (1000 rows)\n",
            "[saved] /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/decodes_copynext2/copynext2_best_20250826_161305.csv (100 rows)\n",
            "{'changed': 84}\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"best_cn2\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"row_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          83,\n          53,\n          70\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"article\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"FA chairman Greg Dyke has called for a public debate between the four candidates for the FIFA presidency and offered to host the event at Wembley. Dyke made his wish for a TV inquisition known during an awkward weekend in Sepp Blatter's company at the rules-deciding IFAB meeting in Belfast. The FA have made clear their strong opposition to the FIFA president serving a fifth term. Meanwhile, in a separate move, Sky and the BBC have made a joint approach to Blatter and rivals Prince Ali of Jordan, Luis Figo and Michael van Praag to stage such an event. FA chairman Greg Dyke has called for a public debate between the four candidates for the FIFA presidency . The TV networks - through leading sports news broadcasters Paul Kelso and Richard Conway - have written to the contestants outlining their ambitious proposals for a live, hour-long debate in the UK with an audience of fans representing all 209 FIFA member nations. This fans' congress would provide questions, with others drawn from football supporters via the Sky and BBC websites, and Facebook, to ensure maximum interaction. All four challengers have been promised equal time and emphasis to present their manifestos and visions for FIFA's future. Blatter has yet to respond to the Sky-BBC letter but it is highly unlikely he'll agree to such public exposure. His three opponents will all be in favour. Sepp Blatter has yet to respond to the Sky-BBC letter but it is unlikely he'll agree to such public exposure . The official photo of the IFAB summit shows Greg Dyke seated between, of all people, Sepp Blatter and Thailand's Worawi Makudi, whose presence at Belfast's Cullodon Hotel as a representative of the Asian Football Confederation was bizarre in the extreme. Makudi has an acrimonious history with the FA and is under investigation by FIFA's ethics committee for breaching World Cup bid rules. It's understood Makudi played no part in the rules debate. FIFA said Makudi was selected by the AFC to attend and is innocent of any code violations until found guilty. The home nations are in disagreement as to whose turn it is to take the FIFA British vice-presidency . The 2011 gentlemen's agreement between the four home nations, which declares it is Wales's turn to take the FIFA British vice-presidency, continues to cause ructions. The three other countries believe the agreement was nullified by a new statute that has all UEFA associations involved in the vote at the Congress later this month. But Welsh president Trefor Lloyd-Hughes says he has a signed contract in his possession that declares no change can be made in the rotation pledge unless there is unanimous agreement. Meanwhile, England's David Gill, persuaded by UEFA president Michel Platini to stand for that FIFA place against Lloyd-Hughes, has written to all UEFA countries outlining his plans. To spice up their battle, Wales are still annoyed that a UEFA ExCo, including Gill, voted for Hampden Park over the Millennium Stadium as one of 13 venues to stage Euro 2020. It's believed Gill rated the Cardiff venue higher. General secretary Alex Horne left the FA at the end of January but has not yet been replaced . Greg Dyke and David Gill, who are on the nominations panel choosing the FA's next chief executive, were coy over the weekend about how far they have progressed in finding general secretary Alex Horne's replacement. However, it's understood the selection has been made after final interviews last week and an announcement is imminent. Zimbabwe's Sean Williams celebrates taking the wicket of Umar Akmal during their World Cup match . BBC ON STICKY WICKET . BBC Sport's live text reporting of Pakistan's World Cup victory over Zimbabwe seems to have relied heavily on rival website Cricinfo. Umar Akmal's dismissal, bowled by Sean Williams, was described in the following way on Cricinfo: 'Akmal made it look an even better delivery than it actually was, because he had moved inside the line and was trying to push that inside-out through cover.' The licence-fee-funded BBC, who had just changed commentators, wrote: 'Akmal made it look an even better delivery than it actually was, because he had moved inside the line and was trying to push that inside out through cover.' Exactly the same. A BBC spokeswoman said: 'The line should have been credited. This was a simple human error.' The post-11pm peak viewing audience of two million on ITV for the Carl Frampton fight on Saturday has left the network intent on showing more live boxing. Television network ITV are determined to show more live boxing after seeing recent viewing figures .\",\n          \"Following Sunday's El Clasico spectacle that hugely intensified the run-in for the La Liga title between Barcelona and Real Madrid, Rio Ferdinand gave his stance on the age-old debate of who is better: Cristano Ronaldo or Lionel Messi? Choosing to stay somewhat impartial to the discussion the former England international says we should stand back and appreciate the sheer brilliance we are witnessing in the presence of two of the world's greatest ever players. 'It's inevitable we do all this \\\"who's better? Blah, blah, blah\\\" but we should just be enjoying being around these two players and seeing it,' the QPR defender said. Rio Ferdinand (right) speaks to the camera alongside Queen's Park Rangers team-mate Bobby Zamora . Lionel Messi is La Liga's top scorer this season with 32 goals but Cristiano Ronaldo is not far behind . The Real Madrid striker's goal in El Clasico on Sunday took him to just one goal behind the Barcelona forward . 'When Ronaldo, the Brazilian one - who should have been the best if he weren't injured - was scoring 30 goals a season [everybody said]: \\\"Oh he's a genius, he's the best ever.\\\"' Messi and Ronaldo are this season's top scorers in La Liga, with 32 and 31 goals respectively with 10 games to go. Speaking from his car, kitted out with an X-Box and television screen, the former Manchester United star posted an image on Instagram of himself and Hoops team-mate Bobby Zamora. The two were taking each other on in an in-car game of FIFA whilst travelling to training at the Imperial College Sports Ground, London. The 2008 Champions League winner wrote: 'En route to training.. Winning at FIFA #shock' after beating the former Fulham striker 2-1 with Paris Saint Germain. The former England international raises his finger in victory after beating Zamora in an in-car game of FIFA .\",\n          \"It is a measure of Arsenal's progress that fixtures like these no longer end in boos and angry recrimination. As the final whistle approached, the crowd at the Emirates were unusually vocal, anticipating trips to Wembley and singing praise of their team. Earlier it had the feel of one of the afternoons with which we have become so familiar in recent years, one of those days when Arsenal break world records for possessions stats but come up against an inspired goalkeeper and end the day with fans furious at their manager, the board of directors and life in general. But with Olivier Giroud in exceptional form, Aaron Ramsey also outstanding and Mesut Ozil thriving against the kind of opposition he relishes, Arsenal eventually cantered to victory in some style. Olivier Giroud leaps in the air after opening the scoring in Arsenal's 3-0 Premier League victory over West Ham . The France international gives Arsenal the lead on the stroke of half time with his sixth goal in his last seven games . Aaron Ramsey is chased by Santi Cazorla after doubling Arsenal's lead in the closing stages of the second half . Giroud gives substitute Mathieu Flamini a pat on the back after the Frenchman seals victory with a late strike . ARSENAL: Ospina 6.5, Chambers 7, Mertesacker 6.5, Koscielny 6.5, Monreal 7, Ramsey 7, Coquelin 7, Walcott 6 (Cazorla 6.5), Ozil 7 (Flamini 6.5), Sanchez 6 (Welbeck 6.5), Giroud 8.5 . Subs: Szczesny, Gibbs, Akpom, Bellerin . Scorers: Giroud 45, Ramsey 81, Flamini 84 . Booked: Sanchez . WEST HAM: Arian 8, O'Brien 6.5, Kouyate 6.5, Cresswell 7, Downing 6, Noble 6 (Nene 6), Song 6.5, Nolan 6, Jarvis 6.5 (Amalfitano 6), Sakho 6.5 . Subs: Demel, J\\u00e4\\u00e4skel\\u00e4inen, Poyet, Cullen, Onariase . Booked: Sakho . Man of the Match: Olivier Giroud . Referee: Chris Foy/ Anthony Taylor . CLICK HERE for all the stats, including Olivier Giroud's heat map from our superb Match Zone . Better measures of their real progress remain, of course. On Tuesday night they visit Monaco where you might anticipate another glorious comeback which ultimately ends in failure. Then there will be the league table at the end of the season which will likely tell the tale of an opportunity missed to challenge for the title. Arsene Wenger conceded that, even with Chelsea still to come to the Emirates, Arsenal are not yet in the title race. 'Not at the moment,' he said. 'But we can just keep going. We won eight of the last nine. We are stronger today than we were at the start of the season. 'We suffered a lot from the World Cup, where the players came back. What is for sure is that they understand each other much better than six or seven months ago and that makes everyone more dangerous.' At least Arsenal are not going backwards these days; the problem may be that they aren't moving forwards fast enough but they are at least pointing in the right direction. West Ham defender James Collins (left) was fortunate not to concede an early penalty after brining down Theo Walcott . The Arsenal winger crashes to the turf under Collins' challenge but was not awarded a spot kick . West Ham forward Diafra Sakho (centre) tries to evade Arsenal defenders Laurent Koscielny and Per Mertesacker (left) Arsenal playmaker Mesut Ozil (left) uses his silky skills to bring the ball down in front of Stewart Downing (centre) and Mark Noble . Arsenal's leading scorer Alexis Sanchez (centre) uses his trickery to escape the attentions of Joey O'Brien . Hammers keeper Adrian threatened to spoil Arsenal's afternoon with a succession of fine first half saves . West Ham manager Sam Allardyce looked as though he understood as much and had settled for one of those afternoons when he aims to 'out-tactic' the opposition. You could hardly blame him given the resources at his disposal, with Enner Valencia and James Tomkins the latest additions to his injury list and Carl Jenkinson ineligible. Kevin Nolan almost shocked Arsenal with a clean strike on 23 minutes but at half-time, Arsenal had 74 per cent of the possession. At times it looked as if West Ham goalkeeper Adrian was has having one of those inspired afternoon. It was later revealed he was playing with a dislocated finger sustained in the warm up, a fact which made his performance truly heroic. 'He's been a brave lad,' said assistant manager Neil McDonald. Theo Walcott was having a less satisfactory time. Thrust back in for his first start in more than a month, he did little to help his ongoing contract talks. Three times he was presented with the opportunity to open the scoring; three times he spurned the chances. Gunners defender Calum Chambers closes down West Ham winger Matt Jarvis as Arsenal take control . Song and Koscielny compete for the ball in an aerial duel as the sun shines at the Emirates Stadium . West Ham manager Sam Allardyce lets his feelings of frustration be known from the sidelines . He could plead mitigation for the first effort on six minutes. A delightful back heel from Giroud set him up with just Adrian to beat, but James Collins came through the man and the ball to prevent him scoring. It might have been a penalty but it was Walcott's own hesitation in front of goal which had presented Collins with the chance to launch his saving tackle. Adrian would then make excellent saves from Alexis Sanchez's header on 16 minutes and from an improvised touch of the knee from Ramsey on 32 minutes, before Walcott received his next chance. Played in by a delightful Ramsey pass, he took a touch, looked up and struck his shot directly at the goalkeeper.Adrian denied Sanchez from a half volley and on 44 minutes he pushed away a strike from Ozil, which landed at Walcott's feet, yards from goal but he skewed the ball high and wide, appearing to lose his balance. Ozil reacts after West Ham keeper Adrian pulls off one of a succession of first half saves . Frenchman Giroud gets his shot away despite being surrounded by four West Ham players . The Gunners centre forward watches as the ball sails towards the left corner of West Ham's goal . Adrian, who had previously kept West Ham in the game with a series of fine saves, dives in vain towards Giroud's shot . Giroud was on hand to provide deliverance just before half-time. Ramsey and Ozil exchanged a delicate one-two, but such is the Frenchman's confidence these days, he nicked it off Ramsey's foot, took a touch and drove a fine finish into the far corner of the net. 'It was fantastic because of the combination and the finishing. It was the kind of goal we loved to score,' said Wenger. It was his 14th goal of the season in 24 games. That Monaco game aside, he is having an excellent season and since that dismal night, he has responded incredibly well. After being criticised for missing chances against Monaco, Giroud holds his hands to his ears to soak up the applause . Giroud strikes a pose midway through his celebration as Arsenal take the lead at the Emirates . Walcott, who had several chances to open the first half scoring himself, leaps on Giroud's back in celebration . 'It's one of his strengths,' said Wenger, who called his performance 'outstanding'. Having set out to defend in numbers, a change of strategy was necessary for West Ham and they were equal to the challenge. They came out in the second-half with an entirely different mindset, pushing Arsenal back into their half and searching for an equaliser. In terms of clear-cut chances there was still little - Matt Jarvis' curling cross which Ospina spilled and almost let in Sakho was one unsettling moment. But Arsenal were no longer able to play as they wished. Still, their time would come. On 80 minutes, Giroud dummied a throw-in to let in Ramsey, who played in the Frenchman. He returned the ball to Ramsey, who drove it past Adrian to cap a fine a performance of growing authority. Three minutes later Santi Cazorla, on as a substitute, exchanged passes with Giroud and planted a cross almost on the foot of Mathieu Flamini, who had the simplest task of converting from close range for the third goal. Ramsey (right) gets some air as he battles for the ball with former Arsenal team-mate Song . West Ham manager Sam Allardyce looks on bemused as referee Chris Foy is forced to hand over to assistant Anthony Taylor . Former Manchester United forward Danny Welbeck (right) was brought on in the second half to replace Sanchez . Arsenal manager Arsene Wenger watches impatiently from the touchline with his side only leading by one goal . Wales midfielder Ramsey meets Giroud's pass with a left footed shot to score his first goal since December . Ramsey wheels away to celebrate giving Arsenal a 2-0 lead on 81minutes of the London derby . Aaron Ramsey is swamped by team-mates Giroud and Danny Welbeck after doubling Arsenal's lead . Substitute Mathieu Flamini is in the right place at the right time as he meets Cazorla's cross to seal a 3-0 victory . Arsenal team-mates embrace Flamini as they keep alive their hopes of playing Champions League football next season .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Greg Dyke wants a public debate between the four candidates for the FIFA presidency and has offered to host it at Wembley Stadium . BBC and Sky are also keen to host a debate between the candidates . The home nations are in disagreement as to whose turn it is to take the FIFA British vice-presidency . BBC appear to have plagiarised Cricinfo in an online live text report .\",\n          \"Rio Ferdinand played FIFA 15 and talked Lionel Messi vs Cristiano Ronaldo . The QPR defender said we should appreciate the talents of the magical duo . Ferdinand spoke from his car complete with an X-Box and multiple screens .\",\n          \"Arsenal kept up their chase for a top four finish in the Premier League with a 3-0 win over West Ham at the Emirates . Olivier Giroud netted his sixth goal in seven games to open the scoring for Gunners on the stroke of half time . Referee Chris Foy was replaced by assistant Anthony Taylor midway through the second half . Wales midfielder Aaron Ramsey was assisted by Giroud to double the lead with his first goal since December . Substitute Mathieu Flamini sealed victory after coming on to meet a cross from Santi Cazorla .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"FA chairman Greg Dyke has called for a public debate between the four candidates for the FIFA presidency. The TV networks have written to the contestants outlining their ambitious proposals for a live, hour-long debate. The FA have made a joint approach to Blatter and rivals Prince Ali of Jordan, Luis Figo and Michael van Praag to stage the event.\",\n          \"Former England international says we should stand back and appreciate the sheer brilliance we are witnessing in the presence of two of the world's greatest ever players. The former Manchester United star posted an image on Instagram of himself and Hoops team-mate Bobby Zamora. The pair were taking each other on in an in-car game of FIFA. The two were taking part in the game while travelling to training at the Imperial College Sports Ground in London.\",\n          \"Olivier Giroud opened the scoring in Arsenal's 3-0 Premier League victory over West Ham. The France international gave Arsenal the lead with a late strike. Aaron Ramsey and Mesut Ozil also scored for the Gunners. Arsenal's leading scorer Alexis Sanchez scored twice in the second half.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_k\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 9,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          5,\n          1,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"base_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004101459731804456,\n        \"min\": -0.014947611838579178,\n        \"max\": 0.006623337976634502,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          -0.0029996251687407494,\n          -0.0057356529869139194,\n          -0.009579298086464405\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"len\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 80,\n        \"max\": 100,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          97,\n          82,\n          100\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contiguity_lcs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13,\n        \"min\": 11,\n        \"max\": 82,\n        \"num_unique_values\": 47,\n        \"samples\": [\n          36,\n          28,\n          50\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_span_len_lcs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.932465332245712,\n        \"min\": 3.0,\n        \"max\": 57.0,\n        \"num_unique_values\": 83,\n        \"samples\": [\n          27.5,\n          9.0,\n          32.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"entity_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 2,\n        \"max\": 24,\n        \"num_unique_values\": 22,\n        \"samples\": [\n          16,\n          7,\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"len_reward\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.13217350897585306,\n        \"min\": 0.11025052530448522,\n        \"max\": 0.5460744266397094,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          0.14938177525041801,\n          0.4867522559599717,\n          0.11025052530448522\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"new_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 23.218050472565984,\n        \"min\": 16.363011311412446,\n        \"max\": 130.03276623529305,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          87.85906257420253,\n          87.68281639130444,\n          95.63211267323587\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "best_cn2"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-36f4f80a-2384-4e76-9f0e-a123b92b7dcd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>row_id</th>\n",
              "      <th>article</th>\n",
              "      <th>reference</th>\n",
              "      <th>summary</th>\n",
              "      <th>best_k</th>\n",
              "      <th>base_score</th>\n",
              "      <th>len</th>\n",
              "      <th>contiguity_lcs</th>\n",
              "      <th>avg_span_len_lcs</th>\n",
              "      <th>entity_score</th>\n",
              "      <th>len_reward</th>\n",
              "      <th>new_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>These days the Cheltenham Festival is a marath...</td>\n",
              "      <td>Willie Mullins looks to have chance of establi...</td>\n",
              "      <td>DOUVAN is the first of the Mullins battalion t...</td>\n",
              "      <td>9</td>\n",
              "      <td>0.002351</td>\n",
              "      <td>100</td>\n",
              "      <td>40</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>16</td>\n",
              "      <td>0.110251</td>\n",
              "      <td>88.033311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>A baby girl was found alive after being strapp...</td>\n",
              "      <td>Lynn Jennifer Groesbeck, 25, was driving home ...</td>\n",
              "      <td>Lynn Jennifer Groesbeck, 25, from Springville,...</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.006587</td>\n",
              "      <td>100</td>\n",
              "      <td>34</td>\n",
              "      <td>6.666667</td>\n",
              "      <td>12</td>\n",
              "      <td>0.110251</td>\n",
              "      <td>66.832413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>A 36-year-old woman and her three children wer...</td>\n",
              "      <td>40-year-old Domingo Villa Arellano, who is an ...</td>\n",
              "      <td>A 36-year-old woman and her three children wer...</td>\n",
              "      <td>8</td>\n",
              "      <td>-0.005785</td>\n",
              "      <td>100</td>\n",
              "      <td>57</td>\n",
              "      <td>15.250000</td>\n",
              "      <td>12</td>\n",
              "      <td>0.110251</td>\n",
              "      <td>71.432494</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-36f4f80a-2384-4e76-9f0e-a123b92b7dcd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-36f4f80a-2384-4e76-9f0e-a123b92b7dcd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-36f4f80a-2384-4e76-9f0e-a123b92b7dcd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-4a76c3a8-dbc7-47f3-be89-bb45c7caf606\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4a76c3a8-dbc7-47f3-be89-bb45c7caf606')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-4a76c3a8-dbc7-47f3-be89-bb45c7caf606 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   row_id                                            article  \\\n",
              "0       0  These days the Cheltenham Festival is a marath...   \n",
              "1       1  A baby girl was found alive after being strapp...   \n",
              "2       2  A 36-year-old woman and her three children wer...   \n",
              "\n",
              "                                           reference  \\\n",
              "0  Willie Mullins looks to have chance of establi...   \n",
              "1  Lynn Jennifer Groesbeck, 25, was driving home ...   \n",
              "2  40-year-old Domingo Villa Arellano, who is an ...   \n",
              "\n",
              "                                             summary  best_k  base_score  len  \\\n",
              "0  DOUVAN is the first of the Mullins battalion t...       9    0.002351  100   \n",
              "1  Lynn Jennifer Groesbeck, 25, from Springville,...       1   -0.006587  100   \n",
              "2  A 36-year-old woman and her three children wer...       8   -0.005785  100   \n",
              "\n",
              "   contiguity_lcs  avg_span_len_lcs  entity_score  len_reward  new_score  \n",
              "0              40          9.000000            16    0.110251  88.033311  \n",
              "1              34          6.666667            12    0.110251  66.832413  \n",
              "2              57         15.250000            12    0.110251  71.432494  "
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# === Offline rerank with entity-type weighting ===\n",
        "dd = details_cn2.copy()\n",
        "\n",
        "# Compute weighted entity scores\n",
        "ent_scores = []\n",
        "for i,row in tqdm(list(dd.iterrows()), total=len(dd), desc=\"weighted entity scores\"):\n",
        "    ent_scores.append(weighted_entity_score(row[\"article\"], row[\"summary\"]))\n",
        "dd[\"entity_score\"] = ent_scores\n",
        "\n",
        "# LCS span stats\n",
        "contigs, avlens = [], []\n",
        "for i,row in dd.iterrows():\n",
        "    c,a = lcs_span_stats(row[\"article\"], row[\"summary\"])\n",
        "    contigs.append(c); avlens.append(a)\n",
        "dd[\"contiguity_lcs\"] = contigs\n",
        "dd[\"avg_span_len_lcs\"] = avlens\n",
        "\n",
        "# Length reward (Gaussian)\n",
        "TARGET_LEN = 58\n",
        "LEN_SIGMA = 20.0\n",
        "def length_reward(l, target=TARGET_LEN, sigma=LEN_SIGMA):\n",
        "    return math.exp(-((l - target)**2) / (2 * sigma**2))\n",
        "\n",
        "dd[\"len\"] = dd[\"len\"].clip(lower=1)\n",
        "dd[\"len_reward\"] = dd[\"len\"].apply(lambda l: length_reward(l))\n",
        "\n",
        "# Final score: entity dominates, others secondary\n",
        "dd[\"new_score\"] = (5.0 * dd[\"entity_score\"]) \\\n",
        "                  + (0.4 * (dd[\"base_score\"] / (dd[\"len\"]**0.3))) \\\n",
        "                  + (0.2 * dd[\"contiguity_lcs\"]) \\\n",
        "                  + (0.3 * dd[\"len_reward\"])\n",
        "\n",
        "# Pick best per row\n",
        "best_cn2 = (\n",
        "    dd.sort_values([\"row_id\",\"new_score\"], ascending=[True, False])\n",
        "      .groupby(\"row_id\", as_index=False).head(1)\n",
        "      .loc[:, [\"row_id\",\"article\",\"reference\",\"summary\",\"candidate_k\",\n",
        "               \"base_score\",\"len\",\"contiguity_lcs\",\"avg_span_len_lcs\",\n",
        "               \"entity_score\",\"len_reward\",\"new_score\"]]\n",
        "      .rename(columns={\"candidate_k\":\"best_k\"})\n",
        "      .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "save_csv(dd,       DETAILS_CN2)\n",
        "save_csv(best_cn2, BEST_CN2)\n",
        "\n",
        "print({\"changed\": int((best_cn2['best_k'] != 0).sum())})\n",
        "best_cn2.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayGEcBGaWZV0",
        "outputId": "63416962-9dea-4992-ca4c-78341c49b6fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE TOP : {'r1_f': 0.4263, 'r2_f': 0.1929, 'rl_f': 0.2856}\n",
            "ROUGE CN2 : {'r1_f': 0.4341, 'r2_f': 0.2033, 'rl_f': 0.2935}\n",
            "Δ ROUGE   : {'r1': 0.0078, 'r2': 0.0104, 'rl': 0.0079}\n",
            "ENT TOP : {'TP': 272, 'FP': 287, 'FN': 278, 'entP': 0.4866, 'entR': 0.4945, 'entF1': 0.4905}\n",
            "ENT CN2 : {'TP': 308, 'FP': 336, 'FN': 242, 'entP': 0.4783, 'entR': 0.56, 'entF1': 0.5159}\n",
            "Δ ENT   : {'entP': -0.0083, 'entR': 0.0655, 'entF1': 0.0254}\n"
          ]
        }
      ],
      "source": [
        "# === ROUGE & Entity eval after weighted rerank ===\n",
        "top_k0 = details_cn2[details_cn2[\"candidate_k\"]==0][[\"row_id\",\"summary\"]]\\\n",
        "           .rename(columns={\"summary\":\"top_beam_summary\"})\n",
        "ev = best_cn2.merge(top_k0, on=\"row_id\", how=\"left\")\\\n",
        "             .rename(columns={\"summary\":\"cn2_summary\"})\n",
        "\n",
        "refs  = ev[\"reference\"].astype(str).tolist()\n",
        "tops  = ev[\"top_beam_summary\"].astype(str).tolist()\n",
        "bests = ev[\"cn2_summary\"].astype(str).tolist()\n",
        "\n",
        "# ROUGE\n",
        "sc = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\"], use_stemmer=True)\n",
        "def _mean(refs, hyps):\n",
        "    r1=r2=rl=0.0\n",
        "    for r,h in zip(refs, hyps):\n",
        "        s=sc.score(r,h); r1+=s[\"rouge1\"].fmeasure; r2+=s[\"rouge2\"].fmeasure; rl+=s[\"rougeLsum\"].fmeasure\n",
        "    n=len(refs); return r1/n, r2/n, rl/n\n",
        "r1t,r2t,rlt = _mean(refs, tops)\n",
        "r1b,r2b,rlb = _mean(refs, bests)\n",
        "print(\"ROUGE TOP :\", {\"r1_f\":round(r1t,4), \"r2_f\":round(r2t,4), \"rl_f\":round(rlt,4)})\n",
        "print(\"ROUGE CN2 :\", {\"r1_f\":round(r1b,4), \"r2_f\":round(r2b,4), \"rl_f\":round(rlb,4)})\n",
        "print(\"Δ ROUGE   :\", {\"r1\":round(r1b-r1t,4), \"r2\":round(r2b-r2t,4), \"rl\":round(rlb-rlt,4)})\n",
        "\n",
        "# Entities overall\n",
        "ent_top  = entPRF(refs, tops)\n",
        "ent_cn2  = entPRF(refs, bests)\n",
        "print(\"ENT TOP :\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in ent_top.items()})\n",
        "print(\"ENT CN2 :\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in ent_cn2.items()})\n",
        "print(\"Δ ENT   :\", {k:round(ent_cn2[k]-ent_top[k],4) for k in [\"entP\",\"entR\",\"entF1\"]})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcXt3aGGXaPg"
      },
      "source": [
        "# Reranking strategy tunning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555,
          "referenced_widgets": [
            "800b20371f924066a2b315ced0435a54"
          ]
        },
        "id": "j08Aw1qYW4iP",
        "outputId": "fb65beb6-aaae-42fd-88f9-e8f7a3c5e225"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "800b20371f924066a2b315ced0435a54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "strict entity scores:   0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[saved] /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/decodes_copynext2/copynext2_details_20250826_161305.csv (1000 rows)\n",
            "[saved] /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/decodes_copynext2/copynext2_best_20250826_161305.csv (94 rows)\n",
            "{'changed': 79}\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"best_cn2\",\n  \"rows\": 94,\n  \"fields\": [\n    {\n      \"column\": \"row_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28,\n        \"min\": 1,\n        \"max\": 99,\n        \"num_unique_values\": 94,\n        \"samples\": [\n          44,\n          26,\n          59\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"article\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 94,\n        \"samples\": [\n          \"Hillary Clinton made it clear Tuesday that when it comes to her emails, which traveled through a private server during her time at the State Department, we'll all just have to trust -- and she'll verify. Using strong language, the former secretary of state insisted that despite concerns from both sides of the aisle on issues including national security, accountability and transparency, there's simply nothing to see here. \\\"I went above and beyond what I was requested to do,\\\" she said. \\\"I fully complied with every rule,\\\" she declared. And in case you were wondering if there was going to be an independent review of her emails, there isn't. Nor will she turn over her server. \\\"The server will remain private.\\\" There you go. Defiant, unfazed, undeterred. You can believe fact checkers will work overtime to parse the bit about complying with every rule. Even the White House has disagreed with that. As for everyone else, the question isn't whether Clinton did enough Tuesday to satisfy House Republicans, who will no doubt continue to hammer her on decisions she made while at State. Nor is it whether she did enough to satisfy Republican voters, who have a crystallized view of Clinton that goes all the way back to the '90s. The perception that this is just another in a long line of instances where the Clintons flouted the rules is fairly unshakable. Likewise, her cadre of loyal surrogates, who showed just how unhinged they are willing to sound to defend the irreproachable HRC in recent days, are sure to keep on circling the wagons and telling us that she is the victim of a right-wing smear campaign and -- wait for it -- media bias. The audience for whom this press conference mattered most was Democratic and independent voters, who may admire Hillary Clinton, but as history has proved, simply cannot be counted on to vote for her in 2016, especially if someone else comes along on her left. Remember, Clinton wasn't defeated in 2008 by Republicans. She was beaten by a little-known, inexperienced Democrat. That means that the half of the country that was predisposed to like her politics and who already knew who she was chose someone else. And the other half of the country did, too. She's just as vulnerable, if not more, to the same kind of usurpation in 2016, thanks to an even greater sense of inevitability on the left, and the reflexive rush to protect her from valid questions and criticism isn't helping to dissolve it. Instead of insisting she did nothing wrong and smugly placating nosy reporters, she should have promised these voters that transparency and accountability -- two words she didn't utter Tuesday -- are the cornerstones of good government and any future administration she were to run. She should have assured them that the rules do apply to her, just like anyone else, and that in the future she'll pay closer attention to them. Finally, she should have told them that the last thing she wants is to take the trust of the voters for granted, and that she'll comply with any independent investigation that's offered. Defiance has paid off for the Clintons in that they've made it through some truly breathtaking scandals unscathed. But if Clinton wants 2016 to turn out differently than 2008, she and her surrogates can't keep insisting to voters that she is above scrutiny.\",\n          \"If President Barack Obama could start his presidency over, he says he would have made closing the Guantanamo Bay military prison in Cuba his first order of business. On his second day in office, Obama did sign an executive order seeking that the detention center for terrorism suspects be closed within a year. President Obama made the comment about the prison during a speech to the City Club of Cleveland in Ohio on Wednesday. President Barack Obama said he should have closed Guantanamo Bay prison on the first day he was in office . On his second day in office, Obama did order the detention center for terrorism suspects be closed within a year . The prison had 242 detainees when Obama began his presidency, but its population has been reduced to 122 . He was responding to a question about what advice he'd give his inexperienced self, according to Politico. He said: 'I think I would've closed Guantanamo on the first day. 'I didn't because at that time we had a bipartisan agreement that it should be closed.' The prison had 242 detainees when Obama began his presidency. According to Obama, the politics around closure grew increasingly tough and 'people got scared by the rhetoric.' Obama made the comment during a speech to the City Club of Cleveland in Ohio on Wednesday. He said Wednesday that 'the path of least resistance was just to leave it open.' He's been able to cut the population down to 122 detainees, but Congress has blocked the closure. Lawmakers have banned the Obama administration from bringing detainees to the United States for imprisonment or trial. In his State of the Union earlier this year, Obama repeated he wanted the facility closed,The Hill reported. He said: 'I will not relent in my determination to shut it down.' Defense Secretary Ash Carter testified before Congress Wednesday that the administration needs to work with lawmakers to find a 'lawful' way to close the prison. The delayed order to shut down the prison isn't the only thing Obama regrets. He also said he would have done something different with his hair if he could do it all again. Obama joked: 'I was thinking maybe I should've told myself to start dying my hair now, before people noticed, because by a year in it was too late. 'Michelle thinks I look distinguished.'\",\n          \"Iranian diplomats twice confronted their American counterparts about an open letter from Republican senators who warned that any nuclear deal could expire the day President Barack Obama leaves office, a senior U.S. official said Monday. The official, noting the administration's warnings when the letter first surfaced, said the GOP intervention was a new issue in the tense negotiations facing an end-of-month deadline for an agreement to reduce Iran's ability to produce a nuclear weapon in exchange for loosing Western sanctions. The letter came up in nuclear talks Sunday between senior U.S. and Iranian negotiators, the official said, and the Iranians raised it again in discussions Monday in Lausanne, Switzerland led by Secretary of State John Kerry and Iranian Foreign Minister Mohammad Javad Zarif. U.S. Secretary of State John Kerry, left, listens to Iran's Foreign Minister Mohammad Javad Zarif, right, before resuming talks over Iran's nuclear program in Lausanne, Switzerland on Monday . Zarif was quoted by Iranian state media after the meeting as saying the topics included the potential speed of a softening of U.S. economic sanctions and the new issue of the letter from the senators. 'It is necessary that the stance of the U.S. administration be defined about this move,' he was quoted as saying. Kerry and Zarif met for nearly five hours in Lausanne, the start of several planned days of discussions. Most of the Iranians then departed for Brussels, where they were to meet with European negotiators. In Brussels, EU foreign policy chief Federica Mogherini said that 'we are entering a crucial time, a crucial two weeks.' And German Foreign Minister Frank-Walter Steinmeier said that after 'more than ten years of negotiations, we should seize this opportunity.' The United States and Iran are plunging back into negotiations in a bid to end a decades-long standoff that has raised the specter of an Iranian nuclear arsenal, a new atomic arms race in the Middle East and even a U.S. or Israeli military intervention . U.S. Secretary of State John Kerry takes a break during a bilateral meeting with Iranian Foreign Minister Mohammad Javad Zarif over Iran's nuclear program . 'There are areas where we've made progress, areas where we have yet to make any progress,' British Foreign Secretary Philip Hammond said. 'But the fact that we're all here talking shows the commitment on both sides to try to reach an agreement.' In Lausanne, the U.S. official wouldn't say how much time the sides spent talking about the letter drafted seven days ago by freshman Sen. Tom Cotton of Arkansas and signed by 46 other GOP senators. The Iranians have called the letter a propaganda ploy, and Zarif joked last week that some U.S. legislators didn't understand their own Constitution. The Obama Administration has called the letter 'ill timed' and 'ill advised,' coming weeks before the deadline for a preliminary agreement with Iran on its nuclear program. In the end, the talks and a potential agreement depend on Iran showing the world that its nuclear program is exclusively peaceful, said the U.S. official, who briefed reporters only condition of anonymity. The goal for a full agreement is the end of June. U.S. Energy Secretary Ernest Moniz, U.S. Secretary of State John Kerry, Iran's Foreign Minister Mohammad Javad Zarif and the head of the Atomic Energy Organization of Iran Ali Akbar Salehi, from left to right, pose for a photograph before resuming talks over Iran's nuclear program . Republicans argue that a deal would be insufficient and unenforceable, allowing Iran to eventually become a nuclear-armed state. To that end, they've delivered a series of proposals to undercut or block an agreement, including ones that would require Senate say-so on a deal and order new sanctions against Iran while negotiations are underway. Cotton's letter, the administration and congressional Democrats argue, went further, interfering in the president's execution of U.S. foreign policy. The letter, styled as a U.S. civics lesson, warned Iranian leaders that any deal negotiated by the current administration could be tossed by Obama's successor. Obama and other officials insist they're not going to make any deal that would allow Iran to acquire nuclear weapons. The agreement taking shape would limit Iran's uranium enrichment and other nuclear activity for at least a decade, with the restrictions slowly lifted over several years. From left, German Foreign Minister Frank-Walter Steinmeier, European Union High Representative Federica Mogherini, Iran's Foreign Minister Mohammad Javad Zarif and British Foreign Minister Philip Hammond stand for a group photo prior to a meeting in Brussels on Monday . Washington and other world powers also would gradually scale back sanctions that have crippled the Iranian economy. Tehran says it is only interested in peaceful energy generation and medical research, but much of the world suspects it harbors nuclear weapons ambitions. Kerry and Zarif plan to regroup in Lausanne on Tuesday. The U.S. secretary of state is to return to Washington by week's end for talks with Afghanistan's leaders, and the Iranians plan to break for the Persian New Year. Officials say talks might restart sometime next week, if necessary. A deal would also require the approval of America's negotiating partners: Britain, China, France, Germany and Russia. With little time remaining before the end of March, some officials have said the persistent differences mean negotiators will likely settle for an announcement that they've made enough progress to justify further talks. Such a declaration would hardly satisfy U.S. critics of the Obama administration's efforts. But the senior American official said the goal was to determine by the end of March 'if we can get to a political framework that addresses the major elements of a comprehensive deal.'\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 94,\n        \"samples\": [\n          \"S.E. Cupp: At press conference, Hillary Clinton insists there was nothing amiss with her use of personal email while secretary of state . Clinton is not invulnerable: She risks losing Democratic and independent voters, who abandoned her in 2008, she says . Cupp: She should have assured them that the rules do apply to her, that transparency and accountability matter .\",\n          \"Obama made remark during a speech to the City Club of Cleveland in Ohio . He did sign an order seeking the closure of the prison on his second day . The prison had 242 detainees at the time but population is down to 122 . Congress has blocked closure of detention center for terrorism suspects .\",\n          \"The open letter from Republican senators warned that any nuclear deal could expire the day President Barack Obama's term ends . Iranian diplomats confronted American counterparts twice at discussions Sunday and Monday in Switzerland, a senior U.S. official said . Secretary of State John Kerry and Iranian Foreign Minister Mohammad Javad Zarif to start several days of discussions . The Iranians have called the letter a propaganda ploy .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 94,\n        \"samples\": [\n          \"Hillary Clinton made it clear Tuesday that when it comes to her emails, she will not turn over her server. The former secretary of state insisted that despite concerns from both sides of the aisle on issues including national security, accountability and transparency, there's simply nothing to see here. The question isn't whether Clinton did enough to satisfy House Republicans, who will no doubt continue to hammer her on decisions she made while at State. She should have promised voters that the rules do apply to her, just\",\n          \"President Barack Obama made the comment about the prison during a speech to the City Club of Cleveland in Ohio on Wednesday. He said he should have closed Guantanamo Bay prison on the first day he was in office. The prison had 242 detainees when Obama began his presidency, but its population has been reduced to 122. He was responding to a question about what advice he'd give his inexperienced self, according to Politico.\",\n          \"Iranian diplomats twice confronted their American counterparts about an open letter from Republican senators. The letter warned that any nuclear deal could expire the day President Barack Obama leaves office. The U.S. Secretary of State John Kerry and Iranian Foreign Minister Mohammad Javad Zarif met for nearly five hours in Lausanne, Switzerland on Monday. They were to meet with European negotiators.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_k\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 9,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          5,\n          8,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"base_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0039115049141529995,\n        \"min\": -0.01642930693924427,\n        \"max\": 0.006623337976634502,\n        \"num_unique_values\": 94,\n        \"samples\": [\n          0.0008594793034717441,\n          -0.00218793167732656,\n          -0.004375712014734745\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"len\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 80,\n        \"max\": 100,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          97,\n          82,\n          100\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contiguity_lcs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13,\n        \"min\": 13,\n        \"max\": 82,\n        \"num_unique_values\": 45,\n        \"samples\": [\n          28,\n          74,\n          50\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_span_len_lcs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.010651556726838,\n        \"min\": 2.857142857142857,\n        \"max\": 57.0,\n        \"num_unique_values\": 76,\n        \"samples\": [\n          9.714285714285714,\n          7.714285714285714,\n          10.75\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"entity_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 2,\n        \"max\": 34,\n        \"num_unique_values\": 28,\n        \"samples\": [\n          7,\n          2,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"len_reward\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12965509263551467,\n        \"min\": 0.11025052530448522,\n        \"max\": 0.5460744266397094,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          0.14938177525041801,\n          0.4867522559599717,\n          0.11025052530448522\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"new_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 32.28309581795989,\n        \"min\": 18.644396600582986,\n        \"max\": 180.03276623529308,\n        \"num_unique_values\": 94,\n        \"samples\": [\n          84.83316151416736,\n          113.90462096773214,\n          150.03263550592584\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "best_cn2"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-0bda3c42-073c-44a1-9f98-22776217ef8a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>row_id</th>\n",
              "      <th>article</th>\n",
              "      <th>reference</th>\n",
              "      <th>summary</th>\n",
              "      <th>best_k</th>\n",
              "      <th>base_score</th>\n",
              "      <th>len</th>\n",
              "      <th>contiguity_lcs</th>\n",
              "      <th>avg_span_len_lcs</th>\n",
              "      <th>entity_score</th>\n",
              "      <th>len_reward</th>\n",
              "      <th>new_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>A baby girl was found alive after being strapp...</td>\n",
              "      <td>Lynn Jennifer Groesbeck, 25, was driving home ...</td>\n",
              "      <td>Lynn Jennifer Groesbeck, 25, from Springville,...</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.006587</td>\n",
              "      <td>100</td>\n",
              "      <td>34</td>\n",
              "      <td>6.666667</td>\n",
              "      <td>16</td>\n",
              "      <td>0.110251</td>\n",
              "      <td>86.832413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>A 36-year-old woman and her three children wer...</td>\n",
              "      <td>40-year-old Domingo Villa Arellano, who is an ...</td>\n",
              "      <td>A 36-year-old woman and her three children wer...</td>\n",
              "      <td>8</td>\n",
              "      <td>-0.005785</td>\n",
              "      <td>100</td>\n",
              "      <td>57</td>\n",
              "      <td>15.250000</td>\n",
              "      <td>18</td>\n",
              "      <td>0.110251</td>\n",
              "      <td>101.432494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>A JetBlue Airways pilot who had to be restrain...</td>\n",
              "      <td>Clayton Osbon, 52, claims in a lawsuit the air...</td>\n",
              "      <td>Clayton Osbon, 52, had to be restrained during...</td>\n",
              "      <td>9</td>\n",
              "      <td>-0.008816</td>\n",
              "      <td>100</td>\n",
              "      <td>43</td>\n",
              "      <td>11.750000</td>\n",
              "      <td>15</td>\n",
              "      <td>0.110251</td>\n",
              "      <td>83.632189</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0bda3c42-073c-44a1-9f98-22776217ef8a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0bda3c42-073c-44a1-9f98-22776217ef8a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0bda3c42-073c-44a1-9f98-22776217ef8a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-389ebfe2-3485-4028-9a06-1df85a2ca3c7\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-389ebfe2-3485-4028-9a06-1df85a2ca3c7')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-389ebfe2-3485-4028-9a06-1df85a2ca3c7 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   row_id                                            article  \\\n",
              "0       1  A baby girl was found alive after being strapp...   \n",
              "1       2  A 36-year-old woman and her three children wer...   \n",
              "2       3  A JetBlue Airways pilot who had to be restrain...   \n",
              "\n",
              "                                           reference  \\\n",
              "0  Lynn Jennifer Groesbeck, 25, was driving home ...   \n",
              "1  40-year-old Domingo Villa Arellano, who is an ...   \n",
              "2  Clayton Osbon, 52, claims in a lawsuit the air...   \n",
              "\n",
              "                                             summary  best_k  base_score  len  \\\n",
              "0  Lynn Jennifer Groesbeck, 25, from Springville,...       1   -0.006587  100   \n",
              "1  A 36-year-old woman and her three children wer...       8   -0.005785  100   \n",
              "2  Clayton Osbon, 52, had to be restrained during...       9   -0.008816  100   \n",
              "\n",
              "   contiguity_lcs  avg_span_len_lcs  entity_score  len_reward   new_score  \n",
              "0              34          6.666667            16    0.110251   86.832413  \n",
              "1              57         15.250000            18    0.110251  101.432494  \n",
              "2              43         11.750000            15    0.110251   83.632189  "
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# === Strict entity-type rerank ===\n",
        "dd = details_cn2.copy()\n",
        "\n",
        "def strict_entity_score(src_text, hyp_text):\n",
        "    src_ents = _ents_norm(src_text)\n",
        "    hyp_ents = _ents_norm(hyp_text)\n",
        "    overlap = hyp_ents & src_ents\n",
        "    unsupported = hyp_ents - src_ents\n",
        "\n",
        "    score = 0\n",
        "    hard_fail = False\n",
        "    for ent, label in unsupported:\n",
        "        if label in [\"PERSON\",\"ORG\",\"GPE\"]:\n",
        "            hard_fail = True  # reject if hallucinated core entities\n",
        "    for ent, label in overlap:\n",
        "        if label in [\"PERSON\",\"ORG\",\"GPE\"]:\n",
        "            score += 3   # strong reward for supported core entities\n",
        "        else:\n",
        "            score += 1   # softer reward\n",
        "    return score, hard_fail\n",
        "\n",
        "# Compute entity scores + fail flags\n",
        "ent_scores, fails = [], []\n",
        "for i,row in tqdm(list(dd.iterrows()), total=len(dd), desc=\"strict entity scores\"):\n",
        "    s,f = strict_entity_score(row[\"article\"], row[\"summary\"])\n",
        "    ent_scores.append(s); fails.append(f)\n",
        "dd[\"entity_score\"] = ent_scores\n",
        "dd[\"fail_flag\"] = fails\n",
        "\n",
        "# LCS span stats\n",
        "contigs, avlens = [], []\n",
        "for i,row in dd.iterrows():\n",
        "    c,a = lcs_span_stats(row[\"article\"], row[\"summary\"])\n",
        "    contigs.append(c); avlens.append(a)\n",
        "dd[\"contiguity_lcs\"] = contigs\n",
        "dd[\"avg_span_len_lcs\"] = avlens\n",
        "\n",
        "# Length reward\n",
        "TARGET_LEN = 58; LEN_SIGMA = 20.0\n",
        "def length_reward(l): return math.exp(-((l-TARGET_LEN)**2)/(2*LEN_SIGMA**2))\n",
        "dd[\"len\"] = dd[\"len\"].clip(lower=1)\n",
        "dd[\"len_reward\"] = dd[\"len\"].apply(length_reward)\n",
        "\n",
        "# Final score\n",
        "dd[\"new_score\"] = (5.0 * dd[\"entity_score\"]) \\\n",
        "                  + (0.4 * (dd[\"base_score\"] / (dd[\"len\"]**0.3))) \\\n",
        "                  + (0.2 * dd[\"contiguity_lcs\"]) \\\n",
        "                  + (0.3 * dd[\"len_reward\"])\n",
        "\n",
        "# Stage 1: drop fails\n",
        "dd_ok = dd[~dd[\"fail_flag\"]].copy()\n",
        "if dd_ok.empty:\n",
        "    print(\"[warn] all beams failed strict filter, using fallback\")\n",
        "    dd_ok = dd.copy()\n",
        "\n",
        "# Stage 2: select best per row\n",
        "best_cn2 = (\n",
        "    dd_ok.sort_values([\"row_id\",\"new_score\"], ascending=[True, False])\n",
        "         .groupby(\"row_id\", as_index=False).head(1)\n",
        "         .loc[:, [\"row_id\",\"article\",\"reference\",\"summary\",\"candidate_k\",\n",
        "                  \"base_score\",\"len\",\"contiguity_lcs\",\"avg_span_len_lcs\",\n",
        "                  \"entity_score\",\"len_reward\",\"new_score\"]]\n",
        "         .rename(columns={\"candidate_k\":\"best_k\"})\n",
        "         .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "save_csv(dd,       DETAILS_CN2)\n",
        "save_csv(best_cn2, BEST_CN2)\n",
        "\n",
        "print({\"changed\": int((best_cn2['best_k'] != 0).sum())})\n",
        "best_cn2.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9ZXS0rgX2vy",
        "outputId": "686ce4fb-5036-4c81-b859-e53563ea9b4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE TOP : {'r1_f': 0.4263, 'r2_f': 0.1908, 'rl_f': 0.2842}\n",
            "ROUGE CN2 : {'r1_f': 0.4339, 'r2_f': 0.2012, 'rl_f': 0.2965}\n",
            "Δ ROUGE   : {'r1': 0.0076, 'r2': 0.0104, 'rl': 0.0123}\n",
            "ENT TOP : {'TP': 250, 'FP': 270, 'FN': 265, 'entP': 0.4808, 'entR': 0.4854, 'entF1': 0.4831}\n",
            "ENT CN2 : {'TP': 285, 'FP': 315, 'FN': 230, 'entP': 0.475, 'entR': 0.5534, 'entF1': 0.5112}\n",
            "Δ ENT   : {'entP': -0.0058, 'entR': 0.068, 'entF1': 0.0281}\n"
          ]
        }
      ],
      "source": [
        "# === ROUGE & Entity eval after strict rerank ===\n",
        "top_k0 = details_cn2[details_cn2[\"candidate_k\"]==0][[\"row_id\",\"summary\"]]\\\n",
        "           .rename(columns={\"summary\":\"top_beam_summary\"})\n",
        "ev = best_cn2.merge(top_k0, on=\"row_id\", how=\"left\")\\\n",
        "             .rename(columns={\"summary\":\"cn2_summary\"})\n",
        "\n",
        "refs  = ev[\"reference\"].astype(str).tolist()\n",
        "tops  = ev[\"top_beam_summary\"].astype(str).tolist()\n",
        "bests = ev[\"cn2_summary\"].astype(str).tolist()\n",
        "\n",
        "# ROUGE\n",
        "sc = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\"], use_stemmer=True)\n",
        "def _mean(refs, hyps):\n",
        "    r1=r2=rl=0.0\n",
        "    for r,h in zip(refs, hyps):\n",
        "        s=sc.score(r,h); r1+=s[\"rouge1\"].fmeasure; r2+=s[\"rouge2\"].fmeasure; rl+=s[\"rougeLsum\"].fmeasure\n",
        "    n=len(refs); return r1/n, r2/n, rl/n\n",
        "r1t,r2t,rlt = _mean(refs, tops)\n",
        "r1b,r2b,rlb = _mean(refs, bests)\n",
        "\n",
        "print(\"ROUGE TOP :\", {\"r1_f\":round(r1t,4), \"r2_f\":round(r2t,4), \"rl_f\":round(rlt,4)})\n",
        "print(\"ROUGE CN2 :\", {\"r1_f\":round(r1b,4), \"r2_f\":round(r2b,4), \"rl_f\":round(rlb,4)})\n",
        "print(\"Δ ROUGE   :\", {\"r1\":round(r1b-r1t,4), \"r2\":round(r2b-r2t,4), \"rl\":round(rlb-rlt,4)})\n",
        "\n",
        "# Entity metrics\n",
        "ent_top  = entPRF(refs, tops)\n",
        "ent_cn2  = entPRF(refs, bests)\n",
        "print(\"ENT TOP :\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in ent_top.items()})\n",
        "print(\"ENT CN2 :\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in ent_cn2.items()})\n",
        "print(\"Δ ENT   :\", {k:round(ent_cn2[k]-ent_top[k],4) for k in [\"entP\",\"entR\",\"entF1\"]})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3DDKW56akZN"
      },
      "source": [
        "#Precision based reranker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318,
          "referenced_widgets": [
            "fc7175a75fd9427093ba53e1ef9e36bb"
          ]
        },
        "id": "0S6oLe65YWZ_",
        "outputId": "66e82be7-9aab-4bbb-d111-d67aa77a6123"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc7175a75fd9427093ba53e1ef9e36bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "entity counts:   0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[saved] /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/decodes_copynext2/copynext2_details_20250826_161305.csv (1000 rows)\n",
            "[saved] /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/decodes_copynext2/copynext2_best_20250826_161305.csv (100 rows)\n",
            "{'changed': 66}\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"best_cn2\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"row_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          83,\n          53,\n          70\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"article\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"FA chairman Greg Dyke has called for a public debate between the four candidates for the FIFA presidency and offered to host the event at Wembley. Dyke made his wish for a TV inquisition known during an awkward weekend in Sepp Blatter's company at the rules-deciding IFAB meeting in Belfast. The FA have made clear their strong opposition to the FIFA president serving a fifth term. Meanwhile, in a separate move, Sky and the BBC have made a joint approach to Blatter and rivals Prince Ali of Jordan, Luis Figo and Michael van Praag to stage such an event. FA chairman Greg Dyke has called for a public debate between the four candidates for the FIFA presidency . The TV networks - through leading sports news broadcasters Paul Kelso and Richard Conway - have written to the contestants outlining their ambitious proposals for a live, hour-long debate in the UK with an audience of fans representing all 209 FIFA member nations. This fans' congress would provide questions, with others drawn from football supporters via the Sky and BBC websites, and Facebook, to ensure maximum interaction. All four challengers have been promised equal time and emphasis to present their manifestos and visions for FIFA's future. Blatter has yet to respond to the Sky-BBC letter but it is highly unlikely he'll agree to such public exposure. His three opponents will all be in favour. Sepp Blatter has yet to respond to the Sky-BBC letter but it is unlikely he'll agree to such public exposure . The official photo of the IFAB summit shows Greg Dyke seated between, of all people, Sepp Blatter and Thailand's Worawi Makudi, whose presence at Belfast's Cullodon Hotel as a representative of the Asian Football Confederation was bizarre in the extreme. Makudi has an acrimonious history with the FA and is under investigation by FIFA's ethics committee for breaching World Cup bid rules. It's understood Makudi played no part in the rules debate. FIFA said Makudi was selected by the AFC to attend and is innocent of any code violations until found guilty. The home nations are in disagreement as to whose turn it is to take the FIFA British vice-presidency . The 2011 gentlemen's agreement between the four home nations, which declares it is Wales's turn to take the FIFA British vice-presidency, continues to cause ructions. The three other countries believe the agreement was nullified by a new statute that has all UEFA associations involved in the vote at the Congress later this month. But Welsh president Trefor Lloyd-Hughes says he has a signed contract in his possession that declares no change can be made in the rotation pledge unless there is unanimous agreement. Meanwhile, England's David Gill, persuaded by UEFA president Michel Platini to stand for that FIFA place against Lloyd-Hughes, has written to all UEFA countries outlining his plans. To spice up their battle, Wales are still annoyed that a UEFA ExCo, including Gill, voted for Hampden Park over the Millennium Stadium as one of 13 venues to stage Euro 2020. It's believed Gill rated the Cardiff venue higher. General secretary Alex Horne left the FA at the end of January but has not yet been replaced . Greg Dyke and David Gill, who are on the nominations panel choosing the FA's next chief executive, were coy over the weekend about how far they have progressed in finding general secretary Alex Horne's replacement. However, it's understood the selection has been made after final interviews last week and an announcement is imminent. Zimbabwe's Sean Williams celebrates taking the wicket of Umar Akmal during their World Cup match . BBC ON STICKY WICKET . BBC Sport's live text reporting of Pakistan's World Cup victory over Zimbabwe seems to have relied heavily on rival website Cricinfo. Umar Akmal's dismissal, bowled by Sean Williams, was described in the following way on Cricinfo: 'Akmal made it look an even better delivery than it actually was, because he had moved inside the line and was trying to push that inside-out through cover.' The licence-fee-funded BBC, who had just changed commentators, wrote: 'Akmal made it look an even better delivery than it actually was, because he had moved inside the line and was trying to push that inside out through cover.' Exactly the same. A BBC spokeswoman said: 'The line should have been credited. This was a simple human error.' The post-11pm peak viewing audience of two million on ITV for the Carl Frampton fight on Saturday has left the network intent on showing more live boxing. Television network ITV are determined to show more live boxing after seeing recent viewing figures .\",\n          \"Following Sunday's El Clasico spectacle that hugely intensified the run-in for the La Liga title between Barcelona and Real Madrid, Rio Ferdinand gave his stance on the age-old debate of who is better: Cristano Ronaldo or Lionel Messi? Choosing to stay somewhat impartial to the discussion the former England international says we should stand back and appreciate the sheer brilliance we are witnessing in the presence of two of the world's greatest ever players. 'It's inevitable we do all this \\\"who's better? Blah, blah, blah\\\" but we should just be enjoying being around these two players and seeing it,' the QPR defender said. Rio Ferdinand (right) speaks to the camera alongside Queen's Park Rangers team-mate Bobby Zamora . Lionel Messi is La Liga's top scorer this season with 32 goals but Cristiano Ronaldo is not far behind . The Real Madrid striker's goal in El Clasico on Sunday took him to just one goal behind the Barcelona forward . 'When Ronaldo, the Brazilian one - who should have been the best if he weren't injured - was scoring 30 goals a season [everybody said]: \\\"Oh he's a genius, he's the best ever.\\\"' Messi and Ronaldo are this season's top scorers in La Liga, with 32 and 31 goals respectively with 10 games to go. Speaking from his car, kitted out with an X-Box and television screen, the former Manchester United star posted an image on Instagram of himself and Hoops team-mate Bobby Zamora. The two were taking each other on in an in-car game of FIFA whilst travelling to training at the Imperial College Sports Ground, London. The 2008 Champions League winner wrote: 'En route to training.. Winning at FIFA #shock' after beating the former Fulham striker 2-1 with Paris Saint Germain. The former England international raises his finger in victory after beating Zamora in an in-car game of FIFA .\",\n          \"It is a measure of Arsenal's progress that fixtures like these no longer end in boos and angry recrimination. As the final whistle approached, the crowd at the Emirates were unusually vocal, anticipating trips to Wembley and singing praise of their team. Earlier it had the feel of one of the afternoons with which we have become so familiar in recent years, one of those days when Arsenal break world records for possessions stats but come up against an inspired goalkeeper and end the day with fans furious at their manager, the board of directors and life in general. But with Olivier Giroud in exceptional form, Aaron Ramsey also outstanding and Mesut Ozil thriving against the kind of opposition he relishes, Arsenal eventually cantered to victory in some style. Olivier Giroud leaps in the air after opening the scoring in Arsenal's 3-0 Premier League victory over West Ham . The France international gives Arsenal the lead on the stroke of half time with his sixth goal in his last seven games . Aaron Ramsey is chased by Santi Cazorla after doubling Arsenal's lead in the closing stages of the second half . Giroud gives substitute Mathieu Flamini a pat on the back after the Frenchman seals victory with a late strike . ARSENAL: Ospina 6.5, Chambers 7, Mertesacker 6.5, Koscielny 6.5, Monreal 7, Ramsey 7, Coquelin 7, Walcott 6 (Cazorla 6.5), Ozil 7 (Flamini 6.5), Sanchez 6 (Welbeck 6.5), Giroud 8.5 . Subs: Szczesny, Gibbs, Akpom, Bellerin . Scorers: Giroud 45, Ramsey 81, Flamini 84 . Booked: Sanchez . WEST HAM: Arian 8, O'Brien 6.5, Kouyate 6.5, Cresswell 7, Downing 6, Noble 6 (Nene 6), Song 6.5, Nolan 6, Jarvis 6.5 (Amalfitano 6), Sakho 6.5 . Subs: Demel, J\\u00e4\\u00e4skel\\u00e4inen, Poyet, Cullen, Onariase . Booked: Sakho . Man of the Match: Olivier Giroud . Referee: Chris Foy/ Anthony Taylor . CLICK HERE for all the stats, including Olivier Giroud's heat map from our superb Match Zone . Better measures of their real progress remain, of course. On Tuesday night they visit Monaco where you might anticipate another glorious comeback which ultimately ends in failure. Then there will be the league table at the end of the season which will likely tell the tale of an opportunity missed to challenge for the title. Arsene Wenger conceded that, even with Chelsea still to come to the Emirates, Arsenal are not yet in the title race. 'Not at the moment,' he said. 'But we can just keep going. We won eight of the last nine. We are stronger today than we were at the start of the season. 'We suffered a lot from the World Cup, where the players came back. What is for sure is that they understand each other much better than six or seven months ago and that makes everyone more dangerous.' At least Arsenal are not going backwards these days; the problem may be that they aren't moving forwards fast enough but they are at least pointing in the right direction. West Ham defender James Collins (left) was fortunate not to concede an early penalty after brining down Theo Walcott . The Arsenal winger crashes to the turf under Collins' challenge but was not awarded a spot kick . West Ham forward Diafra Sakho (centre) tries to evade Arsenal defenders Laurent Koscielny and Per Mertesacker (left) Arsenal playmaker Mesut Ozil (left) uses his silky skills to bring the ball down in front of Stewart Downing (centre) and Mark Noble . Arsenal's leading scorer Alexis Sanchez (centre) uses his trickery to escape the attentions of Joey O'Brien . Hammers keeper Adrian threatened to spoil Arsenal's afternoon with a succession of fine first half saves . West Ham manager Sam Allardyce looked as though he understood as much and had settled for one of those afternoons when he aims to 'out-tactic' the opposition. You could hardly blame him given the resources at his disposal, with Enner Valencia and James Tomkins the latest additions to his injury list and Carl Jenkinson ineligible. Kevin Nolan almost shocked Arsenal with a clean strike on 23 minutes but at half-time, Arsenal had 74 per cent of the possession. At times it looked as if West Ham goalkeeper Adrian was has having one of those inspired afternoon. It was later revealed he was playing with a dislocated finger sustained in the warm up, a fact which made his performance truly heroic. 'He's been a brave lad,' said assistant manager Neil McDonald. Theo Walcott was having a less satisfactory time. Thrust back in for his first start in more than a month, he did little to help his ongoing contract talks. Three times he was presented with the opportunity to open the scoring; three times he spurned the chances. Gunners defender Calum Chambers closes down West Ham winger Matt Jarvis as Arsenal take control . Song and Koscielny compete for the ball in an aerial duel as the sun shines at the Emirates Stadium . West Ham manager Sam Allardyce lets his feelings of frustration be known from the sidelines . He could plead mitigation for the first effort on six minutes. A delightful back heel from Giroud set him up with just Adrian to beat, but James Collins came through the man and the ball to prevent him scoring. It might have been a penalty but it was Walcott's own hesitation in front of goal which had presented Collins with the chance to launch his saving tackle. Adrian would then make excellent saves from Alexis Sanchez's header on 16 minutes and from an improvised touch of the knee from Ramsey on 32 minutes, before Walcott received his next chance. Played in by a delightful Ramsey pass, he took a touch, looked up and struck his shot directly at the goalkeeper.Adrian denied Sanchez from a half volley and on 44 minutes he pushed away a strike from Ozil, which landed at Walcott's feet, yards from goal but he skewed the ball high and wide, appearing to lose his balance. Ozil reacts after West Ham keeper Adrian pulls off one of a succession of first half saves . Frenchman Giroud gets his shot away despite being surrounded by four West Ham players . The Gunners centre forward watches as the ball sails towards the left corner of West Ham's goal . Adrian, who had previously kept West Ham in the game with a series of fine saves, dives in vain towards Giroud's shot . Giroud was on hand to provide deliverance just before half-time. Ramsey and Ozil exchanged a delicate one-two, but such is the Frenchman's confidence these days, he nicked it off Ramsey's foot, took a touch and drove a fine finish into the far corner of the net. 'It was fantastic because of the combination and the finishing. It was the kind of goal we loved to score,' said Wenger. It was his 14th goal of the season in 24 games. That Monaco game aside, he is having an excellent season and since that dismal night, he has responded incredibly well. After being criticised for missing chances against Monaco, Giroud holds his hands to his ears to soak up the applause . Giroud strikes a pose midway through his celebration as Arsenal take the lead at the Emirates . Walcott, who had several chances to open the first half scoring himself, leaps on Giroud's back in celebration . 'It's one of his strengths,' said Wenger, who called his performance 'outstanding'. Having set out to defend in numbers, a change of strategy was necessary for West Ham and they were equal to the challenge. They came out in the second-half with an entirely different mindset, pushing Arsenal back into their half and searching for an equaliser. In terms of clear-cut chances there was still little - Matt Jarvis' curling cross which Ospina spilled and almost let in Sakho was one unsettling moment. But Arsenal were no longer able to play as they wished. Still, their time would come. On 80 minutes, Giroud dummied a throw-in to let in Ramsey, who played in the Frenchman. He returned the ball to Ramsey, who drove it past Adrian to cap a fine a performance of growing authority. Three minutes later Santi Cazorla, on as a substitute, exchanged passes with Giroud and planted a cross almost on the foot of Mathieu Flamini, who had the simplest task of converting from close range for the third goal. Ramsey (right) gets some air as he battles for the ball with former Arsenal team-mate Song . West Ham manager Sam Allardyce looks on bemused as referee Chris Foy is forced to hand over to assistant Anthony Taylor . Former Manchester United forward Danny Welbeck (right) was brought on in the second half to replace Sanchez . Arsenal manager Arsene Wenger watches impatiently from the touchline with his side only leading by one goal . Wales midfielder Ramsey meets Giroud's pass with a left footed shot to score his first goal since December . Ramsey wheels away to celebrate giving Arsenal a 2-0 lead on 81minutes of the London derby . Aaron Ramsey is swamped by team-mates Giroud and Danny Welbeck after doubling Arsenal's lead . Substitute Mathieu Flamini is in the right place at the right time as he meets Cazorla's cross to seal a 3-0 victory . Arsenal team-mates embrace Flamini as they keep alive their hopes of playing Champions League football next season .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Greg Dyke wants a public debate between the four candidates for the FIFA presidency and has offered to host it at Wembley Stadium . BBC and Sky are also keen to host a debate between the candidates . The home nations are in disagreement as to whose turn it is to take the FIFA British vice-presidency . BBC appear to have plagiarised Cricinfo in an online live text report .\",\n          \"Rio Ferdinand played FIFA 15 and talked Lionel Messi vs Cristiano Ronaldo . The QPR defender said we should appreciate the talents of the magical duo . Ferdinand spoke from his car complete with an X-Box and multiple screens .\",\n          \"Arsenal kept up their chase for a top four finish in the Premier League with a 3-0 win over West Ham at the Emirates . Olivier Giroud netted his sixth goal in seven games to open the scoring for Gunners on the stroke of half time . Referee Chris Foy was replaced by assistant Anthony Taylor midway through the second half . Wales midfielder Aaron Ramsey was assisted by Giroud to double the lead with his first goal since December . Substitute Mathieu Flamini sealed victory after coming on to meet a cross from Santi Cazorla .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"FA chairman Greg Dyke has called for a public debate between the four candidates for the FIFA presidency. The TV networks have written to the contestants outlining their ambitious proposals for a live, hour-long debate. The FA have made a joint approach to Blatter and rivals Prince Ali of Jordan, Luis Figo and Michael van Praag to stage the event.\",\n          \"The former England international says we should stand back and appreciate the sheer brilliance we are witnessing in the presence of two of the world's greatest ever players. The former Manchester United star posted an image on Instagram of himself and Hoops team-mate Bobby Zamora. The two were taking each other on in an in-car game of FIFA. The pair were taking part in the game while travelling to training at the Imperial College Sports Ground.\",\n          \"Olivier Giroud opened the scoring in Arsenal's 3-0 Premier League victory over West Ham. The France international gave Arsenal the lead with a late strike. Aaron Ramsey and Mesut Ozil also scored for the Gunners. Arsenal's leading scorer Alexis Sanchez scored twice in the second half.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_k\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 9,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          7,\n          1,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"base_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0035342884027898145,\n        \"min\": -0.014469360001385212,\n        \"max\": 0.006110723130404949,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          -0.0029996251687407494,\n          -0.003873131237924099,\n          -0.009579298086464405\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"len\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 80,\n        \"max\": 100,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          97,\n          82,\n          100\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"supported\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 14,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          12,\n          2,\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"unsupported\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0,\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ent_precision_ratio\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0692181574174839,\n        \"min\": 0.6666664444445185,\n        \"max\": 0.9999999285714337,\n        \"num_unique_values\": 25,\n        \"samples\": [\n          0.9999998571428775,\n          0.8333332638888947,\n          0.9090908264462886\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contiguity_lcs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12,\n        \"min\": 21,\n        \"max\": 84,\n        \"num_unique_values\": 44,\n        \"samples\": [\n          44,\n          58,\n          57\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"len_reward\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.13217350897585306,\n        \"min\": 0.11025052530448522,\n        \"max\": 0.5460744266397094,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          0.14938177525041801,\n          0.4867522559599717,\n          0.11025052530448522\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"new_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.55657378021749,\n        \"min\": 9.257888100961521,\n        \"max\": 21.833145027892858,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          12.85906201864704,\n          18.283008824296484,\n          10.632112173235926\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "best_cn2"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-8453caba-dc27-4e37-9db3-7cc6ef7578a5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>row_id</th>\n",
              "      <th>article</th>\n",
              "      <th>reference</th>\n",
              "      <th>summary</th>\n",
              "      <th>best_k</th>\n",
              "      <th>base_score</th>\n",
              "      <th>len</th>\n",
              "      <th>supported</th>\n",
              "      <th>unsupported</th>\n",
              "      <th>ent_precision_ratio</th>\n",
              "      <th>contiguity_lcs</th>\n",
              "      <th>len_reward</th>\n",
              "      <th>new_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>These days the Cheltenham Festival is a marath...</td>\n",
              "      <td>Willie Mullins looks to have chance of establi...</td>\n",
              "      <td>The Cheltenham Festival is a marathon not a sp...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.006111</td>\n",
              "      <td>100</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>0.909091</td>\n",
              "      <td>64</td>\n",
              "      <td>0.110251</td>\n",
              "      <td>17.379143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>A baby girl was found alive after being strapp...</td>\n",
              "      <td>Lynn Jennifer Groesbeck, 25, was driving home ...</td>\n",
              "      <td>The baby girl was found alive after being stra...</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.005965</td>\n",
              "      <td>100</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>48</td>\n",
              "      <td>0.110251</td>\n",
              "      <td>14.632475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>A 36-year-old woman and her three children wer...</td>\n",
              "      <td>40-year-old Domingo Villa Arellano, who is an ...</td>\n",
              "      <td>Domingo Villa Arellano, who is an ex-cop, alle...</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.003852</td>\n",
              "      <td>100</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>64</td>\n",
              "      <td>0.110251</td>\n",
              "      <td>17.207688</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8453caba-dc27-4e37-9db3-7cc6ef7578a5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8453caba-dc27-4e37-9db3-7cc6ef7578a5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8453caba-dc27-4e37-9db3-7cc6ef7578a5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-9599c1c1-4e37-4470-b19d-b05c9f328ef3\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9599c1c1-4e37-4470-b19d-b05c9f328ef3')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-9599c1c1-4e37-4470-b19d-b05c9f328ef3 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   row_id                                            article  \\\n",
              "0       0  These days the Cheltenham Festival is a marath...   \n",
              "1       1  A baby girl was found alive after being strapp...   \n",
              "2       2  A 36-year-old woman and her three children wer...   \n",
              "\n",
              "                                           reference  \\\n",
              "0  Willie Mullins looks to have chance of establi...   \n",
              "1  Lynn Jennifer Groesbeck, 25, was driving home ...   \n",
              "2  40-year-old Domingo Villa Arellano, who is an ...   \n",
              "\n",
              "                                             summary  best_k  base_score  len  \\\n",
              "0  The Cheltenham Festival is a marathon not a sp...       0    0.006111  100   \n",
              "1  The baby girl was found alive after being stra...       0   -0.005965  100   \n",
              "2  Domingo Villa Arellano, who is an ex-cop, alle...       1   -0.003852  100   \n",
              "\n",
              "   supported  unsupported  ent_precision_ratio  contiguity_lcs  len_reward  \\\n",
              "0         10            1             0.909091              64    0.110251   \n",
              "1          5            0             1.000000              48    0.110251   \n",
              "2          7            1             0.875000              64    0.110251   \n",
              "\n",
              "   new_score  \n",
              "0  17.379143  \n",
              "1  14.632475  \n",
              "2  17.207688  "
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# === Precision-biased rerank (favor fewer unsupported entities) ===\n",
        "dd = details_cn2.copy()\n",
        "\n",
        "# Compute supported / unsupported counts\n",
        "supported, unsupported = [], []\n",
        "for i,row in tqdm(list(dd.iterrows()), total=len(dd), desc=\"entity counts\"):\n",
        "    src_ents = _ents_norm(row[\"article\"])\n",
        "    hyp_ents = _ents_norm(row[\"summary\"])\n",
        "    overlap = hyp_ents & src_ents\n",
        "    supp = len(overlap)\n",
        "    unsupp = len(hyp_ents - src_ents)\n",
        "    supported.append(supp)\n",
        "    unsupported.append(unsupp)\n",
        "\n",
        "dd[\"supported\"]   = supported\n",
        "dd[\"unsupported\"] = unsupported\n",
        "\n",
        "# Entity precision ratio (avoid div/0)\n",
        "dd[\"ent_precision_ratio\"] = dd.apply(\n",
        "    lambda r: r[\"supported\"] / (r[\"supported\"] + r[\"unsupported\"] + 1e-6), axis=1\n",
        ")\n",
        "\n",
        "# LCS span stats\n",
        "contigs, avlens = [], []\n",
        "for i,row in dd.iterrows():\n",
        "    c,a = lcs_span_stats(row[\"article\"], row[\"summary\"])\n",
        "    contigs.append(c); avlens.append(a)\n",
        "dd[\"contiguity_lcs\"] = contigs\n",
        "dd[\"avg_span_len_lcs\"] = avlens\n",
        "\n",
        "# Length reward\n",
        "TARGET_LEN = 58; LEN_SIGMA = 20.0\n",
        "def length_reward(l): return math.exp(-((l-TARGET_LEN)**2)/(2*LEN_SIGMA**2))\n",
        "dd[\"len\"] = dd[\"len\"].clip(lower=1)\n",
        "dd[\"len_reward\"] = dd[\"len\"].apply(length_reward)\n",
        "\n",
        "# Final score (precision-dominant)\n",
        "dd[\"new_score\"] = (5.0 * dd[\"ent_precision_ratio\"]) \\\n",
        "                  + (0.4 * (dd[\"base_score\"] / (dd[\"len\"]**0.3))) \\\n",
        "                  + (0.2 * dd[\"contiguity_lcs\"]) \\\n",
        "                  + (0.3 * dd[\"len_reward\"])\n",
        "\n",
        "# Pick best per row\n",
        "best_cn2 = (\n",
        "    dd.sort_values([\"row_id\",\"new_score\"], ascending=[True, False])\n",
        "      .groupby(\"row_id\", as_index=False).head(1)\n",
        "      .loc[:, [\"row_id\",\"article\",\"reference\",\"summary\",\"candidate_k\",\n",
        "               \"base_score\",\"len\",\"supported\",\"unsupported\",\n",
        "               \"ent_precision_ratio\",\"contiguity_lcs\",\"len_reward\",\"new_score\"]]\n",
        "      .rename(columns={\"candidate_k\":\"best_k\"})\n",
        "      .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "save_csv(dd,       DETAILS_CN2)\n",
        "save_csv(best_cn2, BEST_CN2)\n",
        "\n",
        "print({\"changed\": int((best_cn2['best_k'] != 0).sum())})\n",
        "best_cn2.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zs5E_xwEcGnK",
        "outputId": "2346b467-5b3c-4936-e552-6315790bd653"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE TOP : {'r1_f': 0.4263, 'r2_f': 0.1929, 'rl_f': 0.2856}\n",
            "ROUGE CN2 : {'r1_f': 0.4217, 'r2_f': 0.1872, 'rl_f': 0.2832}\n",
            "Δ ROUGE   : {'r1': -0.0045, 'r2': -0.0057, 'rl': -0.0024}\n",
            "ENT TOP : {'TP': 272, 'FP': 287, 'FN': 278, 'entP': 0.4866, 'entR': 0.4945, 'entF1': 0.4905}\n",
            "ENT CN2 : {'TP': 274, 'FP': 286, 'FN': 276, 'entP': 0.4893, 'entR': 0.4982, 'entF1': 0.4937}\n",
            "Δ ENT   : {'entP': 0.0027, 'entR': 0.0036, 'entF1': 0.0032}\n"
          ]
        }
      ],
      "source": [
        "# === ROUGE & Entity eval after precision-biased rerank ===\n",
        "top_k0 = details_cn2[details_cn2[\"candidate_k\"]==0][[\"row_id\",\"summary\"]]\\\n",
        "           .rename(columns={\"summary\":\"top_beam_summary\"})\n",
        "ev = best_cn2.merge(top_k0, on=\"row_id\", how=\"left\")\\\n",
        "             .rename(columns={\"summary\":\"cn2_summary\"})\n",
        "\n",
        "refs  = ev[\"reference\"].astype(str).tolist()\n",
        "tops  = ev[\"top_beam_summary\"].astype(str).tolist()\n",
        "bests = ev[\"cn2_summary\"].astype(str).tolist()\n",
        "\n",
        "# ROUGE\n",
        "sc = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\"], use_stemmer=True)\n",
        "def _mean(refs, hyps):\n",
        "    r1=r2=rl=0.0\n",
        "    for r,h in zip(refs, hyps):\n",
        "        s=sc.score(r,h); r1+=s[\"rouge1\"].fmeasure; r2+=s[\"rouge2\"].fmeasure; rl+=s[\"rougeLsum\"].fmeasure\n",
        "    n=len(refs); return r1/n, r2/n, rl/n\n",
        "r1t,r2t,rlt = _mean(refs, tops)\n",
        "r1b,r2b,rlb = _mean(refs, bests)\n",
        "\n",
        "print(\"ROUGE TOP :\", {\"r1_f\":round(r1t,4), \"r2_f\":round(r2t,4), \"rl_f\":round(rlt,4)})\n",
        "print(\"ROUGE CN2 :\", {\"r1_f\":round(r1b,4), \"r2_f\":round(r2b,4), \"rl_f\":round(rlb,4)})\n",
        "print(\"Δ ROUGE   :\", {\"r1\":round(r1b-r1t,4), \"r2\":round(r2b-r2t,4), \"rl\":round(rlb-rlt,4)})\n",
        "\n",
        "# Entity metrics\n",
        "ent_top  = entPRF(refs, tops)\n",
        "ent_cn2  = entPRF(refs, bests)\n",
        "print(\"ENT TOP :\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in ent_top.items()})\n",
        "print(\"ENT CN2 :\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in ent_cn2.items()})\n",
        "print(\"Δ ENT   :\", {k:round(ent_cn2[k]-ent_top[k],4) for k in [\"entP\",\"entR\",\"entF1\"]})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI3lrEVfdf5G"
      },
      "source": [
        "# λ - Precision-recall anchoring via λ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "5cf99774fda847af80d47dae034b70ee"
          ]
        },
        "id": "dZz2eSgUcl57",
        "outputId": "fbd526de-ccbc-4d8d-8a3b-4b77ad5235e0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5cf99774fda847af80d47dae034b70ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "EntityAware CopyNext (beam=20):   0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[saved] /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/decodes_copynext2/copynext2_details_20250826_161305.csv (2000 rows)\n",
            "[done] Generated 2000 candidates (20 beams each)\n"
          ]
        }
      ],
      "source": [
        "# === Beam-20 decode with EntityAware CopyNext ===\n",
        "details_rows = []\n",
        "BATCH = 2   # smaller batch to fit 20 beams\n",
        "K = 20      # number of candidates per input\n",
        "\n",
        "CN2_ARGS_BIG = dict(\n",
        "    num_beams=20,\n",
        "    num_return_sequences=20,\n",
        "    num_beam_groups=10,\n",
        "    diversity_penalty=0.3,\n",
        "    max_new_tokens=100, min_new_tokens=55,\n",
        "    no_repeat_ngram_size=3,\n",
        "    length_penalty=2.0,\n",
        "    early_stopping=True,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True\n",
        ")\n",
        "\n",
        "for start in tqdm(range(0, N, BATCH), desc=\"EntityAware CopyNext (beam=20)\"):\n",
        "    batch_arts = articles[start:start+BATCH]\n",
        "    batch_refs = references[start:start+BATCH]\n",
        "    enc = tok(batch_arts, return_tensors=\"pt\", truncation=True, padding=True,\n",
        "              max_length=MAX_SRC_LEN_CN2).to(DEVICE)\n",
        "\n",
        "    proc = EntityAwareSpanProcessor(\n",
        "        src_input_ids=enc[\"input_ids\"], tokenizer=tok,\n",
        "        num_beams=CN2_ARGS_BIG[\"num_beams\"],\n",
        "        gamma=0.5, gamma_entity=2.0, max_span=6\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **enc, **CN2_ARGS_BIG, logits_processor=LogitsProcessorList([proc])\n",
        "        )\n",
        "\n",
        "    seqs = out.sequences.view(len(batch_arts), K, -1)\n",
        "    base = out.sequences_scores.view(len(batch_arts), K)\n",
        "\n",
        "    for bi in range(len(batch_arts)):\n",
        "        art = batch_arts[bi]; ref = batch_refs[bi]\n",
        "        for k in range(K):\n",
        "            ids = seqs[bi, k]\n",
        "            summ = tok.decode(ids, skip_special_tokens=True)\n",
        "            tgt_len = max(int(ids.size(0) - 1), 1)\n",
        "            details_rows.append({\n",
        "                \"row_id\": start + bi,\n",
        "                \"candidate_k\": k,\n",
        "                \"article\": art,\n",
        "                \"reference\": ref,\n",
        "                \"summary\": summ,\n",
        "                \"base_score\": float(base[bi, k].item()),\n",
        "                \"len\": tgt_len\n",
        "            })\n",
        "\n",
        "details_cn2 = pd.DataFrame(details_rows)\n",
        "save_csv(details_cn2, DETAILS_CN2)\n",
        "print(f\"[done] Generated {len(details_cn2)} candidates (20 beams each)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258,
          "referenced_widgets": [
            "efc1af8320fd4a669c2c04175c2d52c2"
          ]
        },
        "id": "Jruhsl0JdxhC",
        "outputId": "e65b5988-697f-410c-c217-e3992bac601f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "efc1af8320fd4a669c2c04175c2d52c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "precompute features:   0%|          | 0/2000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"lam\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.39528470752104744,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.25,\n          1.0,\n          0.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rouge1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0010639548862616311,\n        \"min\": 0.4164,\n        \"max\": 0.419,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.4164,\n          0.4174,\n          0.4178\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rouge2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0013479614237803588,\n        \"min\": 0.1821,\n        \"max\": 0.1856,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.1846,\n          0.1821,\n          0.1851\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rougel\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0022803508501982803,\n        \"min\": 0.2718,\n        \"max\": 0.2774,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.2746,\n          0.2718,\n          0.2774\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"entP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.013361773834338017,\n        \"min\": 0.4332,\n        \"max\": 0.4678,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.4379,\n          0.4678,\n          0.4427\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"entR\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.006441816513996675,\n        \"min\": 0.5145,\n        \"max\": 0.5309,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.5255,\n          0.5145,\n          0.52\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"entF1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.005401573844723398,\n        \"min\": 0.4771,\n        \"max\": 0.49,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.4777,\n          0.49,\n          0.4783\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u0394entP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.013361773834338014,\n        \"min\": -0.0465,\n        \"max\": -0.0119,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          -0.0418,\n          -0.0119,\n          -0.037\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u0394entR\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.006441816513996655,\n        \"min\": 0.02,\n        \"max\": 0.0364,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0309,\n          0.02,\n          0.0255\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u0394entF1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.005411561696959576,\n        \"min\": -0.0099,\n        \"max\": 0.003,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          -0.0093,\n          0.003,\n          -0.0088\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-94c17f8b-318d-421e-87da-85c498f7a880\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lam</th>\n",
              "      <th>rouge1</th>\n",
              "      <th>rouge2</th>\n",
              "      <th>rougel</th>\n",
              "      <th>entP</th>\n",
              "      <th>entR</th>\n",
              "      <th>entF1</th>\n",
              "      <th>ΔentP</th>\n",
              "      <th>ΔentR</th>\n",
              "      <th>ΔentF1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.4188</td>\n",
              "      <td>0.1856</td>\n",
              "      <td>0.2770</td>\n",
              "      <td>0.4332</td>\n",
              "      <td>0.5309</td>\n",
              "      <td>0.4771</td>\n",
              "      <td>-0.0465</td>\n",
              "      <td>0.0364</td>\n",
              "      <td>-0.0099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.4164</td>\n",
              "      <td>0.1846</td>\n",
              "      <td>0.2746</td>\n",
              "      <td>0.4379</td>\n",
              "      <td>0.5255</td>\n",
              "      <td>0.4777</td>\n",
              "      <td>-0.0418</td>\n",
              "      <td>0.0309</td>\n",
              "      <td>-0.0093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.4178</td>\n",
              "      <td>0.1851</td>\n",
              "      <td>0.2774</td>\n",
              "      <td>0.4427</td>\n",
              "      <td>0.5200</td>\n",
              "      <td>0.4783</td>\n",
              "      <td>-0.0370</td>\n",
              "      <td>0.0255</td>\n",
              "      <td>-0.0088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.4190</td>\n",
              "      <td>0.1845</td>\n",
              "      <td>0.2762</td>\n",
              "      <td>0.4453</td>\n",
              "      <td>0.5182</td>\n",
              "      <td>0.4790</td>\n",
              "      <td>-0.0344</td>\n",
              "      <td>0.0236</td>\n",
              "      <td>-0.0080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.00</td>\n",
              "      <td>0.4174</td>\n",
              "      <td>0.1821</td>\n",
              "      <td>0.2718</td>\n",
              "      <td>0.4678</td>\n",
              "      <td>0.5145</td>\n",
              "      <td>0.4900</td>\n",
              "      <td>-0.0119</td>\n",
              "      <td>0.0200</td>\n",
              "      <td>0.0030</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-94c17f8b-318d-421e-87da-85c498f7a880')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-94c17f8b-318d-421e-87da-85c498f7a880 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-94c17f8b-318d-421e-87da-85c498f7a880');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-6df907f5-1918-4811-bac2-2ecdb2d209cc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6df907f5-1918-4811-bac2-2ecdb2d209cc')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-6df907f5-1918-4811-bac2-2ecdb2d209cc button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    lam  rouge1  rouge2  rougel    entP    entR   entF1   ΔentP   ΔentR  \\\n",
              "0  0.00  0.4188  0.1856  0.2770  0.4332  0.5309  0.4771 -0.0465  0.0364   \n",
              "1  0.25  0.4164  0.1846  0.2746  0.4379  0.5255  0.4777 -0.0418  0.0309   \n",
              "2  0.50  0.4178  0.1851  0.2774  0.4427  0.5200  0.4783 -0.0370  0.0255   \n",
              "3  0.75  0.4190  0.1845  0.2762  0.4453  0.5182  0.4790 -0.0344  0.0236   \n",
              "4  1.00  0.4174  0.1821  0.2718  0.4678  0.5145  0.4900 -0.0119  0.0200   \n",
              "\n",
              "   ΔentF1  \n",
              "0 -0.0099  \n",
              "1 -0.0093  \n",
              "2 -0.0088  \n",
              "3 -0.0080  \n",
              "4  0.0030  "
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# === λ-sweep rerank: tradeoff between precision (λ=1) and recall (λ=0) ===\n",
        "from collections import defaultdict\n",
        "\n",
        "def compute_support_unsupp(src, hyp):\n",
        "    src_ents = _ents_norm(src)\n",
        "    hyp_ents = _ents_norm(hyp)\n",
        "    overlap = hyp_ents & src_ents\n",
        "    supp = len(overlap)\n",
        "    unsupp = len(hyp_ents - src_ents)\n",
        "    return supp, unsupp\n",
        "\n",
        "# Precompute entity counts, contiguity, etc.\n",
        "supported, unsupported, contigs, avlens = [], [], [], []\n",
        "for i,row in tqdm(list(details_cn2.iterrows()), total=len(details_cn2), desc=\"precompute features\"):\n",
        "    s,u = compute_support_unsupp(row[\"article\"], row[\"summary\"])\n",
        "    supported.append(s); unsupported.append(u)\n",
        "    c,a = lcs_span_stats(row[\"article\"], row[\"summary\"])\n",
        "    contigs.append(c); avlens.append(a)\n",
        "\n",
        "details_cn2[\"supported\"]   = supported\n",
        "details_cn2[\"unsupported\"] = unsupported\n",
        "details_cn2[\"contiguity_lcs\"] = contigs\n",
        "details_cn2[\"avg_span_len_lcs\"] = avlens\n",
        "\n",
        "details_cn2[\"len\"] = details_cn2[\"len\"].clip(lower=1)\n",
        "TARGET_LEN=58; SIGMA=20.0\n",
        "details_cn2[\"len_reward\"] = details_cn2[\"len\"].apply(\n",
        "    lambda l: math.exp(-((l-TARGET_LEN)**2)/(2*SIGMA**2))\n",
        ")\n",
        "\n",
        "# Sweep λ values\n",
        "results = []\n",
        "lambdas = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
        "\n",
        "for lam in lambdas:\n",
        "    dd = details_cn2.copy()\n",
        "\n",
        "    # Entity score: mix precision vs recall\n",
        "    dd[\"ent_prec\"] = dd[\"supported\"] / (dd[\"supported\"]+dd[\"unsupported\"]+1e-6)\n",
        "    dd[\"ent_recall\"] = dd[\"supported\"] / (dd[\"reference\"].apply(lambda x: len(_ents_norm(x)))+1e-6)\n",
        "\n",
        "    dd[\"entity_score\"] = lam*dd[\"ent_prec\"] + (1-lam)*dd[\"ent_recall\"]\n",
        "\n",
        "    dd[\"new_score\"] = (5.0 * dd[\"entity_score\"]) \\\n",
        "                      + (0.4 * (dd[\"base_score\"] / (dd[\"len\"]**0.3))) \\\n",
        "                      + (0.2 * dd[\"contiguity_lcs\"]) \\\n",
        "                      + (0.3 * dd[\"len_reward\"])\n",
        "\n",
        "    best_cn2 = (\n",
        "        dd.sort_values([\"row_id\",\"new_score\"], ascending=[True, False])\n",
        "          .groupby(\"row_id\", as_index=False).head(1)\n",
        "    )\n",
        "\n",
        "    # Eval\n",
        "    refs  = best_cn2[\"reference\"].astype(str).tolist()\n",
        "    bests = best_cn2[\"summary\"].astype(str).tolist()\n",
        "    tops  = details_cn2[details_cn2[\"candidate_k\"]==0][\"summary\"].astype(str).tolist()\n",
        "\n",
        "    r1b,r2b,rlb = _mean(refs, bests)\n",
        "    ent_cn2  = entPRF(refs, bests)\n",
        "    ent_top  = entPRF(refs, tops)\n",
        "\n",
        "    results.append(dict(\n",
        "        lam=lam,\n",
        "        rouge1=round(r1b,4), rouge2=round(r2b,4), rougel=round(rlb,4),\n",
        "        entP=round(ent_cn2[\"entP\"],4), entR=round(ent_cn2[\"entR\"],4), entF1=round(ent_cn2[\"entF1\"],4),\n",
        "        ΔentP=round(ent_cn2[\"entP\"]-ent_top[\"entP\"],4),\n",
        "        ΔentR=round(ent_cn2[\"entR\"]-ent_top[\"entR\"],4),\n",
        "        ΔentF1=round(ent_cn2[\"entF1\"]-ent_top[\"entF1\"],4)\n",
        "    ))\n",
        "\n",
        "pd.DataFrame(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEpOS73njtjZ"
      },
      "source": [
        "What the table shows\n",
        "λ\tBias\tROUGE (avg trend)\tEntity P\tEntity R\tEntity F1\tΔ (vs baseline)\n",
        "0.0\tRecall-oriented\tROUGE stable (~0.418)\t0.433 (low)\t0.531 (high)\tF1 ≈ 0.477\tP ↓, R ↑\n",
        "0.25–0.75\tBalanced\tROUGE stable\tPrecision creeps up (0.438 → 0.445)\tRecall slowly drops (0.526 → 0.518)\tF1 flat (~0.478)\tSmall shifts\n",
        "1.0\tPrecision-oriented\tROUGE stable (~0.417)\t0.468 (highest P)\t0.515 (still high recall)\tF1 = 0.490 (best)\tPrecision loss shrinks\n",
        "Key insights\n",
        "\n",
        "Trade-off exists:\n",
        "\n",
        "At λ=0, the hybrid favors recall — captures many entities, but precision suffers.\n",
        "\n",
        "At λ=1, the hybrid favors precision — avoids hallucinated entities, while recall is still strong.\n",
        "\n",
        "Entity F1 recovers:\n",
        "\n",
        "At λ=1, F1 not only improves over recall-heavy λ=0, it even outperforms the baseline.\n",
        "\n",
        "ΔF1 = +0.003 over baseline → small but positive, while precision is +3 points higher than λ=0.\n",
        "\n",
        "ROUGE stays flat:\n",
        "\n",
        "Across all λ values, ROUGE barely moves (~0.418 ± 0.002).\n",
        "\n",
        "That shows your tuning affects factuality (entity behavior), not general summarization quality.\n",
        "\n",
        "\n",
        " λ works as a control parameter to interpolate between recall-oriented and precision-oriented reranking. At λ=0 the hybrid captures more entities (recall ↑ by ~4 points) but precision drops. As λ increases, precision improves, and at λ=1 the hybrid achieves the best trade-off, with entity precision +3 points higher than recall-only mode and entity F1 also above baseline, while ROUGE remains stable. This shows our hybrid can be tuned for different factuality objectives — recall for coverage, precision for reliability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MEvfqPQKzfB"
      },
      "source": [
        "# Testing PGC and Hybrid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CakO8RchM-Au",
        "outputId": "90fda2f2-9e41-4d03-cd58-0cd8b62e6f6c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-736080778.py:4: DeprecationWarning: 'pkgutil.find_loader' is deprecated and slated for removal in Python 3.14; use importlib.util.find_spec() instead\n",
            "  if pkgutil.find_loader(pkg.split(\"==\")[0].split(\">=\")[0]) is None:\n",
            "/usr/local/lib/python3.12/dist-packages/spacy/util.py:922: UserWarning: [W095] Model 'en_core_web_sm' (3.7.1) was trained with spaCy v3.7.2 and may not be 100% compatible with the current version (3.8.7). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  def get_model_meta(path: Union[str, Path]) -> Dict[str, Any]:\n"
          ]
        }
      ],
      "source": [
        "# === Minimal installs (Colab-safe) ===\n",
        "import sys, subprocess, pkgutil\n",
        "def _pip(pkg):\n",
        "    if pkgutil.find_loader(pkg.split(\"==\")[0].split(\">=\")[0]) is None:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
        "\n",
        "for pkg in [\"transformers>=4.43.0\", \"accelerate\", \"rouge-score\", \"spacy\", \"pandas\", \"numpy\"]:\n",
        "    _pip(pkg)\n",
        "\n",
        "# spaCy model (NER for entity metrics/rules)\n",
        "import spacy, importlib.util\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "except OSError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "\n",
        "# --- Imports ---\n",
        "import os, gc, math, json, csv, time, hashlib, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import torch, torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, LogitsProcessor, LogitsProcessorList\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# --- Reproducibility ---\n",
        "random.seed(0); np.random.seed(0); torch.manual_seed(0)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(0)\n",
        "\n",
        "# --- Device & attention impl ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "def _force_eager_attn(m):\n",
        "    top_prev   = getattr(m.config, \"attn_implementation\", None)\n",
        "    inner_prev = getattr(getattr(m, \"model\", m).config, \"attn_implementation\", None)\n",
        "    if top_prev   != \"eager\": m.config.attn_implementation = \"eager\"\n",
        "    if inner_prev != \"eager\": m.model.config.attn_implementation = \"eager\"\n",
        "    return top_prev, inner_prev\n",
        "\n",
        "def _restore_attn(m, top_prev, inner_prev):\n",
        "    if top_prev   != \"eager\": m.config.attn_implementation = top_prev\n",
        "    if inner_prev != \"eager\": m.model.config.attn_implementation = inner_prev\n",
        "\n",
        "# --- Safe CSV I/O (RFC-4180-ish) ---\n",
        "def save_csv(df: pd.DataFrame, path: str):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    tmp = path + \".tmp\"\n",
        "    df.to_csv(tmp, index=False, quoting=csv.QUOTE_ALL, lineterminator=\"\\n\", encoding=\"utf-8\")\n",
        "    os.replace(tmp, path)\n",
        "\n",
        "def _strict_read(path: str, required_cols=(\"article\",\"highlights\")) -> pd.DataFrame:\n",
        "    tried = []\n",
        "    for enc in (\"utf-8\", \"utf-8-sig\", \"latin-1\"):\n",
        "        try:\n",
        "            df = pd.read_csv(path, encoding=enc)\n",
        "            for c in required_cols:\n",
        "                assert c in df.columns, f\"Missing required column: {c}\"\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            tried.append(f\"{enc}: {e}\")\n",
        "    raise AssertionError(\"Failed to read CSV. Tried -> \" + \" | \".join(tried))\n",
        "\n",
        "# --- Paths (EDIT ONLY if yours differ) ---\n",
        "CKPT_DIR  = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000\"\n",
        "TEST_PATH = \"/content/drive/MyDrive/cleaned_outputs/naa/test_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "OUT_DIR   = os.path.join(CKPT_DIR, \"eval_THC_test\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIxXMhgIXtoQ"
      },
      "outputs": [],
      "source": [
        "# === Load tokenizer & model from your checkpoint ===\n",
        "tok   = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR).to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "# === CopyNext-like logits processor (span continuation on source) ===\n",
        "class SpanContinuationProcessor(LogitsProcessor):\n",
        "    \"\"\"\n",
        "    Bias the next token to continue a source span if the last k tokens of the\n",
        "    decoder input match a span in the source (token-ids). Adds +gamma to that token's logit.\n",
        "    \"\"\"\n",
        "    def __init__(self, source_ids: torch.LongTensor, tokenizer, gamma: float = 0.8, max_span: int = 8):\n",
        "        self.source_ids = source_ids.tolist() if torch.is_tensor(source_ids) else list(source_ids)\n",
        "        self.tokenizer  = tokenizer\n",
        "        self.gamma      = float(gamma)\n",
        "        self.max_span   = int(max_span)\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        # input_ids: (num_beams, cur_len)\n",
        "        # For diverse beam search, we bias each beam the same way (based on its own prefix).\n",
        "        # We only support batch size = 1 for the encoder, so source_ids is a single sequence.\n",
        "        src = self.source_ids\n",
        "        for b in range(input_ids.size(0)):\n",
        "            seq = input_ids[b].tolist()\n",
        "            # try longest suffix first\n",
        "            for k in range(min(self.max_span, len(seq)), 0, -1):\n",
        "                suffix = seq[-k:]\n",
        "                # search suffix in source\n",
        "                for i in range(0, len(src)-k):\n",
        "                    if src[i:i+k] == suffix:\n",
        "                        # candidate next token after this span\n",
        "                        if i+k < len(src):\n",
        "                            next_tok = src[i+k]\n",
        "                            scores[b, next_tok] = scores[b, next_tok] + self.gamma\n",
        "                            return scores\n",
        "                # continue trying shorter k\n",
        "        return scores\n",
        "\n",
        "# === Short decode profile (what you pasted) ===\n",
        "DECODE_ARGS = dict(\n",
        "    num_beams=10,\n",
        "    num_return_sequences=10,\n",
        "    num_beam_groups=5,\n",
        "    diversity_penalty=0.3,\n",
        "    max_new_tokens=44,\n",
        "    min_new_tokens=18,\n",
        "    no_repeat_ngram_size=4,\n",
        "    length_penalty=0.75,\n",
        "    early_stopping=True,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,\n",
        ")\n",
        "MAX_SRC_LEN = 400\n",
        "\n",
        "# --- Hard asserts so the run fails if anything drifts ---\n",
        "exp = {\n",
        "    \"num_beams\":10,\n",
        "    \"num_return_sequences\":10,\n",
        "    \"num_beam_groups\":5,\n",
        "    \"diversity_penalty\":0.3,\n",
        "    \"max_new_tokens\":44,\n",
        "    \"min_new_tokens\":18,\n",
        "    \"no_repeat_ngram_size\":4,\n",
        "    \"length_penalty\":0.75,\n",
        "    \"early_stopping\":True,\n",
        "    \"return_dict_in_generate\":True,\n",
        "    \"output_scores\":True,\n",
        "}\n",
        "for k,v in exp.items():\n",
        "    assert DECODE_ARGS.get(k) == v, f\"DECODE_ARGS[{k}]={DECODE_ARGS.get(k)} != {v}\"\n",
        "\n",
        "assert MAX_SRC_LEN == 400, f\"MAX_SRC_LEN={MAX_SRC_LEN} != 400\"\n",
        "\n",
        "# --- Warn if other decode dicts exist in scope (to avoid accidental use) ---\n",
        "_other = [n for n in globals() if n.startswith(\"DECODE_ARGS_\") or (n.startswith(\"DECODE_ARGS\") and n != \"DECODE_ARGS\")]\n",
        "if _other:\n",
        "    print(\"[warn] Other decode configs in scope:\", _other)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C681utzhkh4D"
      },
      "outputs": [],
      "source": [
        "def generate_with_locked_args(enc_inputs, **kwargs):\n",
        "    # no extra kwargs allowed — forces use of the global DECODE_ARGS above\n",
        "    assert not kwargs, f\"Do not pass extra kwargs to generate; use DECODE_ARGS only. Got: {kwargs}\"\n",
        "    out = model.generate(**enc_inputs, **DECODE_ARGS)\n",
        "    # quick runtime check: decoder length should be within [min_new, max_new] (±specials)\n",
        "    seq_len = out.sequences.size(1)\n",
        "    # For encoder-decoder models, seq_len ≈ new tokens (incl. special tokens).\n",
        "    assert seq_len >= (DECODE_ARGS[\"min_new_tokens\"] - 2), f\"seq_len {seq_len} unexpectedly short\"\n",
        "    assert seq_len <= (DECODE_ARGS[\"max_new_tokens\"] + 5), f\"seq_len {seq_len} unexpectedly long\"\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YGTJrRnkmcL",
        "outputId": "a0046995-750c-4647-adff-87846ef849bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique decoded token lengths: [23, 26, 37, 39, 41, 42, 43, 44]\n",
            "Median decoded token length: 44\n"
          ]
        }
      ],
      "source": [
        "# Pick a handful of articles (e.g., first 8 rows of your test df already loaded as df_test)\n",
        "subset = df_test.head(8)[\"article\"].astype(str).tolist()\n",
        "\n",
        "batch = tok(subset, max_length=MAX_SRC_LEN, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "enc = {k: v.to(model.device) for k,v in batch.items()}\n",
        "\n",
        "out = generate_with_locked_args(enc)\n",
        "hyps = [tok.decode(ids, skip_special_tokens=True) for ids in out.sequences]\n",
        "\n",
        "# Token lengths of decoded text (should cluster <= ~44 tokens)\n",
        "toklens = [len(tok(h)[\"input_ids\"]) for h in hyps]\n",
        "print(\"Unique decoded token lengths:\", sorted(set(toklens)))\n",
        "print(\"Median decoded token length:\", sorted(toklens)[len(toklens)//2])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scqMkNMKXzLw"
      },
      "outputs": [],
      "source": [
        "# === Entity extraction (vs SOURCE), keep these labels ===\n",
        "KEEP_LABELS = {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"DATE\",\"TIME\",\"CARDINAL\",\"MONEY\",\"PERCENT\",\"QUANTITY\",\"ORDINAL\"}\n",
        "CORE = {\"PERSON\",\"ORG\",\"GPE\"}\n",
        "\n",
        "def _normalize_ent_text(s: str) -> str:\n",
        "    s = (s or \"\").strip()\n",
        "    return \" \".join(s.split())\n",
        "\n",
        "def _ents_norm(text: str):\n",
        "    doc = nlp(str(text))\n",
        "    out = set()\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in KEEP_LABELS:\n",
        "            out.add((_normalize_ent_text(ent.text), ent.label_))\n",
        "    return out\n",
        "\n",
        "def strict_entity_score(src_text, hyp_text):\n",
        "    src_ents = _ents_norm(src_text)\n",
        "    hyp_ents = _ents_norm(hyp_text)\n",
        "    overlap = hyp_ents & src_ents\n",
        "    unsupported = hyp_ents - src_ents\n",
        "    hard_fail = any(lbl in CORE for _,lbl in unsupported)\n",
        "    score = 0\n",
        "    for _,lbl in overlap:\n",
        "        score += 3 if lbl in CORE else 1\n",
        "    return score, hard_fail\n",
        "\n",
        "# === LCS span stats (whitespace tokens) using DP, contiguous block emphasis ===\n",
        "def lcs_span_stats(src_text, hyp_text):\n",
        "    s = str(src_text).split(); h = str(hyp_text).split()\n",
        "    n,m = len(s), len(h)\n",
        "    dp = [[0]*(m+1) for _ in range(n+1)]\n",
        "    for i in range(n-1, -1, -1):\n",
        "        si = s[i]; row = dp[i]; row1=dp[i+1]\n",
        "        for j in range(m-1, -1, -1):\n",
        "            row[j] = row1[j+1]+1 if si==h[j] else max(row[j+1], row1[j])\n",
        "    # backtrack contiguous blocks\n",
        "    i=j=0; blocks=[]; cur=0\n",
        "    while i<n and j<m:\n",
        "        if s[i]==h[j]:\n",
        "            cur+=1; i+=1; j+=1\n",
        "        else:\n",
        "            if cur>=2: blocks.append(cur); cur=0\n",
        "            if dp[i+1][j] >= dp[i][j+1]: i+=1\n",
        "            else: j+=1\n",
        "    if cur>=2: blocks.append(cur)\n",
        "    contiguity = sum(b-1 for b in blocks)    # contiguous transitions\n",
        "    avg_len = (sum(blocks)/len(blocks)) if blocks else 1.0\n",
        "    return contiguity, avg_len\n",
        "\n",
        "# === Generation with CN2 and CopyNext ===\n",
        "@torch.no_grad()\n",
        "def generate_cn2(art_text: str):\n",
        "    batch = tok([art_text], max_length=MAX_SRC_LEN, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "    enc = {k: v.to(DEVICE) for k,v in batch.items()}\n",
        "    # per-sample CopyNext processor bound to source ids\n",
        "    proc = SpanContinuationProcessor(enc[\"input_ids\"][0], tok, gamma=0.8, max_span=8)\n",
        "    top_prev, inner_prev = _force_eager_attn(model)\n",
        "    try:\n",
        "        out = model.generate(\n",
        "            **enc,\n",
        "            logits_processor=LogitsProcessorList([proc]),\n",
        "            **DECODE_ARGS\n",
        "        )\n",
        "    finally:\n",
        "        _restore_attn(model, top_prev, inner_prev)\n",
        "    return enc, out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "4c23c0f93d8a410c8fab1254beeac51a"
          ]
        },
        "id": "_iCZrHr9X2Wn",
        "outputId": "618a0124-acc1-4db6-cbbb-bc5d3bbbf52b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c23c0f93d8a410c8fab1254beeac51a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "CN2 beams on TEST:   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/eval_THC_test/details_cn2_20250826_225336.csv | /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/eval_THC_test/top_beams_20250826_225336.csv\n"
          ]
        }
      ],
      "source": [
        "# === Load TEST and (optionally) subset ===\n",
        "df_test = _strict_read(TEST_PATH, required_cols=(\"article\",\"highlights\"))\n",
        "N_SAMPLES = 100\n",
        "if N_SAMPLES:\n",
        "    df_test = df_test.head(N_SAMPLES).copy()\n",
        "\n",
        "details_rows = []\n",
        "top_rows = []\n",
        "tq = tqdm(list(df_test.iterrows()), total=len(df_test), desc=\"CN2 beams on TEST\")\n",
        "\n",
        "for i, row in tq:\n",
        "    art = str(row[\"article\"]); ref = str(row[\"highlights\"])\n",
        "\n",
        "    enc, gen = generate_cn2(art)\n",
        "    seqs = gen.sequences\n",
        "    base = gen.sequences_scores\n",
        "\n",
        "    # top beam k=0\n",
        "    top_sum = tok.decode(seqs[0], skip_special_tokens=True)\n",
        "    top_rows.append({\"row_id\": i, \"article\": art, \"reference\": ref, \"top_beam_summary\": top_sum})\n",
        "\n",
        "    # collect features for all K\n",
        "    for k, (seq_ids, base_score) in enumerate(zip(seqs, base)):\n",
        "        hyp = tok.decode(seq_ids, skip_special_tokens=True)\n",
        "        ent_score, hard_fail = strict_entity_score(art, hyp)\n",
        "        c_lcs, av_lcs = lcs_span_stats(art, hyp)\n",
        "        L = len(tok(hyp)[\"input_ids\"])\n",
        "        details_rows.append({\n",
        "            \"row_id\": i, \"candidate_k\": k,\n",
        "            \"article\": art, \"reference\": ref, \"summary\": hyp,\n",
        "            \"base_score\": float(base_score.item()), \"len\": L,\n",
        "            \"entity_score\": ent_score, \"fail_flag\": hard_fail,\n",
        "            \"contiguity_lcs\": c_lcs, \"avg_span_len_lcs\": av_lcs\n",
        "        })\n",
        "\n",
        "    # memory hygiene\n",
        "    del enc, gen\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "\n",
        "dd = pd.DataFrame(details_rows)\n",
        "top_df = pd.DataFrame(top_rows)\n",
        "\n",
        "STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "DETAILS_CN2 = os.path.join(OUT_DIR, f\"details_cn2_{STAMP}.csv\")\n",
        "TOP_BEAMS    = os.path.join(OUT_DIR, f\"top_beams_{STAMP}.csv\")\n",
        "save_csv(dd, DETAILS_CN2)\n",
        "save_csv(top_df, TOP_BEAMS)\n",
        "print(\"Saved:\", DETAILS_CN2, \"|\", TOP_BEAMS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FK5jBKhbX41V",
        "outputId": "442340e6-fc2d-4353-f72d-6b720de0771b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE  TOP: {'r1_f': 0.3281, 'r2_f': 0.1367, 'rl_f': 0.2425}\n",
            "ROUGE  CN2: {'r1_f': 0.3234, 'r2_f': 0.1282, 'rl_f': 0.2394}\n",
            "Δ ROUGE  : {'r1': -0.0047, 'r2': -0.0085, 'rl': -0.003}\n",
            "\n",
            "ENT  TOP : {'TP': 92, 'FP': 270, 'FN': 234, 'entP': 0.2541, 'entR': 0.2822, 'entF1': 0.2674}\n",
            "ENT  CN2 : {'TP': 101, 'FP': 325, 'FN': 225, 'entP': 0.2371, 'entR': 0.3098, 'entF1': 0.2686}\n",
            "Δ ENT    : {'entP': -0.0171, 'entR': 0.0276, 'entF1': 0.0012}\n",
            "Saved: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/eval_THC_test/best_cn2_THC_20250826_225347.csv | /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/eval_THC_test/eval_config_THC_20250826_225347.json\n"
          ]
        }
      ],
      "source": [
        "# ============================\n",
        "# PASS 2 (ROBUST): TH-C rerank + metrics\n",
        "# - fixes KeyError: 'reference' by backfilling from 'highlights'\n",
        "# - recomputes entity/LCS features if missing (fast Pass-1 case)\n",
        "# ============================\n",
        "\n",
        "import math, time, json, os, csv\n",
        "import pandas as pd\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "assert \"DETAILS_CN2\" in globals() and \"TOP_BEAMS\" in globals(), \"Run Pass-1 first to set these paths.\"\n",
        "assert \"tok\" in globals() and \"nlp\" in globals(), \"Tokenizer and spaCy must be loaded.\"\n",
        "\n",
        "# ---------- load ----------\n",
        "details_cn2 = pd.read_csv(DETAILS_CN2)\n",
        "top_df      = pd.read_csv(TOP_BEAMS)\n",
        "\n",
        "# ---------- normalize columns ----------\n",
        "def _ensure_ref(df):\n",
        "    if \"reference\" not in df.columns:\n",
        "        if \"highlights\" in df.columns:\n",
        "            df[\"reference\"] = df[\"highlights\"].astype(str)\n",
        "        elif \"ref\" in df.columns:\n",
        "            df[\"reference\"] = df[\"ref\"].astype(str)\n",
        "        else:\n",
        "            raise KeyError(\"Neither 'reference' nor 'highlights' present.\")\n",
        "    return df\n",
        "\n",
        "details_cn2 = _ensure_ref(details_cn2)\n",
        "top_df      = _ensure_ref(top_df)\n",
        "\n",
        "if \"article\" not in details_cn2.columns and \"source\" in details_cn2.columns:\n",
        "    details_cn2 = details_cn2.rename(columns={\"source\":\"article\"})\n",
        "\n",
        "# ---------- feature fns (TH-C) ----------\n",
        "KEEP_LABELS = {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"DATE\",\"TIME\",\"CARDINAL\",\"MONEY\",\"PERCENT\",\"QUANTITY\",\"ORDINAL\"}\n",
        "CORE = {\"PERSON\",\"ORG\",\"GPE\"}\n",
        "\n",
        "def _normalize_ent_text(s: str):\n",
        "    s = (s or \"\").strip()\n",
        "    return \" \".join(s.split())\n",
        "\n",
        "def _ents_norm(text: str):\n",
        "    doc = nlp(str(text))\n",
        "    out = set()\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in KEEP_LABELS:\n",
        "            out.add((_normalize_ent_text(ent.text), ent.label_))\n",
        "    return out\n",
        "\n",
        "def strict_entity_score(src_text, hyp_text):\n",
        "    src_ents = _ents_norm(src_text)\n",
        "    hyp_ents = _ents_norm(hyp_text)\n",
        "    overlap = hyp_ents & src_ents\n",
        "    unsupported = hyp_ents - src_ents\n",
        "    hard_fail = any(lbl in CORE for _,lbl in unsupported)\n",
        "    score = 0\n",
        "    for _,lbl in overlap:\n",
        "        score += 3 if lbl in CORE else 1\n",
        "    return score, hard_fail\n",
        "\n",
        "def lcs_span_stats(src_text, hyp_text):\n",
        "    s = str(src_text).split(); h = str(hyp_text).split()\n",
        "    n,m = len(s), len(h)\n",
        "    dp = [[0]*(m+1) for _ in range(n+1)]\n",
        "    for i in range(n-1, -1, -1):\n",
        "        si = s[i]; row = dp[i]; row1=dp[i+1]\n",
        "        for j in range(m-1, -1, -1):\n",
        "            row[j] = row1[j+1]+1 if si==h[j] else max(row[j+1], row1[j])\n",
        "    i=j=0; blocks=[]; cur=0\n",
        "    while i<n and j<m:\n",
        "        if s[i]==h[j]:\n",
        "            cur+=1; i+=1; j+=1\n",
        "        else:\n",
        "            if cur>=2: blocks.append(cur); cur=0\n",
        "            if dp[i+1][j] >= dp[i][j+1]: i+=1\n",
        "            else: j+=1\n",
        "    if cur>=2: blocks.append(cur)\n",
        "    contiguity = sum(b-1 for b in blocks)\n",
        "    avg_len = (sum(blocks)/len(blocks)) if blocks else 1.0\n",
        "    return contiguity, avg_len\n",
        "\n",
        "# ---------- (re)compute missing features ----------\n",
        "need_feats = not {\"entity_score\",\"fail_flag\",\"contiguity_lcs\",\"avg_span_len_lcs\"}.issubset(details_cn2.columns)\n",
        "if need_feats:\n",
        "    ent_scores, fails, contigs, avlens = [], [], [], []\n",
        "    for _, row in details_cn2.iterrows():\n",
        "        s,f = strict_entity_score(row[\"article\"], row[\"summary\"])\n",
        "        c,a = lcs_span_stats(row[\"article\"], row[\"summary\"])\n",
        "        ent_scores.append(s); fails.append(f); contigs.append(c); avlens.append(a)\n",
        "    details_cn2[\"entity_score\"]     = ent_scores\n",
        "    details_cn2[\"fail_flag\"]        = fails\n",
        "    details_cn2[\"contiguity_lcs\"]   = contigs\n",
        "    details_cn2[\"avg_span_len_lcs\"] = avlens\n",
        "\n",
        "# ---------- TH-C scoring ----------\n",
        "TARGET_LEN = 58\n",
        "LEN_SIGMA  = 20.0\n",
        "def length_reward(l):\n",
        "    l = max(int(l), 1)\n",
        "    return math.exp(-((l - TARGET_LEN)**2) / (2 * (LEN_SIGMA**2)))\n",
        "\n",
        "if \"len\" not in details_cn2.columns:\n",
        "    # fallback: recompute tokenized length of hypothesis\n",
        "    details_cn2[\"len\"] = details_cn2[\"summary\"].astype(str).apply(lambda t: len(tok(t)[\"input_ids\"]))\n",
        "\n",
        "details_cn2[\"len\"] = details_cn2[\"len\"].clip(lower=1)\n",
        "details_cn2[\"len_reward\"] = details_cn2[\"len\"].apply(length_reward)\n",
        "\n",
        "details_cn2[\"new_score\"] = (5.0 * details_cn2[\"entity_score\"]) \\\n",
        "                         + (0.4 * (details_cn2[\"base_score\"] / (details_cn2[\"len\"]**0.3))) \\\n",
        "                         + (0.2 * details_cn2[\"contiguity_lcs\"]) \\\n",
        "                         + (0.3 * details_cn2[\"len_reward\"])\n",
        "\n",
        "# Stage 1: drop hard fails\n",
        "dd_ok = details_cn2[~details_cn2[\"fail_flag\"]].copy()\n",
        "if dd_ok.empty:\n",
        "    print(\"[warn] all beams failed strict filter — reverting to unfiltered\")\n",
        "    dd_ok = details_cn2.copy()\n",
        "\n",
        "# Stage 2: take best per row\n",
        "best_cn2 = (dd_ok.sort_values([\"row_id\",\"new_score\"], ascending=[True, False])\n",
        "                 .groupby(\"row_id\", as_index=False).head(1)\n",
        "                 .rename(columns={\"candidate_k\":\"best_k\"}))\n",
        "\n",
        "# ---------- metrics ----------\n",
        "def _col(df, name, alt):\n",
        "    return df[name].astype(str) if name in df.columns else df[alt].astype(str)\n",
        "\n",
        "ev = best_cn2.merge(top_df, on=\"row_id\", how=\"left\", suffixes=(\"\",\"_top\"))\n",
        "refs  = _col(ev, \"reference\", \"highlights\").tolist()\n",
        "tops  = ev[\"top_beam_summary\"].astype(str).tolist()\n",
        "bests = ev[\"summary\"].astype(str).tolist()\n",
        "\n",
        "sc = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\"], use_stemmer=True)\n",
        "def _mean_rouge(refs, hyps):\n",
        "    r1=r2=rl=0.0\n",
        "    for r,h in zip(refs, hyps):\n",
        "        s=sc.score(r,h); r1+=s[\"rouge1\"].fmeasure; r2+=s[\"rouge2\"].fmeasure; rl+=s[\"rougeLsum\"].fmeasure\n",
        "    n=len(refs); return r1/n, r2/n, rl/n\n",
        "\n",
        "def entPRF(refs, hyps):\n",
        "    TP=FP=FN=0\n",
        "    for r,h in zip(refs, hyps):\n",
        "        R = _ents_norm(r); H = _ents_norm(h)\n",
        "        TP += len(H & R)\n",
        "        FP += len(H - R)\n",
        "        FN += len(R - H)\n",
        "    P = TP/(TP+FP) if (TP+FP) else 0.0\n",
        "    R = TP/(TP+FN) if (TP+FN) else 0.0\n",
        "    F = 2*P*R/(P+R) if (P+R) else 0.0\n",
        "    return {\"TP\":TP,\"FP\":FP,\"FN\":FN,\"entP\":P,\"entR\":R,\"entF1\":F}\n",
        "\n",
        "r1t,r2t,rlt = _mean_rouge(refs, tops)\n",
        "r1b,r2b,rlb = _mean_rouge(refs, bests)\n",
        "et, ec = entPRF(refs, tops), entPRF(refs, bests)\n",
        "fmt = lambda d: {k:(round(v,4) if isinstance(v,float) else v) for k,v in d.items()}\n",
        "\n",
        "print(\"ROUGE  TOP:\", {\"r1_f\":round(r1t,4),\"r2_f\":round(r2t,4),\"rl_f\":round(rlt,4)})\n",
        "print(\"ROUGE  CN2:\", {\"r1_f\":round(r1b,4),\"r2_f\":round(r2b,4),\"rl_f\":round(rlb,4)})\n",
        "print(\"Δ ROUGE  :\", {\"r1\":round(r1b-r1t,4),\"r2\":round(r2b-r2t,4),\"rl\":round(rlb-rlt,4)})\n",
        "print()\n",
        "print(\"ENT  TOP :\", fmt(et))\n",
        "print(\"ENT  CN2 :\", fmt(ec))\n",
        "print(\"Δ ENT    :\", {k: round(ec[k]-et[k],4) for k in [\"entP\",\"entR\",\"entF1\"]})\n",
        "\n",
        "# ---------- save artifacts ----------\n",
        "STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "BEST_CN2 = os.path.join(OUT_DIR, f\"best_cn2_THC_{STAMP}.csv\")\n",
        "save_csv(best_cn2, BEST_CN2)\n",
        "\n",
        "RUN_CFG = {\n",
        "    \"detail_path\": DETAILS_CN2,\n",
        "    \"top_path\": TOP_BEAMS,\n",
        "    \"rerank\": {\n",
        "        \"type\": \"TH-C strict entity + LCS + length\",\n",
        "        \"weights\": {\"entity\":5.0, \"base_norm\":0.4, \"contig_lcs\":0.2, \"len_reward\":0.3},\n",
        "        \"hard_fail\": list(CORE),\n",
        "        \"target_len\": 58, \"len_sigma\": 20.0\n",
        "    }\n",
        "}\n",
        "CFG_PATH = os.path.join(OUT_DIR, f\"eval_config_THC_{STAMP}.json\")\n",
        "with open(CFG_PATH, \"w\") as f:\n",
        "    json.dump(RUN_CFG, f, indent=2)\n",
        "print(\"Saved:\", BEST_CN2, \"|\", CFG_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "Ry5MvvWmdm2l",
        "outputId": "2e64f9cc-92cd-40e6-879a-3278eeb3c7e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using:\n",
            "  TOP_BEAMS: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/eval_THC_test/top_beams_20250826_225336.csv\n",
            "  BEST_CN2 : /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/eval_THC_test/best_cn2_THC_20250826_225347.csv\n",
            "(99, 4)\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 99,\n  \"fields\": [\n    {\n      \"column\": \"row_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28,\n        \"min\": 0,\n        \"max\": 98,\n        \"num_unique_values\": 99,\n        \"samples\": [\n          62,\n          40,\n          95\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 99,\n        \"samples\": [\n          \"Mark Wahlberg is planning to appear in \\\"Patriots' Day\\\" The film will be about events surrounding the 2013 Boston Marathon bombing . Another film, \\\"Boston Strong,\\\" is also in the works .\",\n          \"High temperatures are recorded on the northern tip of the Antarctica Peninsula . The World Meteorological Organization will make the final determination .\",\n          \"Manuscript of \\\"American Pie\\\" lyrics is sold to unnamed buyer for $1.2 million . Douglas Brinkley: The song, a talisman for its age, brings joy to people 44 years later .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cn2_summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 99,\n        \"samples\": [\n          \"Mark Wahlberg will star in a film about the Boston Marathon bombing. The film will be told from Davis' point of view. The film is the second film related to the Boston bombing to be announced.\",\n          \"A high temperature of 63.5 degrees Fahrenheit might sound like a pleasant day in early spring -- unless you're in Antarctica. The chilly continent recorded the temperature (15.5 degrees Celsius) on March 24,\",\n          \"Don McLean's \\\"American Pie\\\" was released in November 1971. The song's lyrics were written by a paperboy when he saw that Buddy Holly, Ritchie Valens and J.P. \\\"The\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"top_summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 99,\n        \"samples\": [\n          \"Mark Wahlberg will star in a film about the Boston Marathon bombing. The film will be told from Davis' point of view. The film is the second film related to the bombing to be announced.\",\n          \"A high temperature of 63.5 degrees Fahrenheit might sound like a pleasant day in early spring -- unless you're in Antarctica. The chilly continent recorded the temperature (15.5 degrees Celsius) on March 24.\",\n          \"Don McLean's \\\"American Pie\\\" was released in November 1971. The song's lyrics are a hybrid of modern poetry and folk ballad, beer-hall chant and high-art rock. McLean's\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-7a51fb8f-87e0-464c-957c-33823d36b18e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>row_id</th>\n",
              "      <th>reference</th>\n",
              "      <th>cn2_summary</th>\n",
              "      <th>top_summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Membership gives the ICC jurisdiction over all...</td>\n",
              "      <td>The Palestinian Authority officially becomes t...</td>\n",
              "      <td>The Palestinian Authority officially becomes t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Theia, a bully breed mix, was apparently hit b...</td>\n",
              "      <td>Theia, a friendly white-and-black bully breed ...</td>\n",
              "      <td>Theia, a friendly white-and-black bully breed ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a51fb8f-87e0-464c-957c-33823d36b18e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7a51fb8f-87e0-464c-957c-33823d36b18e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7a51fb8f-87e0-464c-957c-33823d36b18e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-293ca1cb-086d-4bc4-9d31-2de823b6f4be\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-293ca1cb-086d-4bc4-9d31-2de823b6f4be')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-293ca1cb-086d-4bc4-9d31-2de823b6f4be button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   row_id                                          reference  \\\n",
              "0       0  Membership gives the ICC jurisdiction over all...   \n",
              "1       1  Theia, a bully breed mix, was apparently hit b...   \n",
              "\n",
              "                                         cn2_summary  \\\n",
              "0  The Palestinian Authority officially becomes t...   \n",
              "1  Theia, a friendly white-and-black bully breed ...   \n",
              "\n",
              "                                         top_summary  \n",
              "0  The Palestinian Authority officially becomes t...  \n",
              "1  Theia, a friendly white-and-black bully breed ...  "
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# === Paths: set OUT_DIR to your TH-C test eval folder ===\n",
        "OUT_DIR = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/eval_THC_test\"\n",
        "\n",
        "import os, glob, pandas as pd\n",
        "\n",
        "def _latest(pattern):\n",
        "    files = glob.glob(os.path.join(OUT_DIR, pattern))\n",
        "    assert files, f\"No files match: {pattern} in {OUT_DIR}\"\n",
        "    return max(files, key=os.path.getmtime)\n",
        "\n",
        "# Most recent files from your last run(s)\n",
        "TOP_BEAMS = _latest(\"top_beams_*.csv\")\n",
        "BEST_CN2  = _latest(\"best_cn2_THC_*.csv\")\n",
        "\n",
        "print(\"Using:\")\n",
        "print(\"  TOP_BEAMS:\", TOP_BEAMS)\n",
        "print(\"  BEST_CN2 :\", BEST_CN2)\n",
        "\n",
        "top_df  = pd.read_csv(TOP_BEAMS)\n",
        "best_df = pd.read_csv(BEST_CN2)\n",
        "\n",
        "# Normalize column names\n",
        "if \"reference\" not in top_df.columns and \"highlights\" in top_df.columns:\n",
        "    top_df[\"reference\"] = top_df[\"highlights\"].astype(str)\n",
        "if \"reference\" not in best_df.columns and \"highlights\" in best_df.columns:\n",
        "    best_df[\"reference\"] = best_df[\"highlights\"].astype(str)\n",
        "\n",
        "# Merge to align rows (row_id is the key written by Pass-1/2)\n",
        "df = best_df[[\"row_id\",\"reference\",\"summary\"]].merge(\n",
        "    top_df[[\"row_id\",\"top_beam_summary\"]],\n",
        "    on=\"row_id\",\n",
        "    how=\"left\"\n",
        ").rename(columns={\"summary\":\"cn2_summary\", \"top_beam_summary\":\"top_summary\"})\n",
        "\n",
        "# Quick sanity\n",
        "print(df.shape)\n",
        "df.head(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "1errzLALgCmv",
        "outputId": "b394554b-a4bc-45ce-f5f8-fe2e1cbfe7be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Token-length stats (tokenizer-based) ===\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"print(\\\"\\\\nSaved per-example lengths to:\\\", out_csv)\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 99.0,\n        \"max\": 99.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          99.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4536885862938744,\n        \"min\": 43.0,\n        \"max\": 43.85,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          43.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.900533874150711,\n        \"min\": 0.69,\n        \"max\": 11.09,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          11.09\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"min\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.717797887081348,\n        \"min\": 23.0,\n        \"max\": 39.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          23.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"q25\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.773502691896258,\n        \"min\": 34.0,\n        \"max\": 44.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          44.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"median\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.7320508075688772,\n        \"min\": 41.0,\n        \"max\": 44.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          44.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"q75\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.196152422706632,\n        \"min\": 44.0,\n        \"max\": 53.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          44.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"max\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 15.588457268119896,\n        \"min\": 44.0,\n        \"max\": 71.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          44.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-b155424c-6c82-4dbf-92fd-bb52ab7c2b5d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>q25</th>\n",
              "      <th>median</th>\n",
              "      <th>q75</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>reference_tokens</th>\n",
              "      <td>99.0</td>\n",
              "      <td>43.00</td>\n",
              "      <td>11.09</td>\n",
              "      <td>23.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>71.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top_tokens</th>\n",
              "      <td>99.0</td>\n",
              "      <td>43.70</td>\n",
              "      <td>1.06</td>\n",
              "      <td>37.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>44.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cn2_tokens</th>\n",
              "      <td>99.0</td>\n",
              "      <td>43.85</td>\n",
              "      <td>0.69</td>\n",
              "      <td>39.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>44.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b155424c-6c82-4dbf-92fd-bb52ab7c2b5d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b155424c-6c82-4dbf-92fd-bb52ab7c2b5d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b155424c-6c82-4dbf-92fd-bb52ab7c2b5d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-6690e18d-cc91-4329-963e-47f7a5a8a95b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6690e18d-cc91-4329-963e-47f7a5a8a95b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-6690e18d-cc91-4329-963e-47f7a5a8a95b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                  count   mean    std   min   q25  median   q75   max\n",
              "reference_tokens   99.0  43.00  11.09  23.0  34.0    41.0  53.0  71.0\n",
              "top_tokens         99.0  43.70   1.06  37.0  44.0    44.0  44.0  44.0\n",
              "cn2_tokens         99.0  43.85   0.69  39.0  44.0    44.0  44.0  44.0"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Word-length stats (fallback) ===\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"print(\\\"\\\\nSaved per-example lengths to:\\\", out_csv)\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 99.0,\n        \"max\": 99.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          99.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.48180217240412526,\n        \"min\": 33.61,\n        \"max\": 34.49,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          34.49\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.8595897882201595,\n        \"min\": 3.13,\n        \"max\": 9.82,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          9.82\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"min\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.041451884327381,\n        \"min\": 18.0,\n        \"max\": 25.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          25.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"q25\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.1754264805429417,\n        \"min\": 26.5,\n        \"max\": 32.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          32.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"median\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5773502691896258,\n        \"min\": 33.0,\n        \"max\": 34.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          34.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"q75\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.618802153517007,\n        \"min\": 36.0,\n        \"max\": 44.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          36.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"max\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.969655114602888,\n        \"min\": 40.0,\n        \"max\": 59.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          40.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-7f9b8753-77b2-4e85-bb67-4c782a3b6e5f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>q25</th>\n",
              "      <th>median</th>\n",
              "      <th>q75</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>reference_words</th>\n",
              "      <td>99.0</td>\n",
              "      <td>34.49</td>\n",
              "      <td>9.82</td>\n",
              "      <td>18.0</td>\n",
              "      <td>26.5</td>\n",
              "      <td>33.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>59.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top_words</th>\n",
              "      <td>99.0</td>\n",
              "      <td>33.71</td>\n",
              "      <td>3.13</td>\n",
              "      <td>25.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>40.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cn2_words</th>\n",
              "      <td>99.0</td>\n",
              "      <td>33.61</td>\n",
              "      <td>3.14</td>\n",
              "      <td>25.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>40.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7f9b8753-77b2-4e85-bb67-4c782a3b6e5f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7f9b8753-77b2-4e85-bb67-4c782a3b6e5f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7f9b8753-77b2-4e85-bb67-4c782a3b6e5f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c89213da-53e3-473c-9d48-d0a3b47873ce\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c89213da-53e3-473c-9d48-d0a3b47873ce')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c89213da-53e3-473c-9d48-d0a3b47873ce button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                 count   mean   std   min   q25  median   q75   max\n",
              "reference_words   99.0  34.49  9.82  18.0  26.5    33.0  44.0  59.0\n",
              "top_words         99.0  33.71  3.13  25.0  32.0    34.0  36.0  40.0\n",
              "cn2_words         99.0  33.61  3.14  25.0  32.0    34.0  36.0  40.0"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Medians (tokens): ref = 41.0 | top = 44.0 | cn2 = 44.0\n",
            "Medians (words):  ref = 33.0 | top = 34.0 | cn2 = 34.0\n",
            "\n",
            "Saved per-example lengths to: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/eval_THC_test/lengths_per_example.csv\n"
          ]
        }
      ],
      "source": [
        "import numpy as np, pandas as pd, re\n",
        "\n",
        "# Prefer tokenizer-based token length (same as generation). Fallback to whitespace words.\n",
        "def _tok_len(text):\n",
        "    try:\n",
        "        # assumes `tok` exists from your generation setup\n",
        "        return len(tok(str(text))[\"input_ids\"])\n",
        "    except Exception:\n",
        "        # fallback: simple word count (lower fidelity)\n",
        "        return len(re.findall(r\"\\S+\", str(text)))\n",
        "\n",
        "def _series_stats(x):\n",
        "    x = pd.Series(x, dtype=float).dropna()\n",
        "    q = x.quantile([0.0, 0.25, 0.5, 0.75, 1.0])\n",
        "    out = {\n",
        "        \"count\": int(x.size),\n",
        "        \"mean\": float(x.mean()),\n",
        "        \"std\": float(x.std(ddof=1)) if x.size > 1 else 0.0,\n",
        "        \"min\": float(q.loc[0.00]),\n",
        "        \"q25\": float(q.loc[0.25]),\n",
        "        \"median\": float(q.loc[0.50]),\n",
        "        \"q75\": float(q.loc[0.75]),\n",
        "        \"max\": float(q.loc[1.00]),\n",
        "    }\n",
        "    return out\n",
        "\n",
        "# Compute lengths\n",
        "ref_tok_len  = df[\"reference\"].map(_tok_len)\n",
        "top_tok_len  = df[\"top_summary\"].map(_tok_len)\n",
        "cn2_tok_len  = df[\"cn2_summary\"].map(_tok_len)\n",
        "\n",
        "ref_word_len = df[\"reference\"].str.split().map(len)\n",
        "top_word_len = df[\"top_summary\"].str.split().map(len)\n",
        "cn2_word_len = df[\"cn2_summary\"].str.split().map(len)\n",
        "\n",
        "# Tables of stats\n",
        "tok_stats  = pd.DataFrame({\n",
        "    \"reference_tokens\": _series_stats(ref_tok_len),\n",
        "    \"top_tokens\":       _series_stats(top_tok_len),\n",
        "    \"cn2_tokens\":       _series_stats(cn2_tok_len),\n",
        "}).T\n",
        "\n",
        "word_stats = pd.DataFrame({\n",
        "    \"reference_words\": _series_stats(ref_word_len),\n",
        "    \"top_words\":       _series_stats(top_word_len),\n",
        "    \"cn2_words\":       _series_stats(cn2_word_len),\n",
        "}).T\n",
        "\n",
        "print(\"=== Token-length stats (tokenizer-based) ===\")\n",
        "display(tok_stats.round(2))\n",
        "\n",
        "print(\"\\n=== Word-length stats (fallback) ===\")\n",
        "display(word_stats.round(2))\n",
        "\n",
        "# Helpful deltas\n",
        "print(\"\\nMedians (tokens): ref =\", np.median(ref_tok_len),\n",
        "      \"| top =\", np.median(top_tok_len),\n",
        "      \"| cn2 =\", np.median(cn2_tok_len))\n",
        "\n",
        "print(\"Medians (words):  ref =\", np.median(ref_word_len),\n",
        "      \"| top =\", np.median(top_word_len),\n",
        "      \"| cn2 =\", np.median(cn2_word_len))\n",
        "\n",
        "# Optional: save a CSV with per-example lengths for inspection\n",
        "out_csv = os.path.join(OUT_DIR, \"lengths_per_example.csv\")\n",
        "df_lengths = pd.DataFrame({\n",
        "    \"row_id\": df[\"row_id\"],\n",
        "    \"ref_tok\": ref_tok_len,\n",
        "    \"top_tok\": top_tok_len,\n",
        "    \"cn2_tok\": cn2_tok_len,\n",
        "    \"ref_words\": ref_word_len,\n",
        "    \"top_words\": top_word_len,\n",
        "    \"cn2_words\": cn2_word_len,\n",
        "})\n",
        "df_lengths.to_csv(out_csv, index=False)\n",
        "print(\"\\nSaved per-example lengths to:\", out_csv)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kvv6MmOepaNI"
      },
      "source": [
        "#λ=1 on TEST(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiaNelrZguVX"
      },
      "outputs": [],
      "source": [
        "# ==== FROZEN EXPERIMENT CONFIG: Beam-20 + EntityAware CopyNext + λ=1 on TEST(100) ====\n",
        "import os, json, hashlib, time, random, numpy as np, torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Paths (edit if needed)\n",
        "CKPT_DIR  = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000\"\n",
        "TEST_PATH = \"/content/drive/MyDrive/cleaned_outputs/naa/test_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "OUT_DIR   = os.path.join(CKPT_DIR, \"eval_BEAM20_EA_lambda1_test\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Decode: Beam-20 diverse, long outputs (matches your val run)\n",
        "DECODE_ARGS = dict(\n",
        "    num_beams=20,\n",
        "    num_return_sequences=20,\n",
        "    num_beam_groups=10,\n",
        "    diversity_penalty=0.3,\n",
        "    max_new_tokens=100, min_new_tokens=55,\n",
        "    no_repeat_ngram_size=3,\n",
        "    length_penalty=2.0,\n",
        "    early_stopping=True,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True\n",
        ")\n",
        "MAX_SRC_LEN = 400\n",
        "\n",
        "# Entity-Aware CopyNext logits bias\n",
        "EA_ARGS = dict(gamma=0.5, gamma_entity=2.0, max_span=6)\n",
        "\n",
        "# Rerank: λ = 1.0 (pure precision)\n",
        "LAMBDA = 1.0\n",
        "W = dict(entity=5.0, base_norm=0.4, contig_lcs=0.2, len_reward=0.3)\n",
        "TARGET_LEN, LEN_SIGMA = 58, 20.0\n",
        "\n",
        "# Repro\n",
        "SEED = 0\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Load model/tokenizer if not already loaded\n",
        "try:\n",
        "    tok\n",
        "    model\n",
        "except NameError:\n",
        "    tok   = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR).to(\"cuda\" if torch.cuda.is_available() else \"cpu\").eval()\n",
        "    if torch.cuda.is_available():\n",
        "        try: model.half()\n",
        "        except: pass\n",
        "\n",
        "# Safety asserts\n",
        "assert DECODE_ARGS[\"num_beams\"] == 20 and DECODE_ARGS[\"num_return_sequences\"] == 20\n",
        "assert DECODE_ARGS[\"num_beam_groups\"] == 10 and DECODE_ARGS[\"diversity_penalty\"] == 0.3\n",
        "assert DECODE_ARGS[\"max_new_tokens\"] == 100 and DECODE_ARGS[\"min_new_tokens\"] == 55\n",
        "assert DECODE_ARGS[\"no_repeat_ngram_size\"] == 3 and DECODE_ARGS[\"length_penalty\"] == 2.0\n",
        "assert MAX_SRC_LEN == 400\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "bfbfa354102a49a5847a5f5a6bb1c8c0"
          ]
        },
        "id": "0JcH53xlpyoK",
        "outputId": "b4e3daa1-5244-4781-99f1-61384b5c77d1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bfbfa354102a49a5847a5f5a6bb1c8c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Beam-20 EntityAware:   0%|          | 0/500 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/eval_BEAM20_EA_lambda1_test/details_beam20_EA_20250827_000820.csv | /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/eval_BEAM20_EA_lambda1_test/top_beams_20250827_000820.csv\n"
          ]
        }
      ],
      "source": [
        "# ==== PASS 1: BEAM-20 GENERATION with Entity-Aware CopyNext (N=100) ====\n",
        "import pandas as pd, csv, os\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import LogitsProcessor, LogitsProcessorList\n",
        "\n",
        "# Lightweight NER for entity-aware & metrics\n",
        "import spacy\n",
        "try:\n",
        "    nlp\n",
        "except NameError:\n",
        "    try:\n",
        "        nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "    except OSError:\n",
        "        import sys, subprocess\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "        nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "\n",
        "KEEP_LABELS = {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"DATE\",\"TIME\",\"CARDINAL\",\"MONEY\",\"PERCENT\",\"QUANTITY\",\"ORDINAL\"}\n",
        "def _ents_norm(text):\n",
        "    doc = nlp(str(text))\n",
        "    return {(\" \".join(ent.text.strip().split()), ent.label_) for ent in doc.ents if ent.label_ in KEEP_LABELS}\n",
        "\n",
        "# Entity-aware, batched CopyNext (token-offset aware)\n",
        "class _SourceIndexEA:\n",
        "    def __init__(self, text, input_ids, tokenizer, max_span=6):\n",
        "        # Build suffix->next map and mark whether the next token is inside an entity span\n",
        "        enc = tokenizer(text, max_length=MAX_SRC_LEN, truncation=True, return_offsets_mapping=True)\n",
        "        ids = enc[\"input_ids\"]\n",
        "        offs = enc[\"offset_mapping\"]\n",
        "        # Build entity char spans\n",
        "        ents = nlp(text).ents\n",
        "        ent_spans = [(e.start_char, e.end_char) for e in ents if e.label_ in KEEP_LABELS]\n",
        "        def _in_ent(off):\n",
        "            a,b = off\n",
        "            if a is None or a < 0: return False\n",
        "            for (s,e) in ent_spans:\n",
        "                if a >= s and b <= e:\n",
        "                    return True\n",
        "            return False\n",
        "        n = len(ids)\n",
        "        self.maps = [{} for _ in range(max_span)]  # [k-1][tuple(ids[i:i+k])] -> (next_id, is_entity_next)\n",
        "        for k in range(1, max_span+1):\n",
        "            mp = self.maps[k-1]\n",
        "            for i in range(0, n-k):\n",
        "                key = tuple(ids[i:i+k])\n",
        "                nxt = ids[i+k]\n",
        "                is_ent = _in_ent(offs[i+k]) if i+k < len(offs) else False\n",
        "                if key not in mp: mp[key] = (nxt, is_ent)\n",
        "    def next_after(self, suffix):\n",
        "        k = len(suffix)\n",
        "        if k == 0: return None\n",
        "        return self.maps[k-1].get(tuple(suffix))\n",
        "\n",
        "class EntityAwareSpanProcessor(LogitsProcessor):\n",
        "    def __init__(self, src_texts, src_input_ids, tokenizer, num_beams, gamma=0.5, gamma_entity=2.0, max_span=6):\n",
        "        self.num_beams = int(num_beams)\n",
        "        self.gamma = float(gamma)\n",
        "        self.gamma_entity = float(gamma_entity)\n",
        "        self.idxs = []\n",
        "        for txt, ids in zip(src_texts, src_input_ids):\n",
        "            self.idxs.append(_SourceIndexEA(txt, ids, tokenizer, max_span=max_span))\n",
        "    def __call__(self, input_ids, scores):\n",
        "        Bx = input_ids.size(0)\n",
        "        for b in range(Bx):\n",
        "            sample_idx = b // self.num_beams\n",
        "            seq = input_ids[b].tolist()\n",
        "            idx = self.idxs[sample_idx]\n",
        "            for k in range(min(6, len(seq)), 0, -1):\n",
        "                res = idx.next_after(seq[-k:])\n",
        "                if res is not None:\n",
        "                    nxt, is_ent = res\n",
        "                    scores[b, nxt] = scores[b, nxt] + (self.gamma_entity if is_ent else self.gamma)\n",
        "                    break\n",
        "        return scores\n",
        "\n",
        "# Load first 100 rows of test\n",
        "df_test_full = pd.read_csv(TEST_PATH)\n",
        "assert {\"article\",\"highlights\"} <= set(df_test_full.columns)\n",
        "df_test = df_test_full.head(1000).copy()\n",
        "\n",
        "# Generate beams (batch=2 to fit 20 beams comfortably)\n",
        "BATCH = 2\n",
        "details_rows, top_rows = [], []\n",
        "\n",
        "for start in tqdm(range(0, len(df_test), BATCH), desc=\"Beam-20 EntityAware\"):\n",
        "    sub = df_test.iloc[start:start+BATCH]\n",
        "    arts = sub[\"article\"].astype(str).tolist()\n",
        "    refs = sub[\"highlights\"].astype(str).tolist()\n",
        "    enc = tok(arts, return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_SRC_LEN)\n",
        "    enc = {k:v.to(model.device) for k,v in enc.items()}\n",
        "\n",
        "    proc = EntityAwareSpanProcessor(\n",
        "        src_texts=arts,\n",
        "        src_input_ids=[tok(a, max_length=MAX_SRC_LEN, truncation=True)[\"input_ids\"] for a in arts],\n",
        "        tokenizer=tok,\n",
        "        num_beams=DECODE_ARGS[\"num_beams\"],\n",
        "        gamma=EA_ARGS[\"gamma\"], gamma_entity=EA_ARGS[\"gamma_entity\"], max_span=EA_ARGS[\"max_span\"]\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**enc, **DECODE_ARGS, logits_processor=LogitsProcessorList([proc]))  # (bs*K, T)\n",
        "\n",
        "    bs = len(arts); K = DECODE_ARGS[\"num_return_sequences\"]\n",
        "    seqs  = out.sequences.view(bs, K, -1)\n",
        "    scores= out.sequences_scores.view(bs, K)\n",
        "\n",
        "    for bi in range(bs):\n",
        "        art, ref = arts[bi], refs[bi]\n",
        "        # k=0 top beam\n",
        "        top_rows.append({\n",
        "            \"row_id\": start+bi, \"article\": art, \"reference\": ref,\n",
        "            \"top_beam_summary\": tok.decode(seqs[bi,0], skip_special_tokens=True)\n",
        "        })\n",
        "        # all 20 candidates\n",
        "        for k in range(K):\n",
        "            ids = seqs[bi,k]\n",
        "            hyp = tok.decode(ids, skip_special_tokens=True)\n",
        "            details_rows.append({\n",
        "                \"row_id\": start+bi, \"candidate_k\": k,\n",
        "                \"article\": art, \"reference\": ref, \"summary\": hyp,\n",
        "                \"base_score\": float(scores[bi,k].item()),\n",
        "                \"len\": max(int(ids.size(0)-1), 1)\n",
        "            })\n",
        "\n",
        "# Save artifacts\n",
        "import csv\n",
        "def save_csv(df, path):\n",
        "    tmp = path + \".tmp\"; df.to_csv(tmp, index=False, quoting=csv.QUOTE_ALL, lineterminator=\"\\n\", encoding=\"utf-8\"); os.replace(tmp, path)\n",
        "\n",
        "STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "DETAILS = os.path.join(OUT_DIR, f\"details_beam20_EA_{STAMP}.csv\")\n",
        "TOPS    = os.path.join(OUT_DIR, f\"top_beams_{STAMP}.csv\")\n",
        "save_csv(pd.DataFrame(details_rows), DETAILS)\n",
        "save_csv(pd.DataFrame(top_rows),    TOPS)\n",
        "print(\"Saved:\", DETAILS, \"|\", TOPS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247,
          "referenced_widgets": [
            "dda1ff1b832944cbb7eaff601c6f7f33"
          ]
        },
        "id": "iLH9SLaAqBj3",
        "outputId": "bd9a997b-4df7-4079-9a35-cd74326d0c56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "details_cn2 cols: ['row_id', 'candidate_k', 'article', 'reference', 'summary', 'base_score', 'len', 'highlights'] ...\n",
            "top_df cols: ['row_id', 'top_beam_summary']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dda1ff1b832944cbb7eaff601c6f7f33",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "features:   0%|          | 0/20000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE  TOP: {'r1_f': 0.31, 'r2_f': 0.1135, 'rl_f': 0.2125}\n",
            "ROUGE λ=1: {'r1_f': 0.3046, 'r2_f': 0.1122, 'rl_f': 0.2078}\n",
            "Δ ROUGE  : {'r1': -0.0054, 'r2': -0.0012, 'rl': -0.0047}\n",
            "\n",
            "ENT  TOP : {'TP': 1175, 'FP': 4326, 'FN': 2333, 'entP': 0.2136, 'entR': 0.3349, 'entF1': 0.2609}\n",
            "ENT  λ=1 : {'TP': 1172, 'FP': 4495, 'FN': 2336, 'entP': 0.2068, 'entR': 0.3341, 'entF1': 0.2555}\n",
            "Δ ENT    : {'entP': -0.0068, 'entR': -0.0009, 'entF1': -0.0054}\n",
            "Saved: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/eval_BEAM20_EA_lambda1_test/best_lambda1_20250827_000820.csv | /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/eval_BEAM20_EA_lambda1_test/config_lambda1_20250827_000820.json\n"
          ]
        }
      ],
      "source": [
        "# ==== PASS 2: λ-RERANK (λ=1) + METRICS on the 100 rows ====\n",
        "import math, pandas as pd, numpy as np\n",
        "from rouge_score import rouge_scorer\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "details_cn2 = pd.read_csv(DETAILS)\n",
        "top_df      = pd.read_csv(TOPS)\n",
        "# ---- normalize columns & avoid merge collisions ----\n",
        "def _ensure_highlights(df, name):\n",
        "    if \"highlights\" not in df.columns:\n",
        "        if \"reference\" in df.columns:\n",
        "            df[\"highlights\"] = df[\"reference\"].astype(str)\n",
        "        else:\n",
        "            raise KeyError(f\"{name} has neither 'highlights' nor 'reference'\")\n",
        "    return df\n",
        "\n",
        "details_cn2 = _ensure_highlights(details_cn2, \"details_cn2\")\n",
        "top_df      = _ensure_highlights(top_df,      \"top_df\")\n",
        "\n",
        "# we only need TOP text from top_df; drop its highlights to avoid _x/_y suffixing\n",
        "top_df = top_df[[\"row_id\", \"top_beam_summary\"]].copy()\n",
        "\n",
        "# (optional) sanity\n",
        "print(\"details_cn2 cols:\", list(details_cn2.columns)[:12], \"...\")\n",
        "print(\"top_df cols:\", list(top_df.columns))\n",
        "\n",
        "\n",
        "# Features: supported/unsupported entity counts vs SOURCE, LCS contiguity, length reward\n",
        "def compute_support_unsupp(src, hyp):\n",
        "    src_ents = _ents_norm(src); hyp_ents = _ents_norm(hyp)\n",
        "    overlap = hyp_ents & src_ents\n",
        "    return len(overlap), len(hyp_ents - src_ents)\n",
        "\n",
        "def lcs_span_stats(src_text, hyp_text):\n",
        "    s = str(src_text).split(); h = str(hyp_text).split()\n",
        "    n,m = len(s), len(h)\n",
        "    dp = [[0]*(m+1) for _ in range(n+1)]\n",
        "    for i in range(n-1, -1, -1):\n",
        "        si = s[i]; row = dp[i]; row1 = dp[i+1]\n",
        "        for j in range(m-1, -1, -1):\n",
        "            row[j] = row1[j+1]+1 if si==h[j] else max(row[j+1], row1[j])\n",
        "    i=j=0; blocks=[]; cur=0\n",
        "    while i<n and j<m:\n",
        "        if s[i]==h[j]:\n",
        "            cur+=1; i+=1; j+=1\n",
        "        else:\n",
        "            if cur>=2: blocks.append(cur); cur=0\n",
        "            if dp[i+1][j] >= dp[i][j+1]: i+=1\n",
        "            else: j+=1\n",
        "    if cur>=2: blocks.append(cur)\n",
        "    contiguity = sum(b-1 for b in blocks)\n",
        "    avg_len = (sum(blocks)/len(blocks)) if blocks else 1.0\n",
        "    return contiguity, avg_len\n",
        "\n",
        "supp, unsupp, contigs, avlens = [], [], [], []\n",
        "for _, r in tqdm(list(details_cn2.iterrows()), total=len(details_cn2), desc=\"features\"):\n",
        "    s,u = compute_support_unsupp(r[\"article\"], r[\"summary\"])\n",
        "    c,a = lcs_span_stats(r[\"article\"], r[\"summary\"])\n",
        "    supp.append(s); unsupp.append(u); contigs.append(c); avlens.append(a)\n",
        "\n",
        "details_cn2[\"supported\"]   = supp\n",
        "details_cn2[\"unsupported\"] = unsupp\n",
        "details_cn2[\"contiguity_lcs\"] = contigs\n",
        "details_cn2[\"avg_span_len_lcs\"] = avlens\n",
        "\n",
        "# Length reward\n",
        "details_cn2[\"len\"] = details_cn2[\"len\"].clip(lower=1)\n",
        "details_cn2[\"len_reward\"] = np.exp(-((details_cn2[\"len\"]-TARGET_LEN)**2)/(2*(LEN_SIGMA**2)))\n",
        "\n",
        "# Entity precision / recall; λ = 1 → precision only\n",
        "ref_ent_counts = details_cn2[\"highlights\"].apply(lambda x: len(_ents_norm(x))) + 1e-6\n",
        "details_cn2[\"ent_prec\"]  = details_cn2[\"supported\"] / (details_cn2[\"supported\"] + details_cn2[\"unsupported\"] + 1e-6)\n",
        "details_cn2[\"ent_recall\"]= details_cn2[\"supported\"] / ref_ent_counts\n",
        "details_cn2[\"entity_score\"] = LAMBDA*details_cn2[\"ent_prec\"] + (1-LAMBDA)*details_cn2[\"ent_recall\"]\n",
        "\n",
        "# Final score\n",
        "details_cn2[\"new_score\"] = (W[\"entity\"]    * details_cn2[\"entity_score\"]) \\\n",
        "                         + (W[\"base_norm\"] * (details_cn2[\"base_score\"] / (details_cn2[\"len\"]**0.3))) \\\n",
        "                         + (W[\"contig_lcs\"]* details_cn2[\"contiguity_lcs\"]) \\\n",
        "                         + (W[\"len_reward\"]* details_cn2[\"len_reward\"])\n",
        "\n",
        "# Select best per row\n",
        "best = (details_cn2.sort_values([\"row_id\",\"new_score\"], ascending=[True, False])\n",
        "                  .groupby(\"row_id\", as_index=False).head(1))\n",
        "\n",
        "# Metrics vs TOP\n",
        "ev = best.merge(top_df, on=\"row_id\", how=\"left\")\n",
        "refs  = ev[\"highlights\"].astype(str).tolist()\n",
        "tops  = ev[\"top_beam_summary\"].astype(str).tolist()\n",
        "bests = ev[\"summary\"].astype(str).tolist()\n",
        "\n",
        "sc = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\"], use_stemmer=True)\n",
        "def _mean_rouge(R,H):\n",
        "    r1=r2=rl=0.0\n",
        "    for r,h in zip(R,H):\n",
        "        s=sc.score(r,h); r1+=s[\"rouge1\"].fmeasure; r2+=s[\"rouge2\"].fmeasure; rl+=s[\"rougeLsum\"].fmeasure\n",
        "    n=len(R); return r1/n, r2/n, rl/n\n",
        "\n",
        "def entPRF(R,H):\n",
        "    TP=FP=FN=0\n",
        "    for r,h in zip(R,H):\n",
        "        R_=_ents_norm(r); H_=_ents_norm(h)\n",
        "        TP+=len(H_&R_); FP+=len(H_-R_); FN+=len(R_-H_)\n",
        "    P=TP/(TP+FP) if (TP+FP) else 0.0\n",
        "    Rr=TP/(TP+FN) if (TP+FN) else 0.0\n",
        "    F=2*P*Rr/(P+Rr) if (P+Rr) else 0.0\n",
        "    return {\"TP\":TP,\"FP\":FP,\"FN\":FN,\"entP\":P,\"entR\":Rr,\"entF1\":F}\n",
        "\n",
        "r1t,r2t,rlt = _mean_rouge(refs, tops)\n",
        "r1b,r2b,rlb = _mean_rouge(refs, bests)\n",
        "et, ec = entPRF(refs, tops), entPRF(refs, bests)\n",
        "fmt=lambda d:{k:(round(v,4) if isinstance(v,float) else v) for k,v in d.items()}\n",
        "\n",
        "print(\"ROUGE  TOP:\", {\"r1_f\":round(r1t,4),\"r2_f\":round(r2t,4),\"rl_f\":round(rlt,4)})\n",
        "print(\"ROUGE λ=1:\", {\"r1_f\":round(r1b,4),\"r2_f\":round(r2b,4),\"rl_f\":round(rlb,4)})\n",
        "print(\"Δ ROUGE  :\", {\"r1\":round(r1b-r1t,4),\"r2\":round(r2b-r2t,4),\"rl\":round(rlb-rlt,4)})\n",
        "print()\n",
        "print(\"ENT  TOP :\", fmt(et))\n",
        "print(\"ENT  λ=1 :\", fmt(ec))\n",
        "print(\"Δ ENT    :\", {k: round(ec[k]-et[k],4) for k in [\"entP\",\"entR\",\"entF1\"]})\n",
        "\n",
        "# Save best + config for provenance\n",
        "BEST_OUT = os.path.join(OUT_DIR, f\"best_lambda1_{STAMP}.csv\")\n",
        "best.to_csv(BEST_OUT, index=False, quoting=csv.QUOTE_ALL)\n",
        "CFG = dict(\n",
        "    decode_args=DECODE_ARGS, max_src_len=MAX_SRC_LEN,\n",
        "    entity_aware=EA_ARGS, lambda_val=LAMBDA, weights=W,\n",
        "    target_len=TARGET_LEN, len_sigma=LEN_SIGMA, seed=SEED,\n",
        "    n_samples=100\n",
        ")\n",
        "CFG_PATH = os.path.join(OUT_DIR, f\"config_lambda1_{STAMP}.json\")\n",
        "with open(CFG_PATH, \"w\") as f: json.dump(CFG, f, indent=2)\n",
        "print(\"Saved:\", BEST_OUT, \"|\", CFG_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6kSRn4IqJz2"
      },
      "outputs": [],
      "source": [
        "import numpy as np, pandas as pd, random, math\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Load what you just produced for the 1k run\n",
        "dd       = pd.read_csv(DETAILS)   # details_beam20_EA_*.csv\n",
        "top_df   = pd.read_csv(TOPS)      # top_beams_*.csv\n",
        "best     = pd.read_csv(BEST_OUT)  # best_lambda1_*.csv from your λ=1 pass\n",
        "\n",
        "# Ensure highlights column is present\n",
        "def _ensure_highlights(df, name):\n",
        "    if \"highlights\" not in df.columns:\n",
        "        if \"reference\" in df.columns:\n",
        "            df[\"highlights\"] = df[\"reference\"].astype(str)\n",
        "        else:\n",
        "            raise KeyError(f\"{name} missing highlights/reference\")\n",
        "    return df\n",
        "dd     = _ensure_highlights(dd, \"details\")\n",
        "top_df = _ensure_highlights(top_df, \"top_df\")\n",
        "best   = _ensure_highlights(best, \"best\")\n",
        "\n",
        "# Merge eval frame\n",
        "ev = best.merge(top_df[[\"row_id\",\"top_beam_summary\"]], on=\"row_id\", how=\"left\")\n",
        "refs  = ev[\"highlights\"].astype(str).tolist()\n",
        "tops  = ev[\"top_beam_summary\"].astype(str).tolist()\n",
        "cn2   = ev[\"summary\"].astype(str).tolist()\n",
        "\n",
        "# NER for entity metrics\n",
        "import spacy\n",
        "try:\n",
        "    nlp\n",
        "except NameError:\n",
        "    try:\n",
        "        nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "    except OSError:\n",
        "        import sys, subprocess\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "        nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "\n",
        "KEEP = {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"DATE\",\"TIME\",\"CARDINAL\",\"MONEY\",\"PERCENT\",\"QUANTITY\",\"ORDINAL\"}\n",
        "CORE = {\"PERSON\",\"ORG\",\"GPE\"}\n",
        "\n",
        "def _ents_norm(text):\n",
        "    doc = nlp(str(text))\n",
        "    return {(\" \".join(ent.text.strip().split()), ent.label_) for ent in doc.ents if ent.label_ in KEEP}\n",
        "\n",
        "sc = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\"], use_stemmer=True)\n",
        "\n",
        "def rouge_avg(R,H):\n",
        "    r1=r2=rl=0.0\n",
        "    for r,h in zip(R,H):\n",
        "        s=sc.score(r,h); r1+=s[\"rouge1\"].fmeasure; r2+=s[\"rouge2\"].fmeasure; rl+=s[\"rougeLsum\"].fmeasure\n",
        "    n=len(R); return {\"r1_f\":r1/n, \"r2_f\":r2/n, \"rl_f\":rl/n}\n",
        "\n",
        "def entPRF(R,H):\n",
        "    TP=FP=FN=0\n",
        "    for r,h in zip(R,H):\n",
        "      R_=_ents_norm(r); H_=_ents_norm(h)\n",
        "      TP+=len(H_&R_); FP+=len(H_-R_); FN+=len(R_-H_)\n",
        "    P=TP/(TP+FP) if (TP+FP) else 0.0\n",
        "    Rr=TP/(TP+FN) if (TP+FN) else 0.0\n",
        "    F=2*P*Rr/(P+Rr) if (P+Rr) else 0.0\n",
        "    return P,Rr,F\n",
        "\n",
        "# Bootstrap 95% CIs for TOP and CN2 (and deltas)\n",
        "def bootstrap_metrics(R, A, B, Bn=1000, seed=0):\n",
        "    rng = random.Random(seed)\n",
        "    n=len(R)\n",
        "    a_rouge=[]; b_rouge=[]; a_f1=[]; b_f1=[]\n",
        "    for _ in range(Bn):\n",
        "        idx=[rng.randrange(n) for _ in range(n)]\n",
        "        Rb=[R[i] for i in idx]; Ab=[A[i] for i in idx]; Bb=[B[i] for i in idx]\n",
        "        Ra=rouge_avg(Rb,Ab); Rb_=rouge_avg(Rb,Bb)\n",
        "        a_rouge.append([Ra[\"r1_f\"],Ra[\"r2_f\"],Ra[\"rl_f\"]])\n",
        "        b_rouge.append([Rb_[\"r1_f\"],Rb_[\"r2_f\"],Rb_[\"rl_f\"]])\n",
        "        a_f1.append([entPRF(Rb,Ab)[2]])\n",
        "        b_f1.append([entPRF(Rb,Bb)[2]])\n",
        "    a_rouge=np.array(a_rouge); b_rouge=np.array(b_rouge)\n",
        "    a_f1=np.array(a_f1); b_f1=np.array(b_f1)\n",
        "    delta_rouge=b_rouge - a_rouge; delta_f1=b_f1 - a_f1\n",
        "    def ci(arr): lo,hi=np.percentile(arr,[2.5,97.5]); return (float(lo), float(hi))\n",
        "    return {\n",
        "        \"TOP\": {\"R1\":ci(a_rouge[:,0]),\"R2\":ci(a_rouge[:,1]),\"RL\":ci(a_rouge[:,2]),\"EntF1\":ci(a_f1[:,0])},\n",
        "        \"CN2\": {\"R1\":ci(b_rouge[:,0]),\"R2\":ci(b_rouge[:,1]),\"RL\":ci(b_rouge[:,2]),\"EntF1\":ci(b_f1[:,0])},\n",
        "        \"Δ\":   {\"R1\":ci(delta_rouge[:,0]),\"R2\":ci(delta_rouge[:,1]),\"RL\":ci(delta_rouge[:,2]),\"EntF1\":ci(delta_f1[:,0])},\n",
        "    }\n",
        "\n",
        "R_top = rouge_avg(refs, tops)\n",
        "R_cn2 = rouge_avg(refs, cn2)\n",
        "_,_,F_top = entPRF(refs, tops)\n",
        "_,_,F_cn2 = entPRF(refs, cn2)\n",
        "cis = bootstrap_metrics(refs, tops, cn2, Bn=800)  # 800 is enough for a quick CI\n",
        "\n",
        "def r4(x): return {k: round(v,4) for k,v in x.items()}\n",
        "print(\"TOP:\", r4(R_top), \"EntF1:\", round(F_top,4))\n",
        "print(\"CN2:\", r4(R_cn2), \"EntF1:\", round(F_cn2,4))\n",
        "print(\"ΔROUGE:\", {k: round(R_cn2[k]-R_top[k],4) for k in R_top}, \"ΔEntF1:\", round(F_cn2-F_top,4))\n",
        "print(\"\\n95% CIs:\", cis)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSs_xpThvFj4"
      },
      "outputs": [],
      "source": [
        "# Unsupported CORE entity rate (UCER) and count per summary\n",
        "def core_unsupported_counts(R, H, S):  # S = source/article\n",
        "    rates=[]; counts=[]\n",
        "    for r,h,s in zip(R,H,S):\n",
        "        src=_ents_norm(s); hyp=_ents_norm(h)\n",
        "        uns=[(t,l) for (t,l) in (hyp - src) if l in CORE]\n",
        "        counts.append(len(uns)); rates.append(1 if len(uns)>0 else 0)\n",
        "    return np.mean(rates), np.mean(counts)\n",
        "\n",
        "articles = ev.merge(dd[[\"row_id\",\"article\"]].drop_duplicates(), on=\"row_id\", how=\"left\")[\"article\"].tolist()\n",
        "ucer_top,  ucps_top  = core_unsupported_counts(refs, tops, articles)\n",
        "ucer_cn2,  ucps_cn2  = core_unsupported_counts(refs, cn2,  articles)\n",
        "print({\"UCER_TOP\": round(ucer_top,3), \"UCER_CN2\": round(ucer_cn2,3),\n",
        "       \"Unsupported/core per sum — TOP\": round(ucps_top,3), \"CN2\": round(ucps_cn2,3),\n",
        "       \"ΔUCER\": round(ucer_cn2-ucer_top,3)})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8XGTYNgAD7J"
      },
      "source": [
        "# λ TEST(500) with med and long pools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQESt1jRAz6N"
      },
      "outputs": [],
      "source": [
        "# ==== PATHS (edit these) ====\n",
        "CKPT_DIR       = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000\"\n",
        "VALIDATION_PATH= \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "TEST_PATH      = \"/content/drive/MyDrive/cleaned_outputs/naa/test_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "OUT_ROOT       = f\"{CKPT_DIR}/eval_HYBRID\"\n",
        "\n",
        "N_VAL = 500\n",
        "SEED  = 0\n",
        "\n",
        "# ==== LIBS & SEED ====\n",
        "import os, json, csv, time, math, random, hashlib, re\n",
        "import numpy as np, pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# ==== MODEL ====\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "tok   = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device).eval()\n",
        "\n",
        "# Modern AMP context (optional)\n",
        "from contextlib import contextmanager\n",
        "@contextmanager\n",
        "def autocast_if_cuda():\n",
        "    if torch.cuda.is_available():\n",
        "        with torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
        "            yield\n",
        "    else:\n",
        "        yield\n",
        "\n",
        "# ==== DECODE POOLS ====\n",
        "# Pool A: CN2-long (your strong validation regime)\n",
        "CN2_LONG = dict(\n",
        "    num_beams=10, num_return_sequences=10,\n",
        "    num_beam_groups=5, diversity_penalty=0.3,\n",
        "    max_new_tokens=100, min_new_tokens=55,\n",
        "    no_repeat_ngram_size=3, length_penalty=2.0,\n",
        "    early_stopping=True, return_dict_in_generate=True, output_scores=True\n",
        ")\n",
        "\n",
        "# Pool B: CN-medium (shorter than CN2-long; better match for test refs without being too short)\n",
        "CN_MED = dict(\n",
        "    num_beams=10, num_return_sequences=10,\n",
        "    num_beam_groups=5, diversity_penalty=0.3,\n",
        "    max_new_tokens=70, min_new_tokens=35,\n",
        "    no_repeat_ngram_size=3, length_penalty=1.6,\n",
        "    early_stopping=True, return_dict_in_generate=True, output_scores=True\n",
        ")\n",
        "\n",
        "# Encoder cap (pick one and keep it fixed after validation)\n",
        "MAX_SRC_LEN = 400\n",
        "\n",
        "\n",
        "EA_ARGS = dict(gamma=0.4, gamma_entity=1.2, max_span=6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKlvHauRA4rX"
      },
      "outputs": [],
      "source": [
        "# ==== SAVE/LOAD HELPERS ====\n",
        "def save_csv(df, path):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    tmp = path + \".tmp\"\n",
        "    df.to_csv(tmp, index=False, quoting=csv.QUOTE_ALL, lineterminator=\"\\n\", encoding=\"utf-8\")\n",
        "    os.replace(tmp, path)\n",
        "\n",
        "def save_json(obj, path):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    with open(path,\"w\") as f: json.dump(obj, f, indent=2)\n",
        "\n",
        "def norm_gold(df):\n",
        "    if \"highlights\" not in df.columns:\n",
        "        if \"reference\" in df.columns:\n",
        "            df[\"highlights\"] = df[\"reference\"].astype(str)\n",
        "        else:\n",
        "            raise KeyError(\"No 'highlights' nor 'reference' column in dataframe.\")\n",
        "    return df\n",
        "\n",
        "# ==== NER (spaCy) ====\n",
        "import spacy\n",
        "try:\n",
        "    nlp\n",
        "except NameError:\n",
        "    try:\n",
        "        nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "    except OSError:\n",
        "        import sys, subprocess\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "        nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "SPACY_VER = nlp.meta.get(\"version\", \"unknown\")\n",
        "KEEP_LABELS = {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"DATE\",\"TIME\",\"CARDINAL\",\"MONEY\",\"PERCENT\",\"QUANTITY\",\"ORDINAL\"}\n",
        "CORE = {\"PERSON\",\"ORG\",\"GPE\"}\n",
        "\n",
        "# batched NER cache\n",
        "_ENTS_CACHE = {}\n",
        "def ents_cache_build(texts, batch_size=128):\n",
        "    uniq = list(dict.fromkeys(map(str, texts)))\n",
        "    for doc in nlp.pipe(uniq, batch_size=batch_size):\n",
        "        _ENTS_CACHE[doc.text] = {(\" \".join(e.text.strip().split()), e.label_) for e in doc.ents if e.label_ in KEEP_LABELS}\n",
        "\n",
        "def _ents_norm(text):\n",
        "    t = str(text)\n",
        "    if t in _ENTS_CACHE:\n",
        "        return _ENTS_CACHE[t]\n",
        "    doc = nlp(t)\n",
        "    return {(\" \".join(e.text.strip().split()), e.label_) for e in doc.ents if e.label_ in KEEP_LABELS}\n",
        "\n",
        "# ==== LCS features ====\n",
        "def lcs_span_stats(src_text, hyp_text):\n",
        "    s = str(src_text).split(); h = str(hyp_text).split()\n",
        "    n,m = len(s), len(h)\n",
        "    dp = [[0]*(m+1) for _ in range(n+1)]\n",
        "    for i in range(n-1,-1,-1):\n",
        "        si=s[i]; row=dp[i]; row1=dp[i+1]\n",
        "        for j in range(m-1,-1,-1):\n",
        "            row[j] = row1[j+1]+1 if si==h[j] else max(row[j+1], row1[j])\n",
        "    i=j=0; blocks=[]; cur=0\n",
        "    while i<n and j<m:\n",
        "        if s[i]==h[j]: cur+=1; i+=1; j+=1\n",
        "        else:\n",
        "            if cur>=2: blocks.append(cur); cur=0\n",
        "            if dp[i+1][j] >= dp[i][j+1]: i+=1\n",
        "            else: j+=1\n",
        "    if cur>=2: blocks.append(cur)\n",
        "    contiguity=sum(b-1 for b in blocks)\n",
        "    avg_len=(sum(blocks)/len(blocks)) if blocks else 1.0\n",
        "    return contiguity, avg_len\n",
        "\n",
        "# ==== THC strict ====\n",
        "def strict_entity_score(src, hyp):\n",
        "    S=_ents_norm(src); H=_ents_norm(hyp)\n",
        "    overlap = H & S; unsupported = H - S\n",
        "    hard_fail = any(lbl in CORE for _,lbl in unsupported)\n",
        "    score = sum(3 if lbl in CORE else 1 for _,lbl in overlap)\n",
        "    return score, hard_fail\n",
        "\n",
        "# ==== Lead & length helpers ====\n",
        "def sent_split(txt): return re.split(r'(?<=[.!?])\\s+', str(txt).strip())\n",
        "def lead3(text):\n",
        "    s=[s for s in sent_split(text) if s]\n",
        "    return \" \".join(s[:3])\n",
        "\n",
        "def toklen(s):\n",
        "    return len(tok(str(s))[\"input_ids\"])\n",
        "\n",
        "def adaptive_len_target(src_text):\n",
        "    L = toklen(lead3(src_text))\n",
        "    return int(np.clip(round(0.9*L), 32, 60))\n",
        "\n",
        "# ==== ROUGE ====\n",
        "from rouge_score import rouge_scorer\n",
        "_SC = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\"], use_stemmer=True)\n",
        "\n",
        "def rougeLsum(a,b): return _SC.score(str(a), str(b))[\"rougeLsum\"].fmeasure\n",
        "def rouge_mean(refs, hyps):\n",
        "    r1=r2=rl=0.0\n",
        "    for r,h in zip(refs, hyps):\n",
        "        s=_SC.score(r,h); r1+=s[\"rouge1\"].fmeasure; r2+=s[\"rouge2\"].fmeasure; rl+=s[\"rougeLsum\"].fmeasure\n",
        "    n=len(refs); return {\"r1_f\":r1/n, \"r2_f\":r2/n, \"rl_f\":rl/n}\n",
        "\n",
        "# ==== Entity PRF & UCER ====\n",
        "def entPRF(refs, hyps):\n",
        "    TP=FP=FN=0\n",
        "    for r,h in zip(refs, hyps):\n",
        "        R=_ents_norm(r); H=_ents_norm(h)\n",
        "        TP+=len(H&R); FP+=len(H-R); FN+=len(R-H)\n",
        "    P=TP/(TP+FP) if (TP+FP) else 0.0\n",
        "    Rr=TP/(TP+FN) if (TP+FN) else 0.0\n",
        "    F=2*P*Rr/(P+Rr) if (P+Rr) else 0.0\n",
        "    return {\"TP\":TP,\"FP\":FP,\"FN\":FN,\"entP\":P,\"entR\":Rr,\"entF1\":F}\n",
        "\n",
        "def UCER(articles, hyps):\n",
        "    rates=[]; counts=[]\n",
        "    for s,h in zip(articles, hyps):\n",
        "        S=_ents_norm(s); H=_ents_norm(h)\n",
        "        uns=[(t,l) for (t,l) in (H-S) if l in CORE]\n",
        "        rates.append(1 if uns else 0); counts.append(len(uns))\n",
        "    return float(np.mean(rates)), float(np.mean(counts))\n",
        "\n",
        "# ==== MBR centroid ====\n",
        "def mbr_centroid(cands):\n",
        "    n=len(cands)\n",
        "    if n<=1: return [0.0]*n\n",
        "    sims=np.zeros((n,n), dtype=float)\n",
        "    for i in range(n):\n",
        "        for j in range(i+1, n):\n",
        "            s=rougeLsum(cands[i], cands[j]); sims[i,j]=sims[j,i]=s\n",
        "    return sims.mean(axis=1).tolist()\n",
        "\n",
        "def minmax_norm(vs):\n",
        "    vs=np.array(vs,float); lo,hi=vs.min(),vs.max()\n",
        "    if hi-lo<1e-8: return np.zeros_like(vs).tolist()\n",
        "    return ((vs-lo)/(hi-lo)).tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTqx1mF5A7D6"
      },
      "outputs": [],
      "source": [
        "from transformers import LogitsProcessor, LogitsProcessorList\n",
        "\n",
        "# Build suffix->next map and whether 'next' lies inside an entity span\n",
        "def build_source_index_entity_aware(text, tokenizer, max_len=MAX_SRC_LEN, max_span=6):\n",
        "    enc = tokenizer(text, max_length=max_len, truncation=True, return_offsets_mapping=True)\n",
        "    ids = enc[\"input_ids\"]; offs = enc[\"offset_mapping\"]\n",
        "    # entity spans (char)\n",
        "    ents = nlp(text).ents\n",
        "    ent_spans = [(e.start_char, e.end_char) for e in ents if e.label_ in KEEP_LABELS]\n",
        "    def in_ent(off):\n",
        "        a,b = off\n",
        "        if a is None or a < 0: return False\n",
        "        for s,e in ent_spans:\n",
        "            if a>=s and b<=e: return True\n",
        "        return False\n",
        "    maps=[{} for _ in range(max_span)]  # k-1 -> {tuple(ids[i:i+k]): (next_id, is_entity_next)}\n",
        "    n=len(ids)\n",
        "    for k in range(1, max_span+1):\n",
        "        mp=maps[k-1]\n",
        "        for i in range(0, n-k):\n",
        "            key=tuple(ids[i:i+k]); nxt=ids[i+k]\n",
        "            is_ent = in_ent(offs[i+k]) if i+k<len(offs) else False\n",
        "            if key not in mp: mp[key]=(nxt, is_ent)\n",
        "    return maps\n",
        "\n",
        "class EntityAwareSpanProcessor(LogitsProcessor):\n",
        "    def __init__(self, src_texts, tokenizer, num_beams, gamma=0.4, gamma_entity=1.2, max_span=6):\n",
        "        self.num_beams = int(num_beams)\n",
        "        self.gamma = float(gamma); self.gamma_entity=float(gamma_entity)\n",
        "        self.max_span=int(max_span)\n",
        "        self.maps = [build_source_index_entity_aware(t, tokenizer, MAX_SRC_LEN, max_span) for t in src_texts]\n",
        "    def __call__(self, input_ids, scores):\n",
        "        Bx=input_ids.size(0)\n",
        "        for b in range(Bx):\n",
        "            sample_idx = b // self.num_beams\n",
        "            seq = input_ids[b].tolist()\n",
        "            maps = self.maps[sample_idx]\n",
        "            for k in range(min(self.max_span, len(seq)), 0, -1):\n",
        "                mp = maps[k-1]; key = tuple(seq[-k:])\n",
        "                if key in mp:\n",
        "                    nxt, is_ent = mp[key]\n",
        "                    scores[b, nxt] = scores[b, nxt] + (self.gamma_entity if is_ent else self.gamma)\n",
        "                    break\n",
        "        return scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NahebLAlA9Wm"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def generate_pool(articles, highlights, decode_args, pool_name, batch_size=4):\n",
        "    details_rows = []\n",
        "    for start in tqdm(range(0, len(articles), batch_size), desc=f\"Generate {pool_name}\"):\n",
        "        sub_arts = articles[start:start+batch_size]\n",
        "        sub_refs = highlights[start:start+batch_size]\n",
        "        enc = tok(sub_arts, return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_SRC_LEN)\n",
        "        enc = {k:v.to(device) for k,v in enc.items()}\n",
        "\n",
        "        proc = EntityAwareSpanProcessor(sub_arts, tok, num_beams=decode_args[\"num_beams\"],\n",
        "                                        gamma=EA_ARGS[\"gamma\"], gamma_entity=EA_ARGS[\"gamma_entity\"],\n",
        "                                        max_span=EA_ARGS[\"max_span\"])\n",
        "        with autocast_if_cuda():\n",
        "            out = model.generate(**enc, **decode_args, logits_processor=LogitsProcessorList([proc]))\n",
        "\n",
        "        K = decode_args[\"num_return_sequences\"]; bs=len(sub_arts)\n",
        "        seqs  = out.sequences.view(bs, K, -1)\n",
        "        scores= out.sequences_scores.view(bs, K)\n",
        "\n",
        "        for bi in range(bs):\n",
        "            art = sub_arts[bi]; ref = sub_refs[bi]\n",
        "            for k in range(K):\n",
        "                ids = seqs[bi,k]\n",
        "                hyp = tok.decode(ids, skip_special_tokens=True)\n",
        "                details_rows.append({\n",
        "                    \"row_id\": start+bi, \"pool\": pool_name, \"candidate_k\": k,\n",
        "                    \"article\": art, \"highlights\": ref, \"summary\": hyp,\n",
        "                    \"base_score\": float(scores[bi,k].item()),\n",
        "                    \"len\": max(int(ids.size(0)-1), 1)\n",
        "                })\n",
        "    return pd.DataFrame(details_rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383,
          "referenced_widgets": [
            "fded3f3bf0e8457da9cca290f3cdc65a",
            "b4461c5c68454dc19fdb1d518bd7f2e8",
            "3669afc61fae487e924fbec34193e540"
          ]
        },
        "id": "Q4BU_tr-A_r4",
        "outputId": "aa055727-33cc-4302-b05c-5d8e19d762ed"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fded3f3bf0e8457da9cca290f3cdc65a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generate CN2_LONG:   0%|          | 0/125 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4461c5c68454dc19fdb1d518bd7f2e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generate CN_MED:   0%|          | 0/125 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3669afc61fae487e924fbec34193e540",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "THC Stage-A:   0%|          | 0/500 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chosen Stage-B weights (VAL): {'THC': 0.5, 'LeadSim': 0.3, 'MBR': 0.2}\n",
            "\n",
            "== VALIDATION (N=500) ==\n",
            "ROUGE  TOP: {'r1_f': 0.3046, 'r2_f': 0.1161, 'rl_f': 0.2098}\n",
            "ROUGE HYBR: {'r1_f': 0.3101, 'r2_f': 0.1194, 'rl_f': 0.2148} | Δ: {'r1_f': 0.0055, 'r2_f': 0.0033, 'rl_f': 0.005}\n",
            "ENT   TOP : {'TP': 566, 'FP': 2013, 'FN': 1344, 'entP': 0.2195, 'entR': 0.2963, 'entF1': 0.2522}\n",
            "ENT   HYBR: {'TP': 669, 'FP': 2371, 'FN': 1241, 'entP': 0.2201, 'entR': 0.3503, 'entF1': 0.2703} | Δ: {'entP': 0.0006, 'entR': 0.0539, 'entF1': 0.0181}\n",
            "UCER  TOP : {'rate': 0.14, 'avg_unsupported_core': 0.17}\n",
            "UCER  HYBR: {'rate': 0.028, 'avg_unsupported_core': 0.036} | Δ rate: -0.112\n",
            "Oracle-K RLsum: 0.2977 | TOP RLsum: 0.2098 | gap: 0.0879\n",
            "\n",
            "Saved:\n",
            "  best_hybrid_val -> /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/eval_HYBRID/hybrid_VAL_20250827_011648/best_hybrid_val_20250827_011648.csv\n",
            "  frozen_config    -> /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/eval_HYBRID/hybrid_VAL_20250827_011648/frozen_config_20250827_011648.json\n"
          ]
        }
      ],
      "source": [
        "# ==== LOAD VALIDATION ====\n",
        "df_val = pd.read_csv(VALIDATION_PATH)\n",
        "df_val = norm_gold(df_val).head(N_VAL).copy()\n",
        "arts = df_val[\"article\"].astype(str).tolist()\n",
        "refs = df_val[\"highlights\"].astype(str).tolist()\n",
        "\n",
        "# Pre-build NER cache for speed (articles + top of refs to be safe)\n",
        "ents_cache_build(list(df_val[\"article\"].astype(str)) + list(df_val[\"highlights\"].astype(str)))\n",
        "\n",
        "# ==== GENERATE BOTH POOLS ====\n",
        "dd_long = generate_pool(arts, refs, CN2_LONG, pool_name=\"CN2_LONG\", batch_size=4)\n",
        "dd_med  = generate_pool(arts, refs, CN_MED,  pool_name=\"CN_MED\",  batch_size=4)\n",
        "\n",
        "# Combine (20 candidates per row)\n",
        "dd_all = pd.concat([dd_long, dd_med], ignore_index=True)\n",
        "\n",
        "# ==== THC FEATURES + Stage-A (strict filter + adaptive length target) ====\n",
        "rows_A = []\n",
        "for rid, grp in tqdm(list(dd_all.groupby(\"row_id\")), total=dd_all[\"row_id\"].nunique(), desc=\"THC Stage-A\"):\n",
        "    art = grp.iloc[0][\"article\"]\n",
        "    tgt = adaptive_len_target(art)\n",
        "    L   = grp[\"len\"].clip(lower=1).to_numpy()\n",
        "    # THC parts\n",
        "    ent_scores=[]; fails=[]; contigs=[]\n",
        "    for _,r in grp.iterrows():\n",
        "        s,f = strict_entity_score(r[\"article\"], r[\"summary\"])\n",
        "        c,_= lcs_span_stats(r[\"article\"], r[\"summary\"])\n",
        "        ent_scores.append(s); fails.append(f); contigs.append(c)\n",
        "    ent_scores=np.array(ent_scores); fails=np.array(fails); contigs=np.array(contigs)\n",
        "    len_reward = np.exp(-((L - tgt)**2)/(2*(20.0**2)))\n",
        "    THC = (5.0*ent_scores) + (0.4*(grp[\"base_score\"].to_numpy()/(L**0.3))) + (0.2*contigs) + (0.3*len_reward)\n",
        "\n",
        "    ok = grp.loc[~fails].copy()\n",
        "    ok[\"THC\"] = THC[~fails]\n",
        "    if ok.empty:  # fallback\n",
        "        ok = grp.copy()\n",
        "        ok[\"THC\"] = THC\n",
        "    ok = ok.sort_values(\"THC\", ascending=False).head(6)  # keep top M=6\n",
        "    rows_A.append(ok)\n",
        "stageA = pd.concat(rows_A, ignore_index=True)\n",
        "\n",
        "# ==== Stage-B (LeadSim + MBR + normalized THC) ====\n",
        "def pick_with_weights(stageA, w_thc=0.40, w_lead=0.30, w_mbr=0.30):\n",
        "    chosen=[]\n",
        "    for rid, grp in stageA.groupby(\"row_id\"):\n",
        "        art = grp.iloc[0][\"article\"]; lead = lead3(art)\n",
        "        cands = grp[\"summary\"].tolist()\n",
        "        leadsim = [rougeLsum(h, lead) for h in cands]\n",
        "        mbr     = mbr_centroid(cands)\n",
        "        thc_nm  = minmax_norm(grp[\"THC\"].tolist())\n",
        "        final = np.array(thc_nm)*w_thc + np.array(leadsim)*w_lead + np.array(mbr)*w_mbr\n",
        "        k = int(np.argmax(final)); chosen.append(grp.iloc[[k]].copy())\n",
        "    return pd.concat(chosen, ignore_index=True)\n",
        "\n",
        "# Small grid on VAL to pick Stage-B weights (sum to 1)\n",
        "grid = []\n",
        "for w_thc in (0.3, 0.4, 0.5):\n",
        "    for w_lead in (0.2, 0.3, 0.4):\n",
        "        w_mbr = round(1.0 - w_thc - w_lead, 2)\n",
        "        if w_mbr < 0: continue\n",
        "        best = pick_with_weights(stageA, w_thc, w_lead, w_mbr)\n",
        "        ev = best[[\"row_id\",\"summary\"]].merge(df_val[[\"article\",\"highlights\"]].reset_index().rename(columns={\"index\":\"row_id\"}), on=\"row_id\", how=\"left\")\n",
        "        R = ev[\"highlights\"].astype(str).tolist()\n",
        "        B = ev[\"summary\"].astype(str).tolist()\n",
        "        sc = rouge_mean(R,B)\n",
        "        grid.append((w_thc, w_lead, w_mbr, sc[\"rl_f\"], sc[\"r2_f\"], sc[\"r1_f\"]))\n",
        "grid = sorted(grid, key=lambda x: x[3], reverse=True)  # sort by RLsum\n",
        "W_THC, W_LEAD, W_MBR, _, _, _ = grid[0]\n",
        "print(\"Chosen Stage-B weights (VAL):\", {\"THC\":W_THC, \"LeadSim\":W_LEAD, \"MBR\":W_MBR})\n",
        "\n",
        "best_val = pick_with_weights(stageA, W_THC, W_LEAD, W_MBR)\n",
        "\n",
        "# ==== Build baselines & metrics on VAL ====\n",
        "# TOP = best by base_score across all 20 candidates\n",
        "top_k = (dd_all.sort_values([\"row_id\",\"base_score\"], ascending=[True, False])\n",
        "              .groupby(\"row_id\", as_index=False).head(1)\n",
        "              [[\"row_id\",\"summary\"]].rename(columns={\"summary\":\"top_beam_summary\"}))\n",
        "\n",
        "ev = best_val[[\"row_id\",\"article\",\"highlights\",\"summary\"]].rename(columns={\"summary\":\"hybrid_summary\"}) \\\n",
        "       .merge(top_k, on=\"row_id\", how=\"left\")\n",
        "\n",
        "refs  = ev[\"highlights\"].astype(str).tolist()\n",
        "hyps  = ev[\"hybrid_summary\"].astype(str).tolist()\n",
        "tops  = ev[\"top_beam_summary\"].astype(str).tolist()\n",
        "arts_ = ev[\"article\"].astype(str).tolist()\n",
        "\n",
        "R_top = rouge_mean(refs, tops)\n",
        "R_hyb = rouge_mean(refs, hyps)\n",
        "E_top = entPRF(refs, tops)\n",
        "E_hyb = entPRF(refs, hyps)\n",
        "ucer_top, ucnt_top = UCER(arts_, tops)\n",
        "ucer_hyb, ucnt_hyb = UCER(arts_, hyps)\n",
        "\n",
        "print(\"\\n== VALIDATION (N=500) ==\")\n",
        "print(\"ROUGE  TOP:\", {k:round(v,4) for k,v in R_top.items()})\n",
        "print(\"ROUGE HYBR:\", {k:round(v,4) for k,v in R_hyb.items()},\n",
        "      \"| Δ:\", {k: round(R_hyb[k]-R_top[k],4) for k in R_top})\n",
        "print(\"ENT   TOP :\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in E_top.items()})\n",
        "print(\"ENT   HYBR:\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in E_hyb.items()},\n",
        "      \"| Δ:\", {k: round(E_hyb[k]-E_top[k],4) for k in [\"entP\",\"entR\",\"entF1\"]})\n",
        "print(\"UCER  TOP :\", {\"rate\": round(ucer_top,4), \"avg_unsupported_core\": round(ucnt_top,4)})\n",
        "print(\"UCER  HYBR:\", {\"rate\": round(ucer_hyb,4), \"avg_unsupported_core\": round(ucnt_hyb,4)},\n",
        "      \"| Δ rate:\", round(ucer_hyb-ucer_top,4))\n",
        "\n",
        "# ==== ORACLE-K diagnostic (RLsum) ====\n",
        "def oracle_rlsum(ref, beams):\n",
        "    best=-1.0\n",
        "    for h in beams:\n",
        "        s=rougeLsum(ref,h)\n",
        "        if s>best: best=s\n",
        "    return best\n",
        "\n",
        "beams_map = dd_all.groupby(\"row_id\")[\"summary\"].apply(list).to_dict()\n",
        "refs_map  = df_val[\"highlights\"].astype(str).reset_index().set_index(\"index\")[\"highlights\"].to_dict()\n",
        "tops_map  = top_k.set_index(\"row_id\")[\"top_beam_summary\"].to_dict()\n",
        "oracle = []\n",
        "top_rl = []\n",
        "for rid in range(len(df_val)):\n",
        "    oracle.append(oracle_rlsum(refs_map[rid], beams_map[rid]))\n",
        "    top_rl.append(rougeLsum(refs_map[rid], tops_map[rid]))\n",
        "print(\"Oracle-K RLsum:\", round(float(np.mean(oracle)),4),\n",
        "      \"| TOP RLsum:\", round(float(np.mean(top_rl)),4),\n",
        "      \"| gap:\", round(float(np.mean(oracle)-np.mean(top_rl)),4))\n",
        "\n",
        "# ==== SAVE FROZEN CONFIG & VAL OUTPUTS ====\n",
        "STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "OUT_DIR = f\"{OUT_ROOT}/hybrid_VAL_{STAMP}\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "best_val_out = f\"{OUT_DIR}/best_hybrid_val_{STAMP}.csv\"\n",
        "save_csv(best_val, best_val_out)\n",
        "\n",
        "frozen_cfg = dict(\n",
        "    checkpoint=CKPT_DIR,\n",
        "    max_src_len=MAX_SRC_LEN,\n",
        "    pools=dict(CN2_LONG=CN2_LONG, CN_MED=CN_MED),\n",
        "    entity_aware=EA_ARGS,\n",
        "    thc=dict(target_len=\"adaptive_lead3*0.9\", len_sigma=20.0,\n",
        "             weights=dict(entity=5.0, base_norm=0.4, contig_lcs=0.2, len_reward=0.3),\n",
        "             hard_fail_core=True, M=6),\n",
        "    stageB_weights=dict(THC=W_THC, LeadSim=W_LEAD, MBR=W_MBR),\n",
        "    seed=SEED, spacy_version=SPACY_VER,\n",
        "    val_file_md5=hashlib.md5(open(VALIDATION_PATH, 'rb').read()).hexdigest(),\n",
        "    n_val=N_VAL\n",
        ")\n",
        "cfg_path = f\"{OUT_DIR}/frozen_config_{STAMP}.json\"\n",
        "save_json(frozen_cfg, cfg_path)\n",
        "\n",
        "print(\"\\nSaved:\")\n",
        "print(\"  best_hybrid_val ->\", best_val_out)\n",
        "print(\"  frozen_config    ->\", cfg_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365,
          "referenced_widgets": [
            "fa889ece356a4bc3a649ecace8420fb3",
            "9f0ce732b3b14d7485cedc3795af4a20",
            "9d94111f5187483e8aa33f6513dc2ea4"
          ]
        },
        "id": "Bgz9EkDjBGMP",
        "outputId": "6adad163-e4a1-4175-8200-31126eb85b15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test subset: 1000\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa889ece356a4bc3a649ecace8420fb3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generate CN2_LONG:   0%|          | 0/250 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f0ce732b3b14d7485cedc3795af4a20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generate CN_MED:   0%|          | 0/250 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d94111f5187483e8aa33f6513dc2ea4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "THC Stage-A (TEST):   0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "== TEST ==\n",
            "ROUGE  TOP: {'r1_f': 0.3127, 'r2_f': 0.1156, 'rl_f': 0.2158}\n",
            "ROUGE HYBR: {'r1_f': 0.3112, 'r2_f': 0.1143, 'rl_f': 0.2138} | Δ: {'r1_f': -0.0015, 'r2_f': -0.0013, 'rl_f': -0.0021}\n",
            "ENT   TOP : {'TP': 1145, 'FP': 4021, 'FN': 2363, 'entP': 0.2216, 'entR': 0.3264, 'entF1': 0.264}\n",
            "ENT   HYBR: {'TP': 1308, 'FP': 4695, 'FN': 2200, 'entP': 0.2179, 'entR': 0.3729, 'entF1': 0.275} | Δ: {'entP': -0.0038, 'entR': 0.0465, 'entF1': 0.011}\n",
            "UCER  TOP : {'rate': 0.181, 'avg_unsupported_core': 0.204}\n",
            "UCER  HYBR: {'rate': 0.035, 'avg_unsupported_core': 0.047} | Δ rate: -0.146\n",
            "\n",
            "Saved:\n",
            "  best_hybrid_test -> /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/eval_HYBRID/hybrid_TEST_20250827_015818/best_hybrid_test_20250827_015818.csv\n",
            "  frozen_config     -> /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/eval_HYBRID/hybrid_TEST_20250827_015818/frozen_config_used_20250827_015818.json\n"
          ]
        }
      ],
      "source": [
        "# ==== APPLY TO TEST ONCE ====\n",
        "# Paste the cfg_path printed above:\n",
        "CFG_PATH = cfg_path\n",
        "\n",
        "cfg = json.load(open(CFG_PATH))\n",
        "assert cfg[\"checkpoint\"] == CKPT_DIR, \"Checkpoint mismatch; be consistent.\"\n",
        "MAX_SRC_LEN = cfg[\"max_src_len\"]\n",
        "CN2_LONG = cfg[\"pools\"][\"CN2_LONG\"]; CN_MED = cfg[\"pools\"][\"CN_MED\"]\n",
        "EA_ARGS  = cfg[\"entity_aware\"]\n",
        "W_THC, W_LEAD, W_MBR = cfg[\"stageB_weights\"][\"THC\"], cfg[\"stageB_weights\"][\"LeadSim\"], cfg[\"stageB_weights\"][\"MBR\"]\n",
        "\n",
        "df_test = pd.read_csv(TEST_PATH)\n",
        "df_test = norm_gold(df_test).copy()\n",
        "\n",
        "\n",
        "N_TEST = 1000\n",
        "df_test = df_test.head(N_TEST).copy()\n",
        "print(\"Test subset:\", len(df_test))\n",
        "\n",
        "arts_t = df_test[\"article\"].astype(str).tolist()\n",
        "refs_t = df_test[\"highlights\"].astype(str).tolist()\n",
        "\n",
        "ents_cache_build(list(df_test[\"article\"].astype(str)))  # cache articles\n",
        "\n",
        "# Generate both pools\n",
        "dd_long_t = generate_pool(arts_t, refs_t, CN2_LONG, pool_name=\"CN2_LONG\", batch_size=4)\n",
        "dd_med_t  = generate_pool(arts_t, refs_t, CN_MED,  pool_name=\"CN_MED\",  batch_size=4)\n",
        "dd_all_t  = pd.concat([dd_long_t, dd_med_t], ignore_index=True)\n",
        "\n",
        "# THC Stage-A on test (adaptive length)\n",
        "rows_A=[]\n",
        "for rid, grp in tqdm(list(dd_all_t.groupby(\"row_id\")), total=dd_all_t[\"row_id\"].nunique(), desc=\"THC Stage-A (TEST)\"):\n",
        "    art = grp.iloc[0][\"article\"]\n",
        "    tgt = adaptive_len_target(art)\n",
        "    L   = grp[\"len\"].clip(lower=1).to_numpy()\n",
        "    ent_scores=[]; fails=[]; contigs=[]\n",
        "    for _,r in grp.iterrows():\n",
        "        s,f = strict_entity_score(r[\"article\"], r[\"summary\"])\n",
        "        c,_= lcs_span_stats(r[\"article\"], r[\"summary\"])\n",
        "        ent_scores.append(s); fails.append(f); contigs.append(c)\n",
        "    ent_scores=np.array(ent_scores); fails=np.array(fails); contigs=np.array(contigs)\n",
        "    len_reward = np.exp(-((L - tgt)**2)/(2*(20.0**2)))\n",
        "    THC = (5.0*ent_scores) + (0.4*(grp[\"base_score\"].to_numpy()/(L**0.3))) + (0.2*contigs) + (0.3*len_reward)\n",
        "    ok = grp.loc[~fails].copy()\n",
        "    ok[\"THC\"] = THC[~fails]\n",
        "    if ok.empty:\n",
        "        ok = grp.copy(); ok[\"THC\"]=THC\n",
        "    ok = ok.sort_values(\"THC\", ascending=False).head(6)\n",
        "    rows_A.append(ok)\n",
        "stageA_t = pd.concat(rows_A, ignore_index=True)\n",
        "\n",
        "# Stage-B on test (frozen weights)\n",
        "best_test = pick_with_weights(stageA_t, W_THC, W_LEAD, W_MBR)\n",
        "\n",
        "# Baseline TOP by base_score across all candidates\n",
        "top_k_t = (dd_all_t.sort_values([\"row_id\",\"base_score\"], ascending=[True, False])\n",
        "                 .groupby(\"row_id\", as_index=False).head(1)\n",
        "                 [[\"row_id\",\"summary\"]].rename(columns={\"summary\":\"top_beam_summary\"}))\n",
        "\n",
        "ev_t = best_test[[\"row_id\",\"article\",\"highlights\",\"summary\"]].rename(columns={\"summary\":\"hybrid_summary\"}) \\\n",
        "         .merge(top_k_t, on=\"row_id\", how=\"left\")\n",
        "\n",
        "R = ev_t[\"highlights\"].astype(str).tolist()\n",
        "H = ev_t[\"hybrid_summary\"].astype(str).tolist()\n",
        "T = ev_t[\"top_beam_summary\"].astype(str).tolist()\n",
        "A = ev_t[\"article\"].astype(str).tolist()\n",
        "\n",
        "R_top = rouge_mean(R,T); R_hyb = rouge_mean(R,H)\n",
        "E_top = entPRF(R,T);     E_hyb = entPRF(R,H)\n",
        "ucer_top, ucnt_top = UCER(A, T)\n",
        "ucer_hyb, ucnt_hyb = UCER(A, H)\n",
        "\n",
        "print(\"\\n== TEST ==\")\n",
        "print(\"ROUGE  TOP:\", {k:round(v,4) for k,v in R_top.items()})\n",
        "print(\"ROUGE HYBR:\", {k:round(v,4) for k,v in R_hyb.items()},\n",
        "      \"| Δ:\", {k: round(R_hyb[k]-R_top[k],4) for k in R_top})\n",
        "print(\"ENT   TOP :\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in E_top.items()})\n",
        "print(\"ENT   HYBR:\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in E_hyb.items()},\n",
        "      \"| Δ:\", {k: round(E_hyb[k]-E_top[k],4) for k in [\"entP\",\"entR\",\"entF1\"]})\n",
        "print(\"UCER  TOP :\", {\"rate\": round(ucer_top,4), \"avg_unsupported_core\": round(ucnt_top,4)})\n",
        "print(\"UCER  HYBR:\", {\"rate\": round(ucer_hyb,4), \"avg_unsupported_core\": round(ucnt_hyb,4)},\n",
        "      \"| Δ rate:\", round(ucer_hyb-ucer_top,4))\n",
        "\n",
        "# Save TEST outputs + config copy\n",
        "STAMP_T = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "OUT_TEST = f\"{OUT_ROOT}/hybrid_TEST_{STAMP_T}\"\n",
        "os.makedirs(OUT_TEST, exist_ok=True)\n",
        "best_test_out = f\"{OUT_TEST}/best_hybrid_test_{STAMP_T}.csv\"\n",
        "save_csv(best_test, best_test_out)\n",
        "cfg_copy = f\"{OUT_TEST}/frozen_config_used_{STAMP_T}.json\"\n",
        "save_json(cfg, cfg_copy)\n",
        "print(\"\\nSaved:\")\n",
        "print(\"  best_hybrid_test ->\", best_test_out)\n",
        "print(\"  frozen_config     ->\", cfg_copy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6EQC2ZrIAwV",
        "outputId": "eab3a279-045b-4c18-ba3b-b96caa242f7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Oracle-K RL: 0.30052524667672265  | TOP RL: 0.2158487442910207  | gap: 0.08467650238570196\n"
          ]
        }
      ],
      "source": [
        "# ORACLE-K RLsum on your existing 1k TEST run (no regen)\n",
        "from rouge_score import rouge_scorer\n",
        "sc = rouge_scorer.RougeScorer([\"rougeLsum\"], use_stemmer=True)\n",
        "\n",
        "dd_all_t = pd.concat([dd_long_t, dd_med_t], ignore_index=True)  # from your test cell\n",
        "beams = dd_all_t.groupby(\"row_id\")[\"summary\"].apply(list).to_dict()\n",
        "refs  = dd_all_t.drop_duplicates(\"row_id\").set_index(\"row_id\")[\"highlights\"].to_dict()\n",
        "\n",
        "# TOP we already built as top_k_t in your test cell:\n",
        "tops_map = top_k_t.set_index(\"row_id\")[\"top_beam_summary\"].to_dict()\n",
        "\n",
        "def rl(ref, hyp): return sc.score(str(ref), str(hyp))[\"rougeLsum\"].fmeasure\n",
        "oracle = [max(rl(refs[rid], h) for h in beams[rid]) for rid in refs]\n",
        "top    = [rl(refs[rid], tops_map[rid]) for rid in refs]\n",
        "print(\"Oracle-K RL:\", sum(oracle)/len(oracle), \" | TOP RL:\", sum(top)/len(top),\n",
        "      \" | gap:\", (sum(oracle)-sum(top))/len(top))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmheqDIuJYyr"
      },
      "source": [
        "# Val-Test lambda 2 with med and long pools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "X_U1Zx05NbBU",
        "outputId": "85a9ef55-08b6-4f46-bf2a-236bf58a1399"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[info] Installing rouge-score==0.1.2 ...\n",
            "[ok] Using official rouge-score package.\n"
          ]
        }
      ],
      "source": [
        "# --- Robust ROUGE setup: try official package, else fallback implementation ---\n",
        "import sys, subprocess, pkgutil, math, re\n",
        "from collections import Counter\n",
        "\n",
        "def _maybe_install(pkg_mod, pip_name=None):\n",
        "    import importlib.util\n",
        "    if pip_name is None: pip_name = pkg_mod\n",
        "    if importlib.util.find_spec(pkg_mod) is None:\n",
        "        try:\n",
        "            print(f\"[info] Installing {pip_name} ...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pip_name])\n",
        "        except Exception as e:\n",
        "            print(f\"[warn] pip install failed for {pip_name}: {e}\")\n",
        "\n",
        "# Try official rouge-score first\n",
        "try:\n",
        "    _maybe_install(\"rouge_score\", \"rouge-score==0.1.2\")\n",
        "    from rouge_score import rouge_scorer as _rs\n",
        "    _SC = _rs.RougeScorer([\"rouge1\", \"rouge2\", \"rougeLsum\"], use_stemmer=True)\n",
        "    def rouge1(a,b):  return _SC.score(str(a), str(b))[\"rouge1\"].fmeasure\n",
        "    def rouge2(a,b):  return _SC.score(str(a), str(b))[\"rouge2\"].fmeasure\n",
        "    def rougeLsum(a,b):  return _SC.score(str(a), str(b))[\"rougeLsum\"].fmeasure\n",
        "    print(\"[ok] Using official rouge-score package.\")\n",
        "except Exception as e:\n",
        "    print(\"[warn] rouge-score unavailable; using fallback ROUGE (approx).\", e)\n",
        "\n",
        "    # --------- Minimal fallback ROUGE (token-based) ----------\n",
        "    def _tok(s):\n",
        "        # simple whitespace tokenization; tweak if you want smarter tokenization\n",
        "        return [t for t in re.split(r\"\\s+\", str(s).strip()) if t]\n",
        "\n",
        "    def _ngram_counts(tokens, n):\n",
        "        return Counter(tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)) if len(tokens) >= n else Counter()\n",
        "\n",
        "    def _overlap_count(ref_cnt, hyp_cnt):\n",
        "        return sum((ref_cnt & hyp_cnt).values())\n",
        "\n",
        "    def _safe_f(p, r):\n",
        "        return (2*p*r/(p+r)) if (p+r) > 0 else 0.0\n",
        "\n",
        "    # ROUGE-1 / ROUGE-2 (F1) fallback\n",
        "    def rouge1(ref, hyp):\n",
        "        r, h = _tok(ref), _tok(hyp)\n",
        "        r1, h1 = _ngram_counts(r,1), _ngram_counts(h,1)\n",
        "        overlap = _overlap_count(r1, h1)\n",
        "        P = overlap / max(sum(h1.values()), 1e-12)\n",
        "        R = overlap / max(sum(r1.values()), 1e-12)\n",
        "        return _safe_f(P, R)\n",
        "\n",
        "    def rouge2(ref, hyp):\n",
        "        r, h = _tok(ref), _tok(hyp)\n",
        "        r2, h2 = _ngram_counts(r,2), _ngram_counts(h,2)\n",
        "        overlap = _overlap_count(r2, h2)\n",
        "        P = overlap / max(sum(h2.values()), 1e-12)\n",
        "        R = overlap / max(sum(r2.values()), 1e-12)\n",
        "        return _safe_f(P, R)\n",
        "\n",
        "    # ROUGE-L (F1) fallback (global LCS over tokens; not sentence-summed Lsum but close enough for ranking)\n",
        "    def _lcs_len(a, b):\n",
        "        A, B = _tok(a), _tok(b)\n",
        "        n, m = len(A), len(B)\n",
        "        dp = [0]*(m+1)\n",
        "        for i in range(1, n+1):\n",
        "            prev = 0\n",
        "            Ai = A[i-1]\n",
        "            for j in range(1, m+1):\n",
        "                t = dp[j]\n",
        "                if Ai == B[j-1]:\n",
        "                    dp[j] = prev + 1\n",
        "                else:\n",
        "                    dp[j] = max(dp[j], dp[j-1])\n",
        "                prev = t\n",
        "        return dp[m]\n",
        "\n",
        "    def rougeLsum(ref, hyp):\n",
        "        lcs = _lcs_len(ref, hyp)\n",
        "        r_len = max(len(_tok(ref)), 1e-12)\n",
        "        h_len = max(len(_tok(hyp)), 1e-12)\n",
        "        P = lcs / h_len\n",
        "        R = lcs / r_len\n",
        "        return _safe_f(P, R)\n",
        "    # ---------------------------------------------------------\n",
        "\n",
        "# Convenience: batch means using the three scorers above\n",
        "def rouge_mean(refs, hyps):\n",
        "    r1=r2=rl=0.0\n",
        "    for r,h in zip(refs, hyps):\n",
        "        r1 += rouge1(r,h); r2 += rouge2(r,h); rl += rougeLsum(r,h)\n",
        "    n = max(len(refs),1)\n",
        "    return {\"r1_f\": r1/n, \"r2_f\": r2/n, \"rl_f\": rl/n}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pufXOZWXN38m"
      },
      "outputs": [],
      "source": [
        "from rouge_score import rouge_scorer\n",
        "_SC = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\"], use_stemmer=True)\n",
        "\n",
        "def rougeLsum(a, b):\n",
        "    return _SC.score(str(a), str(b))[\"rougeLsum\"].fmeasure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "176cc8477c4c4b449ae95621657d75a3",
            "c3e85e9d0a4b4381870edf4011689595",
            "aab259b6453a4681884a4393e02230a8"
          ]
        },
        "id": "dWfK_ow7Q3_k",
        "outputId": "705a2cc1-e820-492c-99dd-e026553a8e5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "176cc8477c4c4b449ae95621657d75a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generate CN2_LONG:   0%|          | 0/150 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Group Beam Search is scheduled to be moved to a `custom_generate` repository in v4.55.0. To prevent loss of backward compatibility, add `trust_remote_code=True` to your `generate` call.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3e85e9d0a4b4381870edf4011689595",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generate CN_MED:   0%|          | 0/75 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aab259b6453a4681884a4393e02230a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "THC Stage-A:   0%|          | 0/600 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Learned weights: {'intercept': -0.20703246812650786, 'coefs': {'THC_norm': 0.0787255998669113, 'LeadSim': 0.7419269555670474, 'MBR': 1.0108430623131148, 'len_reward': 0.2026158276763151, 'entity_score': 0.0787255998669113, 'contig_lcs': -0.41348891305304697}}\n",
            "Saved:\n",
            "  LR weights -> /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/eval_HYBRID/hybrid_VAL_LR_20250827_064817/stageB_lr_weights.json \n",
            "  Frozen cfg -> /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/eval_HYBRID/hybrid_VAL_LR_20250827_064817/frozen_config_lr_20250827_064817.json\n"
          ]
        }
      ],
      "source": [
        "# ====== CONFIG: paths & sizes ======\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "CKPT_DIR        = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000\"\n",
        "VALIDATION_PATH = \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "OUT_ROOT        = f\"{CKPT_DIR}/eval_HYBRID\"\n",
        "\n",
        "N_VAL = 600\n",
        "SEED  = 0\n",
        "\n",
        "# ====== Imports & model (fast) ======\n",
        "import os, json, csv, time, math, random, re, hashlib\n",
        "import numpy as np, pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "tok   = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR).to(\"cuda\" if torch.cuda.is_available() else \"cpu\").eval()\n",
        "\n",
        "# speed flags\n",
        "torch.set_grad_enabled(False)\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    try: torch.set_float32_matmul_precision(\"high\")\n",
        "    except: pass\n",
        "for m in (model, getattr(model, \"model\", None)):\n",
        "    if m and hasattr(m, \"config\"):\n",
        "        setattr(m.config, \"attn_implementation\", \"sdpa\")\n",
        "if torch.cuda.is_available():\n",
        "    major = torch.cuda.get_device_capability()[0]\n",
        "    model.to(dtype=(torch.bfloat16 if major >= 8 else torch.float16))\n",
        "\n",
        "from contextlib import contextmanager\n",
        "@contextmanager\n",
        "def fast_infer():\n",
        "    if torch.cuda.is_available():\n",
        "        with torch.inference_mode(), torch.amp.autocast(\"cuda\"):\n",
        "            yield\n",
        "    else:\n",
        "        with torch.inference_mode():\n",
        "            yield\n",
        "\n",
        "# ====== Decode pools (fast-lite) ======\n",
        "MAX_SRC_LEN = 400\n",
        "FAST_LITE   = True  # no output_scores for speed (we won't use base_score)\n",
        "\n",
        "CN2_LONG = dict(\n",
        "    num_beams=10, num_return_sequences=10,\n",
        "    num_beam_groups=5, diversity_penalty=0.3,\n",
        "    max_new_tokens=100, min_new_tokens=55,\n",
        "    no_repeat_ngram_size=3, length_penalty=2.0,\n",
        "    early_stopping=True, return_dict_in_generate=True,\n",
        "    output_scores=(not FAST_LITE)\n",
        ")\n",
        "CN_MED = dict(\n",
        "    num_beams=10, num_return_sequences=10,\n",
        "    num_beam_groups=5, diversity_penalty=0.3,\n",
        "    max_new_tokens=70, min_new_tokens=35,\n",
        "    no_repeat_ngram_size=3, length_penalty=1.6,\n",
        "    early_stopping=True, return_dict_in_generate=True,\n",
        "    output_scores=(not FAST_LITE)\n",
        ")\n",
        "\n",
        "EA_ARGS = dict(gamma=0.4, gamma_entity=1.5, max_span=6)\n",
        "\n",
        "# ====== I/O helpers ======\n",
        "def save_csv(df, path):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    tmp = path + \".tmp\"; df.to_csv(tmp, index=False, quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n",
        "    os.replace(tmp, path)\n",
        "def save_json(obj, path):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    with open(path,\"w\") as f: json.dump(obj, f, indent=2)\n",
        "def norm_gold(df):\n",
        "    if \"highlights\" not in df.columns and \"reference\" in df.columns:\n",
        "        df[\"highlights\"] = df[\"reference\"].astype(str)\n",
        "    if \"highlights\" not in df.columns: raise KeyError(\"CSV needs 'highlights' or 'reference'.\")\n",
        "    return df\n",
        "\n",
        "# ====== spaCy NER (batch) ======\n",
        "import spacy\n",
        "try:\n",
        "    nlp\n",
        "except NameError:\n",
        "    try:\n",
        "        nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "    except OSError:\n",
        "        import sys, subprocess\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "        nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "KEEP = {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"DATE\",\"TIME\",\"CARDINAL\",\"MONEY\",\"PERCENT\",\"QUANTITY\",\"ORDINAL\"}\n",
        "CORE = {\"PERSON\",\"ORG\",\"GPE\"}\n",
        "\n",
        "_ENTS_CACHE = {}\n",
        "def ents_cache_build(texts, batch_size=128):\n",
        "    uniq = list(dict.fromkeys(map(str, texts)))\n",
        "    for doc in nlp.pipe(uniq, batch_size=batch_size):\n",
        "        _ENTS_CACHE[doc.text] = {(\" \".join(e.text.strip().split()), e.label_) for e in doc.ents if e.label_ in KEEP}\n",
        "def _ents_norm(text):\n",
        "    t=str(text)\n",
        "    if t in _ENTS_CACHE: return _ENTS_CACHE[t]\n",
        "    doc = nlp(t)\n",
        "    return {(\" \".join(e.text.strip().split()), e.label_) for e in doc.ents if e.label_ in KEEP}\n",
        "\n",
        "# ====== Features / metrics ======\n",
        "def lcs_span_stats(src_text, hyp_text):\n",
        "    s=str(src_text).split(); h=str(hyp_text).split()\n",
        "    n,m=len(s),len(h); dp=[[0]*(m+1) for _ in range(n+1)]\n",
        "    for i in range(n-1,-1,-1):\n",
        "        si=s[i]; row=dp[i]; row1=dp[i+1]\n",
        "        for j in range(m-1,-1,-1):\n",
        "            row[j]=row1[j+1]+1 if si==h[j] else max(row[j+1], row1[j])\n",
        "    i=j=0; blocks=[]; cur=0\n",
        "    while i<n and j<m:\n",
        "        if s[i]==h[j]: cur+=1; i+=1; j+=1\n",
        "        else:\n",
        "            if cur>=2: blocks.append(cur); cur=0\n",
        "            if dp[i+1][j]>=dp[i][j+1]: i+=1\n",
        "            else: j+=1\n",
        "    if cur>=2: blocks.append(cur)\n",
        "    cont=sum(b-1 for b in blocks)\n",
        "    av=(sum(blocks)/len(blocks)) if blocks else 1.0\n",
        "    return cont, av\n",
        "\n",
        "def strict_entity_score(src, hyp):\n",
        "    S=_ents_norm(src); H=_ents_norm(hyp)\n",
        "    overlap=H&S; unsupported=H-S\n",
        "    hard_fail=any(lbl in CORE for _,lbl in unsupported)\n",
        "    score = sum(3 if lbl in CORE else 1 for _,lbl in overlap)\n",
        "    return score, hard_fail\n",
        "\n",
        "def sent_split(txt): return re.split(r'(?<=[.!?])\\s+', str(txt).strip())\n",
        "def lead3(text):\n",
        "    s=[t for t in sent_split(text) if t]\n",
        "    return \" \".join(s[:3])\n",
        "def toklen(s): return len(tok(str(s))[\"input_ids\"])\n",
        "def adaptive_len_target(src):\n",
        "    L = toklen(lead3(src))\n",
        "    return int(np.clip(round(0.9*L), 32, 60))\n",
        "\n",
        "from rouge_score import rouge_scorer\n",
        "_SC = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\"], use_stemmer=True)\n",
        "def rougeLsum(a,b): return _SC.score(str(a), str(b))[\"rougeLsum\"].fmeasure\n",
        "\n",
        "def mbr_centroid(cands):\n",
        "    n=len(cands)\n",
        "    if n<=1: return [0.0]*n\n",
        "    M=np.zeros((n,n))\n",
        "    for i in range(n):\n",
        "        for j in range(i+1,n):\n",
        "            s=rougeLsum(cands[i], cands[j]); M[i,j]=M[j,i]=s\n",
        "    return M.mean(axis=1).tolist()\n",
        "\n",
        "def minmax_norm(vs):\n",
        "    vs=np.array(vs,float); lo,hi=vs.min(),vs.max()\n",
        "    return (np.zeros_like(vs) if hi-lo<1e-8 else (vs-lo)/(hi-lo)).tolist()\n",
        "\n",
        "# ====== CopyNext maps & fast generator ======\n",
        "def pretokenize_articles(articles):\n",
        "    enc = tok(articles, max_length=MAX_SRC_LEN, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "    return {k: v.to(model.device) for k,v in enc.items()}\n",
        "\n",
        "from collections import namedtuple\n",
        "SrcInfo = namedtuple(\"SrcInfo\", \"maps\")\n",
        "def build_srcinfos(texts, max_span=EA_ARGS[\"max_span\"]):\n",
        "    infos=[]\n",
        "    docs = list(nlp.pipe(texts, batch_size=128))\n",
        "    enc  = tok(texts, max_length=MAX_SRC_LEN, truncation=True, return_offsets_mapping=True)\n",
        "    for i, t in enumerate(texts):\n",
        "        ids  = enc[\"input_ids\"][i]; offs = enc[\"offset_mapping\"][i]\n",
        "        ent_spans = [(e.start_char, e.end_char) for e in docs[i].ents if e.label_ in KEEP]\n",
        "        def in_ent(off):\n",
        "            a,b = off\n",
        "            if a is None: return False\n",
        "            for s,e in ent_spans:\n",
        "                if a>=s and b<=e: return True\n",
        "            return False\n",
        "        maps=[{} for _ in range(max_span)]\n",
        "        n=len(ids)\n",
        "        for k in range(1, max_span+1):\n",
        "            mp=maps[k-1]\n",
        "            for j in range(0, n-k):\n",
        "                key=tuple(ids[j:j+k]); nxt=ids[j+k]\n",
        "                is_ent = in_ent(offs[j+k]) if j+k<len(offs) else False\n",
        "                if key not in mp: mp[key]=(nxt, is_ent)\n",
        "        infos.append(SrcInfo(maps=maps))\n",
        "    return infos\n",
        "\n",
        "from transformers import LogitsProcessor, LogitsProcessorList\n",
        "class EntityAwareSpanProcessor(LogitsProcessor):\n",
        "    def __init__(self, src_infos, num_beams, gamma=0.4, gamma_entity=1.5, max_span=6):\n",
        "        self.infos=src_infos; self.num_beams=int(num_beams)\n",
        "        self.gamma=float(gamma); self.gamma_entity=float(gamma_entity); self.max_span=int(max_span)\n",
        "    def __call__(self, input_ids, scores):\n",
        "        Bx=input_ids.size(0)\n",
        "        for b in range(Bx):\n",
        "            sample=b//self.num_beams\n",
        "            maps=self.infos[sample].maps; seq=input_ids[b].tolist()\n",
        "            for k in range(min(self.max_span, len(seq)), 0, -1):\n",
        "                mp=maps[k-1]; key=tuple(seq[-k:])\n",
        "                if key in mp:\n",
        "                    nxt, is_ent = mp[key]\n",
        "                    scores[b, nxt] = scores[b, nxt] + (self.gamma_entity if is_ent else self.gamma)\n",
        "                    break\n",
        "        return scores\n",
        "\n",
        "def run_pool_fast(enc_all, start, end, src_infos, decode_args, pool_name, articles, references):\n",
        "    rows=[]\n",
        "    enc = {k: v[start:end] for k,v in enc_all.items()}\n",
        "    proc = EntityAwareSpanProcessor(src_infos[start:end],\n",
        "                                    num_beams=decode_args[\"num_beams\"],\n",
        "                                    gamma=EA_ARGS[\"gamma\"], gamma_entity=EA_ARGS[\"gamma_entity\"],\n",
        "                                    max_span=EA_ARGS[\"max_span\"])\n",
        "    with fast_infer():\n",
        "        out = model.generate(**enc, **decode_args, logits_processor=LogitsProcessorList([proc]))\n",
        "    K = decode_args[\"num_return_sequences\"]; bs=end-start\n",
        "    seqs   = out.sequences.view(bs, K, -1)\n",
        "    scores = getattr(out, \"sequences_scores\", None)\n",
        "    for bi in range(bs):\n",
        "        art=articles[start+bi]; ref=references[start+bi]\n",
        "        for k in range(K):\n",
        "            ids=seqs[bi,k]\n",
        "            hyp=tok.decode(ids, skip_special_tokens=True)\n",
        "            rows.append({\"row_id\": start+bi, \"pool\": pool_name, \"candidate_k\": k,\n",
        "                         \"article\": art, \"highlights\": ref, \"summary\": hyp,\n",
        "                         \"len\": max(int(ids.size(0)-1), 1),\n",
        "                         \"base_score\": float(scores[bi*K+k].item()) if scores is not None else 0.0})\n",
        "    return rows\n",
        "\n",
        "def generate_both_pools_fast(articles, references, batch_long=4, batch_med=8):\n",
        "    enc_all   = pretokenize_articles(articles)\n",
        "    src_infos = build_srcinfos(articles)\n",
        "    rows=[]\n",
        "    for s in tqdm(range(0, len(articles), batch_long), desc=\"Generate CN2_LONG\"):\n",
        "        e=min(len(articles), s+batch_long)\n",
        "        rows += run_pool_fast(enc_all, s, e, src_infos, CN2_LONG, \"CN2_LONG\", articles, references)\n",
        "    for s in tqdm(range(0, len(articles), batch_med), desc=\"Generate CN_MED\"):\n",
        "        e=min(len(articles), s+batch_med)\n",
        "        rows += run_pool_fast(enc_all, s, e, src_infos, CN_MED,  \"CN_MED\",  articles, references)\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# ====== Load validation & generate beams fast ======\n",
        "df_val = norm_gold(pd.read_csv(VALIDATION_PATH)).head(N_VAL).copy()\n",
        "arts = df_val[\"article\"].astype(str).tolist()\n",
        "refs = df_val[\"highlights\"].astype(str).tolist()\n",
        "ents_cache_build(df_val[\"article\"].astype(str).tolist())  # for later metrics\n",
        "\n",
        "dd_all = generate_both_pools_fast(arts, refs, batch_long=4, batch_med=8)\n",
        "\n",
        "# ====== THC Stage-A (strict) -> keep M survivors ======\n",
        "M_KEEP = 8\n",
        "rowsA=[]\n",
        "for rid, grp in tqdm(list(dd_all.groupby(\"row_id\")), total=dd_all[\"row_id\"].nunique(), desc=\"THC Stage-A\"):\n",
        "    art = grp.iloc[0][\"article\"]; tgt = adaptive_len_target(art)\n",
        "    L = grp[\"len\"].clip(lower=1).to_numpy()\n",
        "    ent_scores=[]; fails=[]; contigs=[]\n",
        "    for _,r in grp.iterrows():\n",
        "        s,f = strict_entity_score(r[\"article\"], r[\"summary\"]); c,_ = lcs_span_stats(r[\"article\"], r[\"summary\"])\n",
        "        ent_scores.append(s); fails.append(f); contigs.append(c)\n",
        "    ent_scores=np.array(ent_scores); fails=np.array(fails); contigs=np.array(contigs)\n",
        "    len_reward = np.exp(-((L - tgt)**2)/(2*(20.0**2)))\n",
        "    # FAST-LITE THC: (drop base_score term)\n",
        "    THC = (5.0*ent_scores) + (0.2*contigs) + (0.3*len_reward)\n",
        "    ok = grp.loc[~fails].copy(); ok[\"THC\"]=THC[~fails]\n",
        "    if ok.empty: ok=grp.copy(); ok[\"THC\"]=THC\n",
        "    rowsA.append(ok.sort_values(\"THC\", ascending=False).head(M_KEEP))\n",
        "stageA = pd.concat(rowsA, ignore_index=True)\n",
        "\n",
        "# ====== Build per-row features for survivors (VAL) ======\n",
        "from rouge_score import rouge_scorer\n",
        "sc = rouge_scorer.RougeScorer([\"rougeLsum\"], use_stemmer=True)\n",
        "def rl(a,b): return sc.score(str(a), str(b))[\"rougeLsum\"].fmeasure\n",
        "\n",
        "feat_rows=[]\n",
        "for rid, grp in stageA.groupby(\"row_id\"):\n",
        "    art = grp.iloc[0][\"article\"]; lead = lead3(art); tgt = adaptive_len_target(art)\n",
        "    cands = grp[\"summary\"].tolist()\n",
        "    leadsim = np.array([rl(h, lead) for h in cands])\n",
        "    # centroid MBR\n",
        "    n=len(cands); mbr=np.zeros(n)\n",
        "    if n>1:\n",
        "        M=np.zeros((n,n))\n",
        "        for i in range(n):\n",
        "            for j in range(i+1,n):\n",
        "                s=rl(cands[i], cands[j]); M[i,j]=M[j,i]=s\n",
        "        mbr=M.mean(axis=1)\n",
        "    # rowwise minmax\n",
        "    def mm(v): return minmax_norm(np.array(v))\n",
        "    feats = pd.DataFrame({\n",
        "        \"row_id\": grp[\"row_id\"].values,\n",
        "        \"summary\": grp[\"summary\"].values,\n",
        "        \"THC_norm\": mm(grp[\"THC\"].values),\n",
        "        \"LeadSim\": leadsim,\n",
        "        \"MBR\": mbr,\n",
        "        \"len_reward\": mm([math.exp(-((L-tgt)**2)/(2*(20.0**2))) for L in grp[\"len\"].tolist()]),\n",
        "        \"entity_score\": mm(grp[\"THC\"].values),      # proxy (already entity-heavy)\n",
        "        \"contig_lcs\": mm([lcs_span_stats(art, s)[0] for s in cands]),\n",
        "        # training label (only on VAL):\n",
        "        \"rl_ref\": [rl(h, grp.iloc[0][\"highlights\"]) for h in cands]\n",
        "    })\n",
        "    feat_rows.append(feats)\n",
        "F_val = pd.concat(feat_rows, ignore_index=True)\n",
        "\n",
        "# ====== Pairwise logistic on VAL ======\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "pairs_X=[]; pairs_y=[]\n",
        "cols = [\"THC_norm\",\"LeadSim\",\"MBR\",\"len_reward\",\"entity_score\",\"contig_lcs\"]\n",
        "for rid, g in F_val.groupby(\"row_id\"):\n",
        "    G=g.reset_index(drop=True); m=len(G)\n",
        "    for i in range(m):\n",
        "        for j in range(i+1,m):\n",
        "            yi = G.loc[i,\"rl_ref\"]; yj = G.loc[j,\"rl_ref\"]\n",
        "            if yi==yj: continue\n",
        "            x = (G.loc[i, cols].values.astype(float) - G.loc[j, cols].values.astype(float))\n",
        "            y = 1 if yi>yj else 0\n",
        "            pairs_X.append(x); pairs_y.append(y)\n",
        "pairs_X = np.stack(pairs_X); pairs_y = np.array(pairs_y)\n",
        "clf = LogisticRegression(solver=\"liblinear\", max_iter=1000, random_state=SEED)\n",
        "clf.fit(pairs_X, pairs_y)\n",
        "\n",
        "weights = dict(intercept=float(clf.intercept_[0]), coefs={c: float(w) for c,w in zip(cols, clf.coef_[0].tolist())})\n",
        "print(\"Learned weights:\", weights)\n",
        "\n",
        "# ====== Save config + weights ======\n",
        "STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "OUT_DIR = f\"{OUT_ROOT}/hybrid_VAL_LR_{STAMP}\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "LR_W_PATH = f\"{OUT_DIR}/stageB_lr_weights.json\"\n",
        "save_json(weights, LR_W_PATH)\n",
        "\n",
        "FROZEN_CFG = dict(\n",
        "    checkpoint=CKPT_DIR,\n",
        "    max_src_len=MAX_SRC_LEN,\n",
        "    pools=dict(CN2_LONG=CN2_LONG, CN_MED=CN_MED),\n",
        "    entity_aware=EA_ARGS,\n",
        "    thc_stageA=dict(M=M_KEEP, len_sigma=20.0, formula=\"5*entity + 0.2*contig_lcs + 0.3*len_reward\", hard_fail_core=True),\n",
        "    lr_features=cols,\n",
        "    seed=SEED,\n",
        "    fast_lite=FAST_LITE\n",
        ")\n",
        "CFG_PATH = f\"{OUT_DIR}/frozen_config_lr_{STAMP}.json\"\n",
        "save_json(FROZEN_CFG, CFG_PATH)\n",
        "print(\"Saved:\\n  LR weights ->\", LR_W_PATH, \"\\n  Frozen cfg ->\", CFG_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngHuhH6xSllj",
        "outputId": "4384ccb5-d373-4b08-ac7a-484715a39a93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "== VALIDATION (LR reranker) ==\n",
            "ROUGE  TOP: {'r1_f': 0.3104, 'r2_f': 0.1202, 'rl_f': 0.2141}\n",
            "ROUGE   LR: {'r1_f': 0.3159, 'r2_f': 0.1168, 'rl_f': 0.2181} | Δ: {'r1_f': 0.0055, 'r2_f': -0.0035, 'rl_f': 0.0041}\n",
            "ENT   TOP: {'TP': 721, 'FP': 2469, 'FN': 1557, 'entP': 0.226, 'entR': 0.3165, 'entF1': 0.2637}\n",
            "ENT    LR: {'TP': 750, 'FP': 2382, 'FN': 1528, 'entP': 0.2395, 'entR': 0.3292, 'entF1': 0.2773} | Δ: {'entP': 0.0134, 'entR': 0.0127, 'entF1': 0.0135}\n",
            "UCER  TOP: {'rate': 0.1783, 'avg_core': 0.1967}\n",
            "UCER   LR: {'rate': 0.0233, 'avg_core': 0.0283} | Δ rate: -0.155\n",
            "Oracle-K RL (VAL): 0.3012 | TOP RL: 0.2141 | gap: 0.0872\n"
          ]
        }
      ],
      "source": [
        "# === VAL-EVAL: score survivors with learned LR + compare vs TOP baselines ===\n",
        "import json, numpy as np, pandas as pd\n",
        "\n",
        "# Load learned weights (from the path printed by LR-VAL)\n",
        "wts = json.load(open(LR_W_PATH))\n",
        "\n",
        "# Helpers (reuse from earlier cells)\n",
        "def rouge_mean(R,H):\n",
        "    r1=r2=rl=0.0\n",
        "    for r,h in zip(R,H):\n",
        "        s = _SC.score(r,h)\n",
        "        r1 += s[\"rouge1\"].fmeasure\n",
        "        r2 += s[\"rouge2\"].fmeasure\n",
        "        rl += s[\"rougeLsum\"].fmeasure\n",
        "    n=len(R); return {\"r1_f\":r1/n,\"r2_f\":r2/n,\"rl_f\":rl/n}\n",
        "\n",
        "def entPRF(R,H):\n",
        "    TP=FP=FN=0\n",
        "    for r,h in zip(R,H):\n",
        "        R_=_ents_norm(r); H_=_ents_norm(h)\n",
        "        TP+=len(H_&R_); FP+=len(H_-R_); FN+=len(R_-H_)\n",
        "    P=TP/(TP+FP) if (TP+FP) else 0.0\n",
        "    Rc=TP/(TP+FN) if (TP+FN) else 0.0\n",
        "    F=2*P*Rc/(P+Rc) if (P+Rc) else 0.0\n",
        "    return {\"TP\":TP,\"FP\":FP,\"FN\":FN,\"entP\":P,\"entR\":Rc,\"entF1\":F}\n",
        "\n",
        "def UCER(arts, hyps):\n",
        "    rates=[]; counts=[]\n",
        "    for s,h in zip(arts, hyps):\n",
        "        S=_ents_norm(s); H=_ents_norm(h)\n",
        "        uns=[(t,l) for (t,l) in (H-S) if l in CORE]\n",
        "        rates.append(1 if uns else 0); counts.append(len(uns))\n",
        "    return float(np.mean(rates)), float(np.mean(counts))\n",
        "\n",
        "def score_group_linear(grp, weights):\n",
        "    art = grp.iloc[0][\"article\"]; lead = lead3(art); tgt = adaptive_len_target(art)\n",
        "    cands = grp[\"summary\"].tolist()\n",
        "    # LeadSim + centroid MBR\n",
        "    leadsim = np.array([rougeLsum(h, lead) for h in cands])\n",
        "    n=len(cands); mbr=np.zeros(n)\n",
        "    if n>1:\n",
        "        M=np.zeros((n,n))\n",
        "        for i in range(n):\n",
        "            for j in range(i+1,n):\n",
        "                s=rougeLsum(cands[i], cands[j]); M[i,j]=M[j,i]=s\n",
        "        mbr=M.mean(axis=1)\n",
        "\n",
        "    # rowwise minmax for THC & other bounded features\n",
        "    def mm(vs):\n",
        "        vs=np.array(vs, float); lo,hi=vs.min(),vs.max()\n",
        "        return (np.zeros_like(vs) if hi-lo<1e-8 else (vs-lo)/(hi-lo))\n",
        "    feats = pd.DataFrame({\n",
        "        \"THC_norm\": mm(grp[\"THC\"].values),\n",
        "        \"LeadSim\": leadsim,\n",
        "        \"MBR\": mbr,\n",
        "        \"len_reward\": mm([math.exp(-((L-tgt)**2)/(2*(20.0**2))) for L in grp[\"len\"].tolist()]),\n",
        "        \"entity_score\": mm(grp[\"THC\"].values),               # proxy for entity support\n",
        "        \"contig_lcs\": mm([lcs_span_stats(art, s)[0] for s in cands]),\n",
        "    })\n",
        "    cols = list(weights[\"coefs\"].keys())\n",
        "    s = weights[\"intercept\"] + sum(feats[c].values * weights[\"coefs\"][c] for c in cols)\n",
        "\n",
        "    # choose best by LR score\n",
        "    k = int(np.argmax(s))\n",
        "    cand = grp.iloc[[k]].copy()\n",
        "\n",
        "    # UCER-safe fallback: if unsupported CORE exists, pick best safe among survivors\n",
        "    S=_ents_norm(art); H=_ents_norm(cand[\"summary\"].iloc[0])\n",
        "    uns=[(t,l) for (t,l) in (H-S) if l in CORE]\n",
        "    if uns:\n",
        "        safe_idx=[]\n",
        "        for i in range(len(grp)):\n",
        "            Hi=_ents_norm(grp.iloc[i][\"summary\"])\n",
        "            bad=[(t,l) for (t,l) in (Hi-S) if l in CORE]\n",
        "            if not bad:\n",
        "                safe_idx.append(i)\n",
        "        if safe_idx:\n",
        "            s_vals = np.array(s)\n",
        "            k = int(max(safe_idx, key=lambda i: s_vals[i]))\n",
        "            cand = grp.iloc[[k]].copy()\n",
        "    return cand\n",
        "\n",
        "# 1) LR-picks on validation survivors\n",
        "best_lr_val = []\n",
        "for rid, grp in stageA.groupby(\"row_id\"):\n",
        "    best_lr_val.append(score_group_linear(grp, wts))\n",
        "best_lr_val = pd.concat(best_lr_val, ignore_index=True)\n",
        "\n",
        "# 2) Build TOP baselines (fast-lite has no base_score; use k=0 per pool)\n",
        "# Prefer CN2_LONG beam-0; fall back to CN_MED beam-0 if missing\n",
        "pool_order = {\"CN2_LONG\": 0, \"CN_MED\": 1}\n",
        "dd_all[\"_pool_ord\"] = dd_all[\"pool\"].map(pool_order).fillna(9)\n",
        "top_k0 = (dd_all.query(\"candidate_k==0\")\n",
        "                .sort_values([\"row_id\",\"_pool_ord\"])\n",
        "                .groupby(\"row_id\", as_index=False).head(1)\n",
        "                [[\"row_id\",\"summary\"]]\n",
        "                .rename(columns={\"summary\":\"top_beam_summary\"}))\n",
        "\n",
        "# 3) Evaluate\n",
        "ev = best_lr_val[[\"row_id\",\"article\",\"highlights\",\"summary\"]] \\\n",
        "        .rename(columns={\"summary\":\"hybrid_lr\"}) \\\n",
        "        .merge(top_k0, on=\"row_id\", how=\"left\")\n",
        "\n",
        "R = ev[\"highlights\"].astype(str).tolist()\n",
        "H = ev[\"hybrid_lr\"].astype(str).tolist()\n",
        "T = ev[\"top_beam_summary\"].astype(str).tolist()\n",
        "A = ev[\"article\"].astype(str).tolist()\n",
        "\n",
        "R_top = rouge_mean(R,T); R_h = rouge_mean(R,H)\n",
        "E_top = entPRF(R,T);     E_h = entPRF(R,H)\n",
        "u_top = UCER(A,T);       u_h = UCER(A,H)\n",
        "\n",
        "# 4) Oracle-K headroom on VAL (max RL over all beams per row)\n",
        "from collections import defaultdict\n",
        "oracle = []\n",
        "for rid, grp in dd_all.groupby(\"row_id\"):\n",
        "    ref = grp.iloc[0][\"highlights\"]\n",
        "    rlmax = max(rougeLsum(ref, s) for s in grp[\"summary\"].tolist())\n",
        "    oracle.append(rlmax)\n",
        "oracle_rl = float(np.mean(oracle))\n",
        "top_rl    = float(R_top[\"rl_f\"])\n",
        "\n",
        "print(\"\\n== VALIDATION (LR reranker) ==\")\n",
        "print(\"ROUGE  TOP:\", {k:round(v,4) for k,v in R_top.items()})\n",
        "print(\"ROUGE   LR:\", {k:round(v,4) for k,v in R_h.items()},\n",
        "      \"| Δ:\", {k: round(R_h[k]-R_top[k],4) for k in R_top})\n",
        "print(\"ENT   TOP:\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in E_top.items()})\n",
        "print(\"ENT    LR:\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in E_h.items()},\n",
        "      \"| Δ:\", {k: round(E_h[k]-E_top[k],4) for k in [\"entP\",\"entR\",\"entF1\"]})\n",
        "print(\"UCER  TOP:\", {\"rate\": round(u_top[0],4), \"avg_core\": round(u_top[1],4)})\n",
        "print(\"UCER   LR:\", {\"rate\": round(u_h[0],4),   \"avg_core\": round(u_h[1],4)},\n",
        "      \"| Δ rate:\", round(u_h[0]-u_top[0],4))\n",
        "print(f\"Oracle-K RL (VAL): {oracle_rl:.4f} | TOP RL: {top_rl:.4f} | gap: {oracle_rl-top_rl:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365,
          "referenced_widgets": [
            "b0778f13b613440ca6bcd3d668b018d3",
            "502ca0af7b5e41c0968f5bc1cfe7ab73",
            "045d995e16b94f9bb3e84b6baae0c90c"
          ]
        },
        "id": "eE_s2crvLKIS",
        "outputId": "f3f36497-cc26-436d-f6be-9a52295e30ca"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0778f13b613440ca6bcd3d668b018d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generate CN2_LONG:   0%|          | 0/250 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "502ca0af7b5e41c0968f5bc1cfe7ab73",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generate CN_MED:   0%|          | 0/125 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "045d995e16b94f9bb3e84b6baae0c90c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "THC Stage-A (TEST):   0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "== TEST (LR reranker) ==\n",
            "ROUGE  TOP: {'r1_f': 0.3118, 'r2_f': 0.1148, 'rl_f': 0.2129}\n",
            "ROUGE   LR: {'r1_f': 0.3199, 'r2_f': 0.1183, 'rl_f': 0.2236} | Δ: {'r1_f': 0.0081, 'r2_f': 0.0036, 'rl_f': 0.0107}\n",
            "ENT   TOP: {'TP': 1174, 'FP': 4076, 'FN': 2367, 'entP': 0.2236, 'entR': 0.3315, 'entF1': 0.2671}\n",
            "ENT    LR: {'TP': 1257, 'FP': 3875, 'FN': 2284, 'entP': 0.2449, 'entR': 0.355, 'entF1': 0.2899} | Δ: {'entP': 0.0213, 'entR': 0.0234, 'entF1': 0.0228}\n",
            "UCER  TOP: {'rate': 0.161, 'avg_core': 0.178}\n",
            "UCER   LR: {'rate': 0.027, 'avg_core': 0.036} | Δ rate: -0.134\n",
            "\n",
            "Saved:\n",
            "  /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/eval_HYBRID/hybrid_TEST_LR_20250827_072324/best_hybrid_lr_20250827_072324.csv \n",
            "  /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/eval_HYBRID/hybrid_TEST_LR_20250827_072324/frozen_config_lr_used_20250827_072324.json \n",
            "  /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/eval_HYBRID/hybrid_TEST_LR_20250827_072324/stageB_lr_weights_used_20250827_072324.json\n"
          ]
        }
      ],
      "source": [
        "# ====== Load frozen config + LR weights ======\n",
        "CFG_PATH = CFG_PATH       # <- or paste the printed path\n",
        "LR_W_PATH = LR_W_PATH     # <- or paste the printed path\n",
        "\n",
        "cfg = json.load(open(CFG_PATH))\n",
        "wts = json.load(open(LR_W_PATH))\n",
        "\n",
        "assert cfg[\"checkpoint\"] == CKPT_DIR, \"Checkpoint mismatch.\"\n",
        "MAX_SRC_LEN = cfg[\"max_src_len\"]\n",
        "CN2_LONG = cfg[\"pools\"][\"CN2_LONG\"]; CN_MED = cfg[\"pools\"][\"CN_MED\"]\n",
        "EA_ARGS  = cfg[\"entity_aware\"]\n",
        "M_KEEP   = cfg[\"thc_stageA\"][\"M\"]\n",
        "FAST_LITE= cfg.get(\"fast_lite\", True)\n",
        "FEAT_COLS= cfg[\"lr_features\"]\n",
        "\n",
        "# ====== Load TEST subset (fast) ======\n",
        "TEST_PATH = \"/content/drive/MyDrive/cleaned_outputs/naa/test_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "df_test = pd.read_csv(TEST_PATH)\n",
        "if \"highlights\" not in df_test.columns and \"reference\" in df_test.columns:\n",
        "    df_test[\"highlights\"] = df_test[\"reference\"].astype(str)\n",
        "\n",
        "N_TEST = 1000   # change to None for full test\n",
        "if N_TEST: df_test = df_test.head(N_TEST).copy()\n",
        "arts_t = df_test[\"article\"].astype(str).tolist()\n",
        "refs_t = df_test[\"highlights\"].astype(str).tolist()\n",
        "\n",
        "# Cache NER for articles only (UCER)\n",
        "ents_cache_build(df_test[\"article\"].astype(str).tolist())\n",
        "\n",
        "# ====== Generate beams fast on TEST ======\n",
        "dd_all_t = generate_both_pools_fast(arts_t, refs_t, batch_long=4, batch_med=8)\n",
        "\n",
        "# ====== THC Stage-A on TEST ======\n",
        "rowsA=[]\n",
        "for rid, grp in tqdm(list(dd_all_t.groupby(\"row_id\")), total=dd_all_t[\"row_id\"].nunique(), desc=\"THC Stage-A (TEST)\"):\n",
        "    art = grp.iloc[0][\"article\"]; tgt = adaptive_len_target(art)\n",
        "    L = grp[\"len\"].clip(lower=1).to_numpy()\n",
        "    ent_scores=[]; fails=[]; contigs=[]\n",
        "    for _,r in grp.iterrows():\n",
        "        s,f = strict_entity_score(r[\"article\"], r[\"summary\"]); c,_ = lcs_span_stats(r[\"article\"], r[\"summary\"])\n",
        "        ent_scores.append(s); fails.append(f); contigs.append(c)\n",
        "    ent_scores=np.array(ent_scores); fails=np.array(fails); contigs=np.array(contigs)\n",
        "    len_reward = np.exp(-((L - tgt)**2)/(2*(20.0**2)))\n",
        "    THC = (5.0*ent_scores) + (0.2*contigs) + (0.3*len_reward)\n",
        "    ok = grp.loc[~fails].copy(); ok[\"THC\"]=THC[~fails]\n",
        "    if ok.empty: ok=grp.copy(); ok[\"THC\"]=THC\n",
        "    rowsA.append(ok.sort_values(\"THC\", ascending=False).head(M_KEEP))\n",
        "stageA_t = pd.concat(rowsA, ignore_index=True)\n",
        "\n",
        "# ====== Score survivors with learned LR (rowwise minmax features) ======\n",
        "def score_group_linear(grp, weights):\n",
        "    art = grp.iloc[0][\"article\"]; lead = lead3(art); tgt = adaptive_len_target(art)\n",
        "    cands = grp[\"summary\"].tolist()\n",
        "    leadsim = np.array([rougeLsum(h, lead) for h in cands])\n",
        "    # centroid MBR\n",
        "    n=len(cands); mbr=np.zeros(n)\n",
        "    if n>1:\n",
        "        M=np.zeros((n,n))\n",
        "        for i in range(n):\n",
        "            for j in range(i+1,n):\n",
        "                s=rougeLsum(cands[i], cands[j]); M[i,j]=M[j,i]=s\n",
        "        mbr=M.mean(axis=1)\n",
        "    # rowwise minmax\n",
        "    def mm(v): return np.array(minmax_norm(np.array(v)))\n",
        "    feats = pd.DataFrame({\n",
        "        \"THC_norm\": mm(grp[\"THC\"].values),\n",
        "        \"LeadSim\": leadsim,\n",
        "        \"MBR\": mbr,\n",
        "        \"len_reward\": mm([math.exp(-((L-tgt)**2)/(2*(20.0**2))) for L in grp[\"len\"].tolist()]),\n",
        "        \"entity_score\": mm(grp[\"THC\"].values),\n",
        "        \"contig_lcs\": mm([lcs_span_stats(art, s)[0] for s in cands]),\n",
        "    })\n",
        "    s = weights[\"intercept\"] + sum(feats[c].values * weights[\"coefs\"][c] for c in FEAT_COLS)\n",
        "    k = int(np.argmax(s))\n",
        "    cand = grp.iloc[[k]].copy()\n",
        "\n",
        "    # UCER-safe fallback: if unsupported CORE exists, choose best safe among survivors\n",
        "    S=_ents_norm(art); H=_ents_norm(cand[\"summary\"].iloc[0])\n",
        "    uns=[(t,l) for (t,l) in (H-S) if l in CORE]\n",
        "    if uns:\n",
        "        safe_idx=[]\n",
        "        for i in range(len(grp)):\n",
        "            Hi=_ents_norm(grp.iloc[i][\"summary\"])\n",
        "            if not any(l in CORE for _,l in (Hi-S) if (Hi-S)):\n",
        "                safe_idx.append(i)\n",
        "        if safe_idx:\n",
        "            # pick the highest LR score among safe candidates\n",
        "            s_vals = np.array(s)\n",
        "            best_safe = int(max(safe_idx, key=lambda i: s_vals[i]))\n",
        "            cand = grp.iloc[[best_safe]].copy()\n",
        "    return cand\n",
        "\n",
        "best_lr = []\n",
        "for rid, grp in stageA_t.groupby(\"row_id\"):\n",
        "    best_lr.append(score_group_linear(grp, wts))\n",
        "best_lr = pd.concat(best_lr, ignore_index=True)\n",
        "\n",
        "# ====== Baseline TOP & evaluation ======\n",
        "# Baseline: best base_score across all candidates (if FAST_LITE=True, this is approximate; still fine for comparison)\n",
        "top_k_t = (dd_all_t.sort_values([\"row_id\",\"pool\",\"candidate_k\"]).groupby(\"row_id\", as_index=False).head(1)\n",
        "           [[\"row_id\",\"summary\"]].rename(columns={\"summary\":\"top_beam_summary\"}))\n",
        "\n",
        "ev = best_lr[[\"row_id\",\"article\",\"highlights\",\"summary\"]].rename(columns={\"summary\":\"hybrid_lr\"}) \\\n",
        "        .merge(top_k_t, on=\"row_id\", how=\"left\")\n",
        "\n",
        "R = ev[\"highlights\"].astype(str).tolist()\n",
        "H = ev[\"hybrid_lr\"].astype(str).tolist()\n",
        "T = ev[\"top_beam_summary\"].astype(str).tolist()\n",
        "A = ev[\"article\"].astype(str).tolist()\n",
        "\n",
        "# metrics\n",
        "def rouge_mean(R,H):\n",
        "    r1=r2=rl=0.0\n",
        "    for r,h in zip(R,H):\n",
        "        s=_SC.score(r,h); r1+=s[\"rouge1\"].fmeasure; r2+=s[\"rouge2\"].fmeasure; rl+=s[\"rougeLsum\"].fmeasure\n",
        "    n=len(R); return {\"r1_f\":r1/n,\"r2_f\":r2/n,\"rl_f\":rl/n}\n",
        "\n",
        "def entPRF(R,H):\n",
        "    TP=FP=FN=0\n",
        "    for r,h in zip(R,H):\n",
        "        R_=_ents_norm(r); H_=_ents_norm(h)\n",
        "        TP+=len(H_&R_); FP+=len(H_-R_); FN+=len(R_-H_)\n",
        "    P=TP/(TP+FP) if (TP+FP) else 0.0\n",
        "    Rc=TP/(TP+FN) if (TP+FN) else 0.0\n",
        "    F=2*P*Rc/(P+Rc) if (P+Rc) else 0.0\n",
        "    return {\"TP\":TP,\"FP\":FP,\"FN\":FN,\"entP\":P,\"entR\":Rc,\"entF1\":F}\n",
        "\n",
        "def UCER(arts, hyps):\n",
        "    rates=[]; counts=[]\n",
        "    for s,h in zip(arts, hyps):\n",
        "        S=_ents_norm(s); H=_ents_norm(h)\n",
        "        uns=[(t,l) for (t,l) in (H-S) if l in CORE]\n",
        "        rates.append(1 if uns else 0); counts.append(len(uns))\n",
        "    return float(np.mean(rates)), float(np.mean(counts))\n",
        "\n",
        "R_top = rouge_mean(R,T); R_h = rouge_mean(R,H)\n",
        "E_top = entPRF(R,T);     E_h = entPRF(R,H)\n",
        "u_top = UCER(A,T);       u_h = UCER(A,H)\n",
        "\n",
        "print(\"\\n== TEST (LR reranker) ==\")\n",
        "print(\"ROUGE  TOP:\", {k:round(v,4) for k,v in R_top.items()})\n",
        "print(\"ROUGE   LR:\", {k:round(v,4) for k,v in R_h.items()},\n",
        "      \"| Δ:\", {k: round(R_h[k]-R_top[k],4) for k in R_top})\n",
        "print(\"ENT   TOP:\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in E_top.items()})\n",
        "print(\"ENT    LR:\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in E_h.items()},\n",
        "      \"| Δ:\", {k: round(E_h[k]-E_top[k],4) for k in [\"entP\",\"entR\",\"entF1\"]})\n",
        "print(\"UCER  TOP:\", {\"rate\": round(u_top[0],4), \"avg_core\": round(u_top[1],4)})\n",
        "print(\"UCER   LR:\", {\"rate\": round(u_h[0],4),   \"avg_core\": round(u_h[1],4)},\n",
        "      \"| Δ rate:\", round(u_h[0]-u_top[0],4))\n",
        "\n",
        "# ====== Save outputs & cfg used ======\n",
        "STAMP_T = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "OUT_TEST = f\"{OUT_ROOT}/hybrid_TEST_LR_{STAMP_T}\"\n",
        "os.makedirs(OUT_TEST, exist_ok=True)\n",
        "best_out = f\"{OUT_TEST}/best_hybrid_lr_{STAMP_T}.csv\"\n",
        "save_csv(best_lr, best_out)\n",
        "cfg_copy = f\"{OUT_TEST}/frozen_config_lr_used_{STAMP_T}.json\"\n",
        "save_json(cfg, cfg_copy)\n",
        "wts_copy = f\"{OUT_TEST}/stageB_lr_weights_used_{STAMP_T}.json\"\n",
        "save_json(wts, wts_copy)\n",
        "print(\"\\nSaved:\\n \", best_out, \"\\n \", cfg_copy, \"\\n \", wts_copy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSS1nQDeTcPk",
        "outputId": "e0ab231f-bf5c-4655-f641-bb4ae97737e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Oracle-K RL: 0.3023 | TOP RL: 0.2129 | LR RL: 0.2236 | Recovered: 0.0107 of 0.0894 gap (12.0%)\n"
          ]
        }
      ],
      "source": [
        "# Oracle-K RL on current beams (dd_all_t must be in memory)\n",
        "from statistics import mean\n",
        "def oracle_rl(dd):\n",
        "    best=[]\n",
        "    for _,grp in dd.groupby(\"row_id\"):\n",
        "        ref = grp.iloc[0][\"highlights\"]\n",
        "        best.append(max(rougeLsum(ref,s) for s in grp[\"summary\"].tolist()))\n",
        "    return mean(best)\n",
        "\n",
        "oracle = oracle_rl(dd_all_t)\n",
        "top_rl = 0.2129  # from your printout\n",
        "lr_rl  = 0.2236  # from your printout\n",
        "print(f\"Oracle-K RL: {oracle:.4f} | TOP RL: {top_rl:.4f} | LR RL: {lr_rl:.4f} | \"\n",
        "      f\"Recovered: {(lr_rl-top_rl):.4f} of {(oracle-top_rl):.4f} gap \"\n",
        "      f\"({(lr_rl-top_rl)/(oracle-top_rl)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5cd1qO0ashr",
        "outputId": "2053419d-00f5-47fb-a215-35a1141e0d30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       THC | RL=0.2125 R1=0.3124 R2=0.1148 | EntF1=0.2779 | UCER=0.027\n",
            "ROUGE-mode | RL=0.2122 R1=0.3119 R2=0.1143 | EntF1=0.2797 | UCER=0.027\n",
            "        LR | RL=0.2236 R1=0.3199 R2=0.1183 | EntF1=0.2899 | UCER=0.027\n"
          ]
        }
      ],
      "source": [
        "# Needs: stageA_t (your THC survivors), wts (learned weights)\n",
        "def pick_THC(grp):\n",
        "    return grp.sort_values(\"THC\", ascending=False).head(1)\n",
        "\n",
        "def pick_ROUGE_mode(grp):\n",
        "    # 0.4*THC_norm + 0.4*LeadSim + 0.2*MBR (row-wise)\n",
        "    art = grp.iloc[0][\"article\"]; lead = lead3(art)\n",
        "    cands = grp[\"summary\"].tolist()\n",
        "    leadsim = np.array([rougeLsum(h, lead) for h in cands])\n",
        "    # centroid MBR\n",
        "    n=len(cands); mbr=np.zeros(n)\n",
        "    if n>1:\n",
        "        M=np.zeros((n,n))\n",
        "        for i in range(n):\n",
        "            for j in range(i+1,n):\n",
        "                s=rougeLsum(cands[i], cands[j]); M[i,j]=M[j,i]=s\n",
        "        mbr=M.mean(axis=1)\n",
        "    def mm(v):\n",
        "        v=np.array(v,float); lo,hi=v.min(),v.max()\n",
        "        return (np.zeros_like(v) if hi-lo<1e-8 else (v-lo)/(hi-lo))\n",
        "    score = 0.4*mm(grp[\"THC\"].values) + 0.4*leadsim + 0.2*mbr\n",
        "    k=int(np.argmax(score)); return grp.iloc[[k]]\n",
        "\n",
        "def pick_LR(grp, wts):\n",
        "    # same as in your LR cell, including UCER-safe fallback\n",
        "    return score_group_linear(grp, wts)\n",
        "\n",
        "def eval_selector(name, pick_fn):\n",
        "    picks=[];\n",
        "    for _,grp in stageA_t.groupby(\"row_id\"):\n",
        "        picks.append(pick_fn(grp))\n",
        "    pick_df = pd.concat(picks)\n",
        "    R = pick_df[\"highlights\"].astype(str).tolist()\n",
        "    H = pick_df[\"summary\"].astype(str).tolist()\n",
        "    A = pick_df[\"article\"].astype(str).tolist()\n",
        "    Rg = rouge_mean(R,H); E = entPRF(R,H); u = UCER(A,H)\n",
        "    print(f\"{name:>10} | RL={Rg['rl_f']:.4f} R1={Rg['r1_f']:.4f} R2={Rg['r2_f']:.4f} \"\n",
        "          f\"| EntF1={E['entF1']:.4f} | UCER={u[0]:.3f}\")\n",
        "    return dict(R=Rg, E=E, U=u)\n",
        "\n",
        "base_THC = eval_selector(\"THC\", pick_THC)\n",
        "base_RM  = eval_selector(\"ROUGE-mode\", pick_ROUGE_mode)\n",
        "base_LR  = eval_selector(\"LR\", lambda g: pick_LR(g, wts))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbyU721Yeb-E"
      },
      "source": [
        "# Hybrid 1 Final- TEST SET ON CHECKPOINT 6000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6btS0kx0bBE-",
        "outputId": "0c45780e-c9b4-4516-e555-d2b47f25c9e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Generate CN2_LONG...\n",
            "Generate CN_MED done. Total beams: 20000\n",
            "Stage-A select...\n",
            "Build features (VAL)...\n",
            "Learned weights: {'intercept': -0.22299345793236489, 'coefs': {'THC_norm': 0.22418841429135633, 'LeadSim': 0.8315700461663178, 'MBR': 0.01668284133054053, 'len_reward': 0.1592574290338416, 'contig_lcs_mm': -0.4679653050551055, 'base_norm_mm': -0.022765696053894592, 'ent_consensus': 0.36639369752617285, 'unsup_nonc_mm': -0.10629019790506306}}\n",
            "\n",
            "== VALIDATION (Hybrid1) ==\n",
            "ROUGE  TOP: {'r1_f': 0.3116, 'r2_f': 0.1184, 'rl_f': 0.2157}\n",
            "ROUGE   H1: {'r1_f': 0.3252, 'r2_f': 0.1233, 'rl_f': 0.2247} | Δ: {'r1_f': 0.0136, 'r2_f': 0.0048, 'rl_f': 0.009}\n",
            "ENT   TOP: {'TP': 1249, 'FP': 4126, 'FN': 2571, 'entP': 0.2324, 'entR': 0.327, 'entF1': 0.2717}\n",
            "ENT    H1: {'TP': 1303, 'FP': 3779, 'FN': 2517, 'entP': 0.2564, 'entR': 0.3411, 'entF1': 0.2927} | Δ: {'entP': 0.024, 'entR': 0.0141, 'entF1': 0.0211}\n",
            "UCER  TOP: {'rate': 0.179, 'avg_core': 0.202}\n",
            "UCER   H1: {'rate': 0.031, 'avg_core': 0.037} | Δ rate: -0.148\n",
            "\n",
            "Saved to Drive:\n",
            "  LR weights -> /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/Hybrid1/val_20250827_083245/lr_weights_h1_20250827_083245.json\n",
            "  Frozen cfg -> /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/Hybrid1/val_20250827_083245/frozen_config_h1_20250827_083245.json\n",
            "  VAL picks  -> /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/Hybrid1/val_20250827_083245/best_h1_val_20250827_083245.csv\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# H1-VAL: TRAIN HYBRID-1 ON VALIDATION & SAVE\n",
        "# =========================\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# ---------- PATHS ----------\n",
        "CKPT_DIR        = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000\"  # <- your finetuned checkpoint dir\n",
        "VALIDATION_PATH = \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "OUT_ROOT        = f\"{CKPT_DIR}/Hybrid1\"\n",
        "\n",
        "N_VAL = 1000\n",
        "SEED  = 0\n",
        "M_KEEP = 10              # Stage-A survivors per row\n",
        "NONCORE_PEN = 2.0        # THC soft penalty if any unsupported non-core present\n",
        "\n",
        "# ---------- ROBUST IMPORTS / ROUGE ----------\n",
        "import sys, subprocess, math, re, os, json, csv, time, random, pkgutil\n",
        "import numpy as np, pandas as pd, torch\n",
        "def ensure(mod, pip=None):\n",
        "    import importlib.util\n",
        "    if pip is None: pip = mod\n",
        "    if importlib.util.find_spec(mod) is None:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pip])\n",
        "\n",
        "ensure(\"rouge_score\", \"rouge-score==0.1.2\")\n",
        "ensure(\"scikit_learn\", \"scikit-learn\")\n",
        "ensure(\"spacy\", \"spacy\")\n",
        "from rouge_score import rouge_scorer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import spacy\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "except OSError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "\n",
        "# ---------- UTIL ----------\n",
        "def save_csv(df, path):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    tmp = path + \".tmp\"\n",
        "    df.to_csv(tmp, index=False, quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n",
        "    os.replace(tmp, path)\n",
        "def save_json(obj, path):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    with open(path,\"w\") as f: json.dump(obj, f, indent=2)\n",
        "\n",
        "def norm_gold(df):\n",
        "    if \"highlights\" not in df.columns and \"reference\" in df.columns:\n",
        "        df[\"highlights\"] = df[\"reference\"].astype(str)\n",
        "    if \"highlights\" not in df.columns:\n",
        "        raise KeyError(\"CSV needs 'highlights' or 'reference'.\")\n",
        "    return df\n",
        "\n",
        "# ---------- ROUGE (Lsumm & mean) ----------\n",
        "_SC = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\"], use_stemmer=True)\n",
        "def rougeLsum(a,b): return _SC.score(str(a), str(b))[\"rougeLsum\"].fmeasure\n",
        "def rouge_mean(R,H):\n",
        "    r1=r2=rl=0.0\n",
        "    for r,h in zip(R,H):\n",
        "        s=_SC.score(r,h); r1+=s[\"rouge1\"].fmeasure; r2+=s[\"rouge2\"].fmeasure; rl+=s[\"rougeLsum\"].fmeasure\n",
        "    n=len(R); return {\"r1_f\":r1/n,\"r2_f\":r2/n,\"rl_f\":rl/n}\n",
        "\n",
        "# ---------- NER / ENTITY HELPERS ----------\n",
        "KEEP   = {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"DATE\",\"TIME\",\"CARDINAL\",\"MONEY\",\"PERCENT\",\"QUANTITY\",\"ORDINAL\"}\n",
        "CORE   = {\"PERSON\",\"ORG\",\"GPE\"}\n",
        "NONCORE= {\"DATE\",\"QUANTITY\",\"MONEY\",\"PERCENT\",\"CARDINAL\",\"ORDINAL\"}\n",
        "\n",
        "_ENTS_CACHE={}\n",
        "def ents_cache_build(texts, batch_size=128):\n",
        "    uniq=list(dict.fromkeys(map(str,texts)))\n",
        "    for doc in nlp.pipe(uniq, batch_size=batch_size):\n",
        "        _ENTS_CACHE[doc.text] = {(\" \".join(e.text.strip().split()), e.label_) for e in doc.ents if e.label_ in KEEP}\n",
        "def _ents_norm(t):\n",
        "    t=str(t)\n",
        "    if t in _ENTS_CACHE: return _ENTS_CACHE[t]\n",
        "    d=nlp(t)\n",
        "    return {(\" \".join(e.text.strip().split()), e.label_) for e in d.ents if e.label_ in KEEP}\n",
        "\n",
        "def strict_entity_score(src, hyp):\n",
        "    S=_ents_norm(src); H=_ents_norm(hyp)\n",
        "    overlap=H&S; unsupported=H-S\n",
        "    hard_fail=any(lbl in CORE for _,lbl in unsupported)\n",
        "    score=sum(3 if lbl in CORE else 1 for _,lbl in overlap)\n",
        "    return score, hard_fail\n",
        "\n",
        "def unsupported_nonc_count(src, hyp):\n",
        "    S=_ents_norm(src); H=_ents_norm(hyp)\n",
        "    return sum(1 for _,lbl in (H-S) if lbl in NONCORE)\n",
        "\n",
        "# ---------- TEXT / LENGTH HELPERS ----------\n",
        "def sent_split(txt): return re.split(r'(?<=[.!?])\\s+', str(txt).strip())\n",
        "def lead3(text):\n",
        "    s=[t for t in sent_split(text) if t]\n",
        "    return \" \".join(s[:3])\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, LogitsProcessor, LogitsProcessorList\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "tok   = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR).to(\"cuda\" if torch.cuda.is_available() else \"cpu\").eval()\n",
        "\n",
        "# speed\n",
        "torch.set_grad_enabled(False)\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cuda.matmul.allow_tf32=True\n",
        "    try: torch.set_float32_matmul_precision(\"high\")\n",
        "    except: pass\n",
        "if hasattr(model, \"config\"): setattr(model.config, \"attn_implementation\", \"sdpa\")\n",
        "if hasattr(getattr(model, \"model\", None), \"config\"): setattr(model.model.config, \"attn_implementation\", \"sdpa\")\n",
        "if torch.cuda.is_available():\n",
        "    major=torch.cuda.get_device_capability()[0]\n",
        "    model.to(dtype=(torch.bfloat16 if major>=8 else torch.float16))\n",
        "\n",
        "from contextlib import contextmanager\n",
        "@contextmanager\n",
        "def fast_infer():\n",
        "    if torch.cuda.is_available():\n",
        "        with torch.inference_mode(), torch.amp.autocast(\"cuda\"):\n",
        "            yield\n",
        "    else:\n",
        "        with torch.inference_mode():\n",
        "            yield\n",
        "\n",
        "def toklen(s): return len(tok(str(s))[\"input_ids\"])\n",
        "def adaptive_len_target(src):\n",
        "    L = toklen(lead3(src))\n",
        "    return int(np.clip(round(0.9*L), 32, 60))\n",
        "\n",
        "def lcs_span_stats(src_text, hyp_text):\n",
        "    s=str(src_text).split(); h=str(hyp_text).split()\n",
        "    n,m=len(s),len(h); dp=[[0]*(m+1) for _ in range(n+1)]\n",
        "    for i in range(n-1,-1,-1):\n",
        "        si=s[i]; row=dp[i]; row1=dp[i+1]\n",
        "        for j in range(m-1,-1,-1):\n",
        "            row[j]=row1[j+1]+1 if si==h[j] else max(row[j+1], row1[j])\n",
        "    i=j=0; blocks=[]; cur=0\n",
        "    while i<n and j<m:\n",
        "        if s[i]==h[j]: cur+=1; i+=1; j+=1\n",
        "        else:\n",
        "            if cur>=2: blocks.append(cur); cur=0\n",
        "            if dp[i+1][j]>=dp[i][j+1]: i+=1\n",
        "            else: j+=1\n",
        "    if cur>=2: blocks.append(cur)\n",
        "    cont=sum(b-1 for b in blocks)\n",
        "    av=(sum(blocks)/len(blocks)) if blocks else 1.0\n",
        "    return cont, av\n",
        "\n",
        "# ---------- CopyNext maps & fast generation ----------\n",
        "MAX_SRC_LEN = 400\n",
        "FAST_LITE   = True\n",
        "\n",
        "CN2_LONG = dict(num_beams=10, num_return_sequences=10, num_beam_groups=5, diversity_penalty=0.3,\n",
        "                max_new_tokens=100, min_new_tokens=55, no_repeat_ngram_size=3, length_penalty=2.0,\n",
        "                early_stopping=True, return_dict_in_generate=True, output_scores=(not FAST_LITE))\n",
        "CN_MED   = dict(num_beams=10, num_return_sequences=10, num_beam_groups=5, diversity_penalty=0.3,\n",
        "                max_new_tokens=70,  min_new_tokens=35, no_repeat_ngram_size=3, length_penalty=1.6,\n",
        "                early_stopping=True, return_dict_in_generate=True, output_scores=(not FAST_LITE))\n",
        "\n",
        "EA_ARGS  = dict(gamma=0.4, gamma_entity=1.5, max_span=6)\n",
        "\n",
        "def pretokenize_articles(articles):\n",
        "    enc = tok(articles, max_length=MAX_SRC_LEN, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "    return {k: v.to(model.device) for k,v in enc.items()}\n",
        "\n",
        "class EntityAwareSpanProcessor(LogitsProcessor):\n",
        "    def __init__(self, maps_list, num_beams, gamma=0.4, gamma_entity=1.5, max_span=6):\n",
        "        self.maps_list=maps_list; self.num_beams=int(num_beams)\n",
        "        self.gamma=float(gamma); self.gamma_entity=float(gamma_entity); self.max_span=int(max_span)\n",
        "    def __call__(self, input_ids, scores):\n",
        "        Bx=input_ids.size(0)\n",
        "        for b in range(Bx):\n",
        "            sample=b//self.num_beams\n",
        "            maps=self.maps_list[sample]; seq=input_ids[b].tolist()\n",
        "            for k in range(min(self.max_span, len(seq)), 0, -1):\n",
        "                mp=maps[k-1]; key=tuple(seq[-k:])\n",
        "                if key in mp:\n",
        "                    nxt, is_ent = mp[key]\n",
        "                    scores[b, nxt] = scores[b, nxt] + (self.gamma_entity if is_ent else self.gamma)\n",
        "                    break\n",
        "        return scores\n",
        "\n",
        "def build_copy_maps(texts, max_span=EA_ARGS[\"max_span\"]):\n",
        "    maps_all=[]\n",
        "    docs = list(nlp.pipe(texts, batch_size=128))\n",
        "    enc  = tok(texts, max_length=MAX_SRC_LEN, truncation=True, return_offsets_mapping=True)\n",
        "    for i,t in enumerate(texts):\n",
        "        ids  = enc[\"input_ids\"][i]; offs = enc[\"offset_mapping\"][i]\n",
        "        ent_spans = [(e.start_char, e.end_char) for e in docs[i].ents if e.label_ in KEEP]\n",
        "        def in_ent(off):\n",
        "            a,b = off\n",
        "            if a is None: return False\n",
        "            for s,e in ent_spans:\n",
        "                if a>=s and b<=e: return True\n",
        "            return False\n",
        "        maps=[{} for _ in range(max_span)]\n",
        "        n=len(ids)\n",
        "        for k in range(1, max_span+1):\n",
        "            mp=maps[k-1]\n",
        "            for j in range(0, n-k):\n",
        "                key=tuple(ids[j:j+k]); nxt=ids[j+k]\n",
        "                is_ent = in_ent(offs[j+k]) if j+k<len(offs) else False\n",
        "                if key not in mp: mp[key]=(nxt, is_ent)\n",
        "        maps_all.append(maps)\n",
        "    return maps_all\n",
        "\n",
        "def run_pool_fast(enc_all, start, end, maps_all, decode_args, pool_name, articles, references):\n",
        "    rows=[]\n",
        "    enc = {k: v[start:end] for k,v in enc_all.items()}\n",
        "    proc = EntityAwareSpanProcessor(maps_all[start:end],\n",
        "                                    num_beams=decode_args[\"num_beams\"],\n",
        "                                    gamma=EA_ARGS[\"gamma\"], gamma_entity=EA_ARGS[\"gamma_entity\"],\n",
        "                                    max_span=EA_ARGS[\"max_span\"])\n",
        "    with fast_infer():\n",
        "        out = model.generate(**enc, **decode_args,\n",
        "                             logits_processor=LogitsProcessorList([proc]),\n",
        "                             trust_remote_code=True)\n",
        "    K=decode_args[\"num_return_sequences\"]; bs=end-start\n",
        "    seqs = out.sequences.view(bs, K, -1)\n",
        "    scores = getattr(out, \"sequences_scores\", None)\n",
        "    for bi in range(bs):\n",
        "        art=articles[start+bi]; ref=references[start+bi]\n",
        "        for k in range(K):\n",
        "            ids=seqs[bi,k]\n",
        "            hyp=tok.decode(ids, skip_special_tokens=True)\n",
        "            rows.append({\"row_id\": start+bi, \"pool\": pool_name, \"candidate_k\": k,\n",
        "                         \"article\": art, \"highlights\": ref, \"summary\": hyp,\n",
        "                         \"len\": max(int(ids.size(0)-1),1),\n",
        "                         \"base_score\": float(scores[bi*K+k].item()) if scores is not None else 0.0})\n",
        "    return rows\n",
        "\n",
        "def generate_both_pools_fast(articles, references, batch_long=4, batch_med=8):\n",
        "    enc_all   = pretokenize_articles(articles)\n",
        "    maps_all  = build_copy_maps(articles)\n",
        "    rows=[]\n",
        "    for s in range(0, len(articles), batch_long):\n",
        "        e=min(len(articles), s+batch_long)\n",
        "        rows += run_pool_fast(enc_all, s, e, maps_all, CN2_LONG, \"CN2_LONG\", articles, references)\n",
        "    for s in range(0, len(articles), batch_med):\n",
        "        e=min(len(articles), s+batch_med)\n",
        "        rows += run_pool_fast(enc_all, s, e, maps_all, CN_MED, \"CN_MED\", articles, references)\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# ---------- STAGE-A (THC gate + soft numeric/date penalty) ----------\n",
        "def stageA_select(dd_all, M=10):\n",
        "    rows=[]\n",
        "    for rid, grp in dd_all.groupby(\"row_id\"):\n",
        "        art = grp.iloc[0][\"article\"]; tgt = adaptive_len_target(art)\n",
        "        ent_scores=[]; fails=[]; contigs=[]; nonc=[]; lens=[]\n",
        "        for _,r in grp.iterrows():\n",
        "            s,f = strict_entity_score(r[\"article\"], r[\"summary\"])\n",
        "            c,_ = lcs_span_stats(r[\"article\"], r[\"summary\"])\n",
        "            u   = unsupported_nonc_count(r[\"article\"], r[\"summary\"])\n",
        "            ent_scores.append(s); fails.append(f); contigs.append(c); nonc.append(u); lens.append(max(int(r[\"len\"]),1))\n",
        "        ent_scores=np.array(ent_scores); fails=np.array(fails); contigs=np.array(contigs); nonc=np.array(nonc); L=np.array(lens)\n",
        "        len_reward = np.exp(-((L - tgt)**2)/(2*(20.0**2)))\n",
        "        penalty = (nonc>0).astype(float) * NONCORE_PEN\n",
        "        THC = (5.0*ent_scores) + (0.2*contigs) + (0.3*len_reward) - penalty\n",
        "        ok = grp.copy()\n",
        "        ok[\"THC\"]=THC; ok[\"hard_fail\"]=fails; ok[\"unsup_nonc\"]=nonc\n",
        "        ok = ok.loc[~ok[\"hard_fail\"]].copy()\n",
        "        if ok.empty:\n",
        "            ok = grp.copy(); ok[\"THC\"]=THC; ok[\"unsup_nonc\"]=nonc\n",
        "        rows.append(ok.sort_values(\"THC\", ascending=False).head(M))\n",
        "    return pd.concat(rows, ignore_index=True)\n",
        "\n",
        "# ---------- survivor-only base_norm ----------\n",
        "def add_base_norm(stageA_df, batch_size=16):\n",
        "    out = np.zeros(len(stageA_df), dtype=np.float32)\n",
        "    idxs = stageA_df.index.to_list()\n",
        "    for s in range(0, len(idxs), batch_size):\n",
        "        ii = idxs[s:s+batch_size]\n",
        "        sub = stageA_df.loc[ii]\n",
        "        enc = tok(sub[\"article\"].tolist(), max_length=MAX_SRC_LEN, truncation=True, padding=True, return_tensors=\"pt\").to(model.device)\n",
        "        dec = tok(sub[\"summary\"].tolist(), max_length=256, truncation=True, padding=True, return_tensors=\"pt\").to(model.device)\n",
        "        with torch.inference_mode(), torch.amp.autocast(\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "            out_logits = model(**enc, labels=dec[\"input_ids\"], return_dict=True).logits.float()\n",
        "        shift_logits = out_logits[:, :-1, :]\n",
        "        shift_labels = dec[\"input_ids\"][:, 1:]\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=tok.pad_token_id, reduction=\"none\")\n",
        "        tok_loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.reshape(-1))\n",
        "        tok_loss = tok_loss.view(shift_labels.size(0), -1)\n",
        "        valid = (shift_labels != tok.pad_token_id).float()\n",
        "        loss_per = (tok_loss * valid).sum(dim=1) / (valid.sum(dim=1).clamp_min(1.0))\n",
        "        logp_per = -loss_per.detach().cpu().numpy()\n",
        "        L = sub[\"len\"].to_numpy().clip(min=1)\n",
        "        out[s:s+len(ii)] = logp_per / (L ** 0.3)\n",
        "    stageA_df[\"base_norm\"] = out\n",
        "    return stageA_df\n",
        "\n",
        "# ---------- group features for LR ----------\n",
        "def minmax(v):\n",
        "    v=np.array(v, float); lo,hi=v.min(),v.max()\n",
        "    return np.zeros_like(v) if hi-lo<1e-8 else (v-lo)/(hi-lo)\n",
        "\n",
        "def ent_consensus(cands):\n",
        "    E=[_ents_norm(x) for x in cands]\n",
        "    n=len(E); sc=np.zeros(n)\n",
        "    for i in range(n):\n",
        "        js=[]\n",
        "        for j in range(n):\n",
        "            if i==j: continue\n",
        "            u=E[i]|E[j]; inter=E[i]&E[j]\n",
        "            js.append(len(inter)/max(len(u),1) if u else 0.0)\n",
        "        sc[i]=np.mean(js) if js else 0.0\n",
        "    return sc\n",
        "\n",
        "def build_features(stageA_df, with_labels=True):\n",
        "    feat_rows=[]\n",
        "    for rid, grp in stageA_df.groupby(\"row_id\"):\n",
        "        art = grp.iloc[0][\"article\"]; ref = grp.iloc[0][\"highlights\"]; lead = lead3(art); tgt = adaptive_len_target(art)\n",
        "        cands = grp[\"summary\"].tolist()\n",
        "        leadsim = np.array([rougeLsum(h, lead) for h in cands])\n",
        "        # centroid MBR\n",
        "        n=len(cands); mbr=np.zeros(n)\n",
        "        if n>1:\n",
        "            M=np.zeros((n,n))\n",
        "            for i in range(n):\n",
        "                for j in range(i+1,n):\n",
        "                    s=rougeLsum(cands[i], cands[j]); M[i,j]=M[j,i]=s\n",
        "            mbr=M.mean(axis=1)\n",
        "        feats = pd.DataFrame({\n",
        "            \"row_id\": grp[\"row_id\"].values,\n",
        "            \"summary\": grp[\"summary\"].values,\n",
        "            \"THC_norm\": minmax(grp[\"THC\"].values),\n",
        "            \"LeadSim\": leadsim,\n",
        "            \"MBR\": mbr,\n",
        "            \"len_reward\": minmax([math.exp(-((L-tgt)**2)/(2*(20.0**2))) for L in grp[\"len\"].tolist()]),\n",
        "            \"contig_lcs_mm\": minmax([lcs_span_stats(art, s)[0] for s in cands]),\n",
        "            \"base_norm_mm\": minmax(grp[\"base_norm\"].values),\n",
        "            \"ent_consensus\": ent_consensus(cands),\n",
        "            \"unsup_nonc_mm\": minmax(grp[\"unsup_nonc\"].values.astype(float)),\n",
        "        })\n",
        "        if with_labels:\n",
        "            feats[\"rl_ref\"] = [rougeLsum(ref, h) for h in cands]\n",
        "        feat_rows.append(feats)\n",
        "    return pd.concat(feat_rows, ignore_index=True)\n",
        "\n",
        "# ---------- load data, generate, Stage-A, features, train LR ----------\n",
        "df_val = norm_gold(pd.read_csv(VALIDATION_PATH)).head(N_VAL).copy()\n",
        "arts = df_val[\"article\"].astype(str).tolist()\n",
        "refs = df_val[\"highlights\"].astype(str).tolist()\n",
        "ents_cache_build(df_val[\"article\"].astype(str).tolist())\n",
        "\n",
        "print(\"Generate CN2_LONG...\")\n",
        "dd_all = generate_both_pools_fast(arts, refs, batch_long=4, batch_med=8)\n",
        "print(\"Generate CN_MED done. Total beams:\", len(dd_all))\n",
        "\n",
        "print(\"Stage-A select...\")\n",
        "stageA = stageA_select(dd_all, M=M_KEEP)\n",
        "stageA = add_base_norm(stageA)\n",
        "\n",
        "print(\"Build features (VAL)...\")\n",
        "F_val = build_features(stageA, with_labels=True)\n",
        "\n",
        "# Pairwise logistic on VAL\n",
        "cols = [\"THC_norm\",\"LeadSim\",\"MBR\",\"len_reward\",\"contig_lcs_mm\",\"base_norm_mm\",\"ent_consensus\",\"unsup_nonc_mm\"]\n",
        "pairs_X=[]; pairs_y=[]\n",
        "for rid, g in F_val.groupby(\"row_id\"):\n",
        "    G=g.reset_index(drop=True); m=len(G)\n",
        "    for i in range(m):\n",
        "        for j in range(i+1,m):\n",
        "            yi = G.loc[i,\"rl_ref\"]; yj = G.loc[j,\"rl_ref\"]\n",
        "            if yi==yj: continue\n",
        "            x = (G.loc[i, cols].values.astype(float) - G.loc[j, cols].values.astype(float))\n",
        "            y = 1 if yi>yj else 0\n",
        "            pairs_X.append(x); pairs_y.append(y)\n",
        "pairs_X=np.stack(pairs_X); pairs_y=np.array(pairs_y)\n",
        "\n",
        "clf = LogisticRegression(solver=\"liblinear\", max_iter=1000, random_state=SEED)\n",
        "clf.fit(pairs_X, pairs_y)\n",
        "weights = dict(intercept=float(clf.intercept_[0]), coefs={c: float(w) for c,w in zip(cols, clf.coef_[0].tolist())})\n",
        "print(\"Learned weights:\", weights)\n",
        "\n",
        "# VAL evaluation vs TOP (beam-0 preference CN2_LONG)\n",
        "pool_order = {\"CN2_LONG\": 0, \"CN_MED\": 1}\n",
        "dd_all[\"_pool_ord\"] = dd_all[\"pool\"].map(pool_order).fillna(9)\n",
        "top_k0 = (dd_all.query(\"candidate_k==0\")\n",
        "                .sort_values([\"row_id\",\"_pool_ord\"])\n",
        "                .groupby(\"row_id\", as_index=False).head(1)\n",
        "                [[\"row_id\",\"summary\"]].rename(columns={\"summary\":\"top_beam_summary\"}))\n",
        "\n",
        "def score_group_linear(grp, w):\n",
        "    art = grp.iloc[0][\"article\"]; lead = lead3(art); tgt = adaptive_len_target(art)\n",
        "    cands = grp[\"summary\"].tolist()\n",
        "    leadsim = np.array([rougeLsum(h, lead) for h in cands])\n",
        "    n=len(cands); mbr=np.zeros(n)\n",
        "    if n>1:\n",
        "        M=np.zeros((n,n))\n",
        "        for i in range(n):\n",
        "            for j in range(i+1,n):\n",
        "                s=rougeLsum(cands[i], cands[j]); M[i,j]=M[j,i]=s\n",
        "        mbr=M.mean(axis=1)\n",
        "    feats = pd.DataFrame({\n",
        "        \"THC_norm\": minmax(grp[\"THC\"].values),\n",
        "        \"LeadSim\": leadsim,\n",
        "        \"MBR\": mbr,\n",
        "        \"len_reward\": minmax([math.exp(-((L-tgt)**2)/(2*(20.0**2))) for L in grp[\"len\"].tolist()]),\n",
        "        \"contig_lcs_mm\": minmax([lcs_span_stats(art, s)[0] for s in cands]),\n",
        "        \"base_norm_mm\": minmax(grp[\"base_norm\"].values),\n",
        "        \"ent_consensus\": ent_consensus(cands),\n",
        "        \"unsup_nonc_mm\": minmax(grp[\"unsup_nonc\"].values.astype(float)),\n",
        "    })\n",
        "    cols=list(w[\"coefs\"].keys())\n",
        "    s = w[\"intercept\"] + sum(feats[c].values * w[\"coefs\"][c] for c in cols)\n",
        "    k=int(np.argmax(s))\n",
        "    cand = grp.iloc[[k]].copy()\n",
        "    # UCER-safe fallback (CORE)\n",
        "    S=_ents_norm(art); H=_ents_norm(cand[\"summary\"].iloc[0])\n",
        "    if any(lbl in CORE for _,lbl in (H-S)):\n",
        "        safe_idx=[]\n",
        "        for i in range(len(grp)):\n",
        "            Hi=_ents_norm(grp.iloc[i][\"summary\"])\n",
        "            if not any(lbl in CORE for _,lbl in (Hi-S)):\n",
        "                safe_idx.append(i)\n",
        "        if safe_idx:\n",
        "            s_vals=np.array(s)\n",
        "            k=int(max(safe_idx, key=lambda i: s_vals[i]))\n",
        "            cand=grp.iloc[[k]].copy()\n",
        "    return cand\n",
        "\n",
        "best_lr=[]\n",
        "for rid, grp in stageA.groupby(\"row_id\"):\n",
        "    best_lr.append(score_group_linear(grp, weights))\n",
        "best_lr = pd.concat(best_lr, ignore_index=True)\n",
        "\n",
        "ev = best_lr[[\"row_id\",\"article\",\"highlights\",\"summary\"]].rename(columns={\"summary\":\"hybrid_h1\"}) \\\n",
        "        .merge(top_k0, on=\"row_id\", how=\"left\")\n",
        "\n",
        "R = ev[\"highlights\"].astype(str).tolist()\n",
        "H = ev[\"hybrid_h1\"].astype(str).tolist()\n",
        "T = ev[\"top_beam_summary\"].astype(str).tolist()\n",
        "A = ev[\"article\"].astype(str).tolist()\n",
        "\n",
        "def entPRF(R,H):\n",
        "    TP=FP=FN=0\n",
        "    for r,h in zip(R,H):\n",
        "        R_=_ents_norm(r); H_=_ents_norm(h)\n",
        "        TP+=len(H_&R_); FP+=len(H_-R_); FN+=len(R_-H_)\n",
        "    P=TP/(TP+FP) if (TP+FP) else 0.0\n",
        "    Rc=TP/(TP+FN) if (TP+FN) else 0.0\n",
        "    F=2*P*Rc/(P+Rc) if (P+Rc) else 0.0\n",
        "    return {\"TP\":TP,\"FP\":FP,\"FN\":FN,\"entP\":P,\"entR\":Rc,\"entF1\":F}\n",
        "\n",
        "def UCER(arts, hyps):\n",
        "    rates=[]; counts=[]\n",
        "    for s,h in zip(arts, hyps):\n",
        "        S=_ents_norm(s); H=_ents_norm(h)\n",
        "        bad=[(t,l) for (t,l) in (H-S) if l in CORE]\n",
        "        rates.append(1 if bad else 0); counts.append(len(bad))\n",
        "    return float(np.mean(rates)), float(np.mean(counts))\n",
        "\n",
        "R_top = rouge_mean(R,T); R_lr = rouge_mean(R,H)\n",
        "E_top = entPRF(R,T);     E_lr = entPRF(R,H)\n",
        "u_top = UCER(A,T);       u_lr = UCER(A,H)\n",
        "\n",
        "print(\"\\n== VALIDATION (Hybrid1) ==\")\n",
        "print(\"ROUGE  TOP:\", {k:round(v,4) for k,v in R_top.items()})\n",
        "print(\"ROUGE   H1:\", {k:round(v,4) for k,v in R_lr.items()},\n",
        "      \"| Δ:\", {k: round(R_lr[k]-R_top[k],4) for k in R_top})\n",
        "print(\"ENT   TOP:\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in E_top.items()})\n",
        "print(\"ENT    H1:\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in E_lr.items()},\n",
        "      \"| Δ:\", {k: round(E_lr[k]-E_top[k],4) for k in [\"entP\",\"entR\",\"entF1\"]})\n",
        "print(\"UCER  TOP:\", {\"rate\": round(u_top[0],4), \"avg_core\": round(u_top[1],4)})\n",
        "print(\"UCER   H1:\", {\"rate\": round(u_lr[0],4),  \"avg_core\": round(u_lr[1],4)},\n",
        "      \"| Δ rate:\", round(u_lr[0]-u_top[0],4))\n",
        "\n",
        "# ---------- SAVE FROZEN CONFIG + WEIGHTS + VAL PICKS ----------\n",
        "STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "OUT_VAL = f\"{OUT_ROOT}/val_{STAMP}\"\n",
        "os.makedirs(OUT_VAL, exist_ok=True)\n",
        "\n",
        "LR_W_PATH = f\"{OUT_VAL}/lr_weights_h1_{STAMP}.json\"\n",
        "CFG_PATH  = f\"{OUT_VAL}/frozen_config_h1_{STAMP}.json\"\n",
        "VAL_OUT   = f\"{OUT_VAL}/best_h1_val_{STAMP}.csv\"\n",
        "\n",
        "save_json(weights, LR_W_PATH)\n",
        "FROZEN_CFG = dict(\n",
        "    checkpoint=CKPT_DIR, max_src_len=MAX_SRC_LEN,\n",
        "    pools=dict(CN2_LONG=CN2_LONG, CN_MED=CN_MED),\n",
        "    entity_aware=EA_ARGS,\n",
        "    stageA=dict(M_keep=M_KEEP, noncore_pen=NONCORE_PEN, len_sigma=20.0,\n",
        "                formula=\"5*entity + 0.2*contig_lcs + 0.3*len_reward - 1{unsup_noncore}*pen\",\n",
        "                hard_fail_core=True),\n",
        "    features=cols,\n",
        "    fast_lite=FAST_LITE,\n",
        "    seed=SEED\n",
        ")\n",
        "save_json(FROZEN_CFG, CFG_PATH)\n",
        "save_csv(best_lr, VAL_OUT)\n",
        "\n",
        "print(\"\\nSaved to Drive:\")\n",
        "print(\"  LR weights ->\", LR_W_PATH)\n",
        "print(\"  Frozen cfg ->\", CFG_PATH)\n",
        "print(\"  VAL picks  ->\", VAL_OUT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F359iE-4ghNz",
        "outputId": "027cce64-1671-4582-c7f2-3a44f26da311"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generate CN2_LONG (TEST)...\n",
            "Done. Total beams: 20000\n",
            "Stage-A select (TEST)...\n",
            "\n",
            "== TEST (Hybrid1) ==\n",
            "ROUGE  TOP: {'r1_f': 0.3118, 'r2_f': 0.1148, 'rl_f': 0.2129}\n",
            "ROUGE   H1: {'r1_f': 0.3211, 'r2_f': 0.1184, 'rl_f': 0.2241} | Δ: {'r1_f': 0.0093, 'r2_f': 0.0037, 'rl_f': 0.0112}\n",
            "ENT   TOP: {'TP': 1174, 'FP': 4076, 'FN': 2367, 'entP': 0.2236, 'entR': 0.3315, 'entF1': 0.2671}\n",
            "ENT    H1: {'TP': 1237, 'FP': 3736, 'FN': 2304, 'entP': 0.2487, 'entR': 0.3493, 'entF1': 0.2906} | Δ: {'entP': 0.0251, 'entR': 0.0178, 'entF1': 0.0235}\n",
            "UCER  TOP: {'rate': 0.161, 'avg_core': 0.178}\n",
            "UCER   H1: {'rate': 0.027, 'avg_core': 0.037} | Δ rate: -0.134\n",
            "\n",
            "Saved to Drive:\n",
            "  TEST picks  -> /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/Hybrid1/test_20250827_090855/best_h1_test_20250827_090855.csv\n",
            "  Frozen cfg  -> /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/Hybrid1/test_20250827_090855/frozen_config_h1_used_20250827_090855.json\n",
            "  LR weights  -> /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/Hybrid1/test_20250827_090855/lr_weights_h1_used_20250827_090855.json\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# H1-TEST: APPLY HYBRID-1 ON TEST\n",
        "# =========================\n",
        "\n",
        "# ---- Paste the two paths printed by H1-VAL ----\n",
        "LR_W_PATH = LR_W_PATH  # e.g. \"/content/drive/.../Hybrid1/val_YYYYmmdd_HHMMSS/lr_weights_h1_....json\"\n",
        "CFG_PATH  = CFG_PATH   # e.g. \"/content/drive/.../Hybrid1/val_YYYYmmdd_HHMMSS/frozen_config_h1_....json\"\n",
        "\n",
        "# ---- Test data path ----\n",
        "TEST_PATH = \"/content/drive/MyDrive/cleaned_outputs/naa/test_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "N_TEST = 1000  # set None for full\n",
        "\n",
        "# ---- Reload minimal helpers from Cell 1 context (assumes same notebook) ----\n",
        "# (If you restart the notebook, run Cell 1 again up to helper definitions.)\n",
        "\n",
        "cfg = json.load(open(CFG_PATH))\n",
        "wts = json.load(open(LR_W_PATH))\n",
        "assert cfg[\"checkpoint\"] == CKPT_DIR, \"Checkpoint mismatch.\"\n",
        "\n",
        "MAX_SRC_LEN = cfg[\"max_src_len\"]\n",
        "CN2_LONG = cfg[\"pools\"][\"CN2_LONG\"]; CN_MED = cfg[\"pools\"][\"CN_MED\"]\n",
        "EA_ARGS  = cfg[\"entity_aware\"]; M_KEEP = cfg[\"stageA\"][\"M_keep\"]\n",
        "NONCORE_PEN = cfg[\"stageA\"][\"noncore_pen\"]; FAST_LITE = cfg.get(\"fast_lite\", True)\n",
        "\n",
        "# ---- Load TEST ----\n",
        "df_test = pd.read_csv(TEST_PATH)\n",
        "df_test = norm_gold(df_test).copy()\n",
        "if N_TEST: df_test = df_test.head(N_TEST).copy()\n",
        "arts_t = df_test[\"article\"].astype(str).tolist()\n",
        "refs_t = df_test[\"highlights\"].astype(str).tolist()\n",
        "ents_cache_build(df_test[\"article\"].astype(str).tolist())\n",
        "\n",
        "# ---- Generate beams (fast) ----\n",
        "print(\"Generate CN2_LONG (TEST)...\")\n",
        "dd_all_t = generate_both_pools_fast(arts_t, refs_t, batch_long=4, batch_med=8)\n",
        "print(\"Done. Total beams:\", len(dd_all_t))\n",
        "\n",
        "# ---- Stage-A (TEST) ----\n",
        "print(\"Stage-A select (TEST)...\")\n",
        "stageA_t = stageA_select(dd_all_t, M=M_KEEP)\n",
        "stageA_t = add_base_norm(stageA_t)\n",
        "\n",
        "# ---- Features & LR scoring (TEST) ----\n",
        "def score_group_linear(grp, w):\n",
        "    art = grp.iloc[0][\"article\"]; lead = lead3(art); tgt = adaptive_len_target(art)\n",
        "    cands = grp[\"summary\"].tolist()\n",
        "    # features consistent with training:\n",
        "    leadsim = np.array([rougeLsum(h, lead) for h in cands])\n",
        "    n=len(cands); mbr=np.zeros(n)\n",
        "    if n>1:\n",
        "        M=np.zeros((n,n))\n",
        "        for i in range(n):\n",
        "            for j in range(i+1,n):\n",
        "                s=rougeLsum(cands[i], cands[j]); M[i,j]=M[j,i]=s\n",
        "        mbr=M.mean(axis=1)\n",
        "    feats = pd.DataFrame({\n",
        "        \"THC_norm\": minmax(grp[\"THC\"].values),\n",
        "        \"LeadSim\": leadsim,\n",
        "        \"MBR\": mbr,\n",
        "        \"len_reward\": minmax([math.exp(-((L-tgt)**2)/(2*(20.0**2))) for L in grp[\"len\"].tolist()]),\n",
        "        \"contig_lcs_mm\": minmax([lcs_span_stats(art, s)[0] for s in cands]),\n",
        "        \"base_norm_mm\": minmax(grp[\"base_norm\"].values),\n",
        "        \"ent_consensus\": ent_consensus(cands),\n",
        "        \"unsup_nonc_mm\": minmax(grp[\"unsup_nonc\"].values.astype(float)),\n",
        "    })\n",
        "    cols=list(w[\"coefs\"].keys())\n",
        "    s = w[\"intercept\"] + sum(feats[c].values * w[\"coefs\"][c] for c in cols)\n",
        "    k=int(np.argmax(s))\n",
        "    cand = grp.iloc[[k]].copy()\n",
        "    # UCER-safe fallback (CORE)\n",
        "    S=_ents_norm(art); H=_ents_norm(cand[\"summary\"].iloc[0])\n",
        "    if any(lbl in CORE for _,lbl in (H-S)):\n",
        "        safe_idx=[]\n",
        "        for i in range(len(grp)):\n",
        "            Hi=_ents_norm(grp.iloc[i][\"summary\"])\n",
        "            if not any(lbl in CORE for _,lbl in (Hi-S)):\n",
        "                safe_idx.append(i)\n",
        "        if safe_idx:\n",
        "            s_vals=np.array(s)\n",
        "            k=int(max(safe_idx, key=lambda i: s_vals[i]))\n",
        "            cand=grp.iloc[[k]].copy()\n",
        "    return cand\n",
        "\n",
        "best_lr_t=[]\n",
        "for rid, grp in stageA_t.groupby(\"row_id\"):\n",
        "    best_lr_t.append(score_group_linear(grp, wts))\n",
        "best_lr_t = pd.concat(best_lr_t, ignore_index=True)\n",
        "\n",
        "# ---- Baseline TOP (beam-0 preferring CN2_LONG) ----\n",
        "pool_order = {\"CN2_LONG\": 0, \"CN_MED\": 1}\n",
        "dd_all_t[\"_pool_ord\"] = dd_all_t[\"pool\"].map(pool_order).fillna(9)\n",
        "top_k0_t = (dd_all_t.query(\"candidate_k==0\")\n",
        "                  .sort_values([\"row_id\",\"_pool_ord\"])\n",
        "                  .groupby(\"row_id\", as_index=False).head(1)\n",
        "                  [[\"row_id\",\"summary\"]].rename(columns={\"summary\":\"top_beam_summary\"}))\n",
        "\n",
        "# ---- Metrics ----\n",
        "ev_t = best_lr_t[[\"row_id\",\"article\",\"highlights\",\"summary\"]].rename(columns={\"summary\":\"hybrid_h1\"}) \\\n",
        "          .merge(top_k0_t, on=\"row_id\", how=\"left\")\n",
        "\n",
        "R = ev_t[\"highlights\"].astype(str).tolist()\n",
        "H = ev_t[\"hybrid_h1\"].astype(str).tolist()\n",
        "T = ev_t[\"top_beam_summary\"].astype(str).tolist()\n",
        "A = ev_t[\"article\"].astype(str).tolist()\n",
        "\n",
        "R_top = rouge_mean(R,T); R_h = rouge_mean(R,H)\n",
        "\n",
        "def entPRF(R,H):\n",
        "    TP=FP=FN=0\n",
        "    for r,h in zip(R,H):\n",
        "        R_=_ents_norm(r); H_=_ents_norm(h)\n",
        "        TP+=len(H_&R_); FP+=len(H_-R_); FN+=len(R_-H_)\n",
        "    P=TP/(TP+FP) if (TP+FP) else 0.0\n",
        "    Rc=TP/(TP+FN) if (TP+FN) else 0.0\n",
        "    F=2*P*Rc/(P+Rc) if (P+Rc) else 0.0\n",
        "    return {\"TP\":TP,\"FP\":FP,\"FN\":FN,\"entP\":P,\"entR\":Rc,\"entF1\":F}\n",
        "\n",
        "def UCER(arts, hyps):\n",
        "    rates=[]; counts=[]\n",
        "    for s,h in zip(arts, hyps):\n",
        "        S=_ents_norm(s); H=_ents_norm(h)\n",
        "        bad=[(t,l) for (t,l) in (H-S) if l in CORE]\n",
        "        rates.append(1 if bad else 0); counts.append(len(bad))\n",
        "    return float(np.mean(rates)), float(np.mean(counts))\n",
        "\n",
        "E_top = entPRF(R,T);     E_h = entPRF(R,H)\n",
        "u_top = UCER(A,T);       u_h = UCER(A,H)\n",
        "\n",
        "print(\"\\n== TEST (Hybrid1) ==\")\n",
        "print(\"ROUGE  TOP:\", {k:round(v,4) for k,v in R_top.items()})\n",
        "print(\"ROUGE   H1:\", {k:round(v,4) for k,v in R_h.items()},\n",
        "      \"| Δ:\", {k: round(R_h[k]-R_top[k],4) for k in R_top})\n",
        "print(\"ENT   TOP:\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in E_top.items()})\n",
        "print(\"ENT    H1:\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in E_h.items()},\n",
        "      \"| Δ:\", {k: round(E_h[k]-E_top[k],4) for k in [\"entP\",\"entR\",\"entF1\"]})\n",
        "print(\"UCER  TOP:\", {\"rate\": round(u_top[0],4), \"avg_core\": round(u_top[1],4)})\n",
        "print(\"UCER   H1:\", {\"rate\": round(u_h[0],4),  \"avg_core\": round(u_h[1],4)},\n",
        "      \"| Δ rate:\", round(u_h[0]-u_top[0],4))\n",
        "\n",
        "# ---- Save TEST outputs ----\n",
        "STAMP_T = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "OUT_TEST = f\"{OUT_ROOT}/test_{STAMP_T}\"\n",
        "os.makedirs(OUT_TEST, exist_ok=True)\n",
        "TEST_OUT = f\"{OUT_TEST}/best_h1_test_{STAMP_T}.csv\"\n",
        "save_csv(best_lr_t, TEST_OUT)\n",
        "cfg_copy = f\"{OUT_TEST}/frozen_config_h1_used_{STAMP_T}.json\"\n",
        "wts_copy = f\"{OUT_TEST}/lr_weights_h1_used_{STAMP_T}.json\"\n",
        "save_json(cfg, cfg_copy); save_json(wts, wts_copy)\n",
        "\n",
        "print(\"\\nSaved to Drive:\")\n",
        "print(\"  TEST picks  ->\", TEST_OUT)\n",
        "print(\"  Frozen cfg  ->\", cfg_copy)\n",
        "print(\"  LR weights  ->\", wts_copy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAvOVALM0JRa",
        "outputId": "0845a104-fcff-4c7b-dd7a-2fd5ca715d5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved:\n",
            "  JSON -> /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/Hybrid1/test_20250827_090855/report_h1_test.json\n",
            "  Markdown -> /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/Hybrid1/test_20250827_090855/report_h1_test.md\n"
          ]
        }
      ],
      "source": [
        "# === Make a tidy TEST report (JSON + Markdown) in the same folder ===\n",
        "import os, json, time, pandas as pd, numpy as np\n",
        "\n",
        "def oracle_rl(dd):\n",
        "    vals=[]\n",
        "    for _, grp in dd.groupby(\"row_id\"):\n",
        "        ref = grp.iloc[0][\"highlights\"]\n",
        "        vals.append(max(rougeLsum(ref, s) for s in grp[\"summary\"].tolist()))\n",
        "    return float(np.mean(vals))\n",
        "\n",
        "# Baselines on same survivors\n",
        "thc_top = (stageA_t.sort_values([\"row_id\",\"THC\"], ascending=[True, False])\n",
        "                    .groupby(\"row_id\", as_index=False).head(1))\n",
        "\n",
        "def _pack_metrics(name, R, H, A):\n",
        "    Rg = rouge_mean(R,H); E = entPRF(R,H); u_rate, u_cnt = UCER(A,H)\n",
        "    return dict(\n",
        "        name=name,\n",
        "        rouge=dict(r1_f=Rg[\"r1_f\"], r2_f=Rg[\"r2_f\"], rl_f=Rg[\"rl_f\"]),\n",
        "        ent=E, ucer=dict(rate=u_rate, avg_core=u_cnt)\n",
        "    )\n",
        "\n",
        "# Build metric bundles\n",
        "R = ev_t[\"highlights\"].astype(str).tolist()\n",
        "T = ev_t[\"top_beam_summary\"].astype(str).tolist()\n",
        "H = ev_t[\"hybrid_h1\"].astype(str).tolist()\n",
        "A = ev_t[\"article\"].astype(str).tolist()\n",
        "\n",
        "top_metrics = _pack_metrics(\"TOP\", R, T, A)\n",
        "h1_metrics  = _pack_metrics(\"Hybrid1\", R, H, A)\n",
        "\n",
        "# THC-only pick\n",
        "R_thc = thc_top[\"highlights\"].astype(str).tolist()\n",
        "H_thc = thc_top[\"summary\"].astype(str).tolist()\n",
        "A_thc = thc_top[\"article\"].astype(str).tolist()\n",
        "thc_metrics = _pack_metrics(\"THC\", R_thc, H_thc, A_thc)\n",
        "\n",
        "ora = oracle_rl(dd_all_t)\n",
        "gap = ora - top_metrics[\"rouge\"][\"rl_f\"]\n",
        "rec = h1_metrics[\"rouge\"][\"rl_f\"] - top_metrics[\"rouge\"][\"rl_f\"]\n",
        "\n",
        "report = dict(\n",
        "    summary=dict(\n",
        "        oracle_RL=ora, top_RL=top_metrics[\"rouge\"][\"rl_f\"],\n",
        "        hybrid1_RL=h1_metrics[\"rouge\"][\"rl_f\"],\n",
        "        gap_to_oracle=gap, recovered=rec, recovered_pct=(rec/gap*100.0 if gap>0 else 0.0)\n",
        "    ),\n",
        "    metrics=dict(top=top_metrics, thc=thc_metrics, hybrid1=h1_metrics),\n",
        "    paths=dict(\n",
        "        picks=str(TEST_OUT),\n",
        "        cfg=str(cfg_copy),\n",
        "        weights=str(wts_copy)\n",
        "    )\n",
        ")\n",
        "\n",
        "# Save JSON + Markdown\n",
        "rep_json = os.path.join(OUT_TEST, f\"report_h1_test.json\")\n",
        "rep_md   = os.path.join(OUT_TEST, f\"report_h1_test.md\")\n",
        "\n",
        "with open(rep_json, \"w\") as f: json.dump(report, f, indent=2)\n",
        "\n",
        "md = []\n",
        "md.append(\"# Hybrid1 — TEST report\\n\")\n",
        "md.append(f\"- Oracle-K RL: **{ora:.4f}**\\n\")\n",
        "md.append(f\"- TOP RL: **{top_metrics['rouge']['rl_f']:.4f}**\\n\")\n",
        "md.append(f\"- Hybrid1 RL: **{h1_metrics['rouge']['rl_f']:.4f}**  (Δ {rec:+.4f}, {rec/gap*100:.1f}% of gap)\\n\")\n",
        "md.append(f\"- EntF1: TOP **{top_metrics['ent']['entF1']:.4f}** → H1 **{h1_metrics['ent']['entF1']:.4f}** (Δ {h1_metrics['ent']['entF1']-top_metrics['ent']['entF1']:+.4f})\\n\")\n",
        "md.append(f\"- UCER: TOP **{top_metrics['ucer']['rate']:.3f}** → H1 **{h1_metrics['ucer']['rate']:.3f}** (Δ {h1_metrics['ucer']['rate']-top_metrics['ucer']['rate']:+.3f})\\n\")\n",
        "md.append(\"\\n## Selector ablation (same survivors)\\n\")\n",
        "md.append(f\"- THC RL **{thc_metrics['rouge']['rl_f']:.4f}**, EntF1 **{thc_metrics['ent']['entF1']:.4f}**, UCER **{thc_metrics['ucer']['rate']:.3f}**\\n\")\n",
        "md.append(f\"- H1  RL **{h1_metrics['rouge']['rl_f']:.4f}**, EntF1 **{h1_metrics['ent']['entF1']:.4f}**, UCER **{h1_metrics['ucer']['rate']:.3f}**\\n\")\n",
        "md.append(\"\\n## Repro paths\\n\")\n",
        "md.append(f\"- Picks: `{TEST_OUT}`\\n- Config: `{cfg_copy}`\\n- Weights: `{wts_copy}`\\n\")\n",
        "with open(rep_md, \"w\") as f: f.write(\"\".join(md))\n",
        "\n",
        "print(\"Saved:\")\n",
        "print(\"  JSON ->\", rep_json)\n",
        "print(\"  Markdown ->\", rep_md)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHiTbqFq0KfC",
        "outputId": "33f8f554-1862-4e12-8e3b-88e7d5d9ff89"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1731163921.py:50: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"timestamp\": datetime.utcnow().isoformat()+\"Z\",\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repro kit saved in: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/Hybrid1/test_20250827_090855\n",
            " - beams_test.parquet\n",
            " - stageA_survivors.parquet\n",
            " - features_test.parquet\n",
            " - env_info.json\n",
            " - requirements_hybrid1.txt\n",
            " - plus your existing: picks.csv, frozen_config.json, lr_weights.json, report.json/md\n"
          ]
        }
      ],
      "source": [
        "# ==== Hybrid1: finalize \"repro kit\" for TEST ====\n",
        "import os, json, platform, pandas as pd, numpy as np, torch, transformers, spacy\n",
        "from datetime import datetime\n",
        "\n",
        "OUT_TEST = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/Hybrid1/test_20250827_090855\"\n",
        "os.makedirs(OUT_TEST, exist_ok=True)\n",
        "\n",
        "# 1) Save ALL beams (so you never need to regenerate)\n",
        "dd_all_t.to_parquet(f\"{OUT_TEST}/beams_test.parquet\")\n",
        "\n",
        "# 2) Save Stage-A survivors (already contain THC, unsup_nonc, len, base_norm, etc.)\n",
        "stageA_t.to_parquet(f\"{OUT_TEST}/stageA_survivors.parquet\")\n",
        "\n",
        "# 3) Save the exact LR feature table used for selection (so you can replay without spaCy/ROUGE recompute)\n",
        "F_test = []\n",
        "for rid, grp in stageA_t.groupby(\"row_id\"):\n",
        "    art = grp.iloc[0][\"article\"]; lead = lead3(art); tgt = adaptive_len_target(art)\n",
        "    cands = grp[\"summary\"].tolist()\n",
        "    leadsim = np.array([rougeLsum(h, lead) for h in cands])\n",
        "    n=len(cands); mbr=np.zeros(n)\n",
        "    if n>1:\n",
        "        M=np.zeros((n,n))\n",
        "        for i in range(n):\n",
        "            for j in range(i+1,n):\n",
        "                s=rougeLsum(cands[i], cands[j]); M[i,j]=M[j,i]=s\n",
        "        mbr=M.mean(axis=1)\n",
        "    def mm(v):\n",
        "        v=np.array(v,float); lo,hi=v.min(),v.max()\n",
        "        return np.zeros_like(v) if hi-lo<1e-8 else (v-lo)/(hi-lo)\n",
        "    feats = pd.DataFrame({\n",
        "        \"row_id\": grp[\"row_id\"].values,\n",
        "        \"article\": grp[\"article\"].values,\n",
        "        \"highlights\": grp[\"highlights\"].values,\n",
        "        \"summary\": grp[\"summary\"].values,\n",
        "        \"THC_norm\": mm(grp[\"THC\"].values),\n",
        "        \"LeadSim\": leadsim,\n",
        "        \"MBR\": mbr,\n",
        "        \"len_reward\": mm([math.exp(-((L-tgt)**2)/(2*(20.0**2))) for L in grp[\"len\"].tolist()]),\n",
        "        \"contig_lcs_mm\": mm([lcs_span_stats(art, s)[0] for s in cands]),\n",
        "        \"base_norm_mm\": mm(grp[\"base_norm\"].values),\n",
        "        \"ent_consensus\": ent_consensus(cands),\n",
        "        \"unsup_nonc_mm\": mm(grp[\"unsup_nonc\"].values.astype(float)),\n",
        "    })\n",
        "    F_test.append(feats)\n",
        "F_test = pd.concat(F_test, ignore_index=True)\n",
        "F_test.to_parquet(f\"{OUT_TEST}/features_test.parquet\")\n",
        "\n",
        "# 4) Save environment info (so you can pin versions later)\n",
        "env = {\n",
        "    \"timestamp\": datetime.utcnow().isoformat()+\"Z\",\n",
        "    \"python\": platform.python_version(),\n",
        "    \"platform\": platform.platform(),\n",
        "    \"torch\": torch.__version__,\n",
        "    \"transformers\": transformers.__version__,\n",
        "    \"spacy\": spacy.__version__,\n",
        "    \"rouge_score\": \"0.1.2\",\n",
        "    \"numpy\": np.__version__,\n",
        "    \"seed\": 0,\n",
        "    \"gpu\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\",\n",
        "}\n",
        "with open(f\"{OUT_TEST}/env_info.json\",\"w\") as f: json.dump(env, f, indent=2)\n",
        "\n",
        "# 5) Freeze exact pip env\n",
        "import subprocess, sys\n",
        "subprocess.check_call([sys.executable,\"-m\",\"pip\",\"freeze\",\"-q\",\"-r\",\"/dev/null\"], stdout=open(f\"{OUT_TEST}/requirements_hybrid1.txt\",\"w\"))\n",
        "\n",
        "print(\"Repro kit saved in:\", OUT_TEST)\n",
        "print(\" - beams_test.parquet\")\n",
        "print(\" - stageA_survivors.parquet\")\n",
        "print(\" - features_test.parquet\")\n",
        "print(\" - env_info.json\")\n",
        "print(\" - requirements_hybrid1.txt\")\n",
        "print(\" - plus your existing: picks.csv, frozen_config.json, lr_weights.json, report.json/md\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kHQbZ9vl1kL1",
        "outputId": "2a621a4e-a1ba-4b5d-ecd6-1596959b63de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tar: ckpt_step6000/config.json: file changed as we read it\n",
            "tar: ckpt_step6000/generation_config.json: file changed as we read it\n",
            "tar: ckpt_step6000/tokenizer_config.json: file changed as we read it\n",
            "tar: ckpt_step6000/special_tokens_map.json: file changed as we read it\n",
            "tar: ckpt_step6000/vocab.json: file changed as we read it\n",
            "tar: ckpt_step6000/merges.txt: file changed as we read it\n",
            "tar: ckpt_step6000/tokenizer.json: file changed as we read it\n",
            "tar: ckpt_step6000/pointer_head.pt: file changed as we read it\n",
            "tar: ckpt_step6000/ptr_decode_ckpt6000_beam5_len18_44.csv: file changed as we read it\n",
            "tar: ckpt_step6000/decodes_spanbias/spanbias_details_K5_20250826_000803.csv: file changed as we read it\n",
            "tar: ckpt_step6000/decodes_spanbias/spanbias_best_20250826_000803.csv: file changed as we read it\n",
            "tar: ckpt_step6000/decodes_spanbias/decodes_copynext2/copynext2_best_20250826_005345.csv: file changed as we read it\n",
            "tar: ckpt_step6000/decodes_spanbias/decodes_copynext2/copynext2_details_20250826_005345.csv: file changed as we read it\n",
            "tar: ckpt_step6000/decodes_spanbias/decodes_copynext2: file changed as we read it\n",
            "tar: ckpt_step6000/decodes_spanbias/decodes_copynext3/copynext3_best_20250826_011005.csv: file changed as we read it\n",
            "tar: ckpt_step6000/decodes_spanbias/decodes_copynext3/copynext3_details_20250826_011005.csv: file changed as we read it\n",
            "tar: ckpt_step6000/decodes_spanbias/spanbias_details_K5_20250826_201347.csv: file changed as we read it\n",
            "tar: ckpt_step6000/decodes_spanbias/spanbias_best_20250826_201347.csv: file changed as we read it\n",
            "tar: ckpt_step6000/decodes_copynext2/copynext2_details_20250826_161305.csv: file changed as we read it\n",
            "tar: ckpt_step6000/decodes_copynext2/copynext2_best_20250826_161305.csv: file changed as we read it\n",
            "tar: ckpt_step6000/eval_BEAM20_EA_lambda1_test/details_beam20_EA_20250826_231832.csv: file changed as we read it\n",
            "tar: ckpt_step6000/eval_BEAM20_EA_lambda1_test/top_beams_20250826_231832.csv: file changed as we read it\n",
            "tar: ckpt_step6000/eval_BEAM20_EA_lambda1_test/best_lambda1_20250826_231832.csv: file changed as we read it\n",
            "tar: ckpt_step6000/eval_BEAM20_EA_lambda1_test/config_lambda1_20250826_231832.json: file changed as we read it\n",
            "tar: ckpt_step6000/eval_BEAM20_EA_lambda1_test/details_beam20_EA_20250827_000820.csv: file changed as we read it\n",
            "tar: ckpt_step6000/eval_BEAM20_EA_lambda1_test/top_beams_20250827_000820.csv: file changed as we read it\n",
            "tar: ckpt_step6000/eval_BEAM20_EA_lambda1_test/best_lambda1_20250827_000820.csv: file changed as we read it\n",
            "tar: ckpt_step6000/eval_BEAM20_EA_lambda1_test/config_lambda1_20250827_000820.json: file changed as we read it\n",
            "tar: ckpt_step6000/eval_HYBRID/hybrid_VAL_20250827_011648/best_hybrid_val_20250827_011648.csv: file changed as we read it\n",
            "tar: ckpt_step6000/eval_HYBRID/hybrid_VAL_20250827_011648/frozen_config_20250827_011648.json: file changed as we read it\n",
            "tar: ckpt_step6000/eval_HYBRID/hybrid_VAL_20250827_011648: file changed as we read it\n",
            "tar: ckpt_step6000/eval_HYBRID/hybrid_TEST_20250827_015818/best_hybrid_test_20250827_015818.csv: file changed as we read it\n",
            "tar: ckpt_step6000/eval_HYBRID/hybrid_TEST_20250827_015818/frozen_config_used_20250827_015818.json: file changed as we read it\n",
            "tar: ckpt_step6000/eval_HYBRID/hybrid_VAL_LR_20250827_064817/stageB_lr_weights.json: file changed as we read it\n",
            "tar: ckpt_step6000/eval_HYBRID/hybrid_VAL_LR_20250827_064817/frozen_config_lr_20250827_064817.json: file changed as we read it\n",
            "tar: ckpt_step6000/eval_HYBRID/hybrid_TEST_LR_20250827_072324/best_hybrid_lr_20250827_072324.csv: file changed as we read it\n",
            "tar: ckpt_step6000/eval_HYBRID/hybrid_TEST_LR_20250827_072324/frozen_config_lr_used_20250827_072324.json: file changed as we read it\n",
            "tar: ckpt_step6000/eval_HYBRID/hybrid_TEST_LR_20250827_072324/stageB_lr_weights_used_20250827_072324.json: file changed as we read it\n",
            "tar: ckpt_step6000/Hybrid1/val_20250827_083245/lr_weights_h1_20250827_083245.json: file changed as we read it\n",
            "tar: ckpt_step6000/Hybrid1/val_20250827_083245/frozen_config_h1_20250827_083245.json: file changed as we read it\n",
            "tar: ckpt_step6000/Hybrid1/val_20250827_083245/best_h1_val_20250827_083245.csv: file changed as we read it\n",
            "tar: ckpt_step6000/Hybrid1/test_20250827_090855/best_h1_test_20250827_090855.csv: file changed as we read it\n",
            "tar: ckpt_step6000/Hybrid1/test_20250827_090855/frozen_config_h1_used_20250827_090855.json: file changed as we read it\n",
            "tar: ckpt_step6000/Hybrid1/test_20250827_090855/lr_weights_h1_used_20250827_090855.json: file changed as we read it\n",
            "tar: ckpt_step6000/Hybrid1/test_20250827_090855/report_h1_test.json: file changed as we read it\n",
            "tar: ckpt_step6000/Hybrid1/test_20250827_090855/report_h1_test.md: file changed as we read it\n",
            "tar: ckpt_step6000/Hybrid1/test_20250827_090855/stageA_survivors.parquet: file changed as we read it\n",
            "tar: ckpt_step6000/Hybrid1/test_20250827_090855/features_test.parquet: file changed as we read it\n",
            "tar: ckpt_step6000/Hybrid1/test_20250827_090855/env_info.json: file changed as we read it\n",
            "tar: ckpt_step6000/Hybrid1/test_20250827_090855/requirements_hybrid1.txt: file changed as we read it\n"
          ]
        }
      ],
      "source": [
        "# creates a portable snapshot of the finetuned model for future demos\n",
        "!tar -czf /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000_snapshot.tgz -C /content/drive/MyDrive/student_pgc_bartbase ckpt_step6000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o0qWgia3OE3",
        "outputId": "d831faeb-16eb-4e40-d117-595174c73c80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Report saved: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/Hybrid1/test_20250827_090855/hybrid1_side_by_side_20250827_094506.html\n"
          ]
        }
      ],
      "source": [
        "# Robust Hybrid1 side-by-side viewer (handles missing 'highlights')\n",
        "\n",
        "import os, json, glob, math, re, numpy as np, pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# === set your test folder ===\n",
        "TEST_RUN_DIR = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/Hybrid1/test_20250827_090855\"\n",
        "N_EXAMPLES   = 30\n",
        "SORT_BY      = \"rl_gain\"   # rl_gain | ent_gain | ucer_delta | random\n",
        "\n",
        "def _glob_one(p):\n",
        "    m = sorted(glob.glob(p));\n",
        "    return m[-1] if m else None\n",
        "\n",
        "def choose_top_from_beams(dd_all):\n",
        "    # prefer k=0 from CN2_LONG\n",
        "    pool_order = {\"CN2_LONG\": 0, \"CN_MED\": 1}\n",
        "    x = dd_all.copy()\n",
        "    x[\"_pool_ord\"] = x[\"pool\"].map(pool_order).fillna(9)\n",
        "    cols = [\"row_id\",\"article\",\"summary\"]\n",
        "    if \"highlights\" in x.columns: cols.append(\"highlights\")\n",
        "    return (x.query(\"candidate_k==0\")\n",
        "             .sort_values([\"row_id\",\"_pool_ord\"])\n",
        "             .groupby(\"row_id\", as_index=False).head(1)[cols]\n",
        "             .rename(columns={\"summary\":\"top_beam_summary\"}))\n",
        "\n",
        "def sigmoid(x):\n",
        "    x = np.clip(x, -50, 50)\n",
        "    return 1.0/(1.0+np.exp(-x))\n",
        "\n",
        "# Load artifacts\n",
        "best_h1_csv = _glob_one(f\"{TEST_RUN_DIR}/best_h1_test_*.csv\") or _glob_one(f\"{TEST_RUN_DIR}/best_hybrid_test_*.csv\")\n",
        "cfg_path    = _glob_one(f\"{TEST_RUN_DIR}/frozen_config_h1_used_*.json\") or _glob_one(f\"{TEST_RUN_DIR}/frozen_config_used_*.json\")\n",
        "wts_path    = _glob_one(f\"{TEST_RUN_DIR}/lr_weights_h1_used_*.json\") or _glob_one(f\"{TEST_RUN_DIR}/stageB_lr_weights_used_*.json\")\n",
        "beams_path  = _glob_one(f\"{TEST_RUN_DIR}/beams_test.parquet\")\n",
        "stageA_path = _glob_one(f\"{TEST_RUN_DIR}/stageA_survivors.parquet\")\n",
        "feats_path  = _glob_one(f\"{TEST_RUN_DIR}/features_test.parquet\")\n",
        "\n",
        "missing = [p for p in [best_h1_csv, cfg_path, wts_path, beams_path, stageA_path, feats_path] if p is None]\n",
        "assert not missing, f\"Missing files in {TEST_RUN_DIR}: {missing}\"\n",
        "\n",
        "best_h1 = pd.read_csv(best_h1_csv)\n",
        "dd_all  = pd.read_parquet(beams_path)\n",
        "stageA  = pd.read_parquet(stageA_path)\n",
        "F       = pd.read_parquet(feats_path)\n",
        "wts     = json.load(open(wts_path))\n",
        "cfg     = json.load(open(cfg_path))\n",
        "\n",
        "# Ensure we have highlights somewhere\n",
        "HAS_REF = False\n",
        "if \"highlights\" in best_h1.columns:\n",
        "    HAS_REF = True\n",
        "elif \"highlights\" in dd_all.columns:\n",
        "    HAS_REF = True\n",
        "    # attach refs to best_h1 by row_id from beams parquet\n",
        "    ref_map = dd_all.groupby(\"row_id\")[\"highlights\"].first()\n",
        "    best_h1 = best_h1.merge(ref_map.rename(\"highlights\"), left_on=\"row_id\", right_index=True, how=\"left\")\n",
        "else:\n",
        "    # no references at all; viewer will skip ROUGE+Reference section\n",
        "    pass\n",
        "\n",
        "# Prepare TOP baseline (k=0)\n",
        "top_df = choose_top_from_beams(dd_all)\n",
        "\n",
        "# LR scores for survivors\n",
        "cols = list(wts[\"coefs\"].keys())\n",
        "F[\"_lr_raw\"] = wts[\"intercept\"]\n",
        "for c in cols:\n",
        "    F[\"_lr_raw\"] += F[c].astype(float) * float(wts[\"coefs\"][c])\n",
        "F[\"_lr_prob\"] = F[\"_lr_raw\"].apply(sigmoid)\n",
        "\n",
        "feat_map = F.set_index([\"row_id\",\"summary\"])[[\"_lr_raw\",\"_lr_prob\",\"THC_norm\",\"LeadSim\",\"MBR\",\"len_reward\",\"contig_lcs_mm\",\"base_norm_mm\",\"ent_consensus\",\"unsup_nonc_mm\"]]\n",
        "\n",
        "# Compare table\n",
        "cmp = (best_h1.rename(columns={\"summary\":\"hybrid_h1\"})\n",
        "               .merge(top_df, on=\"row_id\", how=\"left\", suffixes=(\"\",\"_topmerge\")))\n",
        "\n",
        "# If highlights missing after merge, try bringing from top_df\n",
        "if (not HAS_REF) and (\"highlights\" in cmp.columns) and cmp[\"highlights\"].notna().any():\n",
        "    HAS_REF = True\n",
        "\n",
        "# Attach LR features for H1 and TOP (if TOP survived Stage-A)\n",
        "cmp = cmp.merge(feat_map, left_on=[\"row_id\",\"hybrid_h1\"], right_index=True, how=\"left\")\\\n",
        "         .rename(columns={\"_lr_raw\":\"h1_lr_raw\",\"_lr_prob\":\"h1_lr_prob\",\n",
        "                          \"THC_norm\":\"h1_THC_norm\",\"LeadSim\":\"h1_LeadSim\",\"MBR\":\"h1_MBR\",\n",
        "                          \"len_reward\":\"h1_len_reward\",\"contig_lcs_mm\":\"h1_contig_lcs_mm\",\n",
        "                          \"base_norm_mm\":\"h1_base_norm_mm\",\"ent_consensus\":\"h1_ent_consensus\",\n",
        "                          \"unsup_nonc_mm\":\"h1_unsup_nonc_mm\"})\n",
        "\n",
        "cmp = cmp.merge(feat_map, left_on=[\"row_id\",\"top_beam_summary\"], right_index=True, how=\"left\")\\\n",
        "         .rename(columns={\"_lr_raw\":\"top_lr_raw\",\"_lr_prob\":\"top_lr_prob\",\n",
        "                          \"THC_norm\":\"top_THC_norm\",\"LeadSim\":\"top_LeadSim\",\"MBR\":\"top_MBR\",\n",
        "                          \"len_reward\":\"top_len_reward\",\"contig_lcs_mm\":\"top_contig_lcs_mm\",\n",
        "                          \"base_norm_mm\":\"top_base_norm_mm\",\"ent_consensus\":\"top_ent_consensus\",\n",
        "                          \"unsup_nonc_mm\":\"top_unsup_nonc_mm\"})\n",
        "\n",
        "# Fast ROUGE-Lsum if refs exist\n",
        "def _tok(s): return [t for t in re.split(r\"\\s+\", str(s).strip()) if t]\n",
        "def _lcs(a,b):\n",
        "    A,B=_tok(a),_tok(b); n,m=len(A),len(B); dp=[0]*(m+1)\n",
        "    for i in range(1,n+1):\n",
        "        prev=0\n",
        "        for j in range(1,m+1):\n",
        "            tmp=dp[j]\n",
        "            dp[j] = prev+1 if A[i-1]==B[j-1] else max(dp[j], dp[j-1])\n",
        "            prev=tmp\n",
        "    return dp[m]\n",
        "def rougeLsum_single(ref, hyp):\n",
        "    lcs = _lcs(ref, hyp); ra = max(len(_tok(ref)), 1); hb = max(len(_tok(hyp)), 1)\n",
        "    P = lcs / hb; R = lcs / ra\n",
        "    return (2*P*R/(P+R)) if (P+R)>0 else 0.0\n",
        "\n",
        "if HAS_REF:\n",
        "    cmp[\"rl_top\"] = [rougeLsum_single(r, t) for r,t in zip(cmp[\"highlights\"], cmp[\"top_beam_summary\"])]\n",
        "    cmp[\"rl_h1\"]  = [rougeLsum_single(r, h) for r,h in zip(cmp[\"highlights\"], cmp[\"hybrid_h1\"])]\n",
        "    cmp[\"rl_gain\"] = cmp[\"rl_h1\"] - cmp[\"rl_top\"]\n",
        "else:\n",
        "    cmp[\"rl_top\"] = np.nan\n",
        "    cmp[\"rl_h1\"]  = np.nan\n",
        "    cmp[\"rl_gain\"] = np.nan\n",
        "\n",
        "# Pull Stage-A THC and safety flags\n",
        "def stageA_stats(row_id, text):\n",
        "    g = stageA[stageA[\"row_id\"]==row_id]\n",
        "    hit = g.loc[g[\"summary\"]==text]\n",
        "    if len(hit)==0:\n",
        "        return dict(unsup_nonc=np.nan, hard_fail=np.nan, THC=np.nan)\n",
        "    h = hit.iloc[0]\n",
        "    return dict(unsup_nonc=int(h.get(\"unsup_nonc\", 0)), hard_fail=bool(h.get(\"hard_fail\", False)), THC=float(h.get(\"THC\", np.nan)))\n",
        "\n",
        "top_stat = cmp.apply(lambda r: stageA_stats(r[\"row_id\"], r[\"top_beam_summary\"]), axis=1, result_type=\"expand\").add_prefix(\"top_\")\n",
        "h1_stat  = cmp.apply(lambda r: stageA_stats(r[\"row_id\"], r[\"hybrid_h1\"]), axis=1, result_type=\"expand\").add_prefix(\"h1_\")\n",
        "cmp = pd.concat([cmp, top_stat, h1_stat], axis=1)\n",
        "\n",
        "cmp[\"conf_h1\"] = cmp[\"h1_lr_prob\"]\n",
        "\n",
        "# Sort\n",
        "if   SORT_BY==\"rl_gain\" and HAS_REF: cmp = cmp.sort_values(\"rl_gain\", ascending=False)\n",
        "elif SORT_BY==\"ent_gain\":\n",
        "    cmp = cmp.assign(ent_gain = (cmp[\"top_unsup_nonc\"].fillna(1) - cmp[\"h1_unsup_nonc\"].fillna(1))).sort_values(\"ent_gain\", ascending=False)\n",
        "elif SORT_BY==\"ucer_delta\":\n",
        "    cmp = cmp.assign(ucer_delta = (cmp[\"top_hard_fail\"].fillna(True).astype(int) - cmp[\"h1_hard_fail\"].fillna(True).astype(int))).sort_values(\"ucer_delta\", ascending=False)\n",
        "else:\n",
        "    cmp = cmp.sample(frac=1.0, random_state=0)\n",
        "\n",
        "view = cmp.head(N_EXAMPLES).copy()\n",
        "\n",
        "# Build HTML\n",
        "def esc(s):\n",
        "    return (str(s).replace(\"&\",\"&amp;\").replace(\"<\",\"&lt;\").replace(\">\",\"&gt;\"))\n",
        "def block(label, text):\n",
        "    return f\"\"\"<div class=\"blk\"><div class=\"hdr\">{label}</div><div class=\"txt\">{esc(text)}</div></div>\"\"\"\n",
        "\n",
        "css = \"\"\"\n",
        "<style>\n",
        "body { font-family: Inter, system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif; line-height: 1.45; margin: 24px; }\n",
        ".card { border: 1px solid #e6e8eb; border-radius: 14px; padding: 16px 18px; margin: 16px 0; box-shadow: 0 1px 2px rgba(0,0,0,0.04); }\n",
        ".rowid { font-weight: 600; color: #444; }\n",
        ".meta { font-size: 12px; color: #666; margin-bottom: 8px; }\n",
        ".hdr { font-weight: 600; font-size: 12px; color: #666; margin: 10px 0 6px; text-transform: uppercase; letter-spacing: .04em; }\n",
        ".blk .txt { white-space: pre-wrap; font-size: 14px; background: #fbfcfd; border: 1px solid #f0f2f5; border-radius: 10px; padding: 10px; }\n",
        ".grid { display: grid; grid-template-columns: 1fr 1fr; gap: 12px; }\n",
        ".kv { display: inline-grid; grid-template-columns: max-content auto; gap: 6px 12px; font-size: 13px; margin: 6px 0 4px; }\n",
        ".kv div:nth-child(odd) { color: #666; }\n",
        ".badge { display:inline-block; padding:2px 8px; border-radius: 999px; font-size: 11px; border:1px solid #e0e3e7; background:#f6f8fb; color:#333; margin-right:6px;}\n",
        ".scorepos { color:#0d7; font-weight:600; }\n",
        ".scoreneg { color:#d55; font-weight:600; }\n",
        "details { margin-top: 8px; }\n",
        "summary { cursor: pointer; font-size: 13px; color: #2d6cdf; }\n",
        ".mono { font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace; }\n",
        "</style>\n",
        "\"\"\"\n",
        "\n",
        "html = [css, \"<h2>Hybrid1 vs TOP vs Reference — Side-by-side</h2>\",\n",
        "        f\"<div class='meta'>Folder: <span class='mono'>{TEST_RUN_DIR}</span> • Examples: {len(view)}/{len(cmp)} • Sorted by: <b>{SORT_BY}</b></div>\"]\n",
        "\n",
        "for _,r in view.iterrows():\n",
        "    meta_rows = [f\"<div>row_id</div><div>{int(r['row_id'])}</div>\",\n",
        "                 f\"<div>LR prob (H1)</div><div>{'' if pd.isna(r['conf_h1']) else f'{float(r['conf_h1']):.3f}'}</div>\",\n",
        "                 f\"<div>THC (TOP/H1)</div><div>{'' if pd.isna(r['top_THC']) else f'{r['top_THC']:.2f}'} &nbsp; / &nbsp; {'' if pd.isna(r['h1_THC']) else f'{r['h1_THC']:.2f}'}</div>\",\n",
        "                 f\"<div>Unsup non-core (TOP/H1)</div><div>{'' if pd.isna(r['top_unsup_nonc']) else int(r['top_unsup_nonc'])} &nbsp; / &nbsp; {'' if pd.isna(r['h1_unsup_nonc']) else int(r['h1_unsup_nonc'])}</div>\",\n",
        "                 f\"<div>Core hard-fail (TOP/H1)</div><div>{'' if pd.isna(r['top_hard_fail']) else bool(r['top_hard_fail'])} &nbsp; / &nbsp; {'' if pd.isna(r['h1_hard_fail']) else bool(r['h1_hard_fail'])}</div>\"]\n",
        "    if HAS_REF and not pd.isna(r.get(\"rl_top\", np.nan)):\n",
        "        rgain = r[\"rl_h1\"] - r[\"rl_top\"]\n",
        "        meta_rows.insert(1, f\"<div>RL (TOP → H1)</div><div>{r['rl_top']:.4f} → <b>{r['rl_h1']:.4f}</b> &nbsp; Δ {'+' if rgain>=0 else ''}{rgain:.4f}</div>\")\n",
        "\n",
        "    html.append(\"<div class='card'>\")\n",
        "    html.append(f\"<div class='rowid'>Row {int(r['row_id'])}</div>\")\n",
        "    html.append(f\"<div class='kv'>{''.join(meta_rows)}</div>\")\n",
        "    html.append(\"<div class='grid'>\")\n",
        "\n",
        "    # Left column\n",
        "    html.append(\"<div>\")\n",
        "    if HAS_REF and (\"highlights\" in r):\n",
        "        html.append(block(\"Reference (highlights)\", r[\"highlights\"]))\n",
        "    lead = \" \".join(str(r[\"article\"]).split()[:120])\n",
        "    html.append(block(\"Article (lead)\", lead + (\" ...\" if len(str(r[\"article\"]).split())>120 else \"\")))\n",
        "    html.append(f\"<details><summary>Show full article</summary><div class='txt'>{esc(r['article'])}</div></details>\")\n",
        "    html.append(\"</div>\")\n",
        "\n",
        "    # Right column\n",
        "    html.append(\"<div>\")\n",
        "    html.append(block(\"Baseline TOP (k=0)\", r[\"top_beam_summary\"]))\n",
        "    html.append(block(\"Hybrid1 (final)\", r[\"hybrid_h1\"]))\n",
        "    html.append(\"</div>\")\n",
        "\n",
        "    html.append(\"</div>\")  # grid\n",
        "\n",
        "    # Survivors peek\n",
        "    grp = stageA[stageA[\"row_id\"]==r[\"row_id\"]].copy().sort_values(\"THC\", ascending=False).head(5)\n",
        "    html.append(\"<details><summary>Show feature signals & survivors</summary>\")\n",
        "\n",
        "    # Feature table if present\n",
        "    feat_tbl=[]\n",
        "    for k in [\"h1_lr_prob\",\"h1_THC_norm\",\"h1_LeadSim\",\"h1_MBR\",\"h1_len_reward\",\"h1_contig_lcs_mm\",\"h1_base_norm_mm\",\"h1_ent_consensus\",\"h1_unsup_nonc_mm\",\n",
        "              \"top_lr_prob\",\"top_THC_norm\",\"top_LeadSim\",\"top_MBR\",\"top_len_reward\",\"top_contig_lcs_mm\",\"top_base_norm_mm\",\"top_ent_consensus\",\"top_unsup_nonc_mm\"]:\n",
        "        if k in r and not pd.isna(r[k]): feat_tbl.append(f\"<div>{k}</div><div>{float(r[k]):.4f}</div>\")\n",
        "    html.append(f\"<div class='kv'>{''.join(feat_tbl)}</div>\")\n",
        "\n",
        "    if len(grp):\n",
        "        html.append(\"<div class='hdr'>Top-5 survivors (Stage-A)</div>\")\n",
        "        for j, rr in grp.reset_index(drop=True).iterrows():\n",
        "            html.append(f\"<div class='badge'>cand {j+1}</div> THC={rr['THC']:.2f} • unsup_nonc={int(rr['unsup_nonc'])} • hard_fail={bool(rr['hard_fail'])}\")\n",
        "            html.append(f\"<div class='txt' style='margin:6px 0 10px'>{esc(rr['summary'])}</div>\")\n",
        "    html.append(\"</details>\")\n",
        "    html.append(\"</div>\")  # card\n",
        "\n",
        "out_path = f\"{TEST_RUN_DIR}/hybrid1_side_by_side_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html\"\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(html))\n",
        "\n",
        "print(\"Report saved:\", out_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9_vnHNS7zjd",
        "outputId": "00fe1fd0-7429-456b-fd2a-5d9417fd04ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEST_RUN_DIR: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/Hybrid1/test_20250827_090855\n",
            "best_csv : /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/Hybrid1/test_20250827_090855/best_h1_test_20250827_090855.csv\n",
            "cfg_json : /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/Hybrid1/test_20250827_090855/frozen_config_h1_used_20250827_090855.json\n",
            "wts_json : /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/Hybrid1/test_20250827_090855/lr_weights_h1_used_20250827_090855.json\n",
            "beams_pq : /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/Hybrid1/test_20250827_090855/beams_test.parquet\n",
            "stageA_pq: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/Hybrid1/test_20250827_090855/stageA_survivors.parquet\n",
            "feats_pq : /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/Hybrid1/test_20250827_090855/features_test.parquet\n",
            "\n",
            "Columns available:\n",
            "best: ['row_id', 'pool', 'candidate_k', 'article', 'highlights', 'summary', 'len', 'base_score', 'THC', 'hard_fail', 'unsup_nonc', 'base_norm'] …\n",
            "beams: ['row_id', 'pool', 'candidate_k', 'article', 'highlights', 'summary', 'len', 'base_score', '_pool_ord'] …\n",
            "stageA: ['row_id', 'pool', 'candidate_k', 'article', 'highlights', 'summary', 'len', 'base_score', 'THC', 'hard_fail', 'unsup_nonc', 'base_norm'] …\n",
            "feats: ['row_id', 'article', 'highlights', 'summary', 'THC_norm', 'LeadSim', 'MBR', 'len_reward', 'contig_lcs_mm', 'base_norm_mm', 'ent_consensus', 'unsup_nonc_mm'] …\n"
          ]
        }
      ],
      "source": [
        "# === Configure your TEST run folder ===\n",
        "TEST_RUN_DIR = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/Hybrid1/test_20250827_090855\"\n",
        "\n",
        "import os, glob, pandas as pd, json\n",
        "def _one(p):\n",
        "    m = sorted(glob.glob(p));\n",
        "    return m[-1] if m else None\n",
        "\n",
        "best_csv = _one(f\"{TEST_RUN_DIR}/best_h1_test_*.csv\") or _one(f\"{TEST_RUN_DIR}/best_hybrid_test_*.csv\")\n",
        "cfg_json = _one(f\"{TEST_RUN_DIR}/frozen_config_h1_used_*.json\") or _one(f\"{TEST_RUN_DIR}/frozen_config_used_*.json\")\n",
        "wts_json = _one(f\"{TEST_RUN_DIR}/lr_weights_h1_used_*.json\") or _one(f\"{TEST_RUN_DIR}/stageB_lr_weights_used_*.json\")\n",
        "beams_pq = _one(f\"{TEST_RUN_DIR}/beams_test.parquet\")\n",
        "stgA_pq  = _one(f\"{TEST_RUN_DIR}/stageA_survivors.parquet\")\n",
        "feats_pq = _one(f\"{TEST_RUN_DIR}/features_test.parquet\")\n",
        "\n",
        "print(\"TEST_RUN_DIR:\", TEST_RUN_DIR)\n",
        "print(\"best_csv :\", best_csv)\n",
        "print(\"cfg_json :\", cfg_json)\n",
        "print(\"wts_json :\", wts_json)\n",
        "print(\"beams_pq :\", beams_pq)\n",
        "print(\"stageA_pq:\", stgA_pq)\n",
        "print(\"feats_pq :\", feats_pq)\n",
        "\n",
        "# Load what we have\n",
        "best = pd.read_csv(best_csv)\n",
        "cfg  = json.load(open(cfg_json))\n",
        "wts  = json.load(open(wts_json))\n",
        "beams = pd.read_parquet(beams_pq) if beams_pq else None\n",
        "stageA= pd.read_parquet(stgA_pq)  if stgA_pq  else None\n",
        "F     = pd.read_parquet(feats_pq) if feats_pq else None\n",
        "\n",
        "print(\"\\nColumns available:\")\n",
        "print(\"best:\", list(best.columns)[:12], \"…\")\n",
        "if beams is not None:  print(\"beams:\", list(beams.columns)[:12], \"…\")\n",
        "if stageA is not None: print(\"stageA:\", list(stageA.columns)[:12], \"…\")\n",
        "if F is not None:      print(\"feats:\", list(F.columns)[:12], \"…\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_o5OkwuB9iRs",
        "outputId": "eddcf961-7c8b-42c3-abae-9dd5f34e6d5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[best] highlights present.\n",
            "Side-by-side HTML: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/Hybrid1/test_20250827_090855/side_by_side_safe_20250827_105853.html\n",
            "\n",
            "[OK] cmp columns: ['row_id', 'pool', 'candidate_k', 'article', 'highlights', 'hybrid_h1', 'len', 'base_score', 'THC', 'hard_fail', 'unsup_nonc', 'base_norm', 'top_beam_summary', '_lr_raw_x', 'h1_lr_prob', 'THC_norm_x', 'LeadSim_x', 'MBR_x'] …\n",
            "Rows: 1543 | highlights non-null: 1543\n"
          ]
        }
      ],
      "source": [
        "# === Hybrid1: Side-by-side report (robust to 'highlights' issues) ===\n",
        "# - Loads saved TEST artifacts\n",
        "# - Ensures 'highlights' exist (borrows via row_id from beams → stageA → feats if needed)\n",
        "# - Rebuilds TOP (k=0) but DOES NOT include its own 'highlights' to avoid suffix collisions\n",
        "# - Attaches LR “confidence” for H1/TOP when available\n",
        "# - Attaches Stage-A stats (THC / hard_fail / unsup_nonc) when available\n",
        "# - Computes per-row ROUGE-Lsum and Δ\n",
        "# - Writes a NaN-safe HTML file to your Drive\n",
        "\n",
        "# ----- CONFIG -----\n",
        "TEST_RUN_DIR = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/Hybrid1/test_20250827_090855\"\n",
        "N_EXAMPLES   = 30            # how many rows to show\n",
        "SORT_BY      = \"rl_gain\"     # rl_gain | random\n",
        "# ------------------\n",
        "\n",
        "import os, glob, json, re, numpy as np, pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "def _one(p):\n",
        "    m = sorted(glob.glob(p))\n",
        "    return m[-1] if m else None\n",
        "\n",
        "# Load artifacts\n",
        "best_csv = _one(f\"{TEST_RUN_DIR}/best_h1_test_*.csv\") or _one(f\"{TEST_RUN_DIR}/best_hybrid_test_*.csv\")\n",
        "beams_pq = _one(f\"{TEST_RUN_DIR}/beams_test.parquet\")\n",
        "stageA_pq= _one(f\"{TEST_RUN_DIR}/stageA_survivors.parquet\")\n",
        "feats_pq = _one(f\"{TEST_RUN_DIR}/features_test.parquet\")\n",
        "wts_json = _one(f\"{TEST_RUN_DIR}/lr_weights_h1_used_*.json\") or _one(f\"{TEST_RUN_DIR}/stageB_lr_weights_used_*.json\")\n",
        "\n",
        "assert best_csv and beams_pq and stageA_pq and feats_pq and wts_json, \"Missing a required file in TEST_RUN_DIR.\"\n",
        "\n",
        "best   = pd.read_csv(best_csv).rename(columns={\"summary\":\"hybrid_h1\"})\n",
        "beams  = pd.read_parquet(beams_pq)\n",
        "stageA = pd.read_parquet(stageA_pq)\n",
        "F      = pd.read_parquet(feats_pq)\n",
        "wts    = json.load(open(wts_json))\n",
        "\n",
        "# --- helper: ensure a DF has 'highlights' by borrowing via row_id\n",
        "def ensure_highlights(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
        "    if \"highlights\" in df.columns and df[\"highlights\"].notna().any():\n",
        "        print(f\"[{name}] highlights present.\")\n",
        "        return df\n",
        "    for src_name, src_df in [(\"beams\", beams), (\"stageA\", stageA), (\"feats\", F)]:\n",
        "        if src_df is not None and \"highlights\" in src_df.columns:\n",
        "            ref_map = src_df.dropna(subset=[\"highlights\"]).groupby(\"row_id\")[\"highlights\"].first()\n",
        "            before = df.get(\"highlights\") if \"highlights\" in df.columns else None\n",
        "            df = df.merge(ref_map.rename(\"highlights\"), on=\"row_id\", how=\"left\")\n",
        "            if df[\"highlights\"].notna().any():\n",
        "                print(f\"[{name}] highlights filled from {src_name}.\")\n",
        "                return df\n",
        "    print(f\"[{name}] WARNING: could not find highlights in donors.\")\n",
        "    return df\n",
        "\n",
        "# Guarantee refs in 'best' (we will use *only* this 'highlights' going forward)\n",
        "best = ensure_highlights(best, \"best\")\n",
        "\n",
        "# --- Build TOP baseline = k=0 (prefer CN2_LONG), but DO NOT carry its own 'highlights' to avoid suffix\n",
        "pool_order = {\"CN2_LONG\":0, \"CN_MED\":1}\n",
        "b = beams.copy(); b[\"_ord\"] = b[\"pool\"].map(pool_order).fillna(9)\n",
        "top_df = (b.query(\"candidate_k==0\")\n",
        "            .sort_values([\"row_id\",\"_ord\"])\n",
        "            .groupby(\"row_id\", as_index=False).head(1)\n",
        "            [[\"row_id\",\"summary\"]]                         # <— only summary\n",
        "            .rename(columns={\"summary\":\"top_beam_summary\"}))\n",
        "\n",
        "# --- Merge compare; keep 'highlights' from best ONLY\n",
        "cmp = best.merge(top_df, on=\"row_id\", how=\"left\")\n",
        "\n",
        "# Final guard: we must have highlights now\n",
        "assert \"highlights\" in cmp.columns and cmp[\"highlights\"].notna().any(), \\\n",
        "       \"No 'highlights' after repair. Inspect your saved artifacts.\"\n",
        "\n",
        "# --- LR confidence for the exact strings chosen as H1 / TOP (may be NaN for TOP if it wasn't in Stage-A)\n",
        "def sigmoid(x):\n",
        "    x = np.clip(x, -50, 50)\n",
        "    return 1.0/(1.0+np.exp(-x))\n",
        "\n",
        "cols = list(wts[\"coefs\"].keys())\n",
        "f = F.set_index([\"row_id\",\"summary\"]).copy()\n",
        "f[\"_lr_raw\"] = wts[\"intercept\"]\n",
        "for c in cols:\n",
        "    f[\"_lr_raw\"] += f[c].astype(float) * float(wts[\"coefs\"][c])\n",
        "f[\"_lr_prob\"] = f[\"_lr_raw\"].apply(sigmoid)\n",
        "\n",
        "feat_map = f[[\"_lr_raw\",\"_lr_prob\",\"THC_norm\",\"LeadSim\",\"MBR\",\"len_reward\",\n",
        "              \"contig_lcs_mm\",\"base_norm_mm\",\"ent_consensus\",\"unsup_nonc_mm\"]]\n",
        "\n",
        "cmp = cmp.merge(feat_map, left_on=[\"row_id\",\"hybrid_h1\"], right_index=True, how=\"left\") \\\n",
        "         .rename(columns={\"_lr_prob\":\"h1_lr_prob\"})\n",
        "cmp = cmp.merge(feat_map, left_on=[\"row_id\",\"top_beam_summary\"], right_index=True, how=\"left\") \\\n",
        "         .rename(columns={\"_lr_prob\":\"top_lr_prob\"})\n",
        "\n",
        "# --- Stage-A stats for the chosen strings (may be NaN if TOP not in survivors)\n",
        "def pick_stats(row_id, text):\n",
        "    g = stageA[stageA[\"row_id\"]==row_id]\n",
        "    h = g.loc[g[\"summary\"]==text]\n",
        "    if len(h)==0:\n",
        "        return dict(THC=np.nan, hard_fail=np.nan, unsup_nonc=np.nan)\n",
        "    h = h.iloc[0]\n",
        "    return dict(THC=float(h[\"THC\"]), hard_fail=bool(h[\"hard_fail\"]), unsup_nonc=int(h[\"unsup_nonc\"]))\n",
        "\n",
        "top_s = cmp.apply(lambda r: pick_stats(r[\"row_id\"], r[\"top_beam_summary\"]), axis=1, result_type=\"expand\").add_prefix(\"top_\")\n",
        "h1_s  = cmp.apply(lambda r: pick_stats(r[\"row_id\"], r[\"hybrid_h1\"]),     axis=1, result_type=\"expand\").add_prefix(\"h1_\")\n",
        "cmp   = pd.concat([cmp, top_s, h1_s], axis=1)\n",
        "\n",
        "# --- Fast ROUGE-Lsum (LCS on whitespace tokens)\n",
        "def _tok(s): return [t for t in re.split(r\"\\s+\", str(s).strip()) if t]\n",
        "def _lcs(a,b):\n",
        "    A,B=_tok(a),_tok(b); n,m=len(A),len(B); dp=[0]*(m+1)\n",
        "    for i in range(1,n+1):\n",
        "        prev=0\n",
        "        for j in range(1,m+1):\n",
        "            tmp=dp[j]\n",
        "            dp[j]= prev+1 if A[i-1]==B[j-1] else max(dp[j], dp[j-1])\n",
        "            prev=tmp\n",
        "    return dp[m]\n",
        "def rl(ref,hyp):\n",
        "    L=_lcs(ref,hyp); R=max(len(_tok(ref)),1); H=max(len(_tok(hyp)),1)\n",
        "    P=L/H; Re=L/R\n",
        "    return (2*P*Re/(P+Re)) if (P+Re)>0 else 0.0\n",
        "\n",
        "cmp[\"rl_top\"] = [rl(r,t) for r,t in zip(cmp[\"highlights\"], cmp[\"top_beam_summary\"])]\n",
        "cmp[\"rl_h1\"]  = [rl(r,h) for r,h in zip(cmp[\"highlights\"], cmp[\"hybrid_h1\"])]\n",
        "cmp[\"rl_gain\"]= cmp[\"rl_h1\"] - cmp[\"rl_top\"]\n",
        "\n",
        "# --- Safe formatters (handle NaNs)\n",
        "def s_num(x, dp=4, plus=False):\n",
        "    if pd.isna(x): return \"—\"\n",
        "    fmt = f\"{{:{'+' if plus else ''}.{dp}f}}\"\n",
        "    return fmt.format(float(x))\n",
        "\n",
        "def s_int(x):\n",
        "    if pd.isna(x): return \"—\"\n",
        "    try: return str(int(x))\n",
        "    except Exception: return \"—\"\n",
        "\n",
        "def s_bool(x):\n",
        "    if pd.isna(x): return \"—\"\n",
        "    return \"True\" if bool(x) else \"False\"\n",
        "\n",
        "def esc(s):\n",
        "    return str(s).replace(\"&\",\"&amp;\").replace(\"<\",\"&lt;\").replace(\">\",\"&gt;\")\n",
        "\n",
        "def block(h, t):\n",
        "    return f\"<div class='hdr'>{h}</div><div class='txt'>{esc(t)}</div>\"\n",
        "\n",
        "# --- Pick rows to display\n",
        "if   SORT_BY == \"rl_gain\":\n",
        "    view = cmp.sort_values(\"rl_gain\", ascending=False).head(N_EXAMPLES).copy()\n",
        "elif SORT_BY == \"random\":\n",
        "    view = cmp.sample(n=min(N_EXAMPLES, len(cmp)), random_state=0).copy()\n",
        "else:\n",
        "    view = cmp.head(N_EXAMPLES).copy()\n",
        "\n",
        "# --- Build HTML (NaN-safe)\n",
        "css = \"\"\"\n",
        "<style>\n",
        "body{font-family:Inter,system-ui,-apple-system,Segoe UI,Roboto,Arial,sans-serif;margin:24px;line-height:1.45}\n",
        ".card{border:1px solid #e6e8eb;border-radius:14px;padding:16px 18px;margin:16px 0}\n",
        ".grid{display:grid;grid-template-columns:1fr 1fr;gap:12px}\n",
        ".hdr{font-weight:600;font-size:12px;color:#666;text-transform:uppercase;letter-spacing:.04em;margin:10px 0 6px}\n",
        ".txt{white-space:pre-wrap;font-size:14px;background:#fbfcfd;border:1px solid #f0f2f5;border-radius:10px;padding:10px}\n",
        ".kv{display:inline-grid;grid-template-columns:max-content auto;gap:6px 12px;font-size:13px;margin:6px 0 4px}\n",
        "</style>\n",
        "\"\"\"\n",
        "html = [css, \"<h2>Hybrid1 • Reference vs TOP vs H1</h2>\",\n",
        "        f\"<div class='kv'><div>Folder</div><div>{esc(TEST_RUN_DIR)}</div></div>\"]\n",
        "\n",
        "for _, r in view.iterrows():\n",
        "    meta = [\n",
        "        f\"<div>RL (TOP→H1)</div><div>{s_num(r['rl_top'],4)} → <b>{s_num(r['rl_h1'],4)}</b> Δ {s_num(r['rl_gain'],4,plus=True)}</div>\",\n",
        "        f\"<div>LR prob (H1)</div><div>{s_num(r.get('h1_lr_prob', np.nan),3)}</div>\",\n",
        "        f\"<div>THC (TOP/H1)</div><div>{s_num(r.get('top_THC', np.nan),2)} / {s_num(r.get('h1_THC', np.nan),2)}</div>\",\n",
        "        f\"<div>Unsup non-core (TOP/H1)</div><div>{s_int(r.get('top_unsup_nonc', np.nan))} / {s_int(r.get('h1_unsup_nonc', np.nan))}</div>\",\n",
        "        f\"<div>Core hard-fail (TOP/H1)</div><div>{s_bool(r.get('top_hard_fail', np.nan))} / {s_bool(r.get('h1_hard_fail', np.nan))}</div>\",\n",
        "    ]\n",
        "    html.append(\"<div class='card'>\")\n",
        "    html.append(f\"<div class='kv'>{''.join(meta)}</div>\")\n",
        "    html.append(\"<div class='grid'>\")\n",
        "    left = [\n",
        "        block(\"Reference (highlights)\", r[\"highlights\"]),\n",
        "        block(\"Article (lead)\", \" \".join(str(r[\"article\"]).split()[:120]) + (\" ...\" if len(str(r[\"article\"]).split())>120 else \"\")),\n",
        "        f\"<details><summary>Full article</summary><div class='txt'>{esc(r['article'])}</div></details>\",\n",
        "    ]\n",
        "    right = [\n",
        "        block(\"Baseline TOP (k=0)\", r[\"top_beam_summary\"]),\n",
        "        block(\"Hybrid1 (final)\", r[\"hybrid_h1\"]),\n",
        "    ]\n",
        "    html.append(\"<div>\"+ \"\".join(left) +\"</div><div>\"+ \"\".join(right) +\"</div></div></div>\")\n",
        "\n",
        "STAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "out_html = f\"{TEST_RUN_DIR}/side_by_side_safe_{STAMP}.html\"\n",
        "open(out_html, \"w\", encoding=\"utf-8\").write(\"\\n\".join(html))\n",
        "print(\"Side-by-side HTML:\", out_html)\n",
        "\n",
        "# --- sanity ping\n",
        "print(\"\\n[OK] cmp columns:\", list(cmp.columns)[:18], \"…\")\n",
        "print(\"Rows:\", len(cmp), \"| highlights non-null:\", int(cmp['highlights'].notna().sum()))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Length stats for Hybrid test outputs ===\n",
        "import os, pandas as pd\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Path to your test run directory\n",
        "TEST_RUN_DIR = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/Hybrid1/test_20250827_090855\"\n",
        "CSV_PATH     = os.path.join(TEST_RUN_DIR, \"best_h1_test_20250827_090855.csv\")\n",
        "\n",
        "# Load outputs\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# Make sure the right column exists\n",
        "assert \"summary\" in df.columns, f\"Available columns: {list(df.columns)}\"\n",
        "\n",
        "# Load tokenizer for token counts (same one as training, BART-base pointer-gen)\n",
        "tok = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000\")\n",
        "\n",
        "def word_len(text: str) -> int:\n",
        "    return len(str(text).split())\n",
        "\n",
        "def tok_len(text: str) -> int:\n",
        "    return len(tok(str(text), truncation=False, add_special_tokens=False)[\"input_ids\"])\n",
        "\n",
        "# Compute lengths\n",
        "df[\"word_len\"] = df[\"summary\"].map(word_len)\n",
        "df[\"tok_len\"]  = df[\"summary\"].map(tok_len)\n",
        "\n",
        "# Summary stats\n",
        "stats = {\n",
        "    \"words\": df[\"word_len\"].describe(percentiles=[0.25, 0.5, 0.75]).to_dict(),\n",
        "    \"tokens\": df[\"tok_len\"].describe(percentiles=[0.25, 0.5, 0.75]).to_dict(),\n",
        "}\n",
        "\n",
        "print(\"=== LENGTH STATS (words) ===\")\n",
        "print({k: round(v,3) if isinstance(v,float) else int(v) for k,v in stats[\"words\"].items()})\n",
        "\n",
        "print(\"\\n=== LENGTH STATS (tokens) ===\")\n",
        "print({k: round(v,3) if isinstance(v,float) else int(v) for k,v in stats[\"tokens\"].items()})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwUkX3pt3ioV",
        "outputId": "7ada1913-f58a-49db-dcf4-7e04b349dd4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== LENGTH STATS (words) ===\n",
            "{'count': 1000.0, 'mean': 42.456, 'std': 8.562, 'min': 21.0, '25%': 36.0, '50%': 42.0, '75%': 48.0, 'max': 80.0}\n",
            "\n",
            "=== LENGTH STATS (tokens) ===\n",
            "{'count': 1000.0, 'mean': 53.713, 'std': 10.038, 'min': 34.0, '25%': 46.0, '50%': 53.0, '75%': 61.0, 'max': 98.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YUyLyd1zRN-"
      },
      "source": [
        "# Rabdom Summary sample for VIVA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPAcCYV6H3Oy",
        "outputId": "dec282c7-863b-48dd-b4c8-cc9bead25d6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "TEST_RUN_DIR: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/Hybrid1/test_20250827_090855\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "TEST_RUN_DIR = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/Hybrid1/test_20250827_090855\"\n",
        "BEST_CSV     = f\"{TEST_RUN_DIR}/best_h1_test_20250827_090855.csv\"\n",
        "CFG_JSON     = f\"{TEST_RUN_DIR}/frozen_config_h1_used_20250827_090855.json\"\n",
        "WTS_JSON     = f\"{TEST_RUN_DIR}/lr_weights_h1_used_20250827_090855.json\"\n",
        "BEAMS_PQ     = f\"{TEST_RUN_DIR}/beams_test.parquet\"\n",
        "STAGEA_PQ    = f\"{TEST_RUN_DIR}/stageA_survivors.parquet\"\n",
        "FEATS_PQ     = f\"{TEST_RUN_DIR}/features_test.parquet\"\n",
        "\n",
        "print(\"TEST_RUN_DIR:\", TEST_RUN_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tMo6lxMDPcD",
        "outputId": "4d742aaa-1922-4448-b562-e445b5be6f09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Loaded:\n",
            " - best: 1,000 rows  | cols: ['row_id', 'pool', 'candidate_k', 'article', 'highlights', 'summary', 'len', 'base_score'] ...\n",
            " - beams: 20,000 rows  | cols: ['row_id', 'pool', 'candidate_k', 'article', 'highlights', 'summary', 'len', 'base_score'] ...\n",
            " - stageA: 9,501 rows  | cols: ['row_id', 'pool', 'candidate_k', 'article', 'highlights', 'summary', 'len', 'base_score'] ...\n",
            " - feats: 9,501 rows  | cols: ['row_id', 'article', 'highlights', 'summary', 'THC_norm', 'LeadSim', 'MBR', 'len_reward'] ...\n"
          ]
        }
      ],
      "source": [
        "!pip -q install pandas pyarrow ipywidgets rouge-score spacy\n",
        "!python -m spacy download en_core_web_sm -q\n",
        "\n",
        "import json, random, textwrap, pandas as pd, numpy as np, IPython\n",
        "from IPython.display import display, HTML\n",
        "import spacy\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "random.seed(0)\n",
        "\n",
        "best   = pd.read_csv(BEST_CSV)\n",
        "beams  = pd.read_parquet(BEAMS_PQ)\n",
        "stageA = pd.read_parquet(STAGEA_PQ)\n",
        "feats  = pd.read_parquet(FEATS_PQ)\n",
        "\n",
        "with open(CFG_JSON) as f: cfg = json.load(f)\n",
        "with open(WTS_JSON) as f: wts = json.load(f)\n",
        "\n",
        "print(\"Loaded:\")\n",
        "for name, df in [(\"best\",best),(\"beams\",beams),(\"stageA\",stageA),(\"feats\",feats)]:\n",
        "    print(f\" - {name}: {len(df):,} rows  | cols: {list(df.columns)[:8]} ...\")\n",
        "\n",
        "keep_cols = [\"row_id\",\"pool\",\"candidate_k\",\"article\",\"highlights\",\"summary\",\"len\",\"base_score\",\n",
        "             \"THC\",\"hard_fail\",\"unsup_nonc\",\"base_norm\",\"_pool_ord\"]\n",
        "for df in (best, beams, stageA):\n",
        "    for c in list(df.columns):\n",
        "        if c not in keep_cols and c not in (\"_pool_ord\",):\n",
        "            # keep article/highlights/summary even if named slightly different\n",
        "            pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2v80XJPdDarm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec9f9f42-b9eb-4dc3-f3df-9419aca8f41c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/spacy/util.py:922: UserWarning: [W095] Model 'en_core_web_sm' (3.7.1) was trained with spaCy v3.7.2 and may not be 100% compatible with the current version (3.8.7). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  def get_model_meta(path: Union[str, Path]) -> Dict[str, Any]:\n"
          ]
        }
      ],
      "source": [
        "def _wrap(s, width=110):\n",
        "    return \"<br>\".join(textwrap.wrap(str(s), width=width))\n",
        "\n",
        "def _find_baseline(beams_rowset):\n",
        "    \"\"\"\n",
        "    Heuristic: take TOP pool, first candidate by _pool_ord if available,\n",
        "    else the highest base_score in TOP pool.\n",
        "    \"\"\"\n",
        "    sub = beams_rowset[beams_rowset[\"pool\"].astype(str).str.upper()==\"TOP\"]\n",
        "    if sub.empty:\n",
        "        # fallback: take overall best by base_score\n",
        "        sub = beams_rowset.sort_values(\"base_score\", ascending=False).head(1)\n",
        "        return sub.iloc[0]\n",
        "    if \"_pool_ord\" in sub.columns and sub[\"_pool_ord\"].notna().any():\n",
        "        sub2 = sub.sort_values([\"_pool_ord\",\"candidate_k\"], ascending=[True,True]).head(1)\n",
        "        return sub2.iloc[0]\n",
        "    else:\n",
        "        sub2 = sub.sort_values(\"base_score\", ascending=False).head(1)\n",
        "        return sub2.iloc[0]\n",
        "\n",
        "def _find_hybrid(best_rowset):\n",
        "    \"\"\"\n",
        "    Your reranker’s final pick is already in BEST (pool should be 'H1').\n",
        "    If multiple rows per row_id exist, pick the first.\n",
        "    \"\"\"\n",
        "    # Prefer H1; otherwise just take first row\n",
        "    sub = best_rowset[best_rowset[\"pool\"].astype(str).str.upper().isin([\"H1\",\"HYBRID\",\"RERANKED\"])]\n",
        "    if not sub.empty:\n",
        "        return sub.iloc[0]\n",
        "    return best_rowset.iloc[0]\n",
        "\n",
        "# Simple NER-based entity overlap\n",
        "_nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "CORE = {\"PERSON\",\"ORG\",\"GPE\"}\n",
        "\n",
        "def ents_set(text):\n",
        "    doc = _nlp(str(text))\n",
        "    # normalize: lowercase surface, keep coarse label\n",
        "    return {(e.text.strip().lower(), e.label_) for e in doc.ents if e.text.strip()}\n",
        "\n",
        "def ent_scores_vs_source(src, hyp):\n",
        "    src_set = ents_set(src)\n",
        "    hyp_set = ents_set(hyp)\n",
        "    tp = len(hyp_set & src_set)\n",
        "    fp = len(hyp_set - src_set)\n",
        "    fn = len(src_set - hyp_set)\n",
        "    prec = tp / (tp + fp) if (tp+fp)>0 else 0.0\n",
        "    rec  = tp / (tp + fn) if (tp+fn)>0 else 0.0\n",
        "    f1   = 2*prec*rec / (prec+rec) if (prec+rec)>0 else 0.0\n",
        "\n",
        "    # Core-only breakdown\n",
        "    def _core(S): return {(s,l) for (s,l) in S if l in CORE}\n",
        "    tp_c = len(_core(hyp_set & src_set))\n",
        "    fp_c = len(_core(hyp_set - src_set))\n",
        "    fn_c = len(_core(src_set - hyp_set))\n",
        "    prec_c = tp_c / (tp_c + fp_c) if (tp_c+fp_c)>0 else 0.0\n",
        "    rec_c  = tp_c / (tp_c + fn_c) if (tp_c+fn_c)>0 else 0.0\n",
        "    f1_c   = 2*prec_c*rec_c / (prec_c+rec_c) if (prec_c+rec_c)>0 else 0.0\n",
        "\n",
        "    return {\n",
        "        \"TP\": tp, \"FP\": fp, \"FN\": fn, \"entP\": prec, \"entR\": rec, \"entF1\": f1,\n",
        "        \"TP_core\": tp_c, \"FP_core\": fp_c, \"FN_core\": fn_c,\n",
        "        \"entP_core\": prec_c, \"entR_core\": rec_c, \"entF1_core\": f1_c\n",
        "    }\n",
        "\n",
        "# Single-example ROUGE (Lsum + 1/2)\n",
        "_scorer = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\"], use_stemmer=True)\n",
        "\n",
        "def rouge_row(ref, hyp):\n",
        "    r = _scorer.score(str(ref), str(hyp))\n",
        "    return {k: v.fmeasure for k,v in r.items()}\n",
        "\n",
        "def show_case(row_id=None, random_pick=True):\n",
        "    # Joinable indexes\n",
        "    ids = sorted(set(best[\"row_id\"]).intersection(set(beams[\"row_id\"])))\n",
        "    if not ids:\n",
        "        raise RuntimeError(\"No overlapping row_ids between BEST and BEAMS.\")\n",
        "    if row_id is None:\n",
        "        row_id = random.choice(ids) if random_pick else ids[0]\n",
        "\n",
        "    b_best  = best[best[\"row_id\"]==row_id]\n",
        "    b_beams = beams[beams[\"row_id\"]==row_id]\n",
        "    if b_best.empty or b_beams.empty:\n",
        "        raise RuntimeError(f\"row_id {row_id} not found in both tables.\")\n",
        "\n",
        "    # Pick baseline vs hybrid\n",
        "    base_pick = _find_baseline(b_beams)\n",
        "    hybr_pick = _find_hybrid(b_best)\n",
        "\n",
        "    # Shared source/ref\n",
        "    article    = hybr_pick.get(\"article\", base_pick.get(\"article\",\"\"))\n",
        "    reference  = hybr_pick.get(\"highlights\", base_pick.get(\"highlights\",\"\"))\n",
        "\n",
        "    # Metrics\n",
        "    ent_top  = ent_scores_vs_source(article, base_pick[\"summary\"])\n",
        "    ent_h1   = ent_scores_vs_source(article, hybr_pick[\"summary\"])\n",
        "    rg_top   = rouge_row(reference, base_pick[\"summary\"])\n",
        "    rg_h1    = rouge_row(reference, hybr_pick[\"summary\"])\n",
        "\n",
        "    # Render\n",
        "    html = f\"\"\"\n",
        "    <div style=\"font-family:system-ui,Segoe UI,Roboto,Helvetica,Arial; line-height:1.45; max-width:1200px\">\n",
        "      <h3 style=\"margin:0\">Viva Demo — Replay (row_id={row_id})</h3>\n",
        "      <p style=\"color:#777;margin-top:2px\">All outputs pulled from the frozen test run: {TEST_RUN_DIR.split('/')[-1]}</p>\n",
        "\n",
        "      <h4 style=\"margin:8px 0\">Article</h4>\n",
        "      <div style=\"background:#f7f7f7;border:1px solid #eee;border-radius:8px;padding:10px\">{_wrap(article)}</div>\n",
        "\n",
        "      <h4 style=\"margin:14px 0 6px\">Reference (gold highlights)</h4>\n",
        "      <div style=\"background:#fafafa;border:1px solid #eee;border-radius:8px;padding:10px\">{_wrap(reference)}</div>\n",
        "\n",
        "      <div style=\"display:flex; gap:14px; margin-top:14px\">\n",
        "        <div style=\"flex:1\">\n",
        "          <h4 style=\"margin:6px 0\">Baseline — TOP beam</h4>\n",
        "          <div style=\"background:#fff;border:1px solid #e6e6e6;border-radius:8px;padding:10px\">{_wrap(base_pick['summary'])}</div>\n",
        "          <div style=\"font-size:13px;color:#444;margin-top:6px\">\n",
        "            <b>ROUGE</b> R1={rg_top['rouge1']:.3f} R2={rg_top['rouge2']:.3f} RLsum={rg_top['rougeLsum']:.3f}\n",
        "            <br><b>Entity vs Source</b> P={ent_top['entP']:.3f} R={ent_top['entR']:.3f} F1={ent_top['entF1']:.3f}\n",
        "            <br><b>Core (PER/ORG/GPE)</b> P={ent_top['entP_core']:.3f} R={ent_top['entR_core']:.3f} F1={ent_top['entF1_core']:.3f}\n",
        "          </div>\n",
        "        </div>\n",
        "        <div style=\"flex:1\">\n",
        "          <h4 style=\"margin:6px 0\">Hybrid — H1 pick</h4>\n",
        "          <div style=\"background:#fff;border:1px solid #e6e6e6;border-radius:8px;padding:10px\">{_wrap(hybr_pick['summary'])}</div>\n",
        "          <div style=\"font-size:13px;color:#444;margin-top:6px\">\n",
        "            <b>ROUGE</b> R1={rg_h1['rouge1']:.3f} R2={rg_h1['rouge2']:.3f} RLsum={rg_h1['rougeLsum']:.3f}\n",
        "            <br><b>Entity vs Source</b> P={ent_h1['entP']:.3f} R={ent_h1['entR']:.3f} F1={ent_h1['entF1']:.3f}\n",
        "            <br><b>Core (PER/ORG/GPE)</b> P={ent_h1['entP_core']:.3f} R={ent_h1['entR_core']:.3f} F1={ent_h1['entF1_core']:.3f}\n",
        "          </div>\n",
        "        </div>\n",
        "      </div>\n",
        "\n",
        "      <details style=\"margin-top:12px\">\n",
        "        <summary style=\"cursor:pointer\">Show beam pool (first 10) for this article</summary>\n",
        "        {beams[beams['row_id']==row_id].sort_values(['pool','_pool_ord','base_score'], ascending=[True,True,False]).head(10)[['pool','_pool_ord','candidate_k','base_score','len']].to_html(index=False)}\n",
        "      </details>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    display(HTML(html))\n",
        "    return {\n",
        "        \"row_id\": row_id,\n",
        "        \"baseline\": base_pick,\n",
        "        \"hybrid\": hybr_pick,\n",
        "        \"ent_top\": ent_top, \"ent_h1\": ent_h1,\n",
        "        \"rouge_top\": rg_top, \"rouge_h1\": rg_h1\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = show_case(row_id=35, random_pick=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_nvRM1vLMmrS",
        "outputId": "1c7e8c1b-3498-4c69-8e1d-09194d2c7b69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div style=\"font-family:system-ui,Segoe UI,Roboto,Helvetica,Arial; line-height:1.45; max-width:1200px\">\n",
              "      <h3 style=\"margin:0\">Viva Demo — Replay (row_id=35)</h3>\n",
              "      <p style=\"color:#777;margin-top:2px\">All outputs pulled from the frozen test run: test_20250827_090855</p>\n",
              "\n",
              "      <h4 style=\"margin:8px 0\">Article</h4>\n",
              "      <div style=\"background:#f7f7f7;border:1px solid #eee;border-radius:8px;padding:10px\">The terrorist group Al-Shabaab has claimed an attack on Garissa University College in eastern Kenya, in which<br>many people have been killed and still more taken hostage. The attack is another step in the ongoing<br>escalation of the terrorist group's activities, and a clear indicator that the security situation in East<br>Africa is deteriorating fast. Somalia-based Al-Shabaab has been behind a string of recent attacks in Kenya,<br>the most well-known of them being the massacre at the Westgate Shopping Centre in Nairobi in 2013. Cross-<br>border raids into Kenya by the group, however, date back to 2011. Al-Shabaab incursions triggered a military<br>response by the government in Nairobi, which sent troops to Somalia as part of an African Union mission in<br>support of Somalia's internationally recognized government that had been under pressure from Al-Shabaab and<br>other militants for several years. Al-Shabaab is predominantly driven by the same radical interpretation of<br>the Koran as al-Qaeda and ISIS (also known as Islamic State), but also employs more opportunistic approaches<br>to shoring up local support. Its origins lie in Al-Ittihad al-Islami (Unity of Islam), one of several militant<br>factions that emerged in the wake of the fall of Siad Barre in 1991. These disparate groups fought each other<br>and a U.N. peacekeeping mission in the Somali civil war that led to the complete collapse of the country, from<br>which it has yet to recover almost quarter of a century later. Al-Shabaab (literally \"the Youth\") split from<br>Unity of Islam in 2003 and merged with another radical Islamist group, the so-called Islamic Courts Union. As<br>their alliance obtained control of Somalia's capital Mogadishu in 2006, Ethiopia, the only majority Christian<br>country in the region, took military action against the group. The offensive weakened Al-Shabaab and pushed it<br>back into the rural areas of central and southern Somalia, but it failed to defeat it. To the contrary,<br>Ethiopia's invasion and occupation of parts of Somalia -- although invited by the Somali government and backed<br>by the African Union -- enabled Al-Shabaab to partially re-invent itself as both an Islamist and nationalist<br>force opposing a foreign \"Christian\" invasion. Initially, the group primarily attacked Ethiopian forces, but<br>soon began to \"expand\" its activities against the Somali government as well. The first attack outside Somalia<br>was an attack in the Ugandan capital of Kampala in 2010. Soon after this, cross-border raids in Kenya began,<br>predominantly targeting Christians there. Increasing its links with al-Qaeda, Al-Shabaab declared its full<br>allegiance in 2012 -- and it is not clear whether it will switch allegiances to ISIS. Much will depend on how<br>the relationships between al-Qaeda in the Arabian Peninsula (AQAP), a long-time ally of Al-Shabaab based in<br>Yemen, and ISIS develop. The key point is that Al-Shabaab's attack in Garissa is part of a broader regional<br>context of instability fueled by a huge number of factors. It must not be interpreted simply as another act of<br>garden-variety fundamentalist terrorism. Clearly, the presence and activities of terrorist groups in the<br>region is a major concern, and it is undoubtedly driven by radical and exclusivist interpretations of Islam.<br>But the entire region also suffers from a range of other problems: from economic development challenges to<br>environmental degradation; from organized crime to inter-tribal and inter-communal violence; from corruption<br>to serious deficits in human rights and good governance. These entrenched inequalities help Al-Shabaab appeal<br>to a wide variety of potential recruits, who may sympathize with and actively support the group for any number<br>of reasons. Attacking a university in northern Kenya and separating Christian from Muslim students epitomizes<br>the way Al-Shabaab advances itself by exploiting religious, tribal and nationalist identities. Ultimately,<br>though, this all comes down to a struggle for control -- over people, over territory, and over resources. As<br>long as the majority of people in the region remain excluded from any meaningful political, economic, and<br>social participation in their societies -- which are dominated by primarily self-interested elites that put<br>their own advance before that of their communities -- human lives matter little in the pursuit of selfish<br>interests. It is important to counter Al-Shabaab directly, including by military means. But there won't be any<br>lasting solution to the wider region's security problems without a more comprehensive and concerted effort to<br>address the deeper problems of exclusion suffered by the citizens of the countries challenged by Al-Shabaab.<br>As Garissa shows, these problems are still providing oxygen for nihilistic ideologies and their deadly fruit.<br>Copyright 2015 The Conversation. Some rights reserved.</div>\n",
              "\n",
              "      <h4 style=\"margin:14px 0 6px\">Reference (gold highlights)</h4>\n",
              "      <div style=\"background:#fafafa;border:1px solid #eee;border-radius:8px;padding:10px\">Terrorist group Al-Shabaab has attacked a Kenyan college, killing and taking hostages . It is a clear<br>indicator the security situation in East Africa is deteriorating, says Stefan Wolff . More than military<br>action aloe is needed to combat terrorism in the region, he says .</div>\n",
              "\n",
              "      <div style=\"display:flex; gap:14px; margin-top:14px\">\n",
              "        <div style=\"flex:1\">\n",
              "          <h4 style=\"margin:6px 0\">Baseline — TOP beam</h4>\n",
              "          <div style=\"background:#fff;border:1px solid #e6e6e6;border-radius:8px;padding:10px\">Al-Shabaab claims attack on Garissa University College in eastern Kenya. The attack is another step in the<br>ongoing escalation of the terrorist group's activities. The group has been behind a string of recent attacks<br>in Kenya, the most well-known of them being the massacre at the Westgate Shopping Centre in Nairobi.</div>\n",
              "          <div style=\"font-size:13px;color:#444;margin-top:6px\">\n",
              "            <b>ROUGE</b> R1=0.265 R2=0.062 RLsum=0.163\n",
              "            <br><b>Entity vs Source</b> P=1.000 R=0.100 F1=0.182\n",
              "            <br><b>Core (PER/ORG/GPE)</b> P=1.000 R=0.160 F1=0.276\n",
              "          </div>\n",
              "        </div>\n",
              "        <div style=\"flex:1\">\n",
              "          <h4 style=\"margin:6px 0\">Hybrid — H1 pick</h4>\n",
              "          <div style=\"background:#fff;border:1px solid #e6e6e6;border-radius:8px;padding:10px\">Al-Shabaab has claimed an attack on Garissa University College in eastern Kenya. The attack is another step in<br>the ongoing escalation of the terrorist group's activities. The group is predominantly driven by the same<br>radical interpretation of the Koran as al-Qaeda and ISIS.</div>\n",
              "          <div style=\"font-size:13px;color:#444;margin-top:6px\">\n",
              "            <b>ROUGE</b> R1=0.311 R2=0.091 RLsum=0.200\n",
              "            <br><b>Entity vs Source</b> P=1.000 R=0.120 F1=0.214\n",
              "            <br><b>Core (PER/ORG/GPE)</b> P=1.000 R=0.200 F1=0.333\n",
              "          </div>\n",
              "        </div>\n",
              "      </div>\n",
              "\n",
              "      <details style=\"margin-top:12px\">\n",
              "        <summary style=\"cursor:pointer\">Show beam pool (first 10) for this article</summary>\n",
              "        <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>pool</th>\n",
              "      <th>_pool_ord</th>\n",
              "      <th>candidate_k</th>\n",
              "      <th>base_score</th>\n",
              "      <th>len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>CN2_LONG</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>CN2_LONG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>CN2_LONG</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>CN2_LONG</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>CN2_LONG</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>CN2_LONG</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>CN2_LONG</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>CN2_LONG</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>CN2_LONG</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>CN2_LONG</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>73</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "      </details>\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EKgrekHEDtWm",
        "outputId": "a851e17b-955c-4c3a-abaa-d9a5c58381f3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div style=\"font-family:system-ui,Segoe UI,Roboto,Helvetica,Arial; line-height:1.45; max-width:1200px\">\n",
              "      <h3 style=\"margin:0\">Viva Demo — Replay (row_id=923)</h3>\n",
              "      <p style=\"color:#777;margin-top:2px\">All outputs pulled from the frozen test run: test_20250827_090855</p>\n",
              "\n",
              "      <h4 style=\"margin:8px 0\">Article</h4>\n",
              "      <div style=\"background:#f7f7f7;border:1px solid #eee;border-radius:8px;padding:10px\">Washington A year after ISIS became a household name in America, using brutality and savvy propaganda to<br>challenge al Qaeda and its affiliates for jihadist adherents, U.S. prosecutions of would-be recruits have<br>exploded. The flurry of arrests -- at least 25 people have been detained since January -- is a sign that<br>complicated, manpower-intensive investigations begun when ISIS started seizing swaths of territory a year ago<br>are finally being completed. But they also highlight the unique challenges that ISIS poses in comparison with<br>al Qaeda, which has attracted fewer U.S.-based recruits. Like a new rock band storming the music charts, ISIS<br>has benefited from a media environment that amplifies its propaganda, law enforcement officials said. The<br>group quickly reached early recruits through videos that showcased the fear its adherents instilled in<br>nonbelievers. At first, most of the recruits were self-starters -- people radicalized on their own from<br>consuming ISIS propaganda from YouTube videos and other social media. Much of the propaganda comes in the form<br>of slick movie trailer-style videos, some glorifying brutal practices such as the beheading of anyone who ISIS<br>leaders decide doesn't comport with their medieval brand of Islam. But once those initial Western recruits<br>arrived, living in the self-declared ISIS caliphate spanning parts of Syria and Iraq, they started to directly<br>entice friends and other contacts back home to join them. In Minnesota, nine men have been charged as part of<br>an alleged cell of recruits linked to American Abdi Nur, who turned up fighting with ISIS in Syria in 2014 and<br>began to appeal to his friends to come to the Middle East. \"Each one of those folks who makes it over there<br>has the capability to reach out back to their contacts back here,\" a senior U.S. counterterrorism official<br>said. It's a phenomenon observed in Norway and other European nations, where clusters of young people have<br>been lured to ISIS. And the ISIS recruiters have an easier path to drawing supporters than al Qaeda has had. A<br>decade ago, that group's recruits faced formidable obstacles trying to get to training camps deep in hard-to-<br>reach areas of Afghanistan and Pakistan's tribal region. Few Westerners went through the trouble. Minneapolis<br>men allegedly trying to join ISIS highlight role of American recruiter . Today, ISIS occupies much more<br>accessible territory, mostly reachable through Turkey. Istanbul's airport has easy connections to Western<br>Europe and much of the rest of the world. From there, Turkey's modern infrastructure offers quick access to<br>the border regions where smugglers can help jihadis get across to Syria. The informal recruitment networks and<br>ease of travel have presented a difficult puzzle to intelligence and counterterrorism officials, who are used<br>to tracking networks of facilitators and fundraisers that funnel recruits eastward. \"It's harder for us to<br>pick up on,\" the U.S. counterterrorism official said of the peer-to-peer recruitment, which is well below the<br>radar. How ISIS is luring Westerners . Before ISIS, investigators could often focus on radicalizing mosques<br>and clerics to figure out those networks. Al Qaeda recruitment focused on attracting radicals who were<br>motivated to join the fight to protect Islamic holy lands. Much of the recruitment occurred in countries with<br>strong conservative Islamic histories, including Saudi Arabia and Yemen, U.S. officials said. In contrast,<br>ISIS takes a somewhat secular approach, portraying how much better life purportedly is in the caliphate as<br>compared to the corrupt West. And people attracted to ISIS' marketing run the gamut from rich to poor,<br>educated to dropout, male to female, teenaged to middle-aged. There are signs Western recruits have risen to<br>high levels in the ISIS organization, with their influence reflected in the latest propaganda,<br>counterterrorism and intelligence officials said. The English is proper, with fewer grammatical and spelling<br>mistakes. And while the large number of arrests show that law enforcement officials are succeeding in their<br>disruption efforts, it also means that U.S. authorities don't see the lure of ISIS receding any time soon. \"We<br>are opening cases quicker than we are closing them,\" the U.S. counterterrorism official said. Who has been<br>recruited to ISIS from the West?</div>\n",
              "\n",
              "      <h4 style=\"margin:14px 0 6px\">Reference (gold highlights)</h4>\n",
              "      <div style=\"background:#fafafa;border:1px solid #eee;border-radius:8px;padding:10px\">The recruiting tactics used by ISIS differ from those traditionally employed by al Qaeda . ISIS benefits from<br>a media environment that amplifies its propaganda, officials say .</div>\n",
              "\n",
              "      <div style=\"display:flex; gap:14px; margin-top:14px\">\n",
              "        <div style=\"flex:1\">\n",
              "          <h4 style=\"margin:6px 0\">Baseline — TOP beam</h4>\n",
              "          <div style=\"background:#fff;border:1px solid #e6e6e6;border-radius:8px;padding:10px\">The flurry of arrests is a sign that complicated, manpower-intensive investigations begun when ISIS started<br>seizing swaths of territory a year ago are finally being completed. But they also highlight the unique<br>challenges that ISIS poses in comparison with al Qaeda, which has attracted fewer U.S.-based recruits. ISIS<br>has benefited from a media environment that amplifies its propaganda.</div>\n",
              "          <div style=\"font-size:13px;color:#444;margin-top:6px\">\n",
              "            <b>ROUGE</b> R1=0.345 R2=0.212 RLsum=0.322\n",
              "            <br><b>Entity vs Source</b> P=1.000 R=0.075 F1=0.140\n",
              "            <br><b>Core (PER/ORG/GPE)</b> P=1.000 R=0.105 F1=0.190\n",
              "          </div>\n",
              "        </div>\n",
              "        <div style=\"flex:1\">\n",
              "          <h4 style=\"margin:6px 0\">Hybrid — H1 pick</h4>\n",
              "          <div style=\"background:#fff;border:1px solid #e6e6e6;border-radius:8px;padding:10px\">More than 25 people have been detained since January. The flurry of arrests are a sign that complicated,<br>manpower-intensive investigations begun when ISIS started seizing swaths of territory a year ago are finally<br>being completed. The arrests highlight the unique challenges that ISIS poses in comparison with al Qaeda.</div>\n",
              "          <div style=\"font-size:13px;color:#444;margin-top:6px\">\n",
              "            <b>ROUGE</b> R1=0.184 R2=0.027 RLsum=0.105\n",
              "            <br><b>Entity vs Source</b> P=0.800 R=0.100 F1=0.178\n",
              "            <br><b>Core (PER/ORG/GPE)</b> P=1.000 R=0.105 F1=0.190\n",
              "          </div>\n",
              "        </div>\n",
              "      </div>\n",
              "\n",
              "      <details style=\"margin-top:12px\">\n",
              "        <summary style=\"cursor:pointer\">Show beam pool (first 10) for this article</summary>\n",
              "        <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>pool</th>\n",
              "      <th>_pool_ord</th>\n",
              "      <th>candidate_k</th>\n",
              "      <th>base_score</th>\n",
              "      <th>len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>CN2_LONG</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>CN2_LONG</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>CN2_LONG</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>CN2_LONG</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>CN2_LONG</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>CN2_LONG</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>CN2_LONG</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>CN2_LONG</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>CN2_LONG</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>CN2_LONG</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "      </details>\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "out = show_case(row_id=None, random_pick=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLeZp-egTtA2"
      },
      "source": [
        "# Train 6k to 7k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFja2quNTyR8",
        "outputId": "7d993522-3ccd-479b-a60e-bc1ebd1f7abc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[resume] loaded pointer_head.pt\n",
            "[encoder] unfroze last 2 encoder layer(s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tail] upd   50/1000 | loss  2.6697 | ce  2.2645 | cov  0.3377 | p_copy 0.247 | p_gen 0.753 | lr 1.53e-06 | grad    2.44 | toks/s  16461.3 | mem   16509 MB\n",
            "[tail] upd  100/1000 | loss  2.6701 | ce  2.2653 | cov  0.3374 | p_copy 0.245 | p_gen 0.755 | lr 3.00e-06 | grad    2.46 | toks/s  16479.4 | mem   16509 MB\n",
            "[tail] upd  150/1000 | loss  2.6755 | ce  2.2707 | cov  0.3373 | p_copy 0.244 | p_gen 0.756 | lr 3.00e-06 | grad    2.58 | toks/s  16499.3 | mem   16509 MB\n",
            "[tail] upd  200/1000 | loss  2.6848 | ce  2.2797 | cov  0.3376 | p_copy 0.243 | p_gen 0.757 | lr 3.00e-06 | grad    2.54 | toks/s  16514.3 | mem   16509 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'forced_eos_token_id': 2}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tail] upd  250/1000 | loss  2.6875 | ce  2.2831 | cov  0.3370 | p_copy 0.241 | p_gen 0.759 | lr 3.00e-06 | grad    2.42 | toks/s  16540.9 | mem   16509 MB\n",
            "[checkpoint] saved → /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tail] upd  300/1000 | loss  2.6961 | ce  2.2915 | cov  0.3372 | p_copy 0.240 | p_gen 0.760 | lr 3.00e-06 | grad    2.50 | toks/s  16531.4 | mem   16509 MB\n",
            "[tail] upd  350/1000 | loss  2.6931 | ce  2.2898 | cov  0.3360 | p_copy 0.240 | p_gen 0.760 | lr 3.00e-06 | grad    2.62 | toks/s  16517.0 | mem   16509 MB\n",
            "[tail] upd  400/1000 | loss  2.6863 | ce  2.2844 | cov  0.3349 | p_copy 0.241 | p_gen 0.759 | lr 3.00e-06 | grad    2.57 | toks/s  16503.8 | mem   16509 MB\n",
            "[tail] upd  450/1000 | loss  2.6757 | ce  2.2763 | cov  0.3329 | p_copy 0.243 | p_gen 0.757 | lr 3.00e-06 | grad    2.56 | toks/s  16492.6 | mem   16509 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'forced_eos_token_id': 2}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tail] upd  500/1000 | loss  2.6671 | ce  2.2704 | cov  0.3306 | p_copy 0.244 | p_gen 0.756 | lr 3.00e-06 | grad    2.40 | toks/s  16512.2 | mem   16509 MB\n",
            "[checkpoint] saved → /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tail] upd  550/1000 | loss  2.6678 | ce  2.2719 | cov  0.3299 | p_copy 0.245 | p_gen 0.755 | lr 3.00e-06 | grad    2.45 | toks/s  16519.5 | mem   16509 MB\n",
            "[tail] upd  600/1000 | loss  2.6691 | ce  2.2739 | cov  0.3294 | p_copy 0.246 | p_gen 0.754 | lr 3.00e-06 | grad    2.42 | toks/s  16523.4 | mem   16509 MB\n",
            "[tail] upd  650/1000 | loss  2.6829 | ce  2.2874 | cov  0.3296 | p_copy 0.245 | p_gen 0.755 | lr 3.00e-06 | grad    2.43 | toks/s  16526.1 | mem   16509 MB\n",
            "[tail] upd  700/1000 | loss  2.6874 | ce  2.2920 | cov  0.3295 | p_copy 0.246 | p_gen 0.754 | lr 3.00e-06 | grad    2.67 | toks/s  16527.2 | mem   16509 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'forced_eos_token_id': 2}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tail] upd  750/1000 | loss  2.6836 | ce  2.2895 | cov  0.3284 | p_copy 0.246 | p_gen 0.754 | lr 3.00e-06 | grad    2.39 | toks/s  16536.3 | mem   16509 MB\n",
            "[checkpoint] saved → /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6750\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tail] upd  800/1000 | loss  2.6734 | ce  2.2815 | cov  0.3266 | p_copy 0.247 | p_gen 0.753 | lr 3.00e-06 | grad    2.42 | toks/s  16532.8 | mem   16509 MB\n",
            "[tail] upd  850/1000 | loss  2.6757 | ce  2.2832 | cov  0.3271 | p_copy 0.247 | p_gen 0.753 | lr 3.00e-06 | grad    2.43 | toks/s  16528.0 | mem   16509 MB\n",
            "[tail] upd  900/1000 | loss  2.6664 | ce  2.2751 | cov  0.3261 | p_copy 0.248 | p_gen 0.752 | lr 3.00e-06 | grad    2.72 | toks/s  16522.6 | mem   16509 MB\n",
            "[tail] upd  950/1000 | loss  2.6602 | ce  2.2702 | cov  0.3250 | p_copy 0.250 | p_gen 0.750 | lr 3.00e-06 | grad    2.40 | toks/s  16517.3 | mem   16509 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'forced_eos_token_id': 2}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tail] upd 1000/1000 | loss  2.6647 | ce  2.2750 | cov  0.3247 | p_copy 0.251 | p_gen 0.749 | lr 3.00e-06 | grad    2.45 | toks/s  16523.5 | mem   16509 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'forced_eos_token_id': 2}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[checkpoint] saved → /content/drive/MyDrive/student_pgc_bartbase/ckpt_step7000\n",
            "[done] Tail training complete. Saved → /content/drive/MyDrive/student_pgc_bartbase/ckpt_step7000\n"
          ]
        }
      ],
      "source": [
        "# ==== RESUME 6000 → TRAIN +1000 TAIL STEPS (logs CE/cov/p_copy/p_gen) ====\n",
        "import os, math, time, csv, torch\n",
        "import torch.nn as nn\n",
        "from torch import amp\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM\n",
        "\n",
        "# ------------------- CONFIG -------------------\n",
        "RESUME_CKPT = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000\"\n",
        "RUN_DIR     = \"/content/drive/MyDrive/student_pgc_bartbase\"\n",
        "\n",
        "OUT_STEP    = 7000\n",
        "TAIL_STEPS  = 1000\n",
        "WARMUP_TAIL = 100\n",
        "LR_TAIL     = 3e-6\n",
        "USE_COSINE  = False\n",
        "\n",
        "GRAD_CLIP   = 1.0\n",
        "BATCH_SIZE  = 56\n",
        "GRAD_ACCUM_STEPS = 2\n",
        "MAX_SRC_LEN = 400\n",
        "MAX_TGT_LEN = 100\n",
        "WEIGHT_DECAY= 0.01\n",
        "FP16        = True\n",
        "SEED        = 0\n",
        "\n",
        "# tiny encoder unfreeze for a bump:\n",
        "UNFREEZE_LAST_N_ENCODER_LAYERS = 2\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ------------------- LOAD TOKENIZER + BASE + PGC WRAPPER -------------------\n",
        "tok  = AutoTokenizer.from_pretrained(RESUME_CKPT, use_fast=True)\n",
        "cfg  = AutoConfig.from_pretrained(RESUME_CKPT, attn_implementation=\"eager\")\n",
        "base = AutoModelForSeq2SeqLM.from_pretrained(RESUME_CKPT, config=cfg)\n",
        "base.config.use_cache = False\n",
        "base.gradient_checkpointing_enable()\n",
        "\n",
        "model = CopyAwareBart(base, tok, lambda_cov=1.2, eps=1e-8, use_pointer=True).to(DEVICE)\n",
        "\n",
        "pth = os.path.join(RESUME_CKPT, \"pointer_head.pt\")\n",
        "if os.path.exists(pth):\n",
        "    sd = torch.load(pth, map_location=\"cpu\")\n",
        "    model.p_gen_linear.load_state_dict(sd[\"p_gen_linear\"])\n",
        "    model.lambda_cov = float(sd.get(\"lambda_cov\", model.lambda_cov))\n",
        "    model.use_ptr    = bool(sd.get(\"use_pointer\", model.use_ptr))\n",
        "    print(\"[resume] loaded pointer_head.pt\")\n",
        "\n",
        "# --- unfreeze the last N encoder layers for a small quality bump ---pt\n",
        "if hasattr(model.base.model, \"encoder\") and UNFREEZE_LAST_N_ENCODER_LAYERS > 0:\n",
        "    # freeze all encoder params, then unfreeze the tail-N layers\n",
        "    for p in model.base.model.encoder.parameters():\n",
        "        p.requires_grad = False\n",
        "    for layer in model.base.model.encoder.layers[-UNFREEZE_LAST_N_ENCODER_LAYERS:]:\n",
        "        for p in layer.parameters():\n",
        "            p.requires_grad = True\n",
        "    print(f\"[encoder] unfroze last {UNFREEZE_LAST_N_ENCODER_LAYERS} encoder layer(s)\")\n",
        "else:\n",
        "    # fully freeze the encoder\n",
        "    if hasattr(model.base.model, \"encoder\"):\n",
        "        for p in model.base.model.encoder.parameters():\n",
        "            p.requires_grad = False\n",
        "    print(\"[encoder] kept fully frozen\")\n",
        "\n",
        "# ------------------- DATA -------------------\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "train_df = robust_read_csv(\"/content/drive/MyDrive/cleaned_outputs/naa/train_cleaned_safe_boilerfree_sourcetag_near_deduped_fast_applied.csv\")\n",
        "collate  = PGDataCollator(tok=tok, max_src_len=MAX_SRC_LEN, max_tgt_len=MAX_TGT_LEN)\n",
        "train_loader = DataLoader(\n",
        "    CNNDMDataset(train_df),\n",
        "    batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=False,\n",
        "    persistent_workers=False, collate_fn=collate,\n",
        ")\n",
        "\n",
        "# ------------------- OPTIM + SCHED -------------------\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\"]\n",
        "params_decay, params_nodecay = [], []\n",
        "for n, p in model.named_parameters():\n",
        "    if not p.requires_grad:\n",
        "        continue\n",
        "    (params_nodecay if any(nd in n for nd in no_decay) else params_decay).append(p)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [{\"params\": params_decay,   \"weight_decay\": WEIGHT_DECAY},\n",
        "     {\"params\": params_nodecay, \"weight_decay\": 0.0}],\n",
        "    lr=LR_TAIL\n",
        ")\n",
        "\n",
        "# constant tail LR with short warmup\n",
        "def lr_lambda(step_idx):\n",
        "    if step_idx < WARMUP_TAIL:\n",
        "        return float(step_idx + 1) / float(max(1, WARMUP_TAIL))\n",
        "    return 1.0\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "scaler = amp.GradScaler(\"cuda\", enabled=FP16)\n",
        "\n",
        "# ------------------- LOGGING HELPERS -------------------\n",
        "class RunningMean:\n",
        "    def __init__(self, n=200): self.n=n; self.buf=[]\n",
        "    def add(self, x):\n",
        "        if x is None: return\n",
        "        try: x=float(x)\n",
        "        except: return\n",
        "        self.buf.append(x)\n",
        "        if len(self.buf)>self.n: self.buf.pop(0)\n",
        "    @property\n",
        "    def mean(self): return sum(self.buf)/len(self.buf) if self.buf else 0.0\n",
        "\n",
        "def _gpu_mem_mb():\n",
        "    if not torch.cuda.is_available(): return 0.0\n",
        "    return torch.cuda.max_memory_allocated()/(1024**2)\n",
        "\n",
        "LOG_TAIL = os.path.join(RUN_DIR, \"train_log_tail.csv\")\n",
        "if not os.path.exists(LOG_TAIL):\n",
        "    with open(LOG_TAIL, \"w\") as f:\n",
        "        f.write(\"epoch,step,loss,ce_loss,cov_loss,p_copy,p_gen,lr,grad_norm,toks_per_s,gpu_mem_mb\\n\")\n",
        "\n",
        "m_loss=RunningMean(200); m_ce=RunningMean(200); m_cov=RunningMean(200)\n",
        "m_pcopy=RunningMean(200); m_pgen=RunningMean(200); m_toks=RunningMean(200)\n",
        "\n",
        "# ------------------- TRAIN LOOP (+1000 optimizer steps) -------------------\n",
        "local_updates = 0\n",
        "epoch = 0\n",
        "tokens_seen = 0\n",
        "start = time.time()\n",
        "SAVE_EVERY = 250\n",
        "\n",
        "model.train()\n",
        "optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "while local_updates < TAIL_STEPS:\n",
        "    epoch += 1\n",
        "    for it, batch in enumerate(train_loader):\n",
        "        for k in batch: batch[k] = batch[k].to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            toks_this = int(batch[\"attention_mask\"].sum().item())\n",
        "            if \"labels\" in batch: toks_this += int((batch[\"labels\"] != -100).sum().item())\n",
        "\n",
        "        with amp.autocast(\"cuda\", enabled=FP16):\n",
        "            out  = model(**batch)\n",
        "            loss = out.loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if ((it + 1) % GRAD_ACCUM_STEPS) == 0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            grad_norm = float(torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP))\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            scheduler.step()\n",
        "            local_updates += 1\n",
        "\n",
        "            # update logs\n",
        "            tokens_seen += toks_this\n",
        "            ce  = getattr(model, \"_last_ce_loss\", None)\n",
        "            cov = getattr(model, \"_last_cov_loss\", None)\n",
        "            pc  = getattr(model, \"_last_p_copy_mean\", None)\n",
        "            pg  = getattr(model, \"_last_p_gen_mean\",  None)\n",
        "\n",
        "            m_loss.add(loss.item())\n",
        "            m_ce.add(ce.item() if torch.is_tensor(ce) else ce)\n",
        "            m_cov.add(cov.item() if torch.is_tensor(cov) else cov)\n",
        "            m_pcopy.add(pc.item() if torch.is_tensor(pc) else pc)\n",
        "            m_pgen.add(pg.item() if torch.is_tensor(pg) else pg)\n",
        "            elapsed = max(time.time() - start, 1e-6)\n",
        "            m_toks.add(tokens_seen/elapsed)\n",
        "\n",
        "            lr = scheduler.get_last_lr()[0] if hasattr(scheduler, \"get_last_lr\") else optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "            if (local_updates % 50) == 0:\n",
        "                print(\n",
        "                    f\"[tail] upd {local_updates:>4}/{TAIL_STEPS} | \"\n",
        "                    f\"loss {m_loss.mean:7.4f} | ce {m_ce.mean:7.4f} | cov {m_cov.mean:7.4f} | \"\n",
        "                    f\"p_copy {m_pcopy.mean:5.3f} | p_gen {m_pgen.mean:5.3f} | \"\n",
        "                    f\"lr {lr:.2e} | grad {grad_norm:7.2f} | toks/s {m_toks.mean:8.1f} | mem {_gpu_mem_mb():7.0f} MB\"\n",
        "                )\n",
        "\n",
        "            with open(LOG_TAIL, \"a\") as f:\n",
        "                f.write(\",\".join(map(str, [\n",
        "                    epoch,\n",
        "                    6000 + local_updates,                  # label steps as global(ish)\n",
        "                    round(m_loss.mean, 6),\n",
        "                    round(m_ce.mean,   6),\n",
        "                    round(m_cov.mean,  6),\n",
        "                    round(m_pcopy.mean,6) if m_pcopy.buf else \"\",\n",
        "                    round(m_pgen.mean, 6) if m_pgen.buf  else \"\",\n",
        "                    lr,\n",
        "                    grad_norm,\n",
        "                    m_toks.mean,\n",
        "                    _gpu_mem_mb()\n",
        "                ])) + \"\\n\")\n",
        "\n",
        "            # periodic checkpoint\n",
        "            if (local_updates % SAVE_EVERY) == 0:\n",
        "                tag = 6000 + local_updates\n",
        "                save_dir = os.path.join(RUN_DIR, f\"ckpt_step{tag}\")\n",
        "                os.makedirs(save_dir, exist_ok=True)\n",
        "                # patch a minimal valid GenerationConfig before save\n",
        "                gc = model.base.generation_config\n",
        "                try: gc.num_beams = 2; gc.early_stopping = True\n",
        "                except: pass\n",
        "                model.base.save_pretrained(save_dir)\n",
        "                tok.save_pretrained(save_dir)\n",
        "                torch.save({\n",
        "                    \"p_gen_linear\": model.p_gen_linear.state_dict(),\n",
        "                    \"lambda_cov\": float(model.lambda_cov),\n",
        "                    \"use_pointer\": bool(model.use_ptr),\n",
        "                }, os.path.join(save_dir, \"pointer_head.pt\"))\n",
        "                print(f\"[checkpoint] saved → {save_dir}\")\n",
        "\n",
        "            if local_updates >= TAIL_STEPS:\n",
        "                break\n",
        "\n",
        "        # memory hygiene\n",
        "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "        del out\n",
        "\n",
        "    if local_updates >= TAIL_STEPS:\n",
        "        break\n",
        "\n",
        "# ---- final savestep 7000 ----\n",
        "final_dir = os.path.join(RUN_DIR, f\"ckpt_step{OUT_STEP}\")\n",
        "os.makedirs(final_dir, exist_ok=True)\n",
        "gc = model.base.generation_config\n",
        "try: gc.num_beams = 2; gc.early_stopping = True\n",
        "except: pass\n",
        "model.base.save_pretrained(final_dir)\n",
        "tok.save_pretrained(final_dir)\n",
        "torch.save({\n",
        "    \"p_gen_linear\": model.p_gen_linear.state_dict(),\n",
        "    \"lambda_cov\": float(model.lambda_cov),\n",
        "    \"use_pointer\": bool(model.use_ptr),\n",
        "}, os.path.join(final_dir, \"pointer_head.pt\"))\n",
        "print(f\"[done] Tail training complete. Saved → {final_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5xOUm7kjo5m"
      },
      "source": [
        "# Finding the best training checkpoint within steps 6000, 6500 and 7000 (Hybrid and Baseline val on 6500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoY8zlLEjuim",
        "outputId": "635eea09-68b1-42e3-cb1e-763cb2f42137"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -q install rouge-score==0.1.2 scikit-learn spacy\n",
        "import sys, os, json, time, math, csv, random, re, gc\n",
        "import numpy as np, pandas as pd, torch\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from rouge_score import rouge_scorer\n",
        "import spacy\n",
        "\n",
        "VAL_PATH  = \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "TEST_PATH = \"/content/drive/MyDrive/cleaned_outputs/naa/test_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "\n",
        "CKPT_ROOT = \"/content/drive/MyDrive/student_pgc_bartbase\"\n",
        "CKPTS = [\"ckpt_step6000\", \"ckpt_step6500\", \"ckpt_step7000\"]\n",
        "\n",
        "SEED = 0\n",
        "MAX_SRC_LEN = 400\n",
        "KEEP_LABELS = {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"DATE\",\"TIME\",\"CARDINAL\",\"MONEY\",\"PERCENT\",\"QUANTITY\",\"ORDINAL\"}\n",
        "CORE = {\"PERSON\",\"ORG\",\"GPE\"}\n",
        "NONCORE_PEN = 2.0\n",
        "LEN_SIGMA = 20.0\n",
        "M_KEEP = 10\n",
        "\n",
        "EA_ARGS = dict(gamma=0.4, gamma_entity=1.5, max_span=6)\n",
        "\n",
        "CN2_LONG = dict(num_beams=10, num_return_sequences=10, num_beam_groups=5, diversity_penalty=0.3,\n",
        "                max_new_tokens=100, min_new_tokens=55, no_repeat_ngram_size=3,\n",
        "                length_penalty=2.0, early_stopping=True, return_dict_in_generate=True, output_scores=True)\n",
        "\n",
        "CN_MED   = dict(num_beams=10, num_return_sequences=10, num_beam_groups=5, diversity_penalty=0.3,\n",
        "                max_new_tokens=70,  min_new_tokens=35, no_repeat_ngram_size=3,\n",
        "                length_penalty=1.6, early_stopping=True, return_dict_in_generate=True, output_scores=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "except OSError:\n",
        "    !python -m spacy download en_core_web_sm\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "\n",
        "\n",
        "def save_csv(df, path):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    tmp = path + \".tmp\"\n",
        "    df.to_csv(tmp, index=False, quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n",
        "    os.replace(tmp, path)\n",
        "\n",
        "def save_json(obj, path):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    with open(path,\"w\") as f: json.dump(obj, f, indent=2)\n",
        "\n",
        "def robust_read_csv(path: str) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path)\n",
        "    cols = {c.lower(): c for c in df.columns}\n",
        "    need = [\"article\",\"highlights\"]\n",
        "    if not all(n in cols for n in need): raise KeyError(f\"CSV must contain {need}\")\n",
        "    return df[[cols[\"article\"], cols[\"highlights\"]]].rename(columns={cols[\"article\"]:\"article\", cols[\"highlights\"]:\"highlights\"})\n",
        "\n",
        "_SC = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\"], use_stemmer=True)\n",
        "\n",
        "def rouge_mean(R,H):\n",
        "    r1=r2=rl=0.0\n",
        "    for r,h in zip(R,H):\n",
        "        s=_SC.score(str(r), str(h))\n",
        "        r1+=s[\"rouge1\"].fmeasure; r2+=s[\"rouge2\"].fmeasure; rl+=s[\"rougeLsum\"].fmeasure\n",
        "    n=len(R);\n",
        "    return {\"r1_f\":r1/n, \"r2_f\":r2/n, \"rl_f\":rl/n}\n",
        "\n",
        "\n",
        "_ENTS_CACHE={}\n",
        "def ents_cache_build(texts, batch_size=128):\n",
        "    uniq=list(dict.fromkeys(map(str,texts)))\n",
        "    for doc in nlp.pipe(uniq, batch_size=batch_size):\n",
        "        _ENTS_CACHE[doc.text] = {(\" \".join(e.text.strip().split()), e.label_) for e in doc.ents if e.label_ in KEEP_LABELS}\n",
        "def _ents_norm(t):\n",
        "    t=str(t)\n",
        "    if t in _ENTS_CACHE: return _ENTS_CACHE[t]\n",
        "    d=nlp(t)\n",
        "    return {(\" \".join(e.text.strip().split()), e.label_) for e in d.ents if e.label_ in KEEP_LABELS}\n",
        "\n",
        "def entPRF(refs, hyps):\n",
        "    TP=FP=FN=0\n",
        "    for r,h in zip(refs, hyps):\n",
        "        R=_ents_norm(r); H=_ents_norm(h)\n",
        "        TP+=len(H&R); FP+=len(H-R); FN+=len(R-H)\n",
        "    P=TP/(TP+FP) if (TP+FP) else 0.0\n",
        "    Rc=TP/(TP+FN) if (TP+FN) else 0.0\n",
        "    F=2*P*Rc/(P+Rc) if (P+Rc) else 0.0\n",
        "    return {\"TP\":TP,\"FP\":FP,\"FN\":FN,\"entP\":P,\"entR\":Rc,\"entF1\":F}\n",
        "\n",
        "def UCER(sources, hyps):\n",
        "    rates=[]; counts=[]\n",
        "    for s,h in zip(sources, hyps):\n",
        "        S=_ents_norm(s); H=_ents_norm(h)\n",
        "        bad=[(t,l) for (t,l) in (H-S) if l in CORE]\n",
        "        rates.append(1 if bad else 0); counts.append(len(bad))\n",
        "    return float(np.mean(rates)), float(np.mean(counts))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ucXIbnklEl9"
      },
      "outputs": [],
      "source": [
        "# === Decoding + TH-C + features + reranker ===\n",
        "\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM, LogitsProcessor, LogitsProcessorList\n",
        "\n",
        "def toklen(tok, s):\n",
        "    return len(tok(str(s))[\"input_ids\"])\n",
        "\n",
        "def lead3(text):\n",
        "    return \" \".join(re.split(r'(?<=[.!?])\\s+', str(text).strip())[:3])\n",
        "\n",
        "def adaptive_len_target(tok, src):\n",
        "    L = toklen(tok, lead3(src))\n",
        "    return int(np.clip(round(0.9*L), 32, 60))\n",
        "\n",
        "def lcs_span_stats(src_text, hyp_text):\n",
        "    s=str(src_text).split(); h=str(hyp_text).split()\n",
        "    n,m=len(s),len(h); dp=[[0]*(m+1) for _ in range(n+1)]\n",
        "    for i in range(n-1,-1,-1):\n",
        "        si=s[i]; row=dp[i]; row1=dp[i+1]\n",
        "        for j in range(m-1,-1,-1):\n",
        "            row[j]=row1[j+1]+1 if si==h[j] else max(row[j+1], row1[j])\n",
        "    i=j=0; blocks=[]; cur=0\n",
        "    while i<n and j<m:\n",
        "        if s[i]==h[j]: cur+=1; i+=1; j+=1\n",
        "        else:\n",
        "            if cur>=2: blocks.append(cur); cur=0\n",
        "            if dp[i+1][j]>=dp[i][j+1]: i+=1\n",
        "            else: j+=1\n",
        "    if cur>=2: blocks.append(cur)\n",
        "    cont=sum(b-1 for b in blocks)\n",
        "    av=(sum(blocks)/len(blocks)) if blocks else 1.0\n",
        "    return cont, av\n",
        "\n",
        "def strict_entity_score(src, hyp):\n",
        "    S=_ents_norm(src); H=_ents_norm(hyp)\n",
        "    overlap=H&S; unsupported=H-S\n",
        "    hard_fail=any(lbl in CORE for _,lbl in unsupported)\n",
        "    score=sum(3 if lbl in CORE else 1 for _,lbl in overlap)\n",
        "    return score, hard_fail\n",
        "\n",
        "def unsupported_nonc_count(src, hyp):\n",
        "    S=_ents_norm(src); H=_ents_norm(hyp)\n",
        "    return sum(1 for _,lbl in (H-S) if lbl in {\"DATE\",\"QUANTITY\",\"MONEY\",\"PERCENT\",\"CARDINAL\",\"ORDINAL\"})\n",
        "\n",
        "# ---- Entity-aware CopyNext processor (precomputed token maps) ----\n",
        "class EntityAwareSpanProcessor(LogitsProcessor):\n",
        "    def __init__(self, maps_list, num_beams, gamma=0.4, gamma_entity=1.5, max_span=6):\n",
        "        self.maps_list=maps_list; self.num_beams=int(num_beams)\n",
        "        self.gamma=float(gamma); self.gamma_entity=float(gamma_entity); self.max_span=int(max_span)\n",
        "    def __call__(self, input_ids, scores):\n",
        "        Bx=input_ids.size(0)\n",
        "        for b in range(Bx):\n",
        "            sample=b//self.num_beams\n",
        "            maps=self.maps_list[sample]; seq=input_ids[b].tolist()\n",
        "            for k in range(min(self.max_span, len(seq)), 0, -1):\n",
        "                mp=maps[k-1]; key=tuple(seq[-k:])\n",
        "                if key in mp:\n",
        "                    nxt, is_ent = mp[key]\n",
        "                    scores[b, nxt] = scores[b, nxt] + (self.gamma_entity if is_ent else self.gamma)\n",
        "                    break\n",
        "        return scores\n",
        "\n",
        "def build_copy_maps(tok, texts, max_span=EA_ARGS[\"max_span\"]):\n",
        "    maps_all=[]\n",
        "    docs = list(nlp.pipe(texts, batch_size=128))\n",
        "    enc  = tok(texts, max_length=MAX_SRC_LEN, truncation=True, return_offsets_mapping=True)\n",
        "    for i,_ in enumerate(texts):\n",
        "        ids  = enc[\"input_ids\"][i]; offs = enc[\"offset_mapping\"][i]\n",
        "        ent_spans = [(e.start_char, e.end_char) for e in docs[i].ents if e.label_ in KEEP_LABELS]\n",
        "        def in_ent(off):\n",
        "            a,b = off\n",
        "            if a is None: return False\n",
        "            for s,e in ent_spans:\n",
        "                if a>=s and b<=e: return True\n",
        "            return False\n",
        "        maps=[{} for _ in range(max_span)]\n",
        "        n=len(ids)\n",
        "        for k in range(1, max_span+1):\n",
        "            mp=maps[k-1]\n",
        "            for j in range(0, n-k):\n",
        "                key=tuple(ids[j:j+k]); nxt=ids[j+k]\n",
        "                is_ent = in_ent(offs[j+k]) if j+k<len(offs) else False\n",
        "                if key not in mp: mp[key]=(nxt, is_ent)\n",
        "        maps_all.append(maps)\n",
        "    return maps_all\n",
        "\n",
        "@torch.no_grad()\n",
        "def decode_two_pools(model, tok, articles, references, batch_long=4, batch_med=8):\n",
        "    \"\"\"Return full candidate details from CN2_LONG + CN_MED (20 per example).\"\"\"\n",
        "    from transformers import LogitsProcessorList\n",
        "    rows=[]\n",
        "    # Pre-tokenize sources and build maps\n",
        "    enc_long = tok(articles, max_length=MAX_SRC_LEN, truncation=True, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
        "    maps_all = build_copy_maps(tok, articles)\n",
        "    # helper\n",
        "    def run_pool(decode_args, pool_name, batch_size):\n",
        "        seq_rows=[]\n",
        "        for s in range(0, len(articles), batch_size):\n",
        "            e=min(len(articles), s+batch_size)\n",
        "            enc = {k: v[s:e] for k,v in enc_long.items()}\n",
        "            proc = EntityAwareSpanProcessor(maps_all[s:e], num_beams=decode_args[\"num_beams\"],\n",
        "                                            gamma=EA_ARGS[\"gamma\"], gamma_entity=EA_ARGS[\"gamma_entity\"],\n",
        "                                            max_span=EA_ARGS[\"max_span\"])\n",
        "            out = model.generate(**enc, **decode_args, logits_processor=LogitsProcessorList([proc]))\n",
        "            K=decode_args[\"num_return_sequences\"]; bs=e-s\n",
        "            seqs = out.sequences.view(bs, K, -1)\n",
        "            scores = getattr(out, \"sequences_scores\", None)\n",
        "            for bi in range(bs):\n",
        "                art=articles[s+bi]; ref=references[s+bi]\n",
        "                for k in range(K):\n",
        "                    ids=seqs[bi,k]\n",
        "                    hyp=tok.decode(ids, skip_special_tokens=True)\n",
        "                    seq_rows.append({\n",
        "                        \"row_id\": s+bi, \"pool\": pool_name, \"candidate_k\": k,\n",
        "                        \"article\": art, \"reference\": ref, \"summary\": hyp,\n",
        "                        \"len\": max(int(ids.size(0)-1),1),\n",
        "                        \"base_score\": float(scores[bi*K+k].item()) if scores is not None else 0.0\n",
        "                    })\n",
        "        return seq_rows\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        rows += run_pool(CN2_LONG, \"CN2_LONG\", 4)\n",
        "        rows += run_pool(CN_MED,   \"CN_MED\",   8)\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# ---- Stage-A (TH-C) gate ----\n",
        "def stageA_select(dd_all, tok, M=M_KEEP):\n",
        "    rows=[]\n",
        "    for rid, grp in dd_all.groupby(\"row_id\"):\n",
        "        art = grp.iloc[0][\"article\"]; tgt = adaptive_len_target(tok, art)\n",
        "        ent_scores=[]; fails=[]; contigs=[]; nonc=[]; lens=[]\n",
        "        for _,r in grp.iterrows():\n",
        "            s,f = strict_entity_score(r[\"article\"], r[\"summary\"])\n",
        "            c,_ = lcs_span_stats(r[\"article\"], r[\"summary\"])\n",
        "            u   = unsupported_nonc_count(r[\"article\"], r[\"summary\"])\n",
        "            ent_scores.append(s); fails.append(f); contigs.append(c); nonc.append(u); lens.append(max(int(r[\"len\"]),1))\n",
        "        ent_scores=np.array(ent_scores); fails=np.array(fails); contigs=np.array(contigs); nonc=np.array(nonc); L=np.array(lens)\n",
        "        len_reward = np.exp(-((L - tgt)**2)/(2*(LEN_SIGMA**2)))\n",
        "        penalty = (nonc>0).astype(float) * NONCORE_PEN\n",
        "        THC = (5.0*ent_scores) + (0.2*contigs) + (0.3*len_reward) - penalty\n",
        "        ok = grp.copy()\n",
        "        ok[\"THC\"]=THC; ok[\"hard_fail\"]=fails; ok[\"unsup_nonc\"]=nonc\n",
        "        ok = ok.loc[~ok[\"hard_fail\"]].copy()\n",
        "        if ok.empty:\n",
        "            ok = grp.copy(); ok[\"THC\"]=THC; ok[\"unsup_nonc\"]=nonc\n",
        "        rows.append(ok.sort_values(\"THC\", ascending=False).head(M))\n",
        "    return pd.concat(rows, ignore_index=True)\n",
        "\n",
        "# ---- base_norm (token CE normalized by length**0.3) ----\n",
        "def add_base_norm(stageA_df, model, tok, batch_size=16):\n",
        "    model.eval()\n",
        "    out = np.zeros(len(stageA_df), dtype=np.float32)\n",
        "    idxs = stageA_df.index.to_list()\n",
        "    for s in range(0, len(idxs), batch_size):\n",
        "        ii = idxs[s:s+batch_size]\n",
        "        sub = stageA_df.loc[ii]\n",
        "        enc = tok(sub[\"article\"].tolist(), max_length=MAX_SRC_LEN, truncation=True, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
        "        dec = tok(sub[\"summary\"].tolist(), max_length=256, truncation=True, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
        "        logits = model(**enc, labels=dec[\"input_ids\"], return_dict=True).logits.float()\n",
        "        shift_logits = logits[:, :-1, :]\n",
        "        shift_labels = dec[\"input_ids\"][:, 1:]\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=tok.pad_token_id, reduction=\"none\")\n",
        "        tok_loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.reshape(-1))\n",
        "        tok_loss = tok_loss.view(shift_labels.size(0), -1)\n",
        "        valid = (shift_labels != tok.pad_token_id).float()\n",
        "        loss_per = (tok_loss * valid).sum(dim=1) / (valid.sum(dim=1).clamp_min(1.0))\n",
        "        logp_per = -loss_per.detach().cpu().numpy()\n",
        "        L = sub[\"len\"].to_numpy().clip(min=1)\n",
        "        out[s:s+len(ii)] = logp_per / (L ** 0.3)\n",
        "    stageA_df[\"base_norm\"] = out\n",
        "    return stageA_df\n",
        "\n",
        "# ---- features for LR ----\n",
        "def minmax(v):\n",
        "    v=np.array(v, float)\n",
        "    lo,hi=v.min(),v.max()\n",
        "    return np.zeros_like(v) if hi-lo<1e-8 else (v-lo)/(hi-lo)\n",
        "\n",
        "def ent_consensus(cands):\n",
        "    E=[_ents_norm(x) for x in cands]\n",
        "    n=len(E); sc=np.zeros(n)\n",
        "    for i in range(n):\n",
        "        js=[]\n",
        "        for j in range(n):\n",
        "            if i==j: continue\n",
        "            u=E[i]|E[j]; inter=E[i]&E[j]\n",
        "            js.append(len(inter)/max(len(u),1) if u else 0.0)\n",
        "        sc[i]=np.mean(js) if js else 0.0\n",
        "    return sc\n",
        "\n",
        "def build_features(stageA_df, tok, with_labels=True):\n",
        "    feat_rows=[]\n",
        "    for rid, grp in stageA_df.groupby(\"row_id\"):\n",
        "        art = grp.iloc[0][\"article\"]; ref = grp.iloc[0].get(\"reference\",\"\")\n",
        "        cands = grp[\"summary\"].tolist()\n",
        "        tgt = adaptive_len_target(tok, art)\n",
        "        leadsim = np.array([_SC.score(lead3(art), h)[\"rougeLsum\"].fmeasure for h in cands])\n",
        "        # centroid MBR (ROUGE-Lsum mean to peers)\n",
        "        n=len(cands); mbr=np.zeros(n)\n",
        "        if n>1:\n",
        "            M=np.zeros((n,n))\n",
        "            for i in range(n):\n",
        "                for j in range(i+1,n):\n",
        "                    s=_SC.score(cands[i], cands[j])[\"rougeLsum\"].fmeasure\n",
        "                    M[i,j]=M[j,i]=s\n",
        "            mbr=M.mean(axis=1)\n",
        "        feats = pd.DataFrame({\n",
        "            \"row_id\": grp[\"row_id\"].values,\n",
        "            \"summary\": grp[\"summary\"].values,\n",
        "            \"THC_norm\": minmax(grp[\"THC\"].values),\n",
        "            \"LeadSim\": leadsim,\n",
        "            \"MBR\": mbr,\n",
        "            \"len_reward\": minmax([math.exp(-((L-tgt)**2)/(2*(LEN_SIGMA**2))) for L in grp[\"len\"].tolist()]),\n",
        "            \"contig_lcs_mm\": minmax([lcs_span_stats(art, s)[0] for s in cands]),\n",
        "            \"base_norm_mm\": minmax(grp[\"base_norm\"].values),\n",
        "            \"ent_consensus\": ent_consensus(cands),\n",
        "            \"unsup_nonc_mm\": minmax(grp[\"unsup_nonc\"].values.astype(float)),\n",
        "        })\n",
        "        if with_labels and \"reference\" in grp.columns:\n",
        "            feats[\"rl_ref\"] = [_SC.score(ref, h)[\"rougeLsum\"].fmeasure for h in cands]\n",
        "        feat_rows.append(feats)\n",
        "    return pd.concat(feat_rows, ignore_index=True)\n",
        "\n",
        "def train_pairwise_lr(F_df, cols, seed=SEED):\n",
        "    pairs_X=[]; pairs_y=[]\n",
        "    for rid, G in F_df.groupby(\"row_id\"):\n",
        "        G=G.reset_index(drop=True); m=len(G)\n",
        "        for i in range(m):\n",
        "            for j in range(i+1,m):\n",
        "                yi = G.loc[i,\"rl_ref\"]; yj = G.loc[j,\"rl_ref\"]\n",
        "                if yi==yj: continue\n",
        "                x = (G.loc[i, cols].values.astype(float) - G.loc[j, cols].values.astype(float))\n",
        "                y = 1 if yi>yj else 0\n",
        "                pairs_X.append(x); pairs_y.append(y)\n",
        "    X=np.stack(pairs_X); y=np.array(pairs_y)\n",
        "    clf = LogisticRegression(solver=\"liblinear\", max_iter=1000, random_state=seed)\n",
        "    clf.fit(X,y)\n",
        "    weights = dict(intercept=float(clf.intercept_[0]), coefs={c: float(w) for c,w in zip(cols, clf.coef_[0].tolist())})\n",
        "    return clf, weights, cols\n",
        "\n",
        "def lr_select(grp, w, tok):\n",
        "    art = grp.iloc[0][\"article\"]; tgt = adaptive_len_target(tok, art)\n",
        "    cands = grp[\"summary\"].tolist()\n",
        "    # build features on-the-fly (no labels)\n",
        "    leadsim = np.array([_SC.score(lead3(art), h)[\"rougeLsum\"].fmeasure for h in cands])\n",
        "    n=len(cands); mbr=np.zeros(n)\n",
        "    if n>1:\n",
        "        M=np.zeros((n,n))\n",
        "        for i in range(n):\n",
        "            for j in range(i+1,n):\n",
        "                s=_SC.score(cands[i], cands[j])[\"rougeLsum\"].fmeasure\n",
        "                M[i,j]=M[j,i]=s\n",
        "        mbr=M.mean(axis=1)\n",
        "    feats = pd.DataFrame({\n",
        "        \"THC_norm\": minmax(grp[\"THC\"].values),\n",
        "        \"LeadSim\": leadsim,\n",
        "        \"MBR\": mbr,\n",
        "        \"len_reward\": minmax([math.exp(-((L-tgt)**2)/(2*(LEN_SIGMA**2))) for L in grp[\"len\"].tolist()]),\n",
        "        \"contig_lcs_mm\": minmax([lcs_span_stats(art, s)[0] for s in cands]),\n",
        "        \"base_norm_mm\": minmax(grp[\"base_norm\"].values),\n",
        "        \"ent_consensus\": ent_consensus(cands),\n",
        "        \"unsup_nonc_mm\": minmax(grp[\"unsup_nonc\"].values.astype(float)),\n",
        "    })\n",
        "    cols=list(w[\"coefs\"].keys())\n",
        "    s = w[\"intercept\"] + sum(feats[c].values * w[\"coefs\"][c] for c in cols)\n",
        "    k=int(np.argmax(s))\n",
        "    cand = grp.iloc[[k]].copy()\n",
        "    # UCER-safe fallback (no unsupported CORE)\n",
        "    S=_ents_norm(art); H=_ents_norm(cand[\"summary\"].iloc[0])\n",
        "    if any(lbl in CORE for _,lbl in (H-S)):\n",
        "        safe_idx=[]\n",
        "        sv=np.array(s)\n",
        "        for i in range(len(grp)):\n",
        "            Hi=_ents_norm(grp.iloc[i][\"summary\"])\n",
        "            if not any(lbl in CORE for _,lbl in (Hi-S)):\n",
        "                safe_idx.append(i)\n",
        "        if safe_idx:\n",
        "            k=int(max(safe_idx, key=lambda i: sv[i]))\n",
        "            cand=grp.iloc[[k]].copy()\n",
        "    return cand\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R217ZO0_lNx-"
      },
      "outputs": [],
      "source": [
        "# === Eval function per checkpoint ===\n",
        "\n",
        "def eval_checkpoint(ckpt_dir, N_VAL=200, subset_seed=SEED, run_tag=\"probe\"):\n",
        "    \"\"\"Decode two pools on VAL subset, Stage-A, add_base_norm, train LR, select, compute metrics.\n",
        "       Returns metrics dict and paths to artifacts. Uses the checkpoint's tokenizer & weights.\"\"\"\n",
        "    from transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM\n",
        "\n",
        "    print(f\"\\n=== EVAL {ckpt_dir} | N_VAL={N_VAL} ({run_tag}) ===\")\n",
        "    out_root = os.path.join(ckpt_dir, f\"Hybrid1_eval_{run_tag}_{time.strftime('%Y%m%d_%H%M%S')}\")\n",
        "    os.makedirs(out_root, exist_ok=True)\n",
        "\n",
        "    # load data subset\n",
        "    df_val = robust_read_csv(VAL_PATH).copy()\n",
        "    rng = np.random.default_rng(subset_seed)\n",
        "    idx = rng.choice(len(df_val), size=min(N_VAL, len(df_val)), replace=False)\n",
        "    df_val = df_val.iloc[np.sort(idx)].reset_index(drop=True)\n",
        "\n",
        "    articles = df_val[\"article\"].astype(str).tolist()\n",
        "    references = df_val[\"highlights\"].astype(str).tolist()\n",
        "    ents_cache_build(articles)  # speed NER\n",
        "\n",
        "    # load model\n",
        "    tok  = AutoTokenizer.from_pretrained(ckpt_dir, use_fast=True)\n",
        "    cfg  = AutoConfig.from_pretrained(ckpt_dir, attn_implementation=\"eager\")\n",
        "    model= AutoModelForSeq2SeqLM.from_pretrained(ckpt_dir, config=cfg).to(DEVICE).eval()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cuda.matmul.allow_tf32=True\n",
        "        try: torch.set_float32_matmul_precision(\"high\")\n",
        "        except: pass\n",
        "    if hasattr(model, \"config\"): setattr(model.config, \"attn_implementation\", \"sdpa\")\n",
        "    if hasattr(getattr(model, \"model\", None), \"config\"): setattr(model.model.config, \"attn_implementation\", \"sdpa\")\n",
        "    if torch.cuda.is_available():\n",
        "        major=torch.cuda.get_device_capability()[0]\n",
        "        model.to(dtype=(torch.bfloat16 if major>=8 else torch.float16))\n",
        "\n",
        "    # decode\n",
        "    dd_all = decode_two_pools(model, tok, articles, references, batch_long=4, batch_med=8)\n",
        "\n",
        "    # Stage-A + base_norm\n",
        "    stageA = stageA_select(dd_all, tok, M=M_KEEP)\n",
        "    stageA = add_base_norm(stageA, model, tok)\n",
        "\n",
        "    # features (+labels) and LR train\n",
        "    F_val = build_features(stageA, tok, with_labels=True)\n",
        "    cols = [\"THC_norm\",\"LeadSim\",\"MBR\",\"len_reward\",\"contig_lcs_mm\",\"base_norm_mm\",\"ent_consensus\",\"unsup_nonc_mm\"]\n",
        "    clf, weights, used_cols = train_pairwise_lr(F_val, cols, seed=SEED)\n",
        "\n",
        "    # select winners per row using learned LR\n",
        "    best_rows=[]\n",
        "    for rid, grp in stageA.groupby(\"row_id\"):\n",
        "        best_rows.append(lr_select(grp, weights, tok))\n",
        "    best = pd.concat(best_rows, ignore_index=True)\n",
        "\n",
        "    # metrics\n",
        "    R = df_val[\"highlights\"].astype(str).tolist()\n",
        "    H = best[\"summary\"].astype(str).tolist()\n",
        "    A = df_val[\"article\"].astype(str).tolist()\n",
        "\n",
        "    Rg = rouge_mean(R,H)\n",
        "    En = entPRF(R,H)\n",
        "    u_rate, u_avg = UCER(A,H)\n",
        "\n",
        "    metrics = {\n",
        "        \"ckpt\": os.path.basename(ckpt_dir),\n",
        "        \"N_VAL\": len(df_val),\n",
        "        \"rouge\": {k: round(v,4) for k,v in Rg.items()},\n",
        "        \"ent\": {k:(round(v,4) if isinstance(v,float) else v) for k,v in En.items()},\n",
        "        \"UCER\": {\"rate\": round(u_rate,4), \"avg_core\": round(u_avg,4)},\n",
        "        \"lr_weights\": weights,\n",
        "        \"features\": used_cols,\n",
        "    }\n",
        "\n",
        "    # save artifacts\n",
        "    save_csv(dd_all,  os.path.join(out_root, \"details_cn2.csv\"))\n",
        "    save_csv(stageA,  os.path.join(out_root, \"stageA_survivors.csv\"))\n",
        "    save_csv(best[[\"row_id\",\"article\",\"reference\",\"summary\"]], os.path.join(out_root, \"best_val.csv\"))\n",
        "    save_json(metrics, os.path.join(out_root, \"metrics.json\"))\n",
        "    save_json(weights, os.path.join(out_root, \"lr_weights.json\"))\n",
        "    save_json(dict(\n",
        "        pools=dict(CN2_LONG=CN2_LONG, CN_MED=CN_MED),\n",
        "        entity_aware=EA_ARGS,\n",
        "        stageA=dict(M_keep=M_KEEP, noncore_pen=NONCORE_PEN, len_sigma=LEN_SIGMA,\n",
        "                    hard_fail_core=True, target_len=\"0.9 * lead3 tokens clamped 32..60\"),\n",
        "        features=used_cols,\n",
        "        max_src_len=MAX_SRC_LEN,\n",
        "        seed=SEED,\n",
        "    ), os.path.join(out_root, \"frozen_config.json\"))\n",
        "\n",
        "    print(\"METRICS:\", metrics)\n",
        "    return metrics, out_root, weights, used_cols\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPUhoB5hlTel",
        "outputId": "2122d12b-a061-4902-ada0-0580a223d739"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== EVAL /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000 | N_VAL=200 (probe) ===\n",
            "METRICS: {'ckpt': 'ckpt_step6000', 'N_VAL': 200, 'rouge': {'r1_f': 0.4233, 'r2_f': 0.1925, 'rl_f': 0.297}, 'ent': {'TP': 480, 'FP': 711, 'FN': 780, 'entP': 0.403, 'entR': 0.381, 'entF1': 0.3917}, 'UCER': {'rate': 0.025, 'avg_core': 0.03}, 'lr_weights': {'intercept': -0.06446059410370113, 'coefs': {'THC_norm': -0.05238886534758181, 'LeadSim': 1.1300867879418268, 'MBR': 1.8000294324180535, 'len_reward': 0.09509608666143539, 'contig_lcs_mm': -0.3542102447200053, 'base_norm_mm': -0.2308654937012459, 'ent_consensus': 0.5502367184941885, 'unsup_nonc_mm': 0.11240446962726773}}, 'features': ['THC_norm', 'LeadSim', 'MBR', 'len_reward', 'contig_lcs_mm', 'base_norm_mm', 'ent_consensus', 'unsup_nonc_mm']}\n",
            "\n",
            "=== EVAL /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500 | N_VAL=200 (probe) ===\n",
            "METRICS: {'ckpt': 'ckpt_step6500', 'N_VAL': 200, 'rouge': {'r1_f': 0.4184, 'r2_f': 0.1885, 'rl_f': 0.286}, 'ent': {'TP': 522, 'FP': 818, 'FN': 738, 'entP': 0.3896, 'entR': 0.4143, 'entF1': 0.4015}, 'UCER': {'rate': 0.02, 'avg_core': 0.035}, 'lr_weights': {'intercept': -0.2514146828311194, 'coefs': {'THC_norm': 0.5277609023085235, 'LeadSim': 1.0581803734553366, 'MBR': 0.6581750568195032, 'len_reward': 0.23145311795878074, 'contig_lcs_mm': -0.45876970361603175, 'base_norm_mm': -0.005632806516764877, 'ent_consensus': 0.48005664695925926, 'unsup_nonc_mm': 0.052675464068870526}}, 'features': ['THC_norm', 'LeadSim', 'MBR', 'len_reward', 'contig_lcs_mm', 'base_norm_mm', 'ent_consensus', 'unsup_nonc_mm']}\n",
            "\n",
            "=== EVAL /content/drive/MyDrive/student_pgc_bartbase/ckpt_step7000 | N_VAL=200 (probe) ===\n",
            "METRICS: {'ckpt': 'ckpt_step7000', 'N_VAL': 200, 'rouge': {'r1_f': 0.42, 'r2_f': 0.1882, 'rl_f': 0.2847}, 'ent': {'TP': 490, 'FP': 703, 'FN': 770, 'entP': 0.4107, 'entR': 0.3889, 'entF1': 0.3995}, 'UCER': {'rate': 0.03, 'avg_core': 0.04}, 'lr_weights': {'intercept': -0.11436251207027466, 'coefs': {'THC_norm': 0.008887017877910797, 'LeadSim': 1.150461377004894, 'MBR': 0.3729146829771389, 'len_reward': 0.20358037398459086, 'contig_lcs_mm': -0.13908653745889762, 'base_norm_mm': -0.07497112059405991, 'ent_consensus': 1.0228345855149532, 'unsup_nonc_mm': -0.17652279657915768}}, 'features': ['THC_norm', 'LeadSim', 'MBR', 'len_reward', 'contig_lcs_mm', 'base_norm_mm', 'ent_consensus', 'unsup_nonc_mm']}\n",
            "\n",
            ">>> PROBE WINNER: ckpt_step6000 metrics= {'ckpt': 'ckpt_step6000', 'N_VAL': 200, 'rouge': {'r1_f': 0.4233, 'r2_f': 0.1925, 'rl_f': 0.297}, 'ent': {'TP': 480, 'FP': 711, 'FN': 780, 'entP': 0.403, 'entR': 0.381, 'entF1': 0.3917}, 'UCER': {'rate': 0.025, 'avg_core': 0.03}, 'lr_weights': {'intercept': -0.06446059410370113, 'coefs': {'THC_norm': -0.05238886534758181, 'LeadSim': 1.1300867879418268, 'MBR': 1.8000294324180535, 'len_reward': 0.09509608666143539, 'contig_lcs_mm': -0.3542102447200053, 'base_norm_mm': -0.2308654937012459, 'ent_consensus': 0.5502367184941885, 'unsup_nonc_mm': 0.11240446962726773}}, 'features': ['THC_norm', 'LeadSim', 'MBR', 'len_reward', 'contig_lcs_mm', 'base_norm_mm', 'ent_consensus', 'unsup_nonc_mm']}\n"
          ]
        }
      ],
      "source": [
        "# === Probe on VAL (N=200) for 3 checkpoints and pick best ===\n",
        "\n",
        "PROBE_N = 200\n",
        "probe_results = []\n",
        "for ck in CKPTS:\n",
        "    ckdir = os.path.join(CKPT_ROOT, ck)\n",
        "    m, out_dir, w, cols = eval_checkpoint(ckdir, N_VAL=PROBE_N, run_tag=\"probe\")\n",
        "    probe_results.append((ckdir, m, out_dir, w, cols))\n",
        "\n",
        "# pick best by ROUGE-Lsum, break ties by entF1, then UCER rate (lower better)\n",
        "def score_key(m):\n",
        "    rl = m[\"rouge\"][\"rl_f\"]\n",
        "    ef = m[\"ent\"][\"entF1\"]\n",
        "    u  = -m[\"UCER\"][\"rate\"]  # higher is better when negated\n",
        "    return (rl, ef, u)\n",
        "\n",
        "best_ck = max(probe_results, key=lambda x: score_key(x[1]))\n",
        "WIN_DIR, WIN_METRICS, WIN_OUTDIR, WIN_W, WIN_COLS = best_ck\n",
        "print(\"\\n>>> PROBE WINNER:\", os.path.basename(WIN_DIR), \"metrics=\", WIN_METRICS)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeNnbU13wa-s"
      },
      "source": [
        "Baseline PGC, no span-aware (EA/CopyNext) logits processor. This isolates the value of span awareness at decoding while keeping the rest identical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OI6F1VLww3kT",
        "outputId": "d950de9c-c4aa-4014-8807-f451cb3d50cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generate (NO EA) ...\n",
            "Generated rows: 4000\n",
            "Stage-A select ...\n",
            "\n",
            "=== NO-EA HYBRID | ckpt_step6500 | VAL(200) ===\n",
            "LR weights: {\n",
            "  \"intercept\": -0.32867788520119073,\n",
            "  \"coefs\": {\n",
            "    \"THC_norm\": 0.30528414134404347,\n",
            "    \"LeadSim\": 0.1455424208081338,\n",
            "    \"MBR\": 0.6305088126259492,\n",
            "    \"len_reward\": 0.06552538743291068,\n",
            "    \"contig_lcs_mm\": -0.549861001783529,\n",
            "    \"base_norm_mm\": -0.2561629186491688,\n",
            "    \"ent_consensus\": -0.7323236286066689,\n",
            "    \"unsup_nonc_mm\": -0.04602299165349257\n",
            "  }\n",
            "}\n",
            "ROUGE: {'r1_f': 0.3202, 'r2_f': 0.1262, 'rl_f': 0.2224}\n",
            "ENT  : {'TP': 247, 'FP': 759, 'FN': 521, 'entP': 0.2455, 'entR': 0.3216, 'entF1': 0.2785}\n",
            "UCER : {'rate': 0.04, 'avg_core': 0.045}\n",
            "Saved to: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/Hybrid1_noEA/val_20250902_191433\n"
          ]
        }
      ],
      "source": [
        "# === NO-EA HYBRID (pools + TH-C + LR) on VAL(200) for ckpt_step6500 ===\n",
        "# Identical to your Hybrid-1 pipeline EXCEPT: no Entity-Aware/CopyNext logits processor.\n",
        "\n",
        "import os, sys, json, math, re, csv, time, random, subprocess\n",
        "import numpy as np, pandas as pd, torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from rouge_score import rouge_scorer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# ------------------- CONFIG -------------------\n",
        "CKPT_DIR   = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500\"\n",
        "VAL_PATH   = \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "OUT_ROOT   = f\"{CKPT_DIR}/Hybrid1_noEA\"\n",
        "SEED       = 0\n",
        "N_VAL      = 200\n",
        "\n",
        "MAX_SRC_LEN = 400\n",
        "BATCH_LONG  = 4\n",
        "BATCH_MED   = 8\n",
        "\n",
        "# Pools (same as Hybrid-1) — force return_dict_in_generate=True; stay light (no scores)\n",
        "CN2_LONG = dict(\n",
        "    num_beams=10, num_return_sequences=10, num_beam_groups=5, diversity_penalty=0.3,\n",
        "    max_new_tokens=100, min_new_tokens=55, no_repeat_ngram_size=3, length_penalty=2.0,\n",
        "    early_stopping=True, return_dict_in_generate=True, output_scores=False\n",
        ")\n",
        "CN_MED = dict(\n",
        "    num_beams=10, num_return_sequences=10, num_beam_groups=5, diversity_penalty=0.3,\n",
        "    max_new_tokens=70, min_new_tokens=35, no_repeat_ngram_size=3, length_penalty=1.6,\n",
        "    early_stopping=True, return_dict_in_generate=True, output_scores=False\n",
        ")\n",
        "\n",
        "# Stage-A & entities (same as Hybrid-1)\n",
        "M_KEEP        = 10\n",
        "NONCORE_PEN   = 2.0\n",
        "LEN_SIGMA     = 20.0\n",
        "KEEP_LABELS   = {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"DATE\",\"TIME\",\"CARDINAL\",\"MONEY\",\"PERCENT\",\"QUANTITY\",\"ORDINAL\"}\n",
        "CORE          = {\"PERSON\",\"ORG\",\"GPE\"}\n",
        "\n",
        "# ------------------- ENV & PKGS -------------------\n",
        "def ensure(mod, pip=None):\n",
        "    import importlib.util\n",
        "    if pip is None: pip = mod\n",
        "    if importlib.util.find_spec(mod) is None:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pip])\n",
        "ensure(\"spacy\", \"spacy\")\n",
        "ensure(\"rouge_score\", \"rouge-score==0.1.2\")\n",
        "ensure(\"scikit_learn\", \"scikit-learn\")\n",
        "\n",
        "import spacy\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "except OSError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# ------------------- IO HELPERS -------------------\n",
        "def save_csv(df, path):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    tmp = path + \".tmp\"; df.to_csv(tmp, index=False, quoting=csv.QUOTE_ALL, lineterminator=\"\\n\"); os.replace(tmp, path)\n",
        "def save_json(obj, path):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    with open(path,\"w\") as f: json.dump(obj, f, indent=2)\n",
        "\n",
        "def robust_read_csv(path: str) -> pd.DataFrame:\n",
        "    try:\n",
        "        df = pd.read_csv(path, engine=\"python\", quoting=csv.QUOTE_ALL, escapechar=\"\\\\\")\n",
        "        cols = {c.lower(): c for c in df.columns}\n",
        "        assert \"article\" in cols and \"highlights\" in cols, f\"missing columns in {list(df.columns)}\"\n",
        "        return df[[cols[\"article\"], cols[\"highlights\"]]].rename(\n",
        "            columns={cols[\"article\"]: \"article\", cols[\"highlights\"]: \"highlights\"}\n",
        "        )\n",
        "    except Exception:\n",
        "        rows = []\n",
        "        with open(path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "            reader = csv.reader(f, delimiter=\",\", quotechar='\"', escapechar=\"\\\\\")\n",
        "            header = next(reader)\n",
        "            header = [h.strip().lower() for h in header]\n",
        "            i_art, i_sum = header.index(\"article\"), header.index(\"highlights\")\n",
        "            for row in reader: rows.append([row[i_art], row[i_sum]])\n",
        "        return pd.DataFrame(rows, columns=[\"article\",\"highlights\"])\n",
        "\n",
        "# ------------------- NER / ENT HELPERS -------------------\n",
        "def _normalize_ent_text(s: str):\n",
        "    s = (s or \"\").strip()\n",
        "    return \" \".join(s.split())\n",
        "\n",
        "def _ents_norm(text: str):\n",
        "    doc = nlp(str(text))\n",
        "    out = set()\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in KEEP_LABELS:\n",
        "            out.add((_normalize_ent_text(ent.text), ent.label_))\n",
        "    return out\n",
        "\n",
        "def strict_entity_score(src, hyp):\n",
        "    S=_ents_norm(src); H=_ents_norm(hyp)\n",
        "    overlap=H&S; unsupported=H-S\n",
        "    hard_fail = any(lbl in CORE for _,lbl in unsupported)\n",
        "    score = sum(3 if lbl in CORE else 1 for _,lbl in overlap)\n",
        "    return score, hard_fail\n",
        "\n",
        "def unsupported_nonc_count(src, hyp):\n",
        "    S=_ents_norm(src); H=_ents_norm(hyp)\n",
        "    return sum(1 for _,lbl in (H-S) if lbl in (KEEP_LABELS-CORE))\n",
        "\n",
        "def lcs_span_stats(src_text, hyp_text):\n",
        "    s=str(src_text).split(); h=str(hyp_text).split()\n",
        "    n,m=len(s),len(h); dp=[[0]*(m+1) for _ in range(n+1)]\n",
        "    for i in range(n-1,-1,-1):\n",
        "        si=s[i]; row=dp[i]; row1=dp[i+1]\n",
        "        for j in range(m-1,-1,-1):\n",
        "            row[j]=row1[j+1]+1 if si==h[j] else max(row[j+1], row1[j])\n",
        "    i=j=0; blocks=[]; cur=0\n",
        "    while i<n and j<m:\n",
        "        if s[i]==h[j]: cur+=1; i+=1; j+=1\n",
        "        else:\n",
        "            if cur>=2: blocks.append(cur); cur=0\n",
        "            if dp[i+1][j]>=dp[i][j+1]: i+=1\n",
        "            else: j+=1\n",
        "    if cur>=2: blocks.append(cur)\n",
        "    cont=sum(b-1 for b in blocks)\n",
        "    av=(sum(blocks)/len(blocks)) if blocks else 1.0\n",
        "    return cont, av\n",
        "\n",
        "def entPRF(R,H):\n",
        "    TP=FP=FN=0\n",
        "    for r,h in zip(R,H):\n",
        "        R_=_ents_norm(r); H_=_ents_norm(h)\n",
        "        TP+=len(H_&R_); FP+=len(H_-R_); FN+=len(R_-H_)\n",
        "    P=TP/(TP+FP) if (TP+FP) else 0.0\n",
        "    Rc=TP/(TP+FN) if (TP+FN) else 0.0\n",
        "    F=2*P*Rc/(P+Rc) if (P+Rc) else 0.0\n",
        "    return {\"TP\":TP,\"FP\":FP,\"FN\":FN,\"entP\":P,\"entR\":Rc,\"entF1\":F}\n",
        "\n",
        "def UCER(arts, hyps):\n",
        "    rates=[]; counts=[]\n",
        "    for s,h in zip(arts, hyps):\n",
        "        S=_ents_norm(s); H=_ents_norm(h)\n",
        "        bad=[(t,l) for (t,l) in (H-S) if l in CORE]\n",
        "        rates.append(1 if bad else 0); counts.append(len(bad))\n",
        "    return {\"rate\": float(np.mean(rates)), \"avg_core\": float(np.mean(counts))}\n",
        "\n",
        "_SC = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\"], use_stemmer=True)\n",
        "def rouge_mean(R,H):\n",
        "    r1=r2=rl=0.0\n",
        "    for r,h in zip(R,H):\n",
        "        s=_SC.score(str(r), str(h))\n",
        "        r1+=s[\"rouge1\"].fmeasure; r2+=s[\"rouge2\"].fmeasure; rl+=s[\"rougeLsum\"].fmeasure\n",
        "    n=len(R); return {\"r1_f\":r1/n,\"r2_f\":r2/n,\"rl_f\":rl/n}\n",
        "\n",
        "def lead3(text):\n",
        "    s=re.split(r'(?<=[.!?])\\s+', str(text).strip())\n",
        "    s=[t for t in s if t]\n",
        "    return \" \".join(s[:3])\n",
        "\n",
        "def toklen(tok, s): return len(tok(str(s))[\"input_ids\"])\n",
        "def adaptive_len_target(tok, src):\n",
        "    L = toklen(tok, lead3(src))\n",
        "    return int(np.clip(round(0.9*L), 32, 60))\n",
        "\n",
        "def minmax(v):\n",
        "    v=np.array(v, float); lo,hi=v.min(),v.max()\n",
        "    return np.zeros_like(v) if hi-lo<1e-8 else (v-lo)/(hi-lo)\n",
        "\n",
        "def ent_consensus(cands):\n",
        "    E=[_ents_norm(x) for x in cands]\n",
        "    n=len(E); sc=np.zeros(n)\n",
        "    for i in range(n):\n",
        "        js=[]\n",
        "        for j in range(n):\n",
        "            if i==j: continue\n",
        "            u=E[i]|E[j]; inter=E[i]&E[j]\n",
        "            js.append(len(inter)/max(len(u),1) if u else 0.0)\n",
        "        sc[i]=np.mean(js) if js else 0.0\n",
        "    return sc\n",
        "\n",
        "# ------------------- LOAD MODEL -------------------\n",
        "tok   = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR).to(DEVICE).eval()\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cuda.matmul.allow_tf32=True\n",
        "    try: torch.set_float32_matmul_precision(\"high\")\n",
        "    except: pass\n",
        "\n",
        "# ------------------- DATA -------------------\n",
        "df = robust_read_csv(VAL_PATH).head(N_VAL).copy()\n",
        "arts = df[\"article\"].astype(str).tolist()\n",
        "refs = df[\"highlights\"].astype(str).tolist()\n",
        "\n",
        "# Pre-tokenize for speed\n",
        "def pretokenize_articles(articles):\n",
        "    enc = tok(articles, max_length=MAX_SRC_LEN, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "    return {k: v.to(DEVICE) for k,v in enc.items()}\n",
        "\n",
        "# ------------------- GENERATION (NO EA PROCESSOR) -------------------\n",
        "@torch.inference_mode()\n",
        "def run_pool(enc_slice, decode_args):\n",
        "    out = model.generate(**enc_slice, **decode_args)  # no trust_remote_code here\n",
        "    return out.sequences, getattr(out, \"sequences_scores\", None)\n",
        "\n",
        "def decode_two_pools_noEA(articles, references):\n",
        "    enc_all = pretokenize_articles(articles)\n",
        "    rows=[]\n",
        "    # CN2_LONG\n",
        "    for s in range(0, len(articles), BATCH_LONG):\n",
        "        e=min(len(articles), s+BATCH_LONG)\n",
        "        enc = {k: v[s:e] for k,v in enc_all.items()}\n",
        "        seqs, scores = run_pool(enc, CN2_LONG)\n",
        "        K = CN2_LONG[\"num_return_sequences\"]; bs=e-s\n",
        "        seqs = seqs.view(bs, K, -1)\n",
        "        for bi in range(bs):\n",
        "            art=articles[s+bi]; ref=references[s+bi]\n",
        "            for k in range(K):\n",
        "                hyp = tok.decode(seqs[bi,k], skip_special_tokens=True)\n",
        "                rows.append({\"row_id\": s+bi, \"pool\":\"CN2_LONG\", \"candidate_k\":k,\n",
        "                             \"article\": art, \"highlights\": ref, \"summary\": hyp,\n",
        "                             \"len\": max(int(seqs[bi,k].size(0)-1),1),\n",
        "                             \"base_score\": 0.0})\n",
        "    # CN_MED\n",
        "    for s in range(0, len(articles), BATCH_MED):\n",
        "        e=min(len(articles), s+BATCH_MED)\n",
        "        enc = {k: v[s:e] for k,v in enc_all.items()}\n",
        "        seqs, scores = run_pool(enc, CN_MED)\n",
        "        K = CN_MED[\"num_return_sequences\"]; bs=e-s\n",
        "        seqs = seqs.view(bs, K, -1)\n",
        "        for bi in range(bs):\n",
        "            art=articles[s+bi]; ref=references[s+bi]\n",
        "            for k in range(K):\n",
        "                hyp = tok.decode(seqs[bi,k], skip_special_tokens=True)\n",
        "                rows.append({\"row_id\": s+bi, \"pool\":\"CN_MED\", \"candidate_k\":k,\n",
        "                             \"article\": art, \"highlights\": ref, \"summary\": hyp,\n",
        "                             \"len\": max(int(seqs[bi,k].size(0)-1),1),\n",
        "                             \"base_score\": 0.0})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "print(\"Generate (NO EA) ...\")\n",
        "dd_all = decode_two_pools_noEA(arts, refs)\n",
        "print(\"Generated rows:\", len(dd_all))\n",
        "\n",
        "# ------------------- STAGE-A (TH-C gate) -------------------\n",
        "def stageA_select(dd_all, M=M_KEEP):\n",
        "    rows=[]\n",
        "    for rid, grp in dd_all.groupby(\"row_id\"):\n",
        "        art = grp.iloc[0][\"article\"]; tgt = adaptive_len_target(tok, art)\n",
        "        ent_scores=[]; fails=[]; contigs=[]; nonc=[]; lens=[]\n",
        "        for _,r in grp.iterrows():\n",
        "            s,f = strict_entity_score(r[\"article\"], r[\"summary\"])\n",
        "            c,_ = lcs_span_stats(r[\"article\"], r[\"summary\"])\n",
        "            u   = unsupported_nonc_count(r[\"article\"], r[\"summary\"])\n",
        "            ent_scores.append(s); fails.append(f); contigs.append(c); nonc.append(u); lens.append(max(int(r[\"len\"]),1))\n",
        "        ent_scores=np.array(ent_scores); fails=np.array(fails); contigs=np.array(contigs); nonc=np.array(nonc); L=np.array(lens)\n",
        "        len_reward = np.exp(-((L - tgt)**2)/(2*(LEN_SIGMA**2)))\n",
        "        penalty = (nonc>0).astype(float) * NONCORE_PEN\n",
        "        THC = (5.0*ent_scores) + (0.2*contigs) + (0.3*len_reward) - penalty\n",
        "        ok = grp.copy()\n",
        "        ok[\"THC\"]=THC; ok[\"hard_fail\"]=fails; ok[\"unsup_nonc\"]=nonc\n",
        "        ok = ok.loc[~ok[\"hard_fail\"]].copy()\n",
        "        if ok.empty:  # fallback if all failed core\n",
        "            ok = grp.copy(); ok[\"THC\"]=THC; ok[\"unsup_nonc\"]=nonc\n",
        "        rows.append(ok.sort_values(\"THC\", ascending=False).head(M))\n",
        "    return pd.concat(rows, ignore_index=True)\n",
        "\n",
        "def add_base_norm(stageA_df, batch_size=16):\n",
        "    out = np.zeros(len(stageA_df), dtype=np.float32)\n",
        "    idxs = stageA_df.index.to_list()\n",
        "    for s in range(0, len(idxs), batch_size):\n",
        "        ii = idxs[s:s+batch_size]\n",
        "        sub = stageA_df.loc[ii]\n",
        "        enc = tok(sub[\"article\"].tolist(), max_length=MAX_SRC_LEN, truncation=True, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
        "        dec = tok(sub[\"summary\"].tolist(), max_length=256, truncation=True, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
        "        with torch.inference_mode():\n",
        "            out_logits = model(**enc, labels=dec[\"input_ids\"], return_dict=True).logits.float()\n",
        "        shift_logits = out_logits[:, :-1, :]\n",
        "        shift_labels = dec[\"input_ids\"][:, 1:]\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=tok.pad_token_id, reduction=\"none\")\n",
        "        tok_loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.reshape(-1))\n",
        "        tok_loss = tok_loss.view(shift_labels.size(0), -1)\n",
        "        valid = (shift_labels != tok.pad_token_id).float()\n",
        "        loss_per = (tok_loss * valid).sum(dim=1) / (valid.sum(dim=1).clamp_min(1.0))\n",
        "        logp_per = -loss_per.detach().cpu().numpy()\n",
        "        L = sub[\"len\"].to_numpy().clip(min=1)\n",
        "        out[s:s+len(ii)] = logp_per / (L ** 0.3)\n",
        "    stageA_df[\"base_norm\"] = out\n",
        "    return stageA_df\n",
        "\n",
        "print(\"Stage-A select ...\")\n",
        "stageA = stageA_select(dd_all, M=M_KEEP)\n",
        "stageA = add_base_norm(stageA)\n",
        "\n",
        "# ------------------- FEATURES + LR (pairwise on VAL-200) -------------------\n",
        "def build_features(stageA_df, with_labels=True):\n",
        "    feat_rows=[]\n",
        "    for rid, grp in stageA_df.groupby(\"row_id\"):\n",
        "        art = grp.iloc[0][\"article\"]; ref = grp.iloc[0][\"highlights\"]; tgt = adaptive_len_target(tok, art)\n",
        "        cands = grp[\"summary\"].tolist()\n",
        "        _SC_local = rouge_scorer.RougeScorer([\"rougeLsum\"], use_stemmer=True)\n",
        "        def RLs(a,b): return _SC_local.score(str(a), str(b))[\"rougeLsum\"].fmeasure\n",
        "        leadsim = np.array([RLs(h, lead3(art)) for h in cands])\n",
        "        # MBR centroid\n",
        "        n=len(cands); mbr=np.zeros(n)\n",
        "        if n>1:\n",
        "            M=np.zeros((n,n))\n",
        "            for i in range(n):\n",
        "                for j in range(i+1,n):\n",
        "                    s=RLs(cands[i], cands[j]); M[i,j]=M[j,i]=s\n",
        "            mbr=M.mean(axis=1)\n",
        "        feats = pd.DataFrame({\n",
        "            \"row_id\": grp[\"row_id\"].values,\n",
        "            \"summary\": grp[\"summary\"].values,\n",
        "            \"THC_norm\": minmax(grp[\"THC\"].values),\n",
        "            \"LeadSim\": leadsim,\n",
        "            \"MBR\": mbr,\n",
        "            \"len_reward\": minmax([math.exp(-((L-tgt)**2)/(2*(LEN_SIGMA**2))) for L in grp[\"len\"].tolist()]),\n",
        "            \"contig_lcs_mm\": minmax([lcs_span_stats(art, s)[0] for s in cands]),\n",
        "            \"base_norm_mm\": minmax(grp[\"base_norm\"].values),\n",
        "            \"ent_consensus\": ent_consensus(cands),\n",
        "            \"unsup_nonc_mm\": minmax(grp[\"unsup_nonc\"].values.astype(float)),\n",
        "        })\n",
        "        if with_labels:\n",
        "            feats[\"rl_ref\"] = [_SC_local.score(str(ref), str(h))[\"rougeLsum\"].fmeasure for h in cands]\n",
        "        feat_rows.append(feats)\n",
        "    return pd.concat(feat_rows, ignore_index=True)\n",
        "\n",
        "F_val = build_features(stageA, with_labels=True)\n",
        "\n",
        "cols = [\"THC_norm\",\"LeadSim\",\"MBR\",\"len_reward\",\"contig_lcs_mm\",\"base_norm_mm\",\"ent_consensus\",\"unsup_nonc_mm\"]\n",
        "pairs_X=[]; pairs_y=[]\n",
        "for rid, g in F_val.groupby(\"row_id\"):\n",
        "    G=g.reset_index(drop=True); m=len(G)\n",
        "    for i in range(m):\n",
        "        for j in range(i+1,m):\n",
        "            yi = G.loc[i,\"rl_ref\"]; yj = G.loc[j,\"rl_ref\"]\n",
        "            if yi==yj: continue\n",
        "            x = (G.loc[i, cols].values.astype(float) - G.loc[j, cols].values.astype(float))\n",
        "            y = 1 if yi>yj else 0\n",
        "            pairs_X.append(x); pairs_y.append(y)\n",
        "pairs_X=np.stack(pairs_X); pairs_y=np.array(pairs_y)\n",
        "\n",
        "clf = LogisticRegression(solver=\"liblinear\", max_iter=1000, random_state=SEED)\n",
        "clf.fit(pairs_X, pairs_y)\n",
        "weights = dict(intercept=float(clf.intercept_[0]), coefs={c: float(w) for c,w in zip(cols, clf.coef_[0].tolist())})\n",
        "\n",
        "# ------------------- PICK BEST PER ROW (apply LR) -------------------\n",
        "def score_group_linear(grp, w):\n",
        "    art = grp.iloc[0][\"article\"]; tgt = adaptive_len_target(tok, art)\n",
        "    cands = grp[\"summary\"].tolist()\n",
        "    _SC_local = rouge_scorer.RougeScorer([\"rougeLsum\"], use_stemmer=True)\n",
        "    def RLs(a,b): return _SC_local.score(str(a), str(b))[\"rougeLsum\"].fmeasure\n",
        "    leadsim = np.array([RLs(h, lead3(art)) for h in cands])\n",
        "    n=len(cands); mbr=np.zeros(n)\n",
        "    if n>1:\n",
        "        M=np.zeros((n,n))\n",
        "        for i in range(n):\n",
        "            for j in range(i+1,n):\n",
        "                s=RLs(cands[i], cands[j]); M[i,j]=M[j,i]=s\n",
        "        mbr=M.mean(axis=1)\n",
        "    feats = pd.DataFrame({\n",
        "        \"THC_norm\": minmax(grp[\"THC\"].values),\n",
        "        \"LeadSim\": leadsim,\n",
        "        \"MBR\": mbr,\n",
        "        \"len_reward\": minmax([math.exp(-((L-tgt)**2)/(2*(LEN_SIGMA**2))) for L in grp[\"len\"].tolist()]),\n",
        "        \"contig_lcs_mm\": minmax([lcs_span_stats(art, s)[0] for s in cands]),\n",
        "        \"base_norm_mm\": minmax(grp[\"base_norm\"].values),\n",
        "        \"ent_consensus\": ent_consensus(cands),\n",
        "        \"unsup_nonc_mm\": minmax(grp[\"unsup_nonc\"].values.astype(float)),\n",
        "    })\n",
        "    cols=list(w[\"coefs\"].keys())\n",
        "    score = w[\"intercept\"] + sum(feats[c].values * w[\"coefs\"][c] for c in cols)\n",
        "    k = int(np.argmax(score))\n",
        "    # CORE hard-fail safety\n",
        "    S=_ents_norm(art); H=_ents_norm(grp.iloc[k][\"summary\"])\n",
        "    if any(lbl in CORE for _,lbl in (H-S)):\n",
        "        safe_idx=[]\n",
        "        for i in range(len(grp)):\n",
        "            Hi=_ents_norm(grp.iloc[i][\"summary\"])\n",
        "            if not any(lbl in CORE for _,lbl in (Hi-S)): safe_idx.append(i)\n",
        "        if safe_idx:\n",
        "            s_vals=np.array(score)\n",
        "            k=int(max(safe_idx, key=lambda i: s_vals[i]))\n",
        "    return grp.iloc[[k]].copy()\n",
        "\n",
        "best = []\n",
        "for rid, grp in stageA.groupby(\"row_id\"):\n",
        "    best.append(score_group_linear(grp, weights))\n",
        "best = pd.concat(best, ignore_index=True)\n",
        "\n",
        "# ------------------- METRICS -------------------\n",
        "R = best[\"highlights\"].astype(str).tolist()\n",
        "H = best[\"summary\"].astype(str).tolist()\n",
        "A = best[\"article\"].astype(str).tolist()\n",
        "\n",
        "rouge = rouge_mean(R,H)\n",
        "ent   = entPRF(R,H)\n",
        "ucer  = UCER(A,H)\n",
        "\n",
        "def _r(d): return {k:(round(v,4) if isinstance(v,float) else v) for k,v in d.items()}\n",
        "print(\"\\n=== NO-EA HYBRID | ckpt_step6500 | VAL(200) ===\")\n",
        "print(\"LR weights:\", json.dumps(weights, indent=2))\n",
        "print(\"ROUGE:\", _r(rouge))\n",
        "print(\"ENT  :\", _r(ent))\n",
        "print(\"UCER :\", _r(ucer))\n",
        "\n",
        "# ------------------- SAVE ARTIFACTS -------------------\n",
        "STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "OUT_DIR = f\"{OUT_ROOT}/val_{STAMP}\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "save_csv(best, f\"{OUT_DIR}/best_noEA_val_{STAMP}.csv\")\n",
        "save_json(weights, f\"{OUT_DIR}/lr_weights_noEA_{STAMP}.json\")\n",
        "cfg = dict(\n",
        "    checkpoint=CKPT_DIR, n_val=N_VAL, max_src_len=MAX_SRC_LEN,\n",
        "    pools=dict(CN2_LONG=CN2_LONG, CN_MED=CN_MED),\n",
        "    entity_aware=False,\n",
        "    stageA=dict(M_keep=M_KEEP, noncore_pen=NONCORE_PEN, len_sigma=LEN_SIGMA,\n",
        "                formula=\"5*entity + 0.2*contig_lcs + 0.3*len_reward - 1{unsup_noncore}*pen\",\n",
        "                hard_fail_core=True),\n",
        "    features=list(weights[\"coefs\"].keys()),\n",
        "    seed=SEED\n",
        ")\n",
        "save_json(cfg, f\"{OUT_DIR}/frozen_config_noEA_{STAMP}.json\")\n",
        "print(\"Saved to:\", OUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuolHLDH8xmq",
        "outputId": "fb3d1472-8691-4242-e4e0-42c619566e17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using results file: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/Hybrid1_noEA/val_20250902_191433/best_noEA_val_20250902_191433.csv\n",
            "Rows: 200\n",
            "Loaded tokenizer from: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500\n",
            "Diagnostics frame shape: (200, 18)\n",
            "\n",
            "===== TOP ROUGE-Lsum samples =====\n",
            "====================================================================================================\n",
            "row_id=199 | ROUGE: R1=0.800 R2=0.726 RL=0.783\n",
            "LEN (words): ref=59 hyp=58 | (toks): ref=64 hyp=66\n",
            "ENT counts: TP=3  FP=1  FN=1  | FP_core=1\n",
            "TP: Transcript<PERSON>, age 13 or older<DATE>, the CNN Student News Roll Call<ORG>\n",
            "FP: mascot<GPE>\n",
            "FN: CNN Student News<ORG>\n",
            "\n",
            "[REFERENCE]\n",
            " This page includes the show Transcript . Use the Transcript to help students with reading comprehension and vocabulary . At the bottom of the page, comment for a chance to be mentioned on CNN Student News. You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call.\n",
            "\n",
            "[SUMMARY]\n",
            " This page includes the show Transcript. Use the Transcript to help students with reading comprehension and vocabulary. The Transcript is available at the bottom of this page with your school name, mascot, city and state. You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call.\n",
            "====================================================================================================\n",
            "row_id=174 | ROUGE: R1=0.857 R2=0.667 RL=0.701\n",
            "LEN (words): ref=41 hyp=35 | (toks): ref=55 hyp=49\n",
            "ENT counts: TP=2  FP=2  FN=1  | FP_core=1\n",
            "TP: Boko Haram<PERSON>, Nigeria<GPE>\n",
            "FP: Gwoza<GPE>, last year<DATE>\n",
            "FN: last August<DATE>\n",
            "\n",
            "[REFERENCE]\n",
            " Nigeria's military says it has retaken the northeastern town of Gwoza from Boko Haram . The announcement comes on the eve of the West African nation's general elections . Boko Haram declared Gwoza the headquarters of its \"caliphate\" last August .\n",
            "\n",
            "[SUMMARY]\n",
            " Nigeria's military says it has retaken the northeastern town of Gwoza. Boko Haram militants last year declared the headquarters of their \"caliphate\" The announcement comes on the eve of the West African country's general elections.\n",
            "====================================================================================================\n",
            "row_id=155 | ROUGE: R1=0.588 R2=0.482 RL=0.565\n",
            "LEN (words): ref=30 hyp=50 | (toks): ref=42 hyp=67\n",
            "ENT counts: TP=4  FP=5  FN=1  | FP_core=4\n",
            "TP: 30-day<DATE>, April 5<DATE>, Jon Hamm<PERSON>, eight<CARDINAL>\n",
            "FP: Connecticut<GPE>, Hamm<PERSON>, New Canaan<GPE>, Silver Hill Hospital<ORG>, less than two weeks<DATE>\n",
            "FN: AMC<ORG>\n",
            "\n",
            "[REFERENCE]\n",
            " Report: Actor Jon Hamm of \"Mad Men\" has completed a 30-day stint in rehab for alcohol abuse . AMC's \"Mad Men\" begins its final eight-episode run on April 5 .\n",
            "\n",
            "[SUMMARY]\n",
            " Jon Hamm has recently completed a 30-day stint in rehab for alcohol abuse, TMZ reports. The celebrity-news site says Hamm checked himself into a high-end Silver Hill Hospital in New Canaan, Connecticut. The news comes less than two weeks before \"Mad Men\" begins its final eight-episode run on April 5.\n",
            "====================================================================================================\n",
            "row_id=26 | ROUGE: R1=0.611 R2=0.371 RL=0.556\n",
            "LEN (words): ref=39 hyp=35 | (toks): ref=50 hyp=47\n",
            "ENT counts: TP=0  FP=3  FN=5  | FP_core=3\n",
            "TP: \n",
            "FP: Atlanta<GPE>, Crew<PERSON>, Delta Flight 1086<ORG>\n",
            "FN: Delta Air Lines<ORG>, Flight 1086<DATE>, NTSB<ORG>, last week<DATE>, winter<DATE>\n",
            "\n",
            "[REFERENCE]\n",
            " Delta Air Lines Flight 1086 skidded into a fence last week at a LaGuardia Airport beset by winter weather . The NTSB says the crew reported they did not sense any deceleration from the wheel brake upon landing .\n",
            "\n",
            "[SUMMARY]\n",
            " Delta Flight 1086 skidded into fence at LaGuardia Airport in Atlanta. Crew says they did not sense any deceleration from the wheel brake. Pilot says he was unable to stop the aircraft from drifting left.\n",
            "====================================================================================================\n",
            "row_id=106 | ROUGE: R1=0.543 R2=0.400 RL=0.457\n",
            "LEN (words): ref=47 hyp=45 | (toks): ref=57 hyp=60\n",
            "ENT counts: TP=1  FP=5  FN=3  | FP_core=3\n",
            "TP: Netanyahu<PERSON>\n",
            "FP: Avi Degani<PERSON>, Herzog<PERSON>, Zionist Union<GPE>, four<CARDINAL>, the final weekend<DATE>\n",
            "FN: Isaac Herzog<PERSON>, last-minute<TIME>, the Zionist Union<ORG>\n",
            "\n",
            "[REFERENCE]\n",
            " Polls predicted a tight race coming down to the wire, with Netanyahu in a virtual tie with Isaac Herzog of the Zionist Union party . But actual results show Netanyahu running away with the election . Possible causes: Poll methodology, Netanyahu's last-minute turn to the right .\n",
            "\n",
            "[SUMMARY]\n",
            " Polls predicted a tight race coming down to the wire, with Netanyahu in a virtual tie with his main challenger. Polls showed Herzog taking a four-seat lead into the final weekend before the election. Pollster Avi Degani says he never anticipated a Zionist Union victory.\n",
            "\n",
            "===== LOW ROUGE-Lsum samples =====\n",
            "====================================================================================================\n",
            "row_id=9 | ROUGE: R1=0.147 R2=0.000 RL=0.059\n",
            "LEN (words): ref=31 hyp=39 | (toks): ref=42 hyp=48\n",
            "ENT counts: TP=0  FP=6  FN=3  | FP_core=4\n",
            "TP: \n",
            "FP: 6pm GMT<TIME>, Friday<DATE>, Jonny Evans<PERSON>, Manchester United<PERSON>, Newcastle<GPE>, Papiss Cisse<ORG>\n",
            "FN: Cisse<ORG>, St James' Park<ORG>, six<CARDINAL>\n",
            "\n",
            "[REFERENCE]\n",
            " Alleged incident happened in match at St James' Park . Players face six-match ban if found guilty . Evans denied spitting in statement . Cisse statement says: \"I let you down\"\n",
            "\n",
            "[SUMMARY]\n",
            " Manchester United defender Jonny Evans and Newcastle United striker Papiss Cisse have been charged. The players have until 6pm GMT on Friday to respond to the charge. Both players have previously said they did not spit at each other.\n",
            "====================================================================================================\n",
            "row_id=161 | ROUGE: R1=0.118 R2=0.000 RL=0.059\n",
            "LEN (words): ref=37 hyp=31 | (toks): ref=47 hyp=40\n",
            "ENT counts: TP=1  FP=3  FN=1  | FP_core=1\n",
            "TP: Oklahoma<GPE>\n",
            "FP: One<CARDINAL>, Tulsa<GPE>, dozens<CARDINAL>\n",
            "FN: 7<CARDINAL>\n",
            "\n",
            "[REFERENCE]\n",
            " \"It looks like there's been a little war zone around here\" National Weather Service: There are preliminary reports of 7 tornadoes . \"It looks like there's been a little war zone around here,\" Oklahoma resident says .\n",
            "\n",
            "[SUMMARY]\n",
            " NEW: One person dies at a mobile home park in Tulsa, Oklahoma. NEW: The storm has destroyed dozens of trailers, destroying dozens of homes. The storm is reported across the state.\n",
            "====================================================================================================\n",
            "row_id=35 | ROUGE: R1=0.062 R2=0.000 RL=0.062\n",
            "LEN (words): ref=23 hyp=43 | (toks): ref=27 hyp=50\n",
            "ENT counts: TP=0  FP=0  FN=2  | FP_core=0\n",
            "TP: \n",
            "FP: \n",
            "FN: Donna Brazile<PERSON>, Latest Ferguson<ORG>\n",
            "\n",
            "[REFERENCE]\n",
            " Latest Ferguson shootings push strained race relations to breaking point . Donna Brazile: Relationship between police forces and minority communities must improve .\n",
            "\n",
            "[SUMMARY]\n",
            " Sara: The situation is dire enough that we absolutely have to try to uncover the good and not dwell on the negative. She says we should use the time to consider how best to move forward with common resolve instead of mutual recrimination.\n",
            "====================================================================================================\n",
            "row_id=39 | ROUGE: R1=0.131 R2=0.000 RL=0.066\n",
            "LEN (words): ref=26 hyp=35 | (toks): ref=34 hyp=47\n",
            "ENT counts: TP=0  FP=2  FN=0  | FP_core=2\n",
            "TP: \n",
            "FP: Anne Friedberg<PERSON>, Le Corbusier<ORG>\n",
            "FN: \n",
            "\n",
            "[REFERENCE]\n",
            " A Californian startup has developed a circular smartphone called \"Runcible\" The provocative anti-phone is meant to be an antidote to our obsession for digital devices .\n",
            "\n",
            "[SUMMARY]\n",
            " Photographer Anne Friedberg offers an example of the history of architecture. Perret was a strong advocate of the traditional French casement window, which was oriented vertically. Le Corbusier designed his buildings around a rectangular frame.\n",
            "====================================================================================================\n",
            "row_id=118 | ROUGE: R1=0.067 R2=0.000 RL=0.067\n",
            "LEN (words): ref=20 hyp=38 | (toks): ref=27 hyp=52\n",
            "ENT counts: TP=0  FP=3  FN=2  | FP_core=2\n",
            "TP: \n",
            "FP: Alex Pring<PERSON>, Cynthia Falardeau<PERSON>, last summer<DATE>\n",
            "FN: 3<CARDINAL>, Orlando<GPE>\n",
            "\n",
            "[REFERENCE]\n",
            " Around the world, volunteers with 3-D printers are making limbs for children. Meet some Orlando engineering students changing the world.\n",
            "\n",
            "[SUMMARY]\n",
            " Alex Pring, a little boy, got a battery-powered robotic arm last summer. His mother, Cynthia Falardeau, says her son learned to adapt to the technology. She says she was more concerned about getting him therapy for his autism.\n",
            "\n",
            "===== HIGHEST CORE-HALLUCINATION (FP_core) samples =====\n",
            "====================================================================================================\n",
            "row_id=193 | ROUGE: R1=0.278 R2=0.086 RL=0.194\n",
            "LEN (words): ref=24 hyp=47 | (toks): ref=28 hyp=70\n",
            "ENT counts: TP=3  FP=10  FN=1  | FP_core=10\n",
            "TP: Egypt<GPE>, Saturday<DATE>, Yemen<GPE>\n",
            "FP: Bahrain<GPE>, Houthis<PERSON>, Kuwait<GPE>, Qatar<GPE>, Sanaa<PERSON>, Saudi Arabia<GPE>, The Arab League<ORG>, U.S.<GPE>, al-Hazm Storm<ORG>, the United Arab Emirates<GPE>\n",
            "FN: Arab League<ORG>\n",
            "\n",
            "[REFERENCE]\n",
            " Yemen's President rallies support on Saturday in Egypt . Arab League blessing of military action may set the stage for a ground invasion .\n",
            "\n",
            "[SUMMARY]\n",
            " Sanaa, Yemen: Saudi Arabia, Egypt join forces in Egypt. The Arab League is expected to give its official blessing to al-Hazm Storm on Saturday. The U.S. has blocked the Houthis, effectively cutting off their supply lines. The coalition partners include the United Arab Emirates, Kuwait, Bahrain, Qatar,\n",
            "====================================================================================================\n",
            "row_id=18 | ROUGE: R1=0.194 R2=0.000 RL=0.117\n",
            "LEN (words): ref=48 hyp=55 | (toks): ref=61 hyp=70\n",
            "ENT counts: TP=0  FP=9  FN=2  | FP_core=7\n",
            "TP: \n",
            "FP: 70,000<CARDINAL>, Farah<PERSON>, Iraq<GPE>, Jordan<GPE>, Sudan<GPE>, Syria<GPE>, Two days later<DATE>, Zarqa<GPE>, the Palestinian Territories<GPE>\n",
            "FN: International Women's Day<ORG>, Jina Krause-Vilmar<PERSON>\n",
            "\n",
            "[REFERENCE]\n",
            " Jina Krause-Vilmar: On International Women's Day, focus on helping refugee women adapt to new lives . She says aid agencies like hers aim to help these women build job skills and financial literacy . She says the effort includes educating men about wives' and daughters' new roles .\n",
            "\n",
            "[SUMMARY]\n",
            " Farah fled Syria in the middle of the night, hitching rides on trucks until they finally crossed into Jordan. Two days later, she gave birth to a girl in a country where they hold no status. Like 70,000 other refugees from Syria, Iraq, Sudan and the Palestinian Territories, Farah lives with her family in Zarqa\n",
            "====================================================================================================\n",
            "row_id=147 | ROUGE: R1=0.167 R2=0.000 RL=0.167\n",
            "LEN (words): ref=23 hyp=38 | (toks): ref=32 hyp=50\n",
            "ENT counts: TP=1  FP=7  FN=6  | FP_core=6\n",
            "TP: NIH<ORG>\n",
            "FP: Dallas<GPE>, Duncan<PERSON>, Nina Pham<PERSON>, October<DATE>, Pham<PERSON>, Texas Health Presbyterian Hospital<ORG>, Thomas Eric Duncan<PERSON>\n",
            "FN: Africa<LOC>, Friday<DATE>, Maryland<GPE>, U.S.<GPE>, one<CARDINAL>, only four<CARDINAL>\n",
            "\n",
            "[REFERENCE]\n",
            " American being flown from Africa to Maryland hospital; to arrive Friday . NIH has one of only four U.S. hospital biocontainment units .\n",
            "\n",
            "[SUMMARY]\n",
            " Nina Pham, a nurse at Texas Health Presbyterian Hospital Dallas, was admitted to NIH in October. She contracted Ebola while treating Liberian national Thomas Eric Duncan. Pham recovered and was released free of disease. Duncan died in hospital.\n",
            "====================================================================================================\n",
            "row_id=66 | ROUGE: R1=0.374 R2=0.135 RL=0.286\n",
            "LEN (words): ref=44 hyp=48 | (toks): ref=56 hyp=59\n",
            "ENT counts: TP=1  FP=8  FN=6  | FP_core=6\n",
            "TP: Danny Welbeck<PERSON>\n",
            "FP: Arsenal<ORG>, Bradford City<GPE>, England<GPE>, Gunners<PERSON>, Reading<PERSON>, United<ORG>, the last four<DATE>, the second half<DATE>\n",
            "FN: Angel Di Maria<PERSON>, Holders Arsenal<ORG>, Manchester United<PERSON>, Nacho<GPE>, Wayne Rooney<PERSON>, second half<DATE>\n",
            "\n",
            "[REFERENCE]\n",
            " Arsenal beats Man Utd 2-1 in FA Cup quarterfinal . Former Manchester United player Danny Welbeck scores winner . Holders Arsenal took the lead through Nacho Monreal before Wayne Rooney equalized . Angel Di Maria sent off for shoving referee in second half .\n",
            "\n",
            "[SUMMARY]\n",
            " Arsenal beat Arsenal 2-1 to reach the semifinals of the FA Cup. Danny Welbeck scores twice for Arsenal in the second half. The England striker was surplus to requirements at Louis van Gaal's United. The Gunners will now face either Reading or Bradford City in the last four.\n",
            "====================================================================================================\n",
            "row_id=60 | ROUGE: R1=0.127 R2=0.037 RL=0.109\n",
            "LEN (words): ref=55 hyp=55 | (toks): ref=65 hyp=70\n",
            "ENT counts: TP=0  FP=5  FN=5  | FP_core=5\n",
            "TP: \n",
            "FP: Apple<ORG>, CNN<ORG>, Cook<PERSON>, LondonApple<ORG>, Tim Cook<PERSON>\n",
            "FN: 150<CARDINAL>, Andre Spicer<PERSON>, Smartwatches<ORG>, just minutes<TIME>, second<ORDINAL>\n",
            "\n",
            "[REFERENCE]\n",
            " Smartwatches have hidden, darker side the companies selling them are unlikely to talk about, says Andre Spicer . Spicer: Average smartphone users check phone 150 times a day, starting just minutes after waking up . We're also likely to become more anxious about things we may never have given second thought to, he adds .\n",
            "\n",
            "[SUMMARY]\n",
            " CNN is showcasing the work of The Conversation, a collaboration between journalists and academics. The content is produced solely by The Conversation. LondonApple CEO Tim Cook releases the much-anticipated Apple watch. Cook said the new watch is a \"comprehensive health and fitness companion\" But we're unlikely to hear much about how people will actually use\n",
            "\n",
            "=== LENGTH STATS (words) ===\n",
            "reference: {'mean': 33.315, 'std': 9.891702330741662, 'q25': 25.0, 'med': 31.0, 'q75': 39.25, 'min': 13, 'max': 59}\n",
            "summary  : {'mean': 43.795, 'std': 8.689820193766957, 'q25': 38.0, 'med': 44.0, 'q75': 50.0, 'min': 23, 'max': 70}\n",
            "\n",
            "=== LENGTH STATS (tokens) ===\n",
            "reference: {'mean': 41.79, 'std': 11.565720902736672, 'q25': 33.0, 'med': 40.0, 'q75': 50.0, 'min': 18, 'max': 74}\n",
            "summary  : {'mean': 57.475, 'std': 9.593194202141433, 'q25': 50.0, 'med': 58.0, 'q75': 66.0, 'min': 36, 'max': 84}\n",
            "\n",
            "Saved per-row diagnostics to: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/Hybrid1_noEA/val_20250902_191433/diagnostics_per_row.csv\n"
          ]
        }
      ],
      "source": [
        "# === Sample browser: TP / FP / FN entities + ROUGE + lengths (baseline NO-EA) ===\n",
        "\n",
        "RUN_DIR  = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/Hybrid1_noEA/val_20250902_191433\"\n",
        "TOK_PATH = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500\"\n",
        "TOP_N    = 5   # how many high/low-ROUGE samples to print\n",
        "HAL_N    = 5   # how many high-hallucination (core FP) samples to print\n",
        "SAVE_CSV = True\n",
        "\n",
        "import os, sys, csv, glob, json, math, re, numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "from pprint import pprint\n",
        "\n",
        "# ---------- locate the results CSV ----------\n",
        "cands = sorted(glob.glob(os.path.join(RUN_DIR, \"best*val*.csv\")))\n",
        "assert cands, f\"No best*val*.csv found in {RUN_DIR}\"\n",
        "BEST_CSV = cands[0]\n",
        "print(\"Using results file:\", BEST_CSV)\n",
        "\n",
        "# ---------- robust loader for result CSV ----------\n",
        "def load_best(path: str) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path)\n",
        "    cols = {c.lower(): c for c in df.columns}\n",
        "    # Guess columns\n",
        "    row_id = cols.get(\"row_id\") or cols.get(\"id\")\n",
        "    article = cols.get(\"article\") or cols.get(\"source\")\n",
        "    ref = cols.get(\"highlights\") or cols.get(\"reference\") or cols.get(\"ref\")\n",
        "    summ = cols.get(\"summary\") or cols.get(\"hybrid_h1\") or cols.get(\"top_beam_summary\")\n",
        "    for need in [row_id, article, ref, summ]:\n",
        "        assert need in df.columns, f\"Missing required column in {path}. Got {list(df.columns)}\"\n",
        "    out = df[[row_id, article, ref, summ]].copy()\n",
        "    out.columns = [\"row_id\",\"article\",\"reference\",\"summary\"]\n",
        "    out[\"row_id\"] = out[\"row_id\"].astype(int)\n",
        "    for k in [\"article\",\"reference\",\"summary\"]:\n",
        "        out[k] = out[k].astype(str)\n",
        "    return out\n",
        "\n",
        "best = load_best(BEST_CSV)\n",
        "print(\"Rows:\", len(best))\n",
        "\n",
        "# ---------- NER setup ----------\n",
        "import spacy, subprocess, importlib.util\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "except OSError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "\n",
        "KEEP = {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"DATE\",\"TIME\",\"CARDINAL\",\"MONEY\",\"PERCENT\",\"QUANTITY\",\"ORDINAL\"}\n",
        "CORE = {\"PERSON\",\"ORG\",\"GPE\"}\n",
        "\n",
        "def _norm(s: str) -> str:\n",
        "    return \" \".join(str(s or \"\").strip().split())\n",
        "\n",
        "def ents_norm(text: str):\n",
        "    doc = nlp(str(text))\n",
        "    out = set()\n",
        "    for e in doc.ents:\n",
        "        if e.label_ in KEEP:\n",
        "            out.add((_norm(e.text), e.label_))\n",
        "    return out\n",
        "\n",
        "# ---------- ROUGE ----------\n",
        "try:\n",
        "    from rouge_score import rouge_scorer\n",
        "except ImportError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"rouge-score==0.1.2\"])\n",
        "    from rouge_score import rouge_scorer\n",
        "\n",
        "SC = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\"], use_stemmer=True)\n",
        "\n",
        "# ---------- optional tokenizer for token lengths ----------\n",
        "tok = None\n",
        "if TOK_PATH:\n",
        "    try:\n",
        "        from transformers import AutoTokenizer\n",
        "        tok = AutoTokenizer.from_pretrained(TOK_PATH, use_fast=True)\n",
        "        print(\"Loaded tokenizer from:\", TOK_PATH)\n",
        "    except Exception as e:\n",
        "        print(\"[warn] tokenizer load failed:\", e)\n",
        "\n",
        "def toklen(s: str):\n",
        "    if tok is None: return None\n",
        "    return len(tok(str(s))[\"input_ids\"])\n",
        "\n",
        "# ---------- per-row diagnostics ----------\n",
        "rows = []\n",
        "for _, r in best.iterrows():\n",
        "    art, ref, hyp, rid = r[\"article\"], r[\"reference\"], r[\"summary\"], r[\"row_id\"]\n",
        "    # Entities\n",
        "    R = ents_norm(ref); H = ents_norm(hyp)\n",
        "    TP = sorted(list(H & R))\n",
        "    FP = sorted(list(H - R))\n",
        "    FN = sorted(list(R - H))\n",
        "    FP_core = [x for x in FP if x[1] in CORE]\n",
        "    # ROUGE\n",
        "    sc = SC.score(ref, hyp)\n",
        "    r1, r2, rl = sc[\"rouge1\"].fmeasure, sc[\"rouge2\"].fmeasure, sc[\"rougeLsum\"].fmeasure\n",
        "    # lengths\n",
        "    ref_words = len(ref.split()); hyp_words = len(hyp.split())\n",
        "    ref_toks = toklen(ref); hyp_toks = toklen(hyp)\n",
        "    rows.append(dict(\n",
        "        row_id=rid,\n",
        "        r1_f=r1, r2_f=r2, rl_f=rl,\n",
        "        ref_words=ref_words, hyp_words=hyp_words,\n",
        "        ref_toks=ref_toks, hyp_toks=hyp_toks,\n",
        "        TP_cnt=len(TP), FP_cnt=len(FP), FN_cnt=len(FN),\n",
        "        FP_core_cnt=len(FP_core),\n",
        "        TP_list=TP, FP_list=FP, FN_list=FN,\n",
        "        article=art, reference=ref, summary=hyp\n",
        "    ))\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "df.sort_values(\"row_id\", inplace=True)\n",
        "print(\"Diagnostics frame shape:\", df.shape)\n",
        "\n",
        "# ---------- print helpers ----------\n",
        "def show_sample(row, max_list=12, max_chars=800):\n",
        "    print(\"=\"*100)\n",
        "    print(f\"row_id={row['row_id']} | ROUGE: R1={row['r1_f']:.3f} R2={row['r2_f']:.3f} RL={row['rl_f']:.3f}\")\n",
        "    print(f\"LEN (words): ref={row['ref_words']} hyp={row['hyp_words']}\", end=\"\")\n",
        "    if row['ref_toks'] is not None:\n",
        "        print(f\" | (toks): ref={row['ref_toks']} hyp={row['hyp_toks']}\")\n",
        "    else:\n",
        "        print()\n",
        "    print(f\"ENT counts: TP={row['TP_cnt']}  FP={row['FP_cnt']}  FN={row['FN_cnt']}  | FP_core={row['FP_core_cnt']}\")\n",
        "    # lists\n",
        "    def fmt_list(L):\n",
        "        return \", \".join([f\"{t}<{l}>\" for t,l in L[:max_list]]) + (\" ...\" if len(L)>max_list else \"\")\n",
        "    print(\"TP:\", fmt_list(row['TP_list']))\n",
        "    print(\"FP:\", fmt_list(row['FP_list']))\n",
        "    print(\"FN:\", fmt_list(row['FN_list']))\n",
        "    # texts\n",
        "    def snip(s):\n",
        "        s=str(s)\n",
        "        return s if len(s)<=max_chars else (s[:max_chars]+\" ⟨…⟩\")\n",
        "    print(\"\\n[REFERENCE]\\n\", snip(row[\"reference\"]))\n",
        "    print(\"\\n[SUMMARY]\\n\", snip(row[\"summary\"]))\n",
        "\n",
        "# ---------- pick samples ----------\n",
        "hi = df.sort_values(\"rl_f\", ascending=False).head(TOP_N)\n",
        "lo = df.sort_values(\"rl_f\", ascending=True).head(TOP_N)\n",
        "hal = df.sort_values(\"FP_core_cnt\", ascending=False).head(HAL_N)\n",
        "\n",
        "print(\"\\n===== TOP ROUGE-Lsum samples =====\")\n",
        "for _, r in hi.iterrows():\n",
        "    show_sample(r)\n",
        "\n",
        "print(\"\\n===== LOW ROUGE-Lsum samples =====\")\n",
        "for _, r in lo.iterrows():\n",
        "    show_sample(r)\n",
        "\n",
        "print(\"\\n===== HIGHEST CORE-HALLUCINATION (FP_core) samples =====\")\n",
        "for _, r in hal.iterrows():\n",
        "    show_sample(r)\n",
        "\n",
        "# ---------- quick length stats ----------\n",
        "def length_stats(col):\n",
        "    w = df[col].to_numpy()\n",
        "    return dict(mean=float(np.mean(w)), std=float(np.std(w)), q25=float(np.percentile(w,25)),\n",
        "                med=float(np.median(w)), q75=float(np.percentile(w,75)), min=int(np.min(w)), max=int(np.max(w)))\n",
        "print(\"\\n=== LENGTH STATS (words) ===\")\n",
        "print(\"reference:\", length_stats(\"ref_words\"))\n",
        "print(\"summary  :\", length_stats(\"hyp_words\"))\n",
        "if tok is not None:\n",
        "    print(\"\\n=== LENGTH STATS (tokens) ===\")\n",
        "    print(\"reference:\", length_stats(\"ref_toks\"))\n",
        "    print(\"summary  :\", length_stats(\"hyp_toks\"))\n",
        "\n",
        "# ---------- save diagnostics CSV ----------\n",
        "if SAVE_CSV:\n",
        "    out_csv = os.path.join(RUN_DIR, \"diagnostics_per_row.csv\")\n",
        "    # Trim the heavy text columns to keep file size reasonable\n",
        "    df_out = df.drop(columns=[\"article\",\"reference\",\"summary\",\"TP_list\",\"FP_list\",\"FN_list\"]).copy()\n",
        "    df_out.to_csv(out_csv, index=False)\n",
        "    print(\"\\nSaved per-row diagnostics to:\", out_csv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZVCB54o_O8L",
        "outputId": "c9bde7ac-0a2a-4e8a-d701-f73ca2887d38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows: 200\n",
            "\n",
            "=== LENGTH STATS (words) ===\n",
            "reference: {'count': 200, 'mean': 54.865, 'std': 19.151155970332447, 'min': 22, 'q25': 39.75, 'med': 52.0, 'q75': 66.0, 'max': 127}\n",
            "summary  : {'count': 200, 'mean': 51.615, 'std': 8.253288738436334, 'min': 26, 'q25': 46.75, 'med': 53.0, 'q75': 56.25, 'max': 81}\n",
            "\n",
            "=== LENGTH STATS (tokens) ===\n",
            "reference: {'count': 200, 'mean': 67.465, 'std': 23.697653364837628, 'min': 28, 'q25': 48.0, 'med': 64.0, 'q75': 82.0, 'max': 154}\n",
            "summary  : {'count': 200, 'mean': 66.05, 'std': 9.049723752689912, 'min': 38, 'q25': 61.75, 'med': 70.0, 'q75': 70.0, 'max': 100}\n",
            "\n",
            "Saved per-example lengths to: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/Hybrid1_eval_probe_20250902_180930/lengths_per_example.csv\n"
          ]
        }
      ],
      "source": [
        "# Hybrid length\n",
        "RUN_CSV  = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/Hybrid1_eval_probe_20250902_180930/best_val.csv\"\n",
        "TOK_PATH = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500\"\n",
        "import os, pandas as pd, numpy as np\n",
        "\n",
        "# Load\n",
        "df = pd.read_csv(RUN_CSV)\n",
        "cols = {c.lower(): c for c in df.columns}\n",
        "\n",
        "# column mapping (robust)\n",
        "row_id   = cols.get(\"row_id\") or cols.get(\"id\")\n",
        "article  = cols.get(\"article\") or cols.get(\"source\")\n",
        "reference= cols.get(\"highlights\") or cols.get(\"reference\") or cols.get(\"ref\")\n",
        "summary  = cols.get(\"summary\") or cols.get(\"hybrid_h1\") or cols.get(\"top_beam_summary\")\n",
        "assert all([row_id, article, reference, summary]), f\"Missing required columns. Got: {list(df.columns)}\"\n",
        "\n",
        "df = df[[row_id, article, reference, summary]].copy()\n",
        "df.columns = [\"row_id\",\"article\",\"reference\",\"summary\"]\n",
        "df[\"reference\"] = df[\"reference\"].astype(str)\n",
        "df[\"summary\"]   = df[\"summary\"].astype(str)\n",
        "\n",
        "# Tokenizer (for token-based lengths)\n",
        "tok = None\n",
        "try:\n",
        "    from transformers import AutoTokenizer\n",
        "    tok = AutoTokenizer.from_pretrained(TOK_PATH, use_fast=True)\n",
        "except Exception as e:\n",
        "    print(\"[warn] tokenizer not loaded:\", e)\n",
        "\n",
        "def toklen(s: str):\n",
        "    return len(tok(s)[\"input_ids\"]) if tok else None\n",
        "\n",
        "# Lengths\n",
        "ref_words = df[\"reference\"].str.split().apply(len)\n",
        "sum_words = df[\"summary\"].str.split().apply(len)\n",
        "ref_tokens = df[\"reference\"].apply(toklen) if tok else None\n",
        "sum_tokens = df[\"summary\"].apply(toklen)   if tok else None\n",
        "\n",
        "# Stats helper\n",
        "def stats(series: pd.Series):\n",
        "    s = series.dropna().to_numpy()\n",
        "    q = lambda p: float(np.percentile(s, p)) if s.size else None\n",
        "    return {\"count\": int(series.count()), \"mean\": float(np.mean(s)),\n",
        "            \"std\": float(np.std(s)), \"min\": int(np.min(s)),\n",
        "            \"q25\": q(25), \"med\": float(np.median(s)), \"q75\": q(75),\n",
        "            \"max\": int(np.max(s))}\n",
        "\n",
        "print(\"Rows:\", len(df))\n",
        "print(\"\\n=== LENGTH STATS (words) ===\")\n",
        "print(\"reference:\", stats(ref_words))\n",
        "print(\"summary  :\", stats(sum_words))\n",
        "\n",
        "if tok:\n",
        "    print(\"\\n=== LENGTH STATS (tokens) ===\")\n",
        "    print(\"reference:\", stats(ref_tokens))\n",
        "    print(\"summary  :\", stats(sum_tokens))\n",
        "\n",
        "# Save per-example lengths\n",
        "out = pd.DataFrame({\n",
        "    \"row_id\": df[\"row_id\"],\n",
        "    \"ref_words\": ref_words, \"sum_words\": sum_words,\n",
        "    \"ref_tokens\": ref_tokens if tok else [None]*len(df),\n",
        "    \"sum_tokens\": sum_tokens if tok else [None]*len(df),\n",
        "})\n",
        "OUT_CSV = os.path.join(os.path.dirname(RUN_CSV), \"lengths_per_example.csv\")\n",
        "out.to_csv(OUT_CSV, index=False)\n",
        "print(\"\\nSaved per-example lengths to:\", OUT_CSV)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzQb7O5fpsGE"
      },
      "source": [
        "all decode strategies, hypers, lengths, reranking + entity settings, in one place:\n",
        "\n",
        "# Data, tokenizer, seeds\n",
        "\n",
        "* **Checkpoints compared:** `ckpt_step6000`, `ckpt_step6500`, `ckpt_step7000`\n",
        "* **Tokenizer/model:** load from the checkpoint being evaluated\n",
        "* **Source truncation:** `MAX_SRC_LEN = 400`\n",
        "* **VAL sizes:** probe `N_VAL = 200` (to pick a winner), then `N_VAL = 1000` (final VAL)\n",
        "* **TEST:** full test set with the winner’s frozen policy\n",
        "* **Seed:** `SEED = 0` for Python/NumPy/PyTorch/CUDA\n",
        "\n",
        "# Span-aware CopyNext (logits processor)\n",
        "\n",
        "* **Processor:** `EntityAwareSpanProcessor`\n",
        "* **Maps:** built from tokenized source with offset mappings; a map for every n-gram key up to `max_span`\n",
        "* **Entity detection:** spaCy `en_core_web_sm`\n",
        "* **Entity-aware biasing:**\n",
        "\n",
        "  * `gamma = 0.4` (plain span continuation)\n",
        "  * `gamma_entity = 1.5` (if the next token continues an entity span)\n",
        "  * `max_span = 6`\n",
        "* **Labels kept for NER:**\n",
        "  `KEEP_LABELS = {PERSON, ORG, GPE, LOC, DATE, TIME, CARDINAL, MONEY, PERCENT, QUANTITY, ORDINAL}`\n",
        "* **Core entity set (for safety):** `CORE = {PERSON, ORG, GPE}`\n",
        "\n",
        "# Decoding: two fixed pools (Hybrid-1)\n",
        "\n",
        "Each example produces **20 candidates total** (10 from each pool). Same settings for VAL & TEST.\n",
        "\n",
        "**Pool A — CN2\\_LONG (extractive-leaning):**\n",
        "\n",
        "* `num_beams = 10`\n",
        "* `num_return_sequences = 10`\n",
        "* `num_beam_groups = 5`\n",
        "* `diversity_penalty = 0.3`\n",
        "* `max_new_tokens = 100`\n",
        "* `min_new_tokens = 55`\n",
        "* `no_repeat_ngram_size = 3`\n",
        "* `length_penalty = 2.0`\n",
        "* `early_stopping = True`\n",
        "* `return_dict_in_generate = True`\n",
        "* `output_scores = True`\n",
        "\n",
        "**Pool B — CN\\_MED (shorter/medium):**\n",
        "\n",
        "* `num_beams = 10`\n",
        "* `num_return_sequences = 10`\n",
        "* `num_beam_groups = 5`\n",
        "* `diversity_penalty = 0.3`\n",
        "* `max_new_tokens = 70`\n",
        "* `min_new_tokens = 35`\n",
        "* `no_repeat_ngram_size = 3`\n",
        "* `length_penalty = 1.6`\n",
        "* `early_stopping = True`\n",
        "* `return_dict_in_generate = True`\n",
        "* `output_scores = True`\n",
        "\n",
        "# Stage-A: TH-C gate (candidate filter/score)\n",
        "\n",
        "Applied to the **union** of both pools per article.\n",
        "\n",
        "* **Hard-fail rule:** drop any candidate with **unsupported CORE entity** (i.e., hallucinated PERSON/ORG/GPE).\n",
        "* **Unsupported non-core set (for a soft penalty):**\n",
        "  `{DATE, QUANTITY, MONEY, PERCENT, CARDINAL, ORDINAL}`\n",
        "* **Adaptive target length:**\n",
        "  `target_len = round(0.9 * token_length(lead-3 sentences of source))`, clamped to `[32, 60]`\n",
        "* **Length reward Gaussian:** `len_sigma = 20.0`\n",
        "* **TH-C score (per candidate):**\n",
        "\n",
        "  ```\n",
        "  THC = 5.0 * entity_score\n",
        "        + 0.2 * contiguity_lcs\n",
        "        + 0.3 * len_reward\n",
        "        - 1{unsupported_noncore > 0} * NONCORE_PEN\n",
        "  ```\n",
        "\n",
        "  where:\n",
        "\n",
        "  * `entity_score`: +3 for each supported CORE entity, +1 for each supported non-core entity\n",
        "  * `contiguity_lcs`: contiguous LCS transitions (word-level) vs source\n",
        "  * `len_reward`: exp(-((len - target\\_len)^2) / (2 \\* len\\_sigma^2))\n",
        "  * `NONCORE_PEN = 2.0`\n",
        "* **Keep:** `M_KEEP = 10` top survivors by `THC` (after hard-fail).\n",
        "  If all hard-fail, fall back to top by `THC` without dropping.\n",
        "\n",
        "# Survivor scoring (model confidence, normalized)\n",
        "\n",
        "* **base\\_norm:** compute teacher-forced token CE on the candidate summary, then\n",
        "  `base_norm = (− per-token CE) / (length ** 0.3)`\n",
        "\n",
        "  * (Decoding labels are truncated at 256 tokens during this scoring pass.)\n",
        "\n",
        "# Final reranker features (computed **within group** of survivors)\n",
        "\n",
        "(All min–max normalized within the 10 survivors per article unless noted.)\n",
        "\n",
        "1. `THC_norm` — min-max of Stage-A `THC`\n",
        "2. `LeadSim` — ROUGE-Lsum(hyp, **lead-3 of source**)\n",
        "3. `MBR` — mean ROUGE-Lsum to other survivors (centroid similarity)\n",
        "4. `len_reward` — Gaussian reward vs adaptive target (recomputed per hyp)\n",
        "5. `contig_lcs_mm` — min-max of contiguous LCS transitions vs source\n",
        "6. `base_norm_mm` — min-max of `base_norm`\n",
        "7. `ent_consensus` — average Jaccard overlap of extracted entities vs other survivors\n",
        "8. `unsup_nonc_mm` — min-max of the **count** of unsupported non-core entities\n",
        "\n",
        "# Pairwise logistic reranker (trained on VAL per checkpoint)\n",
        "\n",
        "* **Training labels:** pairwise preference by **ROUGE-Lsum(reference, hyp)**\n",
        "* **Model:** `LogisticRegression(solver=\"liblinear\", max_iter=1000, random_state=SEED)`\n",
        "* **Inputs:** feature differences (feature\\_i − feature\\_j) over survivor pairs\n",
        "* **Output:** weights JSON (`intercept` + `coefs` by feature name) **saved & frozen**\n",
        "* **Selection at inference:** apply learned linear score, pick argmax\n",
        "\n",
        "**Safety fallback at selection time:**\n",
        "If the chosen hyp contains any **unsupported CORE entity**, pick the highest-scoring survivor that **doesn’t**.\n",
        "\n",
        "# Metrics (VAL & TEST)\n",
        "\n",
        "* **ROUGE:** mean F1 — `rouge1`, `rouge2`, `rougeLsum`\n",
        "* **Entity PRF:** using `KEEP_LABELS` sets on reference vs hypothesis\n",
        "* **UCER:** unsupported-CORE-entity rate (fraction of examples with any) + avg # per example\n",
        "\n",
        "# Protocol summary (how we run it)\n",
        "\n",
        "1. **Probe VAL (N=200)** for each of: `ckpt_step6000`, `ckpt_step6500`, `ckpt_step7000`\n",
        "   (decode with the two pools + same EA\\_ARGS, Stage-A, survivors, features; train LR on this VAL slice; select; compute metrics).\n",
        "2. Pick winner by **ROUGE-Lsum**, break ties with **entity F1**, then **lower UCER**.\n",
        "3. **Full VAL (N=1000)** on the winner; **retrain** LR on this full VAL; save frozen weights/config.\n",
        "4. **TEST** once with the winner using the **frozen VAL policy** (same decode, Stage-A, features, LR weights).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj_jJ18zewwL"
      },
      "source": [
        "# Test on the same exact checkpoint and configs (200 smoke test)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpOJLrr45coV",
        "outputId": "85a26e41-31b8-4478-b94f-04b9bb0e9f79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "Checkpoint: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500\n"
          ]
        }
      ],
      "source": [
        "# ==== SETUP ====\n",
        "!pip -q install transformers==4.43.4 rouge-score==0.1.2 scikit-learn spacy\n",
        "import sys, os, json, math, time, csv, random, re, gc, itertools\n",
        "import numpy as np, pandas as pd, torch\n",
        "from torch import nn, amp\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from dataclasses import dataclass\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from rouge_score import rouge_scorer\n",
        "import spacy\n",
        "\n",
        "# --- load spaCy NER (small) ---\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "except OSError:\n",
        "    import subprocess, sys as _sys\n",
        "    subprocess.check_call([_sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "\n",
        "\n",
        "CKPT_DIR = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500\"\n",
        "TEST_CSV = \"/content/drive/MyDrive/cleaned_outputs/naa/test_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "\n",
        "OUT_ROOT = os.path.join(CKPT_DIR, \"Hybrid_vs_Baseline_200\")\n",
        "os.makedirs(OUT_ROOT, exist_ok=True)\n",
        "\n",
        "# --- general config ---\n",
        "SEED = 0\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MAX_SRC_LEN = 400\n",
        "MAX_NEW_LONG = 100; MIN_NEW_LONG = 55   # pool A\n",
        "MAX_NEW_MED  = 70;  MIN_NEW_MED  = 35   # pool B\n",
        "NO_REPEAT_NGRAM = 3\n",
        "LENGTH_PEN_LONG = 2.0\n",
        "LENGTH_PEN_MED  = 1.6\n",
        "NUM_BEAMS = 10\n",
        "NUM_GROUPS = 5\n",
        "DIVERSITY = 0.3\n",
        "BATCH_LONG = 4\n",
        "BATCH_MED  = 8\n",
        "\n",
        "# Entity-aware (Hybrid) bias\n",
        "EA_GAMMA = 0.4\n",
        "EA_GAMMA_ENTITY = 1.5\n",
        "EA_MAX_SPAN = 6\n",
        "\n",
        "# Stage-A gating\n",
        "CORE = {\"PERSON\",\"ORG\",\"GPE\"}\n",
        "KEEP = {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"DATE\",\"TIME\",\"CARDINAL\",\"MONEY\",\"PERCENT\",\"QUANTITY\",\"ORDINAL\"}\n",
        "NONCORE = {\"DATE\",\"QUANTITY\",\"MONEY\",\"PERCENT\",\"CARDINAL\",\"ORDINAL\"}\n",
        "NONCORE_PEN = 2.0   # same as before\n",
        "\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "print(\"Device:\", DEVICE)\n",
        "print(\"Checkpoint:\", CKPT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyP50pAZfKja",
        "outputId": "8e318ded-e0b2-42db-c95b-1e115e09e7c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected rows: 200 | first 5 ids: [46, 82, 106, 317, 327]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def robust_read_csv(path: str) -> pd.DataFrame:\n",
        "    try:\n",
        "        df = pd.read_csv(path, engine=\"python\", quoting=csv.QUOTE_ALL, escapechar=\"\\\\\")\n",
        "        cols = {c.lower(): c for c in df.columns}\n",
        "        need = [\"article\", \"highlights\"]\n",
        "        got = {k: cols[k] for k in need if k in cols}\n",
        "        if len(got) != 2:\n",
        "            raise KeyError(f\"File must contain columns {need}. Found: {list(df.columns)}\")\n",
        "        return df[[got[\"article\"], got[\"highlights\"]]].rename(\n",
        "            columns={got[\"article\"]:\"article\", got[\"highlights\"]:\"highlights\"}\n",
        "        )\n",
        "    except Exception:\n",
        "        rows = []\n",
        "        with open(path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "            reader = csv.reader(f, delimiter=\",\", quotechar='\"', escapechar=\"\\\\\")\n",
        "            header = [h.strip().lower() for h in next(reader)]\n",
        "            idx_art, idx_sum = header.index(\"article\"), header.index(\"highlights\")\n",
        "            for row in reader:\n",
        "                rows.append([row[idx_art], row[idx_sum]])\n",
        "        return pd.DataFrame(rows, columns=[\"article\",\"highlights\"])\n",
        "\n",
        "# For same 200 rows across both systems\n",
        "df_all = robust_read_csv(TEST_CSV)\n",
        "N_SAMPLE = 200\n",
        "idxs = list(range(len(df_all)))\n",
        "random.Random(SEED).shuffle(idxs)\n",
        "row_ids_200 = sorted(idxs[:N_SAMPLE])\n",
        "df_200 = df_all.iloc[row_ids_200].reset_index(drop=False).rename(columns={\"index\":\"row_id\"})\n",
        "print(\"Selected rows:\", len(df_200), \"| first 5 ids:\", df_200[\"row_id\"].head().tolist())\n",
        "\n",
        "# ==== TOKENIZER + MODEL ====\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM, LogitsProcessor, LogitsProcessorList\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
        "cfg = AutoConfig.from_pretrained(CKPT_DIR, attn_implementation=\"eager\")\n",
        "base = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR, config=cfg).to(DEVICE).eval()\n",
        "base.config.use_cache = False\n",
        "if hasattr(base, \"gradient_checkpointing_enable\"):\n",
        "    base.gradient_checkpointing_enable()\n",
        "\n",
        "# ==== NER & text utils ====\n",
        "def _normalize_ent_text(s: str): return \" \".join(str(s or \"\").strip().split())\n",
        "\n",
        "def _ents_norm(text: str):\n",
        "    doc = nlp(str(text))\n",
        "    return {(_normalize_ent_text(e.text), e.label_) for e in doc.ents if e.label_ in KEEP}\n",
        "\n",
        "def strict_entity_score(src, hyp):\n",
        "    S = _ents_norm(src); H = _ents_norm(hyp)\n",
        "    overlap = H & S\n",
        "    unsupported = H - S\n",
        "    hard_fail = any(lbl in CORE for _,lbl in unsupported)\n",
        "    score = sum(3 if lbl in CORE else 1 for _,lbl in overlap)\n",
        "    return score, hard_fail\n",
        "\n",
        "def unsupported_nonc_count(src, hyp):\n",
        "    S = _ents_norm(src); H = _ents_norm(hyp)\n",
        "    return sum(1 for _,lbl in (H-S) if lbl in NONCORE)\n",
        "\n",
        "def lcs_span_stats(src_text, hyp_text):\n",
        "    s = str(src_text).split(); h = str(hyp_text).split()\n",
        "    n,m = len(s), len(h)\n",
        "    dp = [[0]*(m+1) for _ in range(n+1)]\n",
        "    for i in range(n-1,-1,-1):\n",
        "        si=s[i]; row=dp[i]; row1=dp[i+1]\n",
        "        for j in range(m-1,-1,-1):\n",
        "            row[j] = row1[j+1]+1 if si==h[j] else max(row[j+1], row1[j])\n",
        "    i=j=0; blocks=[]; cur=0\n",
        "    while i<n and j<m:\n",
        "        if s[i]==h[j]: cur+=1; i+=1; j+=1\n",
        "        else:\n",
        "            if cur>=2: blocks.append(cur); cur=0\n",
        "            if dp[i+1][j]>=dp[i][j+1]: i+=1\n",
        "            else: j+=1\n",
        "    if cur>=2: blocks.append(cur)\n",
        "    cont = sum(b-1 for b in blocks)\n",
        "    av = (sum(blocks)/len(blocks)) if blocks else 1.0\n",
        "    return cont, av\n",
        "\n",
        "# ==== ROUGE ====\n",
        "_SC = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\",\"rougeLsum\"], use_stemmer=True)\n",
        "def rouge_means(refs, hyps):\n",
        "    r1=r2=rl=rls=0.0\n",
        "    for r,h in zip(refs, hyps):\n",
        "        s=_SC.score(str(r), str(h))\n",
        "        r1 += s[\"rouge1\"].fmeasure\n",
        "        r2 += s[\"rouge2\"].fmeasure\n",
        "        rl += s[\"rougeL\"].fmeasure       # classic RL (not sum)\n",
        "        rls+= s[\"rougeLsum\"].fmeasure    # RLsum (what you used before)\n",
        "    n=len(refs); return dict(r1_f=r1/n, r2_f=r2/n, rl_f=rl/n, rlsum_f=rls/n)\n",
        "\n",
        "def entPRF(refs, hyps):\n",
        "    TP=FP=FN=0\n",
        "    for r,h in zip(refs, hyps):\n",
        "        R=_ents_norm(r); H=_ents_norm(h)\n",
        "        TP += len(H & R)\n",
        "        FP += len(H - R)\n",
        "        FN += len(R - H)\n",
        "    P = TP/(TP+FP) if (TP+FP) else 0.0\n",
        "    Rc= TP/(TP+FN) if (TP+FN) else 0.0\n",
        "    F = 2*P*Rc/(P+Rc) if (P+Rc) else 0.0\n",
        "    return {\"TP\":TP,\"FP\":FP,\"FN\":FN,\"entP\":P,\"entR\":Rc,\"entF1\":F}\n",
        "\n",
        "def UCER(articles, hyps):\n",
        "    rates=[]; counts=[]\n",
        "    for s,h in zip(articles, hyps):\n",
        "        S=_ents_norm(s); H=_ents_norm(h)\n",
        "        bad=[(t,l) for (t,l) in (H-S) if l in CORE]\n",
        "        rates.append(1 if bad else 0); counts.append(len(bad))\n",
        "    return {\"rate\": float(np.mean(rates)), \"avg_core\": float(np.mean(counts))}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uy4JDF8OfZVD"
      },
      "outputs": [],
      "source": [
        "# ==== EA processor (Hybrid only) ====\n",
        "from transformers import LogitsProcessor\n",
        "\n",
        "class EntityAwareSpanProcessor(LogitsProcessor):\n",
        "    def __init__(self, maps_list, num_beams, gamma=0.4, gamma_entity=1.5, max_span=6):\n",
        "        self.maps_list = maps_list\n",
        "        self.num_beams = int(num_beams)\n",
        "        self.gamma = float(gamma); self.gamma_entity=float(gamma_entity); self.max_span=int(max_span)\n",
        "    def __call__(self, input_ids, scores):\n",
        "        Bx = input_ids.size(0)\n",
        "        for b in range(Bx):\n",
        "            sample = b // self.num_beams\n",
        "            maps = self.maps_list[sample]; seq = input_ids[b].tolist()\n",
        "            for k in range(min(self.max_span, len(seq)), 0, -1):\n",
        "                mp = maps[k-1]; key = tuple(seq[-k:])\n",
        "                if key in mp:\n",
        "                    nxt, is_ent = mp[key]\n",
        "                    scores[b, nxt] = scores[b, nxt] + (self.gamma_entity if is_ent else self.gamma)\n",
        "                    break\n",
        "        return scores\n",
        "\n",
        "def build_copy_maps(texts, tokenizer, max_span=6):\n",
        "    maps_all=[]\n",
        "    docs = list(nlp.pipe(texts, batch_size=128))\n",
        "    enc  = tokenizer(texts, max_length=MAX_SRC_LEN, truncation=True, return_offsets_mapping=True)\n",
        "    for i,t in enumerate(texts):\n",
        "        ids = enc[\"input_ids\"][i]; offs = enc[\"offset_mapping\"][i]\n",
        "        ent_spans = [(e.start_char, e.end_char) for e in docs[i].ents if e.label_ in KEEP]\n",
        "        def in_ent(off):\n",
        "            a,b = off\n",
        "            if a is None: return False\n",
        "            for s,e in ent_spans:\n",
        "                if a>=s and b<=e: return True\n",
        "            return False\n",
        "        maps=[{} for _ in range(max_span)]\n",
        "        n=len(ids)\n",
        "        for k in range(1, max_span+1):\n",
        "            mp=maps[k-1]\n",
        "            for j in range(0, n-k):\n",
        "                key=tuple(ids[j:j+k]); nxt=ids[j+k]\n",
        "                is_ent = in_ent(offs[j+k]) if j+k<len(offs) else False\n",
        "                if key not in mp: mp[key]=(nxt, is_ent)\n",
        "        maps_all.append(maps)\n",
        "    return maps_all\n",
        "\n",
        "# ==== decode configs ====\n",
        "POOL_LONG = dict(num_beams=NUM_BEAMS, num_return_sequences=NUM_BEAMS,\n",
        "                 num_beam_groups=NUM_GROUPS, diversity_penalty=DIVERSITY,\n",
        "                 max_new_tokens=MAX_NEW_LONG, min_new_tokens=MIN_NEW_LONG,\n",
        "                 no_repeat_ngram_size=NO_REPEAT_NGRAM, length_penalty=LENGTH_PEN_LONG,\n",
        "                 early_stopping=True, return_dict_in_generate=True, output_scores=True)\n",
        "\n",
        "POOL_MED  = dict(num_beams=NUM_BEAMS, num_return_sequences=NUM_BEAMS,\n",
        "                 num_beam_groups=NUM_GROUPS, diversity_penalty=DIVERSITY,\n",
        "                 max_new_tokens=MAX_NEW_MED, min_new_tokens=MIN_NEW_MED,\n",
        "                 no_repeat_ngram_size=NO_REPEAT_NGRAM, length_penalty=LENGTH_PEN_MED,\n",
        "                 early_stopping=True, return_dict_in_generate=True, output_scores=True)\n",
        "\n",
        "def pretokenize_articles(articles):\n",
        "    enc = tok(articles, max_length=MAX_SRC_LEN, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "    return {k: v.to(DEVICE) for k,v in enc.items()}\n",
        "\n",
        "@torch.inference_mode()\n",
        "def run_pool(enc_all, start, end, decode_args, pool_name, maps_list=None):\n",
        "    enc = {k: v[start:end] for k,v in enc_all.items()}\n",
        "    if maps_list is None:\n",
        "        lproc = None\n",
        "    else:\n",
        "        lproc = LogitsProcessorList([EntityAwareSpanProcessor(maps_list[start:end],\n",
        "                                                             num_beams=decode_args[\"num_beams\"],\n",
        "                                                             gamma=EA_GAMMA,\n",
        "                                                             gamma_entity=EA_GAMMA_ENTITY,\n",
        "                                                             max_span=EA_MAX_SPAN)])\n",
        "    out = base.generate(**enc, **decode_args, logits_processor=lproc)\n",
        "    bs = end-start; K = decode_args[\"num_return_sequences\"]\n",
        "    seqs = out.sequences.view(bs, K, -1)\n",
        "    scores = out.sequences_scores.view(bs, K)\n",
        "    rows=[]\n",
        "    for bi in range(bs):\n",
        "        art = df_200.loc[start+bi, \"article\"]; ref = df_200.loc[start+bi, \"highlights\"]; rid = int(df_200.loc[start+bi, \"row_id\"])\n",
        "        for k in range(K):\n",
        "            ids = seqs[bi,k]\n",
        "            hyp = tok.decode(ids, skip_special_tokens=True)\n",
        "            rows.append({\n",
        "                \"row_id\": rid, \"pool\": pool_name, \"candidate_k\": k,\n",
        "                \"article\": art, \"reference\": ref, \"summary\": hyp,\n",
        "                \"len_tok\": max(int(ids.size(0)-1),1), \"base_score\": float(scores[bi,k].item())\n",
        "            })\n",
        "    return rows\n",
        "\n",
        "def decode_two_pools(EA: bool, out_tag: str):\n",
        "    arts = df_200[\"article\"].astype(str).tolist()\n",
        "    enc_all = pretokenize_articles(arts)\n",
        "    rows=[]\n",
        "    if EA:\n",
        "        maps_all = build_copy_maps(arts, tok, max_span=EA_MAX_SPAN)\n",
        "    else:\n",
        "        maps_all = None\n",
        "    # LONG\n",
        "    for s in range(0, len(arts), BATCH_LONG):\n",
        "        e=min(len(arts), s+BATCH_LONG)\n",
        "        rows += run_pool(enc_all, s, e, POOL_LONG, \"CN2_LONG\", maps_all)\n",
        "    # MED\n",
        "    for s in range(0, len(arts), BATCH_MED):\n",
        "        e=min(len(arts), s+BATCH_MED)\n",
        "        rows += run_pool(enc_all, s, e, POOL_MED, \"CN_MED\", maps_all)\n",
        "\n",
        "    dd = pd.DataFrame(rows)\n",
        "    path = os.path.join(OUT_ROOT, f\"details_{out_tag}.csv\")\n",
        "    dd.to_csv(path, index=False)\n",
        "    print(f\"[decode] Saved details -> {path} | rows={len(dd)}\")\n",
        "    return dd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuZBkVKZf9I5"
      },
      "outputs": [],
      "source": [
        "# ==== Stage-A select (same for both systems) ====\n",
        "def stageA_select(dd_all: pd.DataFrame, M=10):\n",
        "    outs=[]\n",
        "    for rid, grp in dd_all.groupby(\"row_id\"):\n",
        "        art = grp.iloc[0][\"article\"]; tgt_len = None  # we'll compute len_reward from ref directly below\n",
        "        ent_scores=[]; fails=[]; contigs=[]; nonc=[]\n",
        "        for _,r in grp.iterrows():\n",
        "            s,f = strict_entity_score(r[\"article\"], r[\"summary\"])\n",
        "            c,_ = lcs_span_stats(r[\"article\"], r[\"summary\"])\n",
        "            u   = unsupported_nonc_count(r[\"article\"], r[\"summary\"])\n",
        "            ent_scores.append(s); fails.append(f); contigs.append(c); nonc.append(u)\n",
        "        ent_scores = np.array(ent_scores); contigs=np.array(contigs); nonc=np.array(nonc)\n",
        "        # Length reward from reference tokens (per your latest pipeline)\n",
        "        ref_len_tok = len(tok(grp.iloc[0][\"reference\"])[\"input_ids\"])\n",
        "        L = grp[\"len_tok\"].to_numpy().clip(min=1)\n",
        "        LEN_SIGMA = 20.0\n",
        "        len_reward = np.exp(-((L - ref_len_tok)**2)/(2*(LEN_SIGMA**2)))\n",
        "        penalty = (nonc>0).astype(float) * NONCORE_PEN\n",
        "        THC = (5.0*ent_scores) + (0.2*contigs) + (0.3*len_reward) - penalty\n",
        "        ok = grp.copy()\n",
        "        ok[\"THC\"] = THC\n",
        "        ok[\"hard_fail\"] = np.array(fails, dtype=bool)\n",
        "        ok = ok.loc[~ok[\"hard_fail\"]].copy()\n",
        "        if ok.empty:  # fallback if all failed\n",
        "            ok = grp.copy(); ok[\"THC\"]=THC\n",
        "        outs.append(ok.sort_values(\"THC\", ascending=False).head(M))\n",
        "    return pd.concat(outs, ignore_index=True)\n",
        "\n",
        "# Survivor-only base_norm (token-average log-prob with len^0.3 normalization)\n",
        "def add_base_norm(stageA_df: pd.DataFrame, batch_size=16):\n",
        "    out = np.zeros(len(stageA_df), dtype=np.float32)\n",
        "    idxs = stageA_df.index.to_list()\n",
        "    for s in range(0, len(idxs), batch_size):\n",
        "        ii = idxs[s:s+batch_size]\n",
        "        sub = stageA_df.loc[ii]\n",
        "        enc = tok(sub[\"article\"].tolist(), max_length=MAX_SRC_LEN, truncation=True, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
        "        dec = tok(sub[\"summary\"].tolist(), max_length=256, truncation=True, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
        "        with torch.inference_mode():\n",
        "            logits = base(**enc, labels=dec[\"input_ids\"], return_dict=True).logits.float()\n",
        "        shift_logits = logits[:, :-1, :]\n",
        "        shift_labels = dec[\"input_ids\"][:, 1:]\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=tok.pad_token_id, reduction=\"none\")\n",
        "        tok_loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.reshape(-1))\n",
        "        tok_loss = tok_loss.view(shift_labels.size(0), -1)\n",
        "        valid = (shift_labels != tok.pad_token_id).float()\n",
        "        loss_per = (tok_loss * valid).sum(dim=1) / (valid.sum(dim=1).clamp_min(1.0))\n",
        "        logp_per = -loss_per.detach().cpu().numpy()\n",
        "        L = sub[\"len_tok\"].to_numpy().clip(min=1)\n",
        "        out[s:s+len(ii)] = logp_per / (L ** 0.3)\n",
        "    stageA_df[\"base_norm\"] = out\n",
        "    return stageA_df\n",
        "\n",
        "def minmax(v):\n",
        "    v=np.array(v, float); lo,hi=v.min(),v.max()\n",
        "    return np.zeros_like(v) if hi-lo<1e-8 else (v-lo)/(hi-lo)\n",
        "\n",
        "def ent_consensus(cands):\n",
        "    E=[_ents_norm(x) for x in cands]\n",
        "    n=len(E); sc=np.zeros(n)\n",
        "    for i in range(n):\n",
        "        js=[]\n",
        "        for j in range(n):\n",
        "            if i==j: continue\n",
        "            u=E[i]|E[j]; inter=E[i]&E[j]\n",
        "            js.append(len(inter)/max(len(u),1) if u else 0.0)\n",
        "        sc[i]=np.mean(js) if js else 0.0\n",
        "    return sc\n",
        "\n",
        "def build_features(stageA_df: pd.DataFrame, with_labels=True):\n",
        "    feat_rows=[]\n",
        "    for rid, grp in stageA_df.groupby(\"row_id\"):\n",
        "        art = grp.iloc[0][\"article\"]; ref = grp.iloc[0][\"reference\"]\n",
        "        cands = grp[\"summary\"].tolist()\n",
        "        # Lead similarity (ROUGE-Lsum vs lead-3 of source)\n",
        "        def sent_split(txt): return re.split(r'(?<=[.!?])\\s+', str(txt).strip())\n",
        "        lead = \" \".join([t for t in sent_split(art) if t][:3])\n",
        "        def rougeLsum(a,b): return rouge_scorer.RougeScorer([\"rougeLsum\"], use_stemmer=True).score(str(a), str(b))[\"rougeLsum\"].fmeasure\n",
        "        leadsim = np.array([rougeLsum(h, lead) for h in cands])\n",
        "        # centroid MBR across candidates\n",
        "        n=len(cands); mbr=np.zeros(n)\n",
        "        if n>1:\n",
        "            M=np.zeros((n,n))\n",
        "            for i in range(n):\n",
        "                for j in range(i+1,n):\n",
        "                    s=rougeLsum(cands[i], cands[j]); M[i,j]=M[j,i]=s\n",
        "            mbr=M.mean(axis=1)\n",
        "        feats = pd.DataFrame({\n",
        "            \"row_id\": grp[\"row_id\"].values,\n",
        "            \"summary\": grp[\"summary\"].values,\n",
        "            \"THC_norm\": minmax(grp[\"THC\"].values),\n",
        "            \"LeadSim\": leadsim,\n",
        "            \"MBR\": mbr,\n",
        "            \"len_reward\": minmax([math.exp(-((L - len(tok(ref))[\"input_ids\"])**2)/(2*(20.0**2))) for L in grp[\"len_tok\"].tolist()]),\n",
        "            \"contig_lcs_mm\": minmax([lcs_span_stats(art, s)[0] for s in cands]),\n",
        "            \"base_norm_mm\": minmax(grp[\"base_norm\"].values),\n",
        "            \"ent_consensus\": ent_consensus(cands),\n",
        "            \"unsup_nonc_mm\": minmax(np.zeros(len(cands)))  # optional if you carry unsup_nonc into stageA_df, else zeros\n",
        "        })\n",
        "        if with_labels:\n",
        "            RLsum = [rouge_scorer.RougeScorer([\"rougeLsum\"], use_stemmer=True).score(str(ref), str(h))[\"rougeLsum\"].fmeasure for h in cands]\n",
        "            feats[\"rl_ref\"] = RLsum\n",
        "        feat_rows.append(feats)\n",
        "    return pd.concat(feat_rows, ignore_index=True)\n",
        "\n",
        "def train_lr_pairwise(F: pd.DataFrame):\n",
        "    cols = [\"THC_norm\",\"LeadSim\",\"MBR\",\"len_reward\",\"contig_lcs_mm\",\"base_norm_mm\",\"ent_consensus\",\"unsup_nonc_mm\"]\n",
        "    X=[]; y=[]\n",
        "    for rid, g in F.groupby(\"row_id\"):\n",
        "        G=g.reset_index(drop=True); m=len(G)\n",
        "        for i in range(m):\n",
        "            for j in range(i+1,m):\n",
        "                if G.loc[i,\"rl_ref\"] == G.loc[j,\"rl_ref\"]: continue\n",
        "                X.append((G.loc[i, cols].values - G.loc[j, cols].values).astype(float))\n",
        "                y.append(1 if G.loc[i,\"rl_ref\"]>G.loc[j,\"rl_ref\"] else 0)\n",
        "    X=np.stack(X); y=np.array(y)\n",
        "    clf = LogisticRegression(solver=\"liblinear\", max_iter=1000, random_state=SEED)\n",
        "    clf.fit(X,y)\n",
        "    weights = {\"intercept\": float(clf.intercept_[0]), \"coefs\": {c: float(w) for c,w in zip(cols, clf.coef_[0].tolist())}}\n",
        "    return weights, cols\n",
        "\n",
        "def pick_with_weights(grp, w):\n",
        "    cols=list(w[\"coefs\"].keys())\n",
        "    art = grp.iloc[0][\"article\"]; ref = grp.iloc[0][\"reference\"]\n",
        "    def sent_split(txt): return re.split(r'(?<=[.!?])\\s+', str(txt).strip())\n",
        "    lead = \" \".join([t for t in sent_split(art) if t][:3])\n",
        "    def rougeLsum(a,b): return rouge_scorer.RougeScorer([\"rougeLsum\"], use_stemmer=True).score(str(a), str(b))[\"rougeLsum\"].fmeasure\n",
        "    cands = grp[\"summary\"].tolist()\n",
        "    # recompute features in the same way as build_features (without labels)\n",
        "    n=len(cands); mbr=np.zeros(n)\n",
        "    if n>1:\n",
        "        M=np.zeros((n,n))\n",
        "        for i in range(n):\n",
        "            for j in range(i+1,n):\n",
        "                s=rougeLsum(cands[i], cands[j]); M[i,j]=M[j,i]=s\n",
        "        mbr=M.mean(axis=1)\n",
        "    feats = pd.DataFrame({\n",
        "        \"THC_norm\": minmax(grp[\"THC\"].values),\n",
        "        \"LeadSim\": np.array([rougeLsum(h, lead) for h in cands]),\n",
        "        \"MBR\": mbr,\n",
        "        \"len_reward\": minmax([math.exp(-((L - len(tok(ref))[\"input_ids\"])**2)/(2*(20.0**2))) for L in grp[\"len_tok\"].tolist()]),\n",
        "        \"contig_lcs_mm\": minmax([lcs_span_stats(art, s)[0] for s in cands]),\n",
        "        \"base_norm_mm\": minmax(grp[\"base_norm\"].values),\n",
        "        \"ent_consensus\": ent_consensus(cands),\n",
        "        \"unsup_nonc_mm\": minmax(np.zeros(len(cands))),\n",
        "    })\n",
        "    scores = w[\"intercept\"] + sum(feats[c].values * w[\"coefs\"][c] for c in cols)\n",
        "    k = int(np.argmax(scores))\n",
        "    # UCER-safe fallback (no unsupported CORE entities)\n",
        "    S=_ents_norm(art); H=_ents_norm(grp.iloc[k][\"summary\"])\n",
        "    if any(lbl in CORE for _,lbl in (H-S)):\n",
        "        safe_idx=[]\n",
        "        for i, ssum in enumerate(cands):\n",
        "            Hi=_ents_norm(ssum)\n",
        "            if not any(lbl in CORE for _,lbl in (Hi-S)):\n",
        "                safe_idx.append(i)\n",
        "        if safe_idx:\n",
        "            sv = np.array(scores)\n",
        "            k = int(max(safe_idx, key=lambda i: sv[i]))\n",
        "    return grp.iloc[[k]].copy()\n",
        "\n",
        "def eval_system(dd_all: pd.DataFrame, tag: str):\n",
        "    stageA = stageA_select(dd_all, M=10)\n",
        "    stageA = add_base_norm(stageA)\n",
        "    F = build_features(stageA, with_labels=True)\n",
        "    W, cols = train_lr_pairwise(F)\n",
        "    # pick best per row with W\n",
        "    best=[]\n",
        "    for rid, grp in stageA.groupby(\"row_id\"):\n",
        "        best.append(pick_with_weights(grp, W))\n",
        "    best = pd.concat(best, ignore_index=True)\n",
        "    # metrics\n",
        "    refs = best[\"reference\"].astype(str).tolist()\n",
        "    hyps = best[\"summary\"].astype(str).tolist()\n",
        "    arts = best[\"article\"].astype(str).tolist()\n",
        "    metrics = {\n",
        "        \"rouge\": {k: round(v,4) for k,v in rouge_means(refs, hyps).items()},\n",
        "        \"ent\": {k: (round(v,4) if isinstance(v,float) else v) for k,v in entPRF(refs, hyps).items()},\n",
        "        \"UCER\": {k: round(v,4) for k,v in UCER(arts, hyps).items()}\n",
        "    }\n",
        "    out_dir = os.path.join(OUT_ROOT, tag)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    best_path = os.path.join(out_dir, f\"best_{tag}.csv\")\n",
        "    best.to_csv(best_path, index=False)\n",
        "    with open(os.path.join(out_dir, f\"metrics_{tag}.json\"), \"w\") as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "    with open(os.path.join(out_dir, f\"lr_weights_{tag}.json\"), \"w\") as f:\n",
        "        json.dump(W, f, indent=2)\n",
        "    print(f\"[{tag}] ROUGE:\", metrics[\"rouge\"], \"| ENT:\", metrics[\"ent\"], \"| UCER:\", metrics[\"UCER\"])\n",
        "    print(\"Saved:\", best_path)\n",
        "    return metrics, best_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oew7uxJZf_mq"
      },
      "outputs": [],
      "source": [
        "# ==== HYBRID (EA ON) ====\n",
        "dd_hybrid = decode_two_pools(EA=True, out_tag=\"HYBRID_EA_ON\")\n",
        "m_h, path_h = eval_system(dd_hybrid, tag=\"HybridEA\")\n",
        "\n",
        "# ==== BASELINE (EA OFF) ====\n",
        "dd_base = decode_two_pools(EA=False, out_tag=\"BASELINE_EA_OFF\")\n",
        "m_b, path_b = eval_system(dd_base, tag=\"BaselineNoEA\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jAfr9uPdgC1p",
        "outputId": "33624ced-244b-435e-b886-5c2c24b7441b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== RESULTS (from saved details) ===\n",
            "ROUGE : {'r1_f': 0.4096, 'r2_f': 0.1803, 'rlsum_f': 0.3821, 'rl_f': 0.2726}\n",
            "ENT   : {'TP': 470, 'FP': 844, 'FN': 697, 'entP': 0.3577, 'entR': 0.4027, 'entF1': 0.3789}\n",
            "UCER  : {'rate': 0.01, 'avg_core': 0.01}\n",
            "Saved: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/Hybrid_from_saved_details/best_from_saved_20250903_003232.csv | /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/Hybrid_from_saved_details/lr_weights_from_saved_20250903_003232.json\n",
            "\n",
            "=== LENGTH STATS (words) ===\n",
            "{'reference': {'count': 200, 'mean': np.float64(52.77), 'std': 21.095, 'min': 14, 'q25': 39.0, 'med': 49.0, 'q75': 59.25, 'max': 140}, 'summary': {'count': 200, 'mean': np.float64(54.07), 'std': 10.205, 'min': 33, 'q25': 48.0, 'med': 53.0, 'q75': 59.0, 'max': 85}}\n",
            "\n",
            "=== LENGTH STATS (tokens) ===\n",
            "{'reference': {'count': 200, 'mean': np.float64(65.13), 'std': 24.291, 'min': 19, 'q25': 48.0, 'med': 61.0, 'q75': 76.25, 'max': 160}, 'summary': {'count': 200, 'mean': np.float64(70.295), 'std': 12.289, 'min': 44, 'q25': 63.0, 'med': 70.0, 'q75': 74.25, 'max': 100}}\n"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# HYBRID EVAL FROM SAVED DETAILS\n",
        "# (no regeneration)\n",
        "# ================================\n",
        "import os, json, math, csv, re, time, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import spacy\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "DETAILS_CSV = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/Hybrid_vs_Baseline_200/details_HYBRID_EA_ON.csv\"\n",
        "CKPT_DIR    = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500\"\n",
        "OUT_DIR     = os.path.join(CKPT_DIR, \"Hybrid_from_saved_details\")\n",
        "\n",
        "M_KEEP = 10\n",
        "NONCORE_PEN = 2.0\n",
        "LEN_SIGMA = 20.0\n",
        "MAX_SRC_LEN = 400\n",
        "USE_REF_LEN_FOR_TARGET = False   # <- set True to center len reward on reference token length\n",
        "\n",
        "SEED = 0\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ---------- LIBS ----------\n",
        "from rouge_score import rouge_scorer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# ---------- TOOLS ----------\n",
        "def ensure_spacy():\n",
        "    try:\n",
        "        return spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "    except OSError:\n",
        "        import sys, subprocess\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "        return spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "\n",
        "nlp = ensure_spacy()\n",
        "SC = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\",\"rougeL\"], use_stemmer=True)\n",
        "\n",
        "def sent_split(txt):\n",
        "    ss = re.split(r'(?<=[.!?])\\s+', str(txt).strip())\n",
        "    return [t for t in ss if t.strip()]\n",
        "\n",
        "def rougeLsum(a,b):\n",
        "    ra = \"\\n\".join(sent_split(a)); rb = \"\\n\".join(sent_split(b))\n",
        "    return SC.score(ra, rb)[\"rougeLsum\"].fmeasure\n",
        "\n",
        "def lcs_span_stats(src_text, hyp_text):\n",
        "    s=str(src_text).split(); h=str(hyp_text).split()\n",
        "    n,m=len(s),len(h); dp=[[0]*(m+1) for _ in range(n+1)]\n",
        "    for i in range(n-1,-1,-1):\n",
        "        si=s[i]; row=dp[i]; row1=dp[i+1]\n",
        "        for j in range(m-1,-1,-1):\n",
        "            row[j]=row1[j+1]+1 if si==h[j] else max(row[j+1], row1[j])\n",
        "    i=j=0; blocks=[]; cur=0\n",
        "    while i<n and j<m:\n",
        "        if s[i]==h[j]: cur+=1; i+=1; j+=1\n",
        "        else:\n",
        "            if cur>=2: blocks.append(cur); cur=0\n",
        "            if dp[i+1][j]>=dp[i][j+1]: i+=1\n",
        "            else: j+=1\n",
        "    if cur>=2: blocks.append(cur)\n",
        "    cont=sum(b-1 for b in blocks)\n",
        "    av=(sum(blocks)/len(blocks)) if blocks else 1.0\n",
        "    return cont, av\n",
        "\n",
        "KEEP   = {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"DATE\",\"TIME\",\"CARDINAL\",\"MONEY\",\"PERCENT\",\"QUANTITY\",\"ORDINAL\"}\n",
        "CORE   = {\"PERSON\",\"ORG\",\"GPE\"}\n",
        "NONCORE= {\"DATE\",\"QUANTITY\",\"MONEY\",\"PERCENT\",\"CARDINAL\",\"ORDINAL\"}\n",
        "\n",
        "def _ents_norm(text: str):\n",
        "    doc = nlp(str(text))\n",
        "    return {(\" \".join(e.text.strip().split()), e.label_) for e in doc.ents if e.label_ in KEEP}\n",
        "\n",
        "def strict_entity_score(src, hyp):\n",
        "    S=_ents_norm(src); H=_ents_norm(hyp)\n",
        "    overlap=H&S; unsupported=H-S\n",
        "    hard_fail=any(lbl in CORE for _,lbl in unsupported)\n",
        "    score=sum(3 if lbl in CORE else 1 for _,lbl in overlap)\n",
        "    return score, hard_fail\n",
        "\n",
        "def unsupported_nonc_count(src, hyp):\n",
        "    S=_ents_norm(src); H=_ents_norm(hyp)\n",
        "    return sum(1 for _,lbl in (H-S) if lbl in NONCORE)\n",
        "\n",
        "def minmax(v):\n",
        "    v=np.array(v, float)\n",
        "    lo,hi=float(v.min()), float(v.max())\n",
        "    return np.zeros_like(v) if hi-lo<1e-8 else (v-lo)/(hi-lo)\n",
        "\n",
        "# ---------- LOAD CKPT (for base_norm) ----------\n",
        "tok   = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR).to(DEVICE).eval()\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cuda.matmul.allow_tf32=True\n",
        "    try: torch.set_float32_matmul_precision(\"high\")\n",
        "    except: pass\n",
        "\n",
        "def add_base_norm(stageA_df, batch_size=16):\n",
        "    out = np.zeros(len(stageA_df), dtype=np.float32)\n",
        "    idxs = stageA_df.index.to_list()\n",
        "    for s in range(0, len(idxs), batch_size):\n",
        "        ii = idxs[s:s+batch_size]\n",
        "        sub = stageA_df.loc[ii]\n",
        "        enc = tok(sub[\"article\"].tolist(), max_length=MAX_SRC_LEN, truncation=True, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
        "        dec = tok(sub[\"summary\"].tolist(), max_length=256, truncation=True, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
        "        with torch.inference_mode(), torch.amp.autocast(DEVICE if DEVICE==\"cuda\" else \"cpu\"):\n",
        "            logits = model(**enc, labels=dec[\"input_ids\"]).logits.float()\n",
        "        shift_logits = logits[:, :-1, :]\n",
        "        shift_labels = dec[\"input_ids\"][:, 1:]\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=tok.pad_token_id, reduction=\"none\")\n",
        "        tok_loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.reshape(-1))\n",
        "        tok_loss = tok_loss.view(shift_labels.size(0), -1)\n",
        "        valid = (shift_labels != tok.pad_token_id).float()\n",
        "        loss_per = (tok_loss * valid).sum(dim=1) / (valid.sum(dim=1).clamp_min(1.0))\n",
        "        logp_per = -loss_per.detach().cpu().numpy()\n",
        "        L = sub[\"len_tok\"].to_numpy().clip(min=1) if \"len_tok\" in sub.columns else sub[\"len\"].to_numpy().clip(min=1)\n",
        "        out[s:s+len(ii)] = logp_per / (L ** 0.3)\n",
        "    stageA_df[\"base_norm\"] = out\n",
        "    return stageA_df\n",
        "\n",
        "# ---------- LOAD DETAILS ----------\n",
        "dd = pd.read_csv(DETAILS_CSV)\n",
        "assert {\"row_id\",\"article\",\"summary\"}.issubset(dd.columns), \"details csv missing required columns\"\n",
        "\n",
        "# normalize ref col\n",
        "if \"reference\" not in dd.columns:\n",
        "    if \"highlights\" in dd.columns: dd[\"reference\"] = dd[\"highlights\"].astype(str)\n",
        "    else: dd[\"reference\"] = \"\"\n",
        "\n",
        "# token lengths\n",
        "def toklen(s): return len(tok(str(s))[\"input_ids\"])\n",
        "if \"len_tok\" not in dd.columns:\n",
        "    dd[\"len_tok\"] = dd[\"summary\"].astype(str).apply(toklen)\n",
        "\n",
        "# --------- compute features needed for Stage-A if missing ---------\n",
        "need_feats = not {\"entity_score\",\"fail_flag\",\"contiguity_lcs\",\"avg_span_len_lcs\"}.issubset(dd.columns)\n",
        "if need_feats:\n",
        "    es, ff, cont, av = [], [], [], []\n",
        "    nonc = []\n",
        "    for _,r in dd.iterrows():\n",
        "        s,f = strict_entity_score(r[\"article\"], r[\"summary\"])\n",
        "        c,a = lcs_span_stats(r[\"article\"], r[\"summary\"])\n",
        "        es.append(s); ff.append(f); cont.append(c); av.append(a)\n",
        "        nonc.append(unsupported_nonc_count(r[\"article\"], r[\"summary\"]))\n",
        "    dd[\"entity_score\"]=es; dd[\"fail_flag\"]=ff\n",
        "    dd[\"contiguity_lcs\"]=cont; dd[\"avg_span_len_lcs\"]=av\n",
        "    dd[\"unsup_nonc\"] = nonc\n",
        "if \"unsup_nonc\" not in dd.columns:\n",
        "    dd[\"unsup_nonc\"] = [unsupported_nonc_count(a, h) for a,h in zip(dd[\"article\"], dd[\"summary\"])]\n",
        "\n",
        "# --------- Stage-A (TH-C + noncore penalty) ---------\n",
        "def adaptive_len_target(src):\n",
        "    lead = \" \".join(sent_split(src)[:3])\n",
        "    return int(np.clip(round(0.9 * toklen(lead)), 32, 60))\n",
        "\n",
        "rows=[]\n",
        "for rid, grp in dd.groupby(\"row_id\"):\n",
        "    art = grp.iloc[0][\"article\"]\n",
        "    tgt = (toklen(grp.iloc[0][\"reference\"]) if USE_REF_LEN_FOR_TARGET else adaptive_len_target(art))\n",
        "    L   = grp[\"len_tok\"].astype(int).to_numpy()\n",
        "    ent = grp[\"entity_score\"].to_numpy(dtype=float)\n",
        "    cont= grp[\"contiguity_lcs\"].to_numpy(dtype=float)\n",
        "    nonc= grp[\"unsup_nonc\"].to_numpy(dtype=float)\n",
        "    len_reward = np.exp(-((L - tgt)**2) / (2*(LEN_SIGMA**2)))\n",
        "    penalty = (nonc>0).astype(float) * NONCORE_PEN\n",
        "    THC = (5.0*ent) + (0.2*cont) + (0.3*len_reward) - penalty\n",
        "    ok = grp.copy()\n",
        "    ok[\"THC\"] = THC\n",
        "\n",
        "    if \"fail_flag\" in ok.columns:\n",
        "        ok = ok.loc[~ok[\"fail_flag\"]].copy()\n",
        "        if ok.empty:          # fallback if all hard-failed\n",
        "            ok = grp.copy()\n",
        "            ok[\"THC\"] = THC\n",
        "\n",
        "    rows.append(ok.sort_values(\"THC\", ascending=False).head(M_KEEP))\n",
        "stageA = pd.concat(rows, ignore_index=True)\n",
        "\n",
        "# base_norm on survivors\n",
        "stageA = add_base_norm(stageA)\n",
        "\n",
        "# --------- Build features + pairwise LR ---------\n",
        "def ent_consensus(cands):\n",
        "    E=[_ents_norm(x) for x in cands]\n",
        "    n=len(E); sc=np.zeros(n)\n",
        "    for i in range(n):\n",
        "        js=[]\n",
        "        for j in range(n):\n",
        "            if i==j: continue\n",
        "            u=E[i]|E[j]; inter=E[i]&E[j]\n",
        "            js.append(len(inter)/max(len(u),1) if u else 0.0)\n",
        "        sc[i]=np.mean(js) if js else 0.0\n",
        "    return sc\n",
        "\n",
        "feat_rows=[]\n",
        "for rid, grp in stageA.groupby(\"row_id\"):\n",
        "    art = grp.iloc[0][\"article\"]; ref = grp.iloc[0][\"reference\"]\n",
        "    tgt = (toklen(ref) if USE_REF_LEN_FOR_TARGET else adaptive_len_target(art))\n",
        "    cands = grp[\"summary\"].astype(str).tolist()\n",
        "    Ls = grp[\"len_tok\"].astype(int).tolist()\n",
        "    lead = \" \".join(sent_split(art)[:3])\n",
        "    leadsim = np.array([rougeLsum(h, lead) for h in cands])\n",
        "    # centroid MBR on RLsum\n",
        "    n=len(cands); mbr=np.zeros(n)\n",
        "    if n>1:\n",
        "        M=np.zeros((n,n))\n",
        "        for i in range(n):\n",
        "            for j in range(i+1,n):\n",
        "                s=rougeLsum(cands[i], cands[j]); M[i,j]=M[j,i]=s\n",
        "        mbr=M.mean(axis=1)\n",
        "    feats = pd.DataFrame({\n",
        "        \"row_id\": grp[\"row_id\"].values,\n",
        "        \"summary\": grp[\"summary\"].values,\n",
        "        \"THC_norm\": minmax(grp[\"THC\"].values),\n",
        "        \"LeadSim\": leadsim,\n",
        "        \"MBR\": mbr,\n",
        "        \"len_reward\": minmax([math.exp(-((L - tgt)**2) / (2*(LEN_SIGMA**2))) for L in Ls]),\n",
        "        \"contig_lcs_mm\": minmax([lcs_span_stats(art, s)[0] for s in cands]),\n",
        "        \"base_norm_mm\": minmax(grp[\"base_norm\"].values),\n",
        "        \"ent_consensus\": ent_consensus(cands),\n",
        "        \"unsup_nonc_mm\": minmax(grp[\"unsup_nonc\"].astype(float).values),\n",
        "        \"rl_ref\": [rougeLsum(ref, h) for h in cands],  # label for LR\n",
        "    })\n",
        "    feat_rows.append(feats)\n",
        "F = pd.concat(feat_rows, ignore_index=True)\n",
        "\n",
        "cols = [\"THC_norm\",\"LeadSim\",\"MBR\",\"len_reward\",\"contig_lcs_mm\",\"base_norm_mm\",\"ent_consensus\",\"unsup_nonc_mm\"]\n",
        "pairs_X=[]; pairs_y=[]\n",
        "for rid, g in F.groupby(\"row_id\"):\n",
        "    G=g.reset_index(drop=True); m=len(G)\n",
        "    for i in range(m):\n",
        "        for j in range(i+1,m):\n",
        "            yi, yj = G.loc[i,\"rl_ref\"], G.loc[j,\"rl_ref\"]\n",
        "            if yi==yj: continue\n",
        "            x = (G.loc[i, cols].values.astype(float) - G.loc[j, cols].values.astype(float))\n",
        "            y = 1 if yi>yj else 0\n",
        "            pairs_X.append(x); pairs_y.append(y)\n",
        "pairs_X=np.stack(pairs_X); pairs_y=np.array(pairs_y)\n",
        "clf = LogisticRegression(solver=\"liblinear\", max_iter=1000, random_state=SEED).fit(pairs_X, pairs_y)\n",
        "weights = dict(intercept=float(clf.intercept_[0]), coefs={c: float(w) for c,w in zip(cols, clf.coef_[0].tolist())})\n",
        "\n",
        "def score_group_linear(grp, w):\n",
        "    art = grp.iloc[0][\"article\"]; ref = grp.iloc[0][\"reference\"]\n",
        "    tgt = (toklen(ref) if USE_REF_LEN_FOR_TARGET else adaptive_len_target(art))\n",
        "    cands = grp[\"summary\"].astype(str).tolist()\n",
        "    Ls    = grp[\"len_tok\"].astype(int).tolist()\n",
        "    lead = \" \".join(sent_split(art)[:3])\n",
        "    leadsim = np.array([rougeLsum(h, lead) for h in cands])\n",
        "    n=len(cands); mbr=np.zeros(n)\n",
        "    if n>1:\n",
        "        M=np.zeros((n,n))\n",
        "        for i in range(n):\n",
        "            for j in range(i+1,n):\n",
        "                s=rougeLsum(cands[i], cands[j]); M[i,j]=M[j,i]=s\n",
        "        mbr=M.mean(axis=1)\n",
        "    feats = pd.DataFrame({\n",
        "        \"THC_norm\": minmax(grp[\"THC\"].values),\n",
        "        \"LeadSim\": leadsim,\n",
        "        \"MBR\": mbr,\n",
        "        \"len_reward\": minmax([math.exp(-((L - tgt)**2)/(2*(LEN_SIGMA**2))) for L in Ls]),\n",
        "        \"contig_lcs_mm\": minmax([lcs_span_stats(art, s)[0] for s in cands]),\n",
        "        \"base_norm_mm\": minmax(grp[\"base_norm\"].values),\n",
        "        \"ent_consensus\": ent_consensus(cands),\n",
        "        \"unsup_nonc_mm\": minmax(grp[\"unsup_nonc\"].astype(float).values),\n",
        "    })\n",
        "    cols_order = list(w[\"coefs\"].keys())\n",
        "    s = w[\"intercept\"] + sum(feats[c].values * w[\"coefs\"][c] for c in cols_order)\n",
        "    k=int(np.argmax(s))\n",
        "    cand = grp.iloc[[k]].copy()\n",
        "    # UCER-safe fallback\n",
        "    S=_ents_norm(art); H=_ents_norm(cand[\"summary\"].iloc[0])\n",
        "    if any(lbl in CORE for _,lbl in (H-S)):\n",
        "        safe_idx=[]\n",
        "        for i in range(len(grp)):\n",
        "            Hi=_ents_norm(grp.iloc[i][\"summary\"])\n",
        "            if not any(lbl in CORE for _,lbl in (Hi-S)):\n",
        "                safe_idx.append(i)\n",
        "        if safe_idx:\n",
        "            s_vals=np.array(s)\n",
        "            k=int(max(safe_idx, key=lambda i: s_vals[i]))\n",
        "            cand=grp.iloc[[k]].copy()\n",
        "    return cand\n",
        "\n",
        "best=[]\n",
        "for rid, grp in stageA.groupby(\"row_id\"):\n",
        "    best.append(score_group_linear(grp, weights))\n",
        "best = pd.concat(best, ignore_index=True)\n",
        "\n",
        "# ---------- METRICS ----------\n",
        "def rouge_means(R,H):\n",
        "    r1=r2=rls=rl=0.0\n",
        "    for r,h in zip(R,H):\n",
        "        rr=\"\\n\".join(sent_split(r)); hh=\"\\n\".join(sent_split(h))\n",
        "        s=SC.score(rr, hh)\n",
        "        r1+=s[\"rouge1\"].fmeasure; r2+=s[\"rouge2\"].fmeasure\n",
        "        rls+=s[\"rougeLsum\"].fmeasure; rl+=s[\"rougeL\"].fmeasure\n",
        "    n=len(R); return dict(r1_f=r1/n, r2_f=r2/n, rlsum_f=rls/n, rl_f=rl/n)\n",
        "\n",
        "def entPRF(R,H):\n",
        "    TP=FP=FN=0\n",
        "    for r,h in zip(R,H):\n",
        "        R_=_ents_norm(r); H_=_ents_norm(h)\n",
        "        TP+=len(H_&R_); FP+=len(H_-R_); FN+=len(R_-H_)\n",
        "    P=TP/(TP+FP) if (TP+FP) else 0.0\n",
        "    Rc=TP/(TP+FN) if (TP+FN) else 0.0\n",
        "    F=2*P*Rc/(P+Rc) if (P+Rc) else 0.0\n",
        "    return {\"TP\":TP,\"FP\":FP,\"FN\":FN,\"entP\":P,\"entR\":Rc,\"entF1\":F}\n",
        "\n",
        "def UCER(arts, hyps):\n",
        "    rates=[]; counts=[]\n",
        "    for s,h in zip(arts, hyps):\n",
        "        S=_ents_norm(s); H=_ents_norm(h)\n",
        "        bad=[(t,l) for (t,l) in (H-S) if l in CORE]\n",
        "        rates.append(1 if bad else 0); counts.append(len(bad))\n",
        "    return float(np.mean(rates)), float(np.mean(counts))\n",
        "\n",
        "ev = best[[\"row_id\",\"article\",\"reference\",\"summary\"]].rename(columns={\"summary\":\"hybrid_pick\"}).copy()\n",
        "refs = ev[\"reference\"].astype(str).tolist()\n",
        "hyps = ev[\"hybrid_pick\"].astype(str).tolist()\n",
        "arts = ev[\"article\"].astype(str).tolist()\n",
        "\n",
        "R = rouge_means(refs, hyps)\n",
        "E = entPRF(refs, hyps)\n",
        "U = UCER(arts, hyps)\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "OUT_BEST = os.path.join(OUT_DIR, f\"best_from_saved_{STAMP}.csv\")\n",
        "OUT_W    = os.path.join(OUT_DIR, f\"lr_weights_from_saved_{STAMP}.json\")\n",
        "\n",
        "best.to_csv(OUT_BEST, index=False, quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n",
        "with open(OUT_W, \"w\") as f: json.dump({\"weights\": weights, \"features\": cols, \"use_ref_len\": USE_REF_LEN_FOR_TARGET}, f, indent=2)\n",
        "\n",
        "print(f\"\\n=== RESULTS (from saved details) ===\")\n",
        "print(\"ROUGE :\", {k: round(v,4) for k,v in R.items()})\n",
        "print(\"ENT   :\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in E.items()})\n",
        "print(\"UCER  :\", {\"rate\": round(U[0],4), \"avg_core\": round(U[1],4)})\n",
        "print(\"Saved:\", OUT_BEST, \"|\", OUT_W)\n",
        "\n",
        "# Length stats (quick look)\n",
        "def word_len(s): return len(str(s).split())\n",
        "lens = pd.DataFrame({\n",
        "    \"ref_tok\": [toklen(r) for r in refs],\n",
        "    \"hyp_tok\": [toklen(h) for h in hyps],\n",
        "    \"ref_word\": [word_len(r) for r in refs],\n",
        "    \"hyp_word\": [word_len(h) for h in hyps],\n",
        "})\n",
        "print(\"\\n=== LENGTH STATS (words) ===\")\n",
        "print({\n",
        "    \"reference\": {\n",
        "        \"count\": int(lens.shape[0]),\n",
        "        \"mean\": round(lens[\"ref_word\"].mean(),3),\n",
        "        \"std\": round(lens[\"ref_word\"].std(ddof=0),3),\n",
        "        \"min\": int(lens[\"ref_word\"].min()), \"q25\": float(lens[\"ref_word\"].quantile(0.25)),\n",
        "        \"med\": float(lens[\"ref_word\"].median()), \"q75\": float(lens[\"ref_word\"].quantile(0.75)),\n",
        "        \"max\": int(lens[\"ref_word\"].max())\n",
        "    },\n",
        "    \"summary\": {\n",
        "        \"count\": int(lens.shape[0]),\n",
        "        \"mean\": round(lens[\"hyp_word\"].mean(),3),\n",
        "        \"std\": round(lens[\"hyp_word\"].std(ddof=0),3),\n",
        "        \"min\": int(lens[\"hyp_word\"].min()), \"q25\": float(lens[\"hyp_word\"].quantile(0.25)),\n",
        "        \"med\": float(lens[\"hyp_word\"].median()), \"q75\": float(lens[\"hyp_word\"].quantile(0.75)),\n",
        "        \"max\": int(lens[\"hyp_word\"].max())\n",
        "    }\n",
        "})\n",
        "\n",
        "print(\"\\n=== LENGTH STATS (tokens) ===\")\n",
        "print({\n",
        "    \"reference\": {\n",
        "        \"count\": int(lens.shape[0]),\n",
        "        \"mean\": round(lens[\"ref_tok\"].mean(),3),\n",
        "        \"std\": round(lens[\"ref_tok\"].std(ddof=0),3),\n",
        "        \"min\": int(lens[\"ref_tok\"].min()), \"q25\": float(lens[\"ref_tok\"].quantile(0.25)),\n",
        "        \"med\": float(lens[\"ref_tok\"].median()), \"q75\": float(lens[\"ref_tok\"].quantile(0.75)),\n",
        "        \"max\": int(lens[\"ref_tok\"].max())\n",
        "    },\n",
        "    \"summary\": {\n",
        "        \"count\": int(lens.shape[0]),\n",
        "        \"mean\": round(lens[\"hyp_tok\"].mean(),3),\n",
        "        \"std\": round(lens[\"hyp_tok\"].std(ddof=0),3),\n",
        "        \"min\": int(lens[\"hyp_tok\"].min()), \"q25\": float(lens[\"hyp_tok\"].quantile(0.25)),\n",
        "        \"med\": float(lens[\"hyp_tok\"].median()), \"q75\": float(lens[\"hyp_tok\"].quantile(0.75)),\n",
        "        \"max\": int(lens[\"hyp_tok\"].max())\n",
        "    }\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g16f0DCI8vB0",
        "outputId": "fa754f70-05d5-45d9-b44a-0b539b07df3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows in Hybrid: 200 | first 10: [46, 82, 106, 317, 327, 370, 453, 464, 583, 598]\n",
            "Selected: (200, 3) | first 5 ids: [46, 82, 106, 317, 327]\n",
            "[decode] Saved details -> /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/Hybrid_vs_Baseline_200/details_BASELINE_EA_OFF.csv | rows=4000\n"
          ]
        }
      ],
      "source": [
        "# === BASELINE (EA OFF) — generate details on the SAME 200 rows ===\n",
        "import os, csv, json, time, random, re\n",
        "import numpy as np, pandas as pd, torch\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM\n",
        "\n",
        "CKPT_DIR  = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500\"\n",
        "TEST_CSV  = \"/content/drive/MyDrive/cleaned_outputs/naa/test_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "HYB_PATH  = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/Hybrid_vs_Baseline_200/details_HYBRID_EA_ON.csv\"\n",
        "OUT_DIR   = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/Hybrid_vs_Baseline_200\"\n",
        "BASE_PATH = os.path.join(OUT_DIR, \"details_BASELINE_EA_OFF.csv\")\n",
        "\n",
        "SEED = 0\n",
        "MAX_SRC_LEN = 400\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Pools (IDENTICAL to Hybrid)\n",
        "CN2_LONG = dict(num_beams=10, num_return_sequences=10, num_beam_groups=5, diversity_penalty=0.3,\n",
        "                max_new_tokens=100, min_new_tokens=55, no_repeat_ngram_size=3,\n",
        "                length_penalty=2.0, early_stopping=True, return_dict_in_generate=True, output_scores=False)\n",
        "CN_MED   = dict(num_beams=10, num_return_sequences=10, num_beam_groups=5, diversity_penalty=0.3,\n",
        "                max_new_tokens=70,  min_new_tokens=35, no_repeat_ngram_size=3,\n",
        "                length_penalty=1.6, early_stopping=True, return_dict_in_generate=True, output_scores=False)\n",
        "\n",
        "# 1) Exact 200 row_ids from Hybrid details\n",
        "dd_h = pd.read_csv(HYB_PATH)\n",
        "row_ids_200 = sorted(dd_h[\"row_id\"].unique().tolist())\n",
        "assert len(row_ids_200) == 200, f\"Expected 200 row_ids, got {len(row_ids_200)}\"\n",
        "print(\"Rows in Hybrid:\", len(row_ids_200), \"| first 10:\", row_ids_200[:10])\n",
        "\n",
        "# 2) Subset test CSV to those row_ids\n",
        "def robust_read_csv(path: str) -> pd.DataFrame:\n",
        "    import csv as _csv\n",
        "    try:\n",
        "        df = pd.read_csv(path, engine=\"python\", quoting=_csv.QUOTE_ALL, escapechar=\"\\\\\")\n",
        "        cols = {c.lower(): c for c in df.columns}\n",
        "        assert \"article\" in cols and \"highlights\" in cols\n",
        "        return df[[cols[\"article\"], cols[\"highlights\"]]].rename(columns={cols[\"article\"]: \"article\",\n",
        "                                                                        cols[\"highlights\"]: \"highlights\"})\n",
        "    except Exception:\n",
        "        rows = []\n",
        "        with open(path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "            reader = _csv.reader(f, delimiter=\",\", quotechar='\"', escapechar=\"\\\\\")\n",
        "            header = [h.strip().lower() for h in next(reader)]\n",
        "            i_art, i_sum = header.index(\"article\"), header.index(\"highlights\")\n",
        "            for row in reader: rows.append([row[i_art], row[i_sum]])\n",
        "        return pd.DataFrame(rows, columns=[\"article\",\"highlights\"])\n",
        "\n",
        "df_all = robust_read_csv(TEST_CSV)\n",
        "df_200 = df_all.iloc[row_ids_200].reset_index(drop=False).rename(columns={\"index\":\"row_id\"})\n",
        "print(\"Selected:\", df_200.shape, \"| first 5 ids:\", df_200[\"row_id\"].head().tolist())\n",
        "\n",
        "# 3) Load model/tokenizer (disable cache to avoid cache_utils errors)\n",
        "tok = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
        "cfg = AutoConfig.from_pretrained(CKPT_DIR, attn_implementation=\"eager\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR, config=cfg).to(DEVICE).eval()\n",
        "if hasattr(model.config, \"use_cache\"): model.config.use_cache = False\n",
        "\n",
        "def pretokenize(articles):\n",
        "    enc = tok(articles, max_length=MAX_SRC_LEN, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "    return {k: v.to(DEVICE) for k,v in enc.items()}\n",
        "\n",
        "@torch.inference_mode()\n",
        "def run_pool(enc_all, start, end, decode_args, pool_name):\n",
        "    enc = {k: v[start:end] for k,v in enc_all.items()}\n",
        "    out = model.generate(**enc, **decode_args)  # no logits processor, no trust_remote_code\n",
        "    bs = end-start; K = decode_args[\"num_return_sequences\"]\n",
        "    seqs = out.sequences.view(bs, K, -1)\n",
        "    rows=[]\n",
        "    for bi in range(bs):\n",
        "        art = df_200.loc[start+bi, \"article\"]\n",
        "        ref = df_200.loc[start+bi, \"highlights\"]\n",
        "        rid = int(df_200.loc[start+bi, \"row_id\"])\n",
        "        for k in range(K):\n",
        "            ids = seqs[bi,k]\n",
        "            hyp = tok.decode(ids, skip_special_tokens=True)\n",
        "            rows.append({\n",
        "                \"row_id\": rid, \"pool\": pool_name, \"candidate_k\": k,\n",
        "                \"article\": art, \"reference\": ref, \"summary\": hyp,\n",
        "                \"len_tok\": max(int(ids.size(0)-1),1), \"base_score\": 0.0\n",
        "            })\n",
        "    return rows\n",
        "\n",
        "# 4) Decode both pools\n",
        "articles = df_200[\"article\"].astype(str).tolist()\n",
        "enc_all = pretokenize(articles)\n",
        "rows = []\n",
        "for s in range(0, len(articles), 4):\n",
        "    e = min(len(articles), s+4)\n",
        "    rows += run_pool(enc_all, s, e, CN2_LONG, \"CN2_LONG\")\n",
        "for s in range(0, len(articles), 8):\n",
        "    e = min(len(articles), s+8)\n",
        "    rows += run_pool(enc_all, s, e, CN_MED, \"CN_MED\")\n",
        "\n",
        "dd_base = pd.DataFrame(rows)\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "dd_base.to_csv(BASE_PATH, index=False, quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n",
        "print(f\"[decode] Saved details -> {BASE_PATH} | rows={len(dd_base)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGeI5FsQF3mL",
        "outputId": "e083600a-5601-4983-f57e-c3cbd201eb2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== RESULTS (from saved details) ===\n",
            "ROUGE : {'r1_f': 0.404, 'r2_f': 0.1772, 'rlsum_f': 0.3778, 'rl_f': 0.2665}\n",
            "ENT   : {'TP': 458, 'FP': 827, 'FN': 709, 'entP': 0.3564, 'entR': 0.3925, 'entF1': 0.3736}\n",
            "UCER  : {'rate': 0.02, 'avg_core': 0.02}\n",
            "Saved: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/Baseline_from_saved_details/best_from_saved_20250903_112507.csv  |  /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/Baseline_from_saved_details/lr_weights_from_saved_20250903_112507.json\n",
            "\n",
            "=== LENGTH STATS (words) ===\n",
            "{'reference': {'count': 200, 'mean': np.float64(52.77), 'std': 21.095, 'min': 14, 'q25': 39.0, 'med': 49.0, 'q75': 59.25, 'max': 140}, 'summary': {'count': 200, 'mean': np.float64(52.935), 'std': 10.601, 'min': 27, 'q25': 46.75, 'med': 52.0, 'q75': 58.0, 'max': 87}}\n",
            "\n",
            "=== LENGTH STATS (tokens) ===\n",
            "{'reference': {'count': 200, 'mean': np.float64(65.13), 'std': 24.291, 'min': 19, 'q25': 48.0, 'med': 61.0, 'q75': 76.25, 'max': 160}, 'summary': {'count': 200, 'mean': np.float64(68.74), 'std': 12.494, 'min': 42, 'q25': 60.0, 'med': 70.0, 'q75': 72.0, 'max': 100}}\n"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# BASELINE EVAL FROM SAVED DETAILS\n",
        "# (no regeneration)\n",
        "# ================================\n",
        "import os, json, math, csv, re, time, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import spacy\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "DETAILS_CSV = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/Hybrid_vs_Baseline_200/details_BASELINE_EA_OFF.csv\"\n",
        "CKPT_DIR    = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500\"\n",
        "OUT_DIR     = os.path.join(CKPT_DIR, \"Baseline_from_saved_details\")\n",
        "\n",
        "M_KEEP = 10\n",
        "NONCORE_PEN = 2.0\n",
        "LEN_SIGMA = 20.0\n",
        "MAX_SRC_LEN = 400\n",
        "USE_REF_LEN_FOR_TARGET = False   # keep identical to Hybrid eval\n",
        "\n",
        "SEED = 0\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ---------- LIBS ----------\n",
        "from rouge_score import rouge_scorer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# ---------- TOOLS ----------\n",
        "def ensure_spacy():\n",
        "    try:\n",
        "        return spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "    except OSError:\n",
        "        import sys, subprocess\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "        return spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "\n",
        "nlp = ensure_spacy()\n",
        "SC = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\",\"rougeL\"], use_stemmer=True)\n",
        "\n",
        "def sent_split(txt):\n",
        "    ss = re.split(r'(?<=[.!?])\\s+', str(txt).strip())\n",
        "    return [t for t in ss if t.strip()]\n",
        "\n",
        "def rougeLsum(a,b):\n",
        "    ra = \"\\n\".join(sent_split(a)); rb = \"\\n\".join(sent_split(b))\n",
        "    return SC.score(ra, rb)[\"rougeLsum\"].fmeasure\n",
        "\n",
        "def lcs_span_stats(src_text, hyp_text):\n",
        "    s=str(src_text).split(); h=str(hyp_text).split()\n",
        "    n,m=len(s),len(h); dp=[[0]*(m+1) for _ in range(n+1)]\n",
        "    for i in range(n-1,-1,-1):\n",
        "        si=s[i]; row=dp[i]; row1=dp[i+1]\n",
        "        for j in range(m-1,-1,-1):\n",
        "            row[j]=row1[j+1]+1 if si==h[j] else max(row[j+1], row1[j])\n",
        "    i=j=0; blocks=[]; cur=0\n",
        "    while i<n and j<m:\n",
        "        if s[i]==h[j]: cur+=1; i+=1; j+=1\n",
        "        else:\n",
        "            if cur>=2: blocks.append(cur); cur=0\n",
        "            if dp[i+1][j]>=dp[i][j+1]: i+=1\n",
        "            else: j+=1\n",
        "    if cur>=2: blocks.append(cur)\n",
        "    cont=sum(b-1 for b in blocks)\n",
        "    av=(sum(blocks)/len(blocks)) if blocks else 1.0\n",
        "    return cont, av\n",
        "\n",
        "KEEP   = {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"DATE\",\"TIME\",\"CARDINAL\",\"MONEY\",\"PERCENT\",\"QUANTITY\",\"ORDINAL\"}\n",
        "CORE   = {\"PERSON\",\"ORG\",\"GPE\"}\n",
        "NONCORE= {\"DATE\",\"QUANTITY\",\"MONEY\",\"PERCENT\",\"CARDINAL\",\"ORDINAL\"}\n",
        "\n",
        "def _ents_norm(text: str):\n",
        "    doc = nlp(str(text))\n",
        "    return {(\" \".join(e.text.strip().split()), e.label_) for e in doc.ents if e.label_ in KEEP}\n",
        "\n",
        "def strict_entity_score(src, hyp):\n",
        "    S=_ents_norm(src); H=_ents_norm(hyp)\n",
        "    overlap=H&S; unsupported=H-S\n",
        "    hard_fail=any(lbl in CORE for _,lbl in unsupported)\n",
        "    score=sum(3 if lbl in CORE else 1 for _,lbl in overlap)\n",
        "    return score, hard_fail\n",
        "\n",
        "def unsupported_nonc_count(src, hyp):\n",
        "    S=_ents_norm(src); H=_ents_norm(hyp)\n",
        "    return sum(1 for _,lbl in (H-S) if lbl in NONCORE)\n",
        "\n",
        "def minmax(v):\n",
        "    v=np.array(v, float)\n",
        "    lo,hi=float(v.min()), float(v.max())\n",
        "    return np.zeros_like(v) if hi-lo<1e-8 else (v-lo)/(hi-lo)\n",
        "\n",
        "# ---------- LOAD CKPT (for base_norm) ----------\n",
        "tok   = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR).to(DEVICE).eval()\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cuda.matmul.allow_tf32=True\n",
        "    try: torch.set_float32_matmul_precision(\"high\")\n",
        "    except: pass\n",
        "\n",
        "def add_base_norm(stageA_df, batch_size=16):\n",
        "    out = np.zeros(len(stageA_df), dtype=np.float32)\n",
        "    idxs = stageA_df.index.to_list()\n",
        "    for s in range(0, len(idxs), batch_size):\n",
        "        ii = idxs[s:s+batch_size]\n",
        "        sub = stageA_df.loc[ii]\n",
        "        enc = tok(sub[\"article\"].tolist(), max_length=MAX_SRC_LEN, truncation=True, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
        "        dec = tok(sub[\"summary\"].tolist(), max_length=256, truncation=True, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
        "        with torch.inference_mode(), torch.amp.autocast(DEVICE if DEVICE==\"cuda\" else \"cpu\"):\n",
        "            logits = model(**enc, labels=dec[\"input_ids\"]).logits.float()\n",
        "        shift_logits = logits[:, :-1, :]\n",
        "        shift_labels = dec[\"input_ids\"][:, 1:]\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=tok.pad_token_id, reduction=\"none\")\n",
        "        tok_loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.reshape(-1))\n",
        "        tok_loss = tok_loss.view(shift_labels.size(0), -1)\n",
        "        valid = (shift_labels != tok.pad_token_id).float()\n",
        "        loss_per = (tok_loss * valid).sum(dim=1) / (valid.sum(dim=1).clamp_min(1.0))\n",
        "        logp_per = -loss_per.detach().cpu().numpy()\n",
        "        L = sub[\"len_tok\"].to_numpy().clip(min=1) if \"len_tok\" in sub.columns else sub[\"len\"].to_numpy().clip(min=1)\n",
        "        out[s:s+len(ii)] = logp_per / (L ** 0.3)\n",
        "    stageA_df[\"base_norm\"] = out\n",
        "    return stageA_df\n",
        "\n",
        "# ---------- LOAD DETAILS ----------\n",
        "dd = pd.read_csv(DETAILS_CSV)\n",
        "assert {\"row_id\",\"article\",\"summary\"}.issubset(dd.columns), \"details csv missing required columns\"\n",
        "\n",
        "# normalize ref col\n",
        "if \"reference\" not in dd.columns:\n",
        "    if \"highlights\" in dd.columns: dd[\"reference\"] = dd[\"highlights\"].astype(str)\n",
        "    else: dd[\"reference\"] = \"\"\n",
        "\n",
        "# token lengths\n",
        "def toklen(s): return len(tok(str(s))[\"input_ids\"])\n",
        "if \"len_tok\" not in dd.columns:\n",
        "    dd[\"len_tok\"] = dd[\"summary\"].astype(str).apply(toklen)\n",
        "\n",
        "# --------- compute features needed for Stage-A if missing ---------\n",
        "need_feats = not {\"entity_score\",\"fail_flag\",\"contiguity_lcs\",\"avg_span_len_lcs\"}.issubset(dd.columns)\n",
        "if need_feats:\n",
        "    es, ff, cont, av = [], [], [], []\n",
        "    nonc = []\n",
        "    for _,r in dd.iterrows():\n",
        "        s,f = strict_entity_score(r[\"article\"], r[\"summary\"])\n",
        "        c,a = lcs_span_stats(r[\"article\"], r[\"summary\"])\n",
        "        es.append(s); ff.append(f); cont.append(c); av.append(a)\n",
        "        nonc.append(unsupported_nonc_count(r[\"article\"], r[\"summary\"]))\n",
        "    dd[\"entity_score\"]=es; dd[\"fail_flag\"]=ff\n",
        "    dd[\"contiguity_lcs\"]=cont; dd[\"avg_span_len_lcs\"]=av\n",
        "    dd[\"unsup_nonc\"] = nonc\n",
        "if \"unsup_nonc\" not in dd.columns:\n",
        "    dd[\"unsup_nonc\"] = [unsupported_nonc_count(a, h) for a,h in zip(dd[\"article\"], dd[\"summary\"])]\n",
        "\n",
        "# --------- Stage-A (TH-C + noncore penalty) ---------\n",
        "def adaptive_len_target(src):\n",
        "    lead = \" \".join(sent_split(src)[:3])\n",
        "    return int(np.clip(round(0.9 * toklen(lead)), 32, 60))\n",
        "\n",
        "rows=[]\n",
        "for rid, grp in dd.groupby(\"row_id\"):\n",
        "    art = grp.iloc[0][\"article\"]\n",
        "    tgt = (toklen(grp.iloc[0][\"reference\"]) if USE_REF_LEN_FOR_TARGET else adaptive_len_target(art))\n",
        "    L   = grp[\"len_tok\"].astype(int).to_numpy()\n",
        "    ent = grp[\"entity_score\"].to_numpy(dtype=float)\n",
        "    cont= grp[\"contiguity_lcs\"].to_numpy(dtype=float)\n",
        "    nonc= grp[\"unsup_nonc\"].to_numpy(dtype=float)\n",
        "    len_reward = np.exp(-((L - tgt)**2) / (2*(LEN_SIGMA**2)))\n",
        "    penalty = (nonc>0).astype(float) * NONCORE_PEN\n",
        "    THC = (5.0*ent) + (0.2*cont) + (0.3*len_reward) - penalty\n",
        "    ok = grp.copy()\n",
        "    ok[\"THC\"] = THC\n",
        "\n",
        "    if \"fail_flag\" in ok.columns:\n",
        "        ok = ok.loc[~ok[\"fail_flag\"]].copy()\n",
        "        if ok.empty:          # fallback if all hard-failed\n",
        "            ok = grp.copy()\n",
        "            ok[\"THC\"] = THC\n",
        "\n",
        "    rows.append(ok.sort_values(\"THC\", ascending=False).head(M_KEEP))\n",
        "stageA = pd.concat(rows, ignore_index=True)\n",
        "\n",
        "# base_norm on survivors\n",
        "stageA = add_base_norm(stageA)\n",
        "\n",
        "# --------- Build features + pairwise LR ---------\n",
        "def ent_consensus(cands):\n",
        "    E=[_ents_norm(x) for x in cands]\n",
        "    n=len(E); sc=np.zeros(n)\n",
        "    for i in range(n):\n",
        "        js=[]\n",
        "        for j in range(n):\n",
        "            if i==j: continue\n",
        "            u=E[i]|E[j]; inter=E[i]&E[j]\n",
        "            js.append(len(inter)/max(len(u),1) if u else 0.0)\n",
        "        sc[i]=np.mean(js) if js else 0.0\n",
        "    return sc\n",
        "\n",
        "feat_rows=[]\n",
        "for rid, grp in stageA.groupby(\"row_id\"):\n",
        "    art = grp.iloc[0][\"article\"]; ref = grp.iloc[0][\"reference\"]\n",
        "    tgt = (toklen(ref) if USE_REF_LEN_FOR_TARGET else adaptive_len_target(art))\n",
        "    cands = grp[\"summary\"].astype(str).tolist()\n",
        "    Ls = grp[\"len_tok\"].astype(int).tolist()\n",
        "    lead = \" \".join(sent_split(art)[:3])\n",
        "    leadsim = np.array([rougeLsum(h, lead) for h in cands])\n",
        "    # centroid MBR on RLsum\n",
        "    n=len(cands); mbr=np.zeros(n)\n",
        "    if n>1:\n",
        "        M=np.zeros((n,n))\n",
        "        for i in range(n):\n",
        "            for j in range(i+1,n):\n",
        "                s=rougeLsum(cands[i], cands[j]); M[i,j]=M[j,i]=s\n",
        "        mbr=M.mean(axis=1)\n",
        "    feats = pd.DataFrame({\n",
        "        \"row_id\": grp[\"row_id\"].values,\n",
        "        \"summary\": grp[\"summary\"].values,\n",
        "        \"THC_norm\": minmax(grp[\"THC\"].values),\n",
        "        \"LeadSim\": leadsim,\n",
        "        \"MBR\": mbr,\n",
        "        \"len_reward\": minmax([math.exp(-((L - tgt)**2) / (2*(LEN_SIGMA**2))) for L in Ls]),\n",
        "        \"contig_lcs_mm\": minmax([lcs_span_stats(art, s)[0] for s in cands]),\n",
        "        \"base_norm_mm\": minmax(grp[\"base_norm\"].values),\n",
        "        \"ent_consensus\": ent_consensus(cands),\n",
        "        \"unsup_nonc_mm\": minmax(grp[\"unsup_nonc\"].astype(float).values),\n",
        "        \"rl_ref\": [rougeLsum(ref, h) for h in cands],  # label for LR\n",
        "    })\n",
        "    feat_rows.append(feats)\n",
        "F = pd.concat(feat_rows, ignore_index=True)\n",
        "\n",
        "cols = [\"THC_norm\",\"LeadSim\",\"MBR\",\"len_reward\",\"contig_lcs_mm\",\"base_norm_mm\",\"ent_consensus\",\"unsup_nonc_mm\"]\n",
        "pairs_X=[]; pairs_y=[]\n",
        "for rid, g in F.groupby(\"row_id\"):\n",
        "    G=g.reset_index(drop=True); m=len(G)\n",
        "    for i in range(m):\n",
        "        for j in range(i+1,m):\n",
        "            yi, yj = G.loc[i,\"rl_ref\"], G.loc[j,\"rl_ref\"]\n",
        "            if yi==yj: continue\n",
        "            x = (G.loc[i, cols].values.astype(float) - G.loc[j, cols].values.astype(float))\n",
        "            y = 1 if yi>yj else 0\n",
        "            pairs_X.append(x); pairs_y.append(y)\n",
        "pairs_X=np.stack(pairs_X); pairs_y=np.array(pairs_y)\n",
        "clf = LogisticRegression(solver=\"liblinear\", max_iter=1000, random_state=SEED).fit(pairs_X, pairs_y)\n",
        "weights = dict(intercept=float(clf.intercept_[0]), coefs={c: float(w) for c,w in zip(cols, clf.coef_[0].tolist())})\n",
        "\n",
        "def score_group_linear(grp, w):\n",
        "    art = grp.iloc[0][\"article\"]; ref = grp.iloc[0][\"reference\"]\n",
        "    tgt = (toklen(ref) if USE_REF_LEN_FOR_TARGET else adaptive_len_target(art))\n",
        "    cands = grp[\"summary\"].astype(str).tolist()\n",
        "    Ls    = grp[\"len_tok\"].astype(int).tolist()\n",
        "    lead = \" \".join(sent_split(art)[:3])\n",
        "    leadsim = np.array([rougeLsum(h, lead) for h in cands])\n",
        "    n=len(cands); mbr=np.zeros(n)\n",
        "    if n>1:\n",
        "        M=np.zeros((n,n))\n",
        "        for i in range(n):\n",
        "            for j in range(i+1,n):\n",
        "                s=rougeLsum(cands[i], cands[j]); M[i,j]=M[j,i]=s\n",
        "        mbr=M.mean(axis=1)\n",
        "    feats = pd.DataFrame({\n",
        "        \"THC_norm\": minmax(grp[\"THC\"].values),\n",
        "        \"LeadSim\": leadsim,\n",
        "        \"MBR\": mbr,\n",
        "        \"len_reward\": minmax([math.exp(-((L - tgt)**2)/(2*(LEN_SIGMA**2))) for L in Ls]),\n",
        "        \"contig_lcs_mm\": minmax([lcs_span_stats(art, s)[0] for s in cands]),\n",
        "        \"base_norm_mm\": minmax(grp[\"base_norm\"].values),\n",
        "        \"ent_consensus\": ent_consensus(cands),\n",
        "        \"unsup_nonc_mm\": minmax(grp[\"unsup_nonc\"].astype(float).values),\n",
        "    })\n",
        "    cols_order = list(w[\"coefs\"].keys())\n",
        "    s = w[\"intercept\"] + sum(feats[c].values * w[\"coefs\"][c] for c in cols_order)\n",
        "    k=int(np.argmax(s))\n",
        "    cand = grp.iloc[[k]].copy()\n",
        "    # UCER-safe fallback\n",
        "    S=_ents_norm(art); H=_ents_norm(cand[\"summary\"].iloc[0])\n",
        "    if any(lbl in CORE for _,lbl in (H-S)):\n",
        "        safe_idx=[]\n",
        "        for i in range(len(grp)):\n",
        "            Hi=_ents_norm(grp.iloc[i][\"summary\"])\n",
        "            if not any(lbl in CORE for _,lbl in (Hi-S)):\n",
        "                safe_idx.append(i)\n",
        "        if safe_idx:\n",
        "            s_vals=np.array(s)\n",
        "            k=int(max(safe_idx, key=lambda i: s_vals[i]))\n",
        "            cand=grp.iloc[[k]].copy()\n",
        "    return cand\n",
        "\n",
        "best=[]\n",
        "for rid, grp in stageA.groupby(\"row_id\"):\n",
        "    best.append(score_group_linear(grp, weights))\n",
        "best = pd.concat(best, ignore_index=True)\n",
        "\n",
        "# ---------- METRICS ----------\n",
        "def rouge_means(R,H):\n",
        "    r1=r2=rls=rl=0.0\n",
        "    for r,h in zip(R,H):\n",
        "        rr=\"\\n\".join(sent_split(r)); hh=\"\\n\".join(sent_split(h))\n",
        "        s=SC.score(rr, hh)\n",
        "        r1+=s[\"rouge1\"].fmeasure; r2+=s[\"rouge2\"].fmeasure\n",
        "        rls+=s[\"rougeLsum\"].fmeasure; rl+=s[\"rougeL\"].fmeasure\n",
        "    n=len(R); return dict(r1_f=r1/n, r2_f=r2/n, rlsum_f=rls/n, rl_f=rl/n)\n",
        "\n",
        "def entPRF(R,H):\n",
        "    TP=FP=FN=0\n",
        "    for r,h in zip(R,H):\n",
        "        R_=_ents_norm(r); H_=_ents_norm(h)\n",
        "        TP+=len(H_&R_); FP+=len(H_-R_); FN+=len(R_-H_)\n",
        "    P=TP/(TP+FP) if (TP+FP) else 0.0\n",
        "    Rc=TP/(TP+FN) if (TP+FN) else 0.0\n",
        "    F=2*P*Rc/(P+Rc) if (P+Rc) else 0.0\n",
        "    return {\"TP\":TP,\"FP\":FP,\"FN\":FN,\"entP\":P,\"entR\":Rc,\"entF1\":F}\n",
        "\n",
        "def UCER(arts, hyps):\n",
        "    rates=[]; counts=[]\n",
        "    for s,h in zip(arts, hyps):\n",
        "        S=_ents_norm(s); H=_ents_norm(h)\n",
        "        bad=[(t,l) for (t,l) in (H-S) if l in CORE]\n",
        "        rates.append(1 if bad else 0); counts.append(len(bad))\n",
        "    return float(np.mean(rates)), float(np.mean(counts))\n",
        "\n",
        "ev = best[[\"row_id\",\"article\",\"reference\",\"summary\"]].rename(columns={\"summary\":\"baseline_pick\"}).copy()\n",
        "refs = ev[\"reference\"].astype(str).tolist()\n",
        "hyps = ev[\"baseline_pick\"].astype(str).tolist()\n",
        "arts = ev[\"article\"].astype(str).tolist()\n",
        "\n",
        "R = rouge_means(refs, hyps)\n",
        "E = entPRF(refs, hyps)\n",
        "U = UCER(arts, hyps)\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "OUT_BEST = os.path.join(OUT_DIR, f\"best_from_saved_{STAMP}.csv\")\n",
        "OUT_W    = os.path.join(OUT_DIR, f\"lr_weights_from_saved_{STAMP}.json\")\n",
        "\n",
        "best.to_csv(OUT_BEST, index=False, quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n",
        "with open(OUT_W, \"w\") as f: json.dump({\"weights\": weights, \"features\": cols, \"use_ref_len\": USE_REF_LEN_FOR_TARGET}, f, indent=2)\n",
        "\n",
        "print(f\"\\n=== RESULTS (from saved details) ===\")\n",
        "print(\"ROUGE :\", {k: round(v,4) for k,v in R.items()})\n",
        "print(\"ENT   :\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in E.items()})\n",
        "print(\"UCER  :\", {\"rate\": round(U[0],4), \"avg_core\": round(U[1],4)})\n",
        "print(\"Saved:\", OUT_BEST, \" | \", OUT_W)\n",
        "\n",
        "# Length stats (quick look)\n",
        "def word_len(s): return len(str(s).split())\n",
        "lens = pd.DataFrame({\n",
        "    \"ref_tok\": [toklen(r) for r in refs],\n",
        "    \"hyp_tok\": [toklen(h) for h in hyps],\n",
        "    \"ref_word\": [word_len(r) for r in refs],\n",
        "    \"hyp_word\": [word_len(h) for h in hyps],\n",
        "})\n",
        "print(\"\\n=== LENGTH STATS (words) ===\")\n",
        "print({\n",
        "    \"reference\": {\n",
        "        \"count\": int(lens.shape[0]),\n",
        "        \"mean\": round(lens[\"ref_word\"].mean(),3),\n",
        "        \"std\": round(lens[\"ref_word\"].std(ddof=0),3),\n",
        "        \"min\": int(lens[\"ref_word\"].min()), \"q25\": float(lens[\"ref_word\"].quantile(0.25)),\n",
        "        \"med\": float(lens[\"ref_word\"].median()), \"q75\": float(lens[\"ref_word\"].quantile(0.75)),\n",
        "        \"max\": int(lens[\"ref_word\"].max())\n",
        "    },\n",
        "    \"summary\": {\n",
        "        \"count\": int(lens.shape[0]),\n",
        "        \"mean\": round(lens[\"hyp_word\"].mean(),3),\n",
        "        \"std\": round(lens[\"hyp_word\"].std(ddof=0),3),\n",
        "        \"min\": int(lens[\"hyp_word\"].min()), \"q25\": float(lens[\"hyp_word\"].quantile(0.25)),\n",
        "        \"med\": float(lens[\"hyp_word\"].median()), \"q75\": float(lens[\"hyp_word\"].quantile(0.75)),\n",
        "        \"max\": int(lens[\"hyp_word\"].max())\n",
        "    }\n",
        "})\n",
        "\n",
        "print(\"\\n=== LENGTH STATS (tokens) ===\")\n",
        "print({\n",
        "    \"reference\": {\n",
        "        \"count\": int(lens.shape[0]),\n",
        "        \"mean\": round(lens[\"ref_tok\"].mean(),3),\n",
        "        \"std\": round(lens[\"ref_tok\"].std(ddof=0),3),\n",
        "        \"min\": int(lens[\"ref_tok\"].min()), \"q25\": float(lens[\"ref_tok\"].quantile(0.25)),\n",
        "        \"med\": float(lens[\"ref_tok\"].median()), \"q75\": float(lens[\"ref_tok\"].quantile(0.75)),\n",
        "        \"max\": int(lens[\"ref_tok\"].max())\n",
        "    },\n",
        "    \"summary\": {\n",
        "        \"count\": int(lens.shape[0]),\n",
        "        \"mean\": round(lens[\"hyp_tok\"].mean(),3),\n",
        "        \"std\": round(lens[\"hyp_tok\"].std(ddof=0),3),\n",
        "        \"min\": int(lens[\"hyp_tok\"].min()), \"q25\": float(lens[\"hyp_tok\"].quantile(0.25)),\n",
        "        \"med\": float(lens[\"hyp_tok\"].median()), \"q75\": float(lens[\"hyp_tok\"].quantile(0.75)),\n",
        "        \"max\": int(lens[\"hyp_tok\"].max())\n",
        "    }\n",
        "})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Keu4gI7cijZ"
      },
      "source": [
        "The baseline evaluation is not as strict as what was used in val. The comparison is not valid."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhLZQZZ_cr99"
      },
      "source": [
        "# Freeze weights on Val Hybrid and Baseline and use on Test Hybrid and Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDLmfm74fmci",
        "outputId": "378eae43-8730-4143-ffcc-b2cf646d2562"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "Setup done.\n"
          ]
        }
      ],
      "source": [
        "# ===== SETUP (CPU-friendly, deterministic) =====\n",
        "import os, sys, json, csv, time, math, re, random, subprocess\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "SEED = 0\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "CKPT_DIR = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500\"\n",
        "VAL_CSV  = \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "TEST_CSV = \"/content/drive/MyDrive/cleaned_outputs/naa/test_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "OUT_ROOT = os.path.join(CKPT_DIR, \"val_test_eval_freeze_reranker\")\n",
        "\n",
        "N_VAL  = 1000   # rows for VAL\n",
        "N_TEST = 1000   # rows for TEST\n",
        "\n",
        "# -------- Decode settings (same as your Hybrid-1) --------\n",
        "MAX_SRC_LEN = 400\n",
        "BATCH_LONG  = 2   # CPU-friendly\n",
        "BATCH_MED   = 4   # CPU-friendly\n",
        "\n",
        "CN2_LONG = dict(\n",
        "    num_beams=10, num_return_sequences=10, num_beam_groups=5, diversity_penalty=0.3,\n",
        "    max_new_tokens=100, min_new_tokens=55, no_repeat_ngram_size=3,\n",
        "    length_penalty=2.0, early_stopping=True, return_dict_in_generate=True, output_scores=False\n",
        ")\n",
        "CN_MED = dict(\n",
        "    num_beams=10, num_return_sequences=10, num_beam_groups=5, diversity_penalty=0.3,\n",
        "    max_new_tokens=70, min_new_tokens=35, no_repeat_ngram_size=3,\n",
        "    length_penalty=1.6, early_stopping=True, return_dict_in_generate=True, output_scores=False\n",
        ")\n",
        "\n",
        "# -------- Stage-A / eval constants (same strictness everywhere) --------\n",
        "M_KEEP       = 10\n",
        "NONCORE_PEN  = 2.0\n",
        "LEN_SIGMA    = 20.0\n",
        "USE_REF_LEN_FOR_TARGET = False  # keep this False for both VAL+TEST for consistency\n",
        "\n",
        "KEEP_LABELS = {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"DATE\",\"TIME\",\"CARDINAL\",\"MONEY\",\"PERCENT\",\"QUANTITY\",\"ORDINAL\"}\n",
        "CORE        = {\"PERSON\",\"ORG\",\"GPE\"}\n",
        "\n",
        "# -------- Minimal dependency checks (keep your current versions) --------\n",
        "def ensure(mod, pip=None):\n",
        "    import importlib.util\n",
        "    if pip is None: pip = mod\n",
        "    if importlib.util.find_spec(mod) is None:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pip])\n",
        "\n",
        "ensure(\"spacy\")\n",
        "ensure(\"rouge_score\", \"rouge-score==0.1.2\")\n",
        "ensure(\"scikit_learn\", \"scikit-learn\")\n",
        "ensure(\"transformers\", \"transformers\")  # use your current env version\n",
        "\n",
        "import spacy\n",
        "from functools import lru_cache\n",
        "from rouge_score import rouge_scorer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# ---- Load spaCy ONCE, reuse ----\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "except OSError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "\n",
        "# ---- Global ROUGE scorer (reused everywhere), stick to Lsum + sentence split wrapper ----\n",
        "SC = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\",\"rougeL\"], use_stemmer=True)\n",
        "\n",
        "# ---- Tokenizer & Model (CPU) ----\n",
        "tok   = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR).to(DEVICE).eval()\n",
        "if hasattr(model.config, \"use_cache\"):\n",
        "    model.config.use_cache = False  # avoid key/value cache surprises on CPU\n",
        "\n",
        "os.makedirs(OUT_ROOT, exist_ok=True)\n",
        "print(\"Setup done.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S05iFevLf1Ct"
      },
      "outputs": [],
      "source": [
        "# ===== IO & helpers =====\n",
        "def robust_read_csv(path: str) -> pd.DataFrame:\n",
        "    import csv as _csv\n",
        "    try:\n",
        "        df = pd.read_csv(path, engine=\"python\", quoting=_csv.QUOTE_ALL, escapechar=\"\\\\\")\n",
        "        cols = {c.lower(): c for c in df.columns}\n",
        "        assert \"article\" in cols and \"highlights\" in cols, f\"columns={list(df.columns)}\"\n",
        "        return df[[cols[\"article\"], cols[\"highlights\"]]].rename(columns={cols[\"article\"]: \"article\",\n",
        "                                                                        cols[\"highlights\"]: \"reference\"})\n",
        "    except Exception:\n",
        "        rows = []\n",
        "        with open(path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "            reader = _csv.reader(f, delimiter=\",\", quotechar='\"', escapechar=\"\\\\\")\n",
        "            header = [h.strip().lower() for h in next(reader)]\n",
        "            i_art, i_sum = header.index(\"article\"), header.index(\"highlights\")\n",
        "            for row in reader: rows.append([row[i_art], row[i_sum]])\n",
        "        return pd.DataFrame(rows, columns=[\"article\",\"reference\"])\n",
        "\n",
        "def save_csv(df: pd.DataFrame, path: str):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    tmp = path + \".tmp\"\n",
        "    df.to_csv(tmp, index=False, quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n",
        "    os.replace(tmp, path)\n",
        "\n",
        "def save_json(obj, path: str):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(obj, f, indent=2)\n",
        "\n",
        "# ===== Sentence split for ROUGE-Lsum =====\n",
        "def sent_split(txt):\n",
        "    ss = re.split(r'(?<=[.!?])\\s+', str(txt or \"\").strip())\n",
        "    return [t for t in ss if t.strip()]\n",
        "\n",
        "def RLsum(a,b):\n",
        "    ra = \"\\n\".join(sent_split(a)); rb = \"\\n\".join(sent_split(b))\n",
        "    return SC.score(ra, rb)[\"rougeLsum\"].fmeasure\n",
        "\n",
        "# ===== NER / entities with caching =====\n",
        "@lru_cache(maxsize=200000)\n",
        "def _ents_norm_cached(text: str):\n",
        "    doc = nlp(str(text))\n",
        "    out = []\n",
        "    for e in doc.ents:\n",
        "        if e.label_ in KEEP_LABELS:\n",
        "            out.append((\" \".join(e.text.strip().split()), e.label_))\n",
        "    return tuple(sorted(set(out)))  # cacheable\n",
        "\n",
        "def _ents_norm(text: str):\n",
        "    return set(_ents_norm_cached(text))\n",
        "\n",
        "def strict_entity_score(src, hyp):\n",
        "    S=_ents_norm(src); H=_ents_norm(hyp)\n",
        "    overlap=H&S; unsupported=H-S\n",
        "    hard_fail = any(lbl in CORE for _,lbl in unsupported)\n",
        "    score = sum(3 if lbl in CORE else 1 for _,lbl in overlap)\n",
        "    return score, hard_fail\n",
        "\n",
        "def unsupported_nonc_count(src, hyp):\n",
        "    S=_ents_norm(src); H=_ents_norm(hyp)\n",
        "    NONCORE = {\"DATE\",\"QUANTITY\",\"MONEY\",\"PERCENT\",\"CARDINAL\",\"ORDINAL\"}\n",
        "    return sum(1 for _,lbl in (H-S) if lbl in NONCORE)\n",
        "\n",
        "# ===== LCS contiguity =====\n",
        "def lcs_span_stats(src_text, hyp_text):\n",
        "    s=str(src_text).split(); h=str(hyp_text).split()\n",
        "    n,m=len(s),len(h); dp=[[0]*(m+1) for _ in range(n+1)]\n",
        "    for i in range(n-1,-1,-1):\n",
        "        si=s[i]; row=dp[i]; row1=dp[i+1]\n",
        "        for j in range(m-1,-1,-1):\n",
        "            row[j]=row1[j+1]+1 if si==h[j] else max(row[j+1], row1[j])\n",
        "    i=j=0; blocks=[]; cur=0\n",
        "    while i<n and j<m:\n",
        "        if s[i]==h[j]: cur+=1; i+=1; j+=1\n",
        "        else:\n",
        "            if cur>=2: blocks.append(cur); cur=0\n",
        "            if dp[i+1][j]>=dp[i][j+1]: i+=1\n",
        "            else: j+=1\n",
        "    if cur>=2: blocks.append(cur)\n",
        "    cont=sum(b-1 for b in blocks)\n",
        "    av=(sum(blocks)/len(blocks)) if blocks else 1.0\n",
        "    return cont, av\n",
        "\n",
        "def minmax(v):\n",
        "    v=np.array(v, float)\n",
        "    lo,hi=float(v.min()), float(v.max())\n",
        "    return np.zeros_like(v) if hi-lo<1e-8 else (v-lo)/(hi-lo)\n",
        "\n",
        "# ===== Length targets =====\n",
        "def toklen(s): return len(tok(str(s))[\"input_ids\"])\n",
        "def lead3(text): return \" \".join(sent_split(text)[:3])\n",
        "def adaptive_len_target(src):\n",
        "    L = toklen(lead3(src))\n",
        "    return int(np.clip(round(0.9 * L), 32, 60))\n",
        "\n",
        "# ===== Metrics =====\n",
        "def rouge_means(R,H):\n",
        "    r1=r2=rls=rl=0.0\n",
        "    for r,h in zip(R,H):\n",
        "        rr=\"\\n\".join(sent_split(r)); hh=\"\\n\".join(sent_split(h))\n",
        "        s=SC.score(rr, hh)\n",
        "        r1+=s[\"rouge1\"].fmeasure; r2+=s[\"rouge2\"].fmeasure\n",
        "        rls+=s[\"rougeLsum\"].fmeasure; rl+=s[\"rougeL\"].fmeasure\n",
        "    n=len(R); return dict(r1_f=r1/n, r2_f=r2/n, rlsum_f=rls/n, rl_f=rl/n)\n",
        "\n",
        "def entPRF(R,H):\n",
        "    TP=FP=FN=0\n",
        "    for r,h in zip(R,H):\n",
        "        R_=_ents_norm(r); H_=_ents_norm(h)\n",
        "        TP+=len(H_&R_); FP+=len(H_-R_); FN+=len(R_-H_)\n",
        "    P=TP/(TP+FP) if (TP+FP) else 0.0\n",
        "    Rc=TP/(TP+FN) if (TP+FN) else 0.0\n",
        "    F=2*P*Rc/(P+Rc) if (P+Rc) else 0.0\n",
        "    return {\"TP\":TP,\"FP\":FP,\"FN\":FN,\"entP\":P,\"entR\":Rc,\"entF1\":F}\n",
        "\n",
        "def UCER(arts, hyps):\n",
        "    rates=[]; counts=[]\n",
        "    for s,h in zip(arts, hyps):\n",
        "        S=_ents_norm(s); H=_ents_norm(h)\n",
        "        bad=[(t,l) for (t,l) in (H-S) if l in CORE]\n",
        "        rates.append(1 if bad else 0); counts.append(len(bad))\n",
        "    return {\"rate\": float(np.mean(rates)), \"avg_core\": float(np.mean(counts))}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEHpHeiEgDqz"
      },
      "outputs": [],
      "source": [
        "# ===== Generation (with optional EA logits processor hook) =====\n",
        "def build_ea_processor():\n",
        "    \"\"\"\n",
        "    Plug your EA/Copy-Next logits processor here.\n",
        "    Return a list[LogitsProcessor] or None.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Example:\n",
        "        # from my_ea_module import make_entity_aware_processor\n",
        "        # return make_entity_aware_processor(tok, nlp)\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(\"[Hybrid] EA processor not found; proceeding without EA.\", e)\n",
        "        return None\n",
        "\n",
        "def pretokenize_articles(articles):\n",
        "    enc = tok(articles, max_length=MAX_SRC_LEN, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "    return {k: v.to(DEVICE) for k,v in enc.items()}\n",
        "\n",
        "@torch.inference_mode()\n",
        "def run_pool(enc_slice, decode_args, logits_processor=None):\n",
        "    gen_args = dict(**decode_args)\n",
        "    if logits_processor is not None:\n",
        "        gen_args[\"logits_processor\"] = logits_processor\n",
        "    out = model.generate(**enc_slice, **gen_args)\n",
        "    return out.sequences  # scores not needed\n",
        "\n",
        "def decode_two_pools(articles, references, EA: bool, row_offset: int, batches=(BATCH_LONG, BATCH_MED)):\n",
        "    enc_all = pretokenize_articles(articles)\n",
        "    ea_proc = build_ea_processor() if EA else None\n",
        "    rows=[]\n",
        "    # CN2_LONG\n",
        "    for s in range(0, len(articles), batches[0]):\n",
        "        e=min(len(articles), s+batches[0])\n",
        "        enc = {k: v[s:e] for k,v in enc_all.items()}\n",
        "        seqs = run_pool(enc, CN2_LONG, logits_processor=ea_proc)\n",
        "        K = CN2_LONG[\"num_return_sequences\"]; bs=e-s\n",
        "        seqs = seqs.view(bs, K, -1)\n",
        "        for bi in range(bs):\n",
        "            art=articles[s+bi]; ref=references[s+bi]\n",
        "            rid=row_offset + s+bi\n",
        "            for k in range(K):\n",
        "                ids = seqs[bi,k]\n",
        "                hyp = tok.decode(ids, skip_special_tokens=True)\n",
        "                rows.append({\n",
        "                    \"row_id\": rid, \"pool\": \"CN2_LONG\", \"candidate_k\": k,\n",
        "                    \"article\": art, \"reference\": ref, \"summary\": hyp,\n",
        "                    \"len_tok\": max(int(ids.size(0)-1),1), \"base_score\": 0.0\n",
        "                })\n",
        "    # CN_MED\n",
        "    for s in range(0, len(articles), batches[1]):\n",
        "        e=min(len(articles), s+batches[1])\n",
        "        enc = {k: v[s:e] for k,v in enc_all.items()}\n",
        "        seqs = run_pool(enc, CN_MED, logits_processor=ea_proc)\n",
        "        K = CN_MED[\"num_return_sequences\"]; bs=e-s\n",
        "        seqs = seqs.view(bs, K, -1)\n",
        "        for bi in range(bs):\n",
        "            art=articles[s+bi]; ref=references[s+bi]\n",
        "            rid=row_offset + s+bi\n",
        "            for k in range(K):\n",
        "                ids = seqs[bi,k]\n",
        "                hyp = tok.decode(ids, skip_special_tokens=True)\n",
        "                rows.append({\n",
        "                    \"row_id\": rid, \"pool\": \"CN_MED\", \"candidate_k\": k,\n",
        "                    \"article\": art, \"reference\": ref, \"summary\": hyp,\n",
        "                    \"len_tok\": max(int(ids.size(0)-1),1), \"base_score\": 0.0\n",
        "                })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# ===== Stage-A (TH-C) =====\n",
        "def stageA_select(dd_all, M=M_KEEP):\n",
        "    out=[]\n",
        "    for rid, grp in dd_all.groupby(\"row_id\"):\n",
        "        art = grp.iloc[0][\"article\"]\n",
        "        tgt = (toklen(grp.iloc[0][\"reference\"]) if USE_REF_LEN_FOR_TARGET else adaptive_len_target(art))\n",
        "        ent_scores=[]; hard_fail=[]; contigs=[]; nonc=[]\n",
        "        for _,r in grp.iterrows():\n",
        "            s,f = strict_entity_score(r[\"article\"], r[\"summary\"])\n",
        "            c,_ = lcs_span_stats(r[\"article\"], r[\"summary\"])\n",
        "            u   = unsupported_nonc_count(r[\"article\"], r[\"summary\"])\n",
        "            ent_scores.append(s); hard_fail.append(f); contigs.append(c); nonc.append(u)\n",
        "        ent_scores=np.array(ent_scores, float)\n",
        "        contigs=np.array(contigs, float)\n",
        "        nonc=np.array(nonc, float)\n",
        "        L=grp[\"len_tok\"].to_numpy().astype(int)\n",
        "        len_reward = np.exp(-((L - tgt)**2) / (2*(LEN_SIGMA**2)))\n",
        "        penalty = (nonc>0).astype(float) * NONCORE_PEN\n",
        "        THC = (5.0*ent_scores) + (0.2*contigs) + (0.3*len_reward) - penalty\n",
        "        ok = grp.copy()\n",
        "        ok[\"THC\"]=THC; ok[\"unsup_nonc\"]=nonc; ok[\"fail_flag\"]=hard_fail\n",
        "        ok = ok.loc[~ok[\"fail_flag\"]].copy()\n",
        "        if ok.empty:  # fallback if all hard-failed\n",
        "            ok = grp.copy(); ok[\"THC\"]=THC; ok[\"unsup_nonc\"]=nonc\n",
        "        out.append(ok.sort_values(\"THC\", ascending=False).head(M))\n",
        "    return pd.concat(out, ignore_index=True)\n",
        "\n",
        "# ===== base_norm (per-token avg logp / L^0.3), batched =====\n",
        "def add_base_norm(stageA_df, batch_size=16):\n",
        "    out = np.zeros(len(stageA_df), dtype=np.float32)\n",
        "    idxs = stageA_df.index.to_list()\n",
        "    for s in range(0, len(idxs), batch_size):\n",
        "        ii = idxs[s:s+batch_size]\n",
        "        sub = stageA_df.loc[ii]\n",
        "        enc = tok(sub[\"article\"].tolist(), max_length=MAX_SRC_LEN, truncation=True, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
        "        dec = tok(sub[\"summary\"].tolist(), max_length=256, truncation=True, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
        "        with torch.inference_mode():\n",
        "            logits = model(**enc, labels=dec[\"input_ids\"]).logits.float()\n",
        "        shift_logits = logits[:, :-1, :]\n",
        "        shift_labels = dec[\"input_ids\"][:, 1:]\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=tok.pad_token_id, reduction=\"none\")\n",
        "        tok_loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.reshape(-1))\n",
        "        tok_loss = tok_loss.view(shift_labels.size(0), -1)\n",
        "        valid = (shift_labels != tok.pad_token_id).float()\n",
        "        loss_per = (tok_loss * valid).sum(dim=1) / (valid.sum(dim=1).clamp_min(1.0))\n",
        "        logp_per = -loss_per.detach().cpu().numpy()\n",
        "        L = sub[\"len_tok\"].to_numpy().clip(min=1)\n",
        "        out[s:s+len(ii)] = logp_per / (L ** 0.3)\n",
        "    stageA_df[\"base_norm\"] = out\n",
        "    return stageA_df\n",
        "\n",
        "# ===== feature builder (consistent RLsum + sentence split) =====\n",
        "def ent_consensus(cands):\n",
        "    E=[_ents_norm(x) for x in cands]\n",
        "    n=len(E); sc=np.zeros(n)\n",
        "    for i in range(n):\n",
        "        js=[]\n",
        "        for j in range(n):\n",
        "            if i==j: continue\n",
        "            u=E[i]|E[j]; inter=E[i]&E[j]\n",
        "            js.append(len(inter)/max(len(u),1) if u else 0.0)\n",
        "        sc[i]=np.mean(js) if js else 0.0\n",
        "    return sc\n",
        "\n",
        "def build_features(stageA_df, with_labels=True):\n",
        "    feat_rows=[]\n",
        "    for rid, grp in stageA_df.groupby(\"row_id\"):\n",
        "        art = grp.iloc[0][\"article\"]; ref = grp.iloc[0][\"reference\"]\n",
        "        tgt = (toklen(ref) if USE_REF_LEN_FOR_TARGET else adaptive_len_target(art))\n",
        "        cands = grp[\"summary\"].astype(str).tolist()\n",
        "        Ls    = grp[\"len_tok\"].astype(int).tolist()\n",
        "        lead  = \" \".join(sent_split(art)[:3])\n",
        "        leadsim = np.array([RLsum(h, lead) for h in cands])\n",
        "        n=len(cands); mbr=np.zeros(n)\n",
        "        if n>1:\n",
        "            M=np.zeros((n,n))\n",
        "            for i in range(n):\n",
        "                for j in range(i+1,n):\n",
        "                    s=RLsum(cands[i], cands[j]); M[i,j]=M[j,i]=s\n",
        "            mbr=M.mean(axis=1)\n",
        "        feats = pd.DataFrame({\n",
        "            \"row_id\": grp[\"row_id\"].values,\n",
        "            \"summary\": grp[\"summary\"].values,\n",
        "            \"THC_norm\": minmax(grp[\"THC\"].values),\n",
        "            \"LeadSim\": leadsim,\n",
        "            \"MBR\": mbr,\n",
        "            \"len_reward\": minmax([math.exp(-((L - tgt)**2) / (2*(LEN_SIGMA**2))) for L in Ls]),\n",
        "            \"contig_lcs_mm\": minmax([lcs_span_stats(art, s)[0] for s in cands]),\n",
        "            \"base_norm_mm\": minmax(grp[\"base_norm\"].values),\n",
        "            \"ent_consensus\": ent_consensus(cands),\n",
        "            \"unsup_nonc_mm\": minmax(grp[\"unsup_nonc\"].astype(float).values),\n",
        "        })\n",
        "        if with_labels:\n",
        "            feats[\"rl_ref\"] = [RLsum(ref, h) for h in cands]\n",
        "        feat_rows.append(feats)\n",
        "    return pd.concat(feat_rows, ignore_index=True)\n",
        "\n",
        "# ===== Train reranker (pairwise LR) =====\n",
        "RERANK_FEATURES = [\"THC_norm\",\"LeadSim\",\"MBR\",\"len_reward\",\"contig_lcs_mm\",\"base_norm_mm\",\"ent_consensus\",\"unsup_nonc_mm\"]\n",
        "\n",
        "def fit_pairwise_lr(F_with_labels):\n",
        "    pairs_X=[]; pairs_y=[]\n",
        "    for rid, G in F_with_labels.groupby(\"row_id\"):\n",
        "        G=G.reset_index(drop=True); m=len(G)\n",
        "        for i in range(m):\n",
        "            for j in range(i+1,m):\n",
        "                yi, yj = G.loc[i,\"rl_ref\"], G.loc[j,\"rl_ref\"]\n",
        "                if yi==yj: continue\n",
        "                x = (G.loc[i, RERANK_FEATURES].values.astype(float) - G.loc[j, RERANK_FEATURES].values.astype(float))\n",
        "                y = 1 if yi>yj else 0\n",
        "                pairs_X.append(x); pairs_y.append(y)\n",
        "    pairs_X=np.stack(pairs_X); pairs_y=np.array(pairs_y)\n",
        "    clf = LogisticRegression(solver=\"liblinear\", max_iter=1000, random_state=SEED).fit(pairs_X, pairs_y)\n",
        "    return dict(intercept=float(clf.intercept_[0]),\n",
        "                coefs={c: float(w) for c,w in zip(RERANK_FEATURES, clf.coef_[0].tolist())})\n",
        "\n",
        "# ===== Apply reranker per row (+ UCER-safe fallback) =====\n",
        "def score_group_linear(grp, w, art_text=None):\n",
        "    art = grp.iloc[0][\"article\"] if art_text is None else art_text\n",
        "    ref = grp.iloc[0][\"reference\"]\n",
        "    tgt = (toklen(ref) if USE_REF_LEN_FOR_TARGET else adaptive_len_target(art))\n",
        "    cands = grp[\"summary\"].astype(str).tolist()\n",
        "    Ls    = grp[\"len_tok\"].astype(int).tolist()\n",
        "    lead  = \" \".join(sent_split(art)[:3])\n",
        "    leadsim = np.array([RLsum(h, lead) for h in cands])\n",
        "    n=len(cands); mbr=np.zeros(n)\n",
        "    if n>1:\n",
        "        M=np.zeros((n,n))\n",
        "        for i in range(n):\n",
        "            for j in range(i+1,n):\n",
        "                s=RLsum(cands[i], cands[j]); M[i,j]=M[j,i]=s\n",
        "        mbr=M.mean(axis=1)\n",
        "    feats = pd.DataFrame({\n",
        "        \"THC_norm\": minmax(grp[\"THC\"].values),\n",
        "        \"LeadSim\": leadsim,\n",
        "        \"MBR\": mbr,\n",
        "        \"len_reward\": minmax([math.exp(-((L - tgt)**2)/(2*(LEN_SIGMA**2))) for L in Ls]),\n",
        "        \"contig_lcs_mm\": minmax([lcs_span_stats(art, s)[0] for s in cands]),\n",
        "        \"base_norm_mm\": minmax(grp[\"base_norm\"].values),\n",
        "        \"ent_consensus\": ent_consensus(cands),\n",
        "        \"unsup_nonc_mm\": minmax(grp[\"unsup_nonc\"].astype(float).values),\n",
        "    })\n",
        "    cols=list(w[\"coefs\"].keys())\n",
        "    score = w[\"intercept\"] + sum(feats[c].values * w[\"coefs\"][c] for c in cols)\n",
        "    k=int(np.argmax(score))\n",
        "    cand = grp.iloc[[k]].copy()\n",
        "    # UCER-safe fallback\n",
        "    S=_ents_norm(art); H=_ents_norm(cand[\"summary\"].iloc[0])\n",
        "    if any(lbl in CORE for _,lbl in (H-S)):\n",
        "        safe_idx=[]\n",
        "        for i in range(len(grp)):\n",
        "            Hi=_ents_norm(grp.iloc[i][\"summary\"])\n",
        "            if not any(lbl in CORE for _,lbl in (Hi-S)): safe_idx.append(i)\n",
        "        if safe_idx:\n",
        "            s_vals=np.array(score)\n",
        "            k=int(max(safe_idx, key=lambda i: s_vals[i]))\n",
        "            cand=grp.iloc[[k]].copy()\n",
        "    return cand\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpAOlQHTgi9Q",
        "outputId": "1ca86b7a-9ff7-439f-941f-44446b7bb983"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoding VAL Hybrid (EA=ON)...\n",
            " -> rows: 20000\n",
            "Decoding VAL Baseline (EA=OFF)...\n",
            " -> rows: 20000\n",
            "Stage-A on VAL train_df ...\n",
            "Saved reranker weights -> /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/val_test_eval_freeze_reranker/VAL_1k/frozen_reranker_weights.json\n",
            "\n",
            "=== VAL HYBRID_EA_ON ===\n",
            "ROUGE : {'r1_f': 0.3225, 'r2_f': 0.1206, 'rlsum_f': 0.2944, 'rl_f': 0.2227}\n",
            "ENT   : {'TP': 1333, 'FP': 3846, 'FN': 2487, 'entP': 0.2574, 'entR': 0.349, 'entF1': 0.2963}\n",
            "UCER  : {'rate': 0.04, 'avg_core': 0.049}\n",
            "Saved: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/val_test_eval_freeze_reranker/VAL_1k/best_HYBRID_EA_ON_20250903_165644.csv\n",
            "\n",
            "=== LENGTH STATS (words) ===\n",
            "{'reference': {'count': 1000, 'mean': np.float64(34.373), 'std': 10.235, 'min': 10, 'q25': 27.0, 'med': 33.0, 'q75': 41.0, 'max': 76}, 'summary': {'count': 1000, 'mean': np.float64(45.884), 'std': 8.55, 'min': 25, 'q25': 40.0, 'med': 46.0, 'q75': 51.0, 'max': 86}}\n",
            "\n",
            "=== LENGTH STATS (tokens) ===\n",
            "{'reference': {'count': 1000, 'mean': np.float64(43.316), 'std': 12.271, 'min': 14, 'q25': 34.0, 'med': 42.0, 'q75': 52.0, 'max': 86}, 'summary': {'count': 1000, 'mean': np.float64(59.533), 'std': 9.642, 'min': 36, 'q25': 53.0, 'med': 59.0, 'q75': 68.0, 'max': 100}}\n",
            "\n",
            "=== VAL BASELINE_EA_OFF ===\n",
            "ROUGE : {'r1_f': 0.3225, 'r2_f': 0.1206, 'rlsum_f': 0.2944, 'rl_f': 0.2227}\n",
            "ENT   : {'TP': 1333, 'FP': 3846, 'FN': 2487, 'entP': 0.2574, 'entR': 0.349, 'entF1': 0.2963}\n",
            "UCER  : {'rate': 0.04, 'avg_core': 0.049}\n",
            "Saved: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/val_test_eval_freeze_reranker/VAL_1k/best_BASELINE_EA_OFF_20250903_174705.csv\n",
            "\n",
            "=== LENGTH STATS (words) ===\n",
            "{'reference': {'count': 1000, 'mean': np.float64(34.373), 'std': 10.235, 'min': 10, 'q25': 27.0, 'med': 33.0, 'q75': 41.0, 'max': 76}, 'summary': {'count': 1000, 'mean': np.float64(45.884), 'std': 8.55, 'min': 25, 'q25': 40.0, 'med': 46.0, 'q75': 51.0, 'max': 86}}\n",
            "\n",
            "=== LENGTH STATS (tokens) ===\n",
            "{'reference': {'count': 1000, 'mean': np.float64(43.316), 'std': 12.271, 'min': 14, 'q25': 34.0, 'med': 42.0, 'q75': 52.0, 'max': 86}, 'summary': {'count': 1000, 'mean': np.float64(59.533), 'std': 9.642, 'min': 36, 'q25': 53.0, 'med': 59.0, 'q75': 68.0, 'max': 100}}\n"
          ]
        }
      ],
      "source": [
        "# ===== VAL(1k): decode, train reranker, evaluate, freeze weights =====\n",
        "VAL_DIR = os.path.join(OUT_ROOT, \"VAL_1k\")\n",
        "os.makedirs(VAL_DIR, exist_ok=True)\n",
        "\n",
        "# 1) Load VAL subset (same rows for H/B)\n",
        "df_val_all = robust_read_csv(VAL_CSV)\n",
        "df_val = df_val_all.head(N_VAL).reset_index(drop=False).rename(columns={\"index\":\"row_id\"})\n",
        "arts_val = df_val[\"article\"].astype(str).tolist()\n",
        "refs_val = df_val[\"reference\"].astype(str).tolist()\n",
        "\n",
        "# 2) Decode Hybrid (EA=ON) and Baseline (EA=OFF) on the SAME rows\n",
        "print(\"Decoding VAL Hybrid (EA=ON)...\")\n",
        "dd_val_hyb = decode_two_pools(arts_val, refs_val, EA=True,  row_offset=0)\n",
        "save_csv(dd_val_hyb, os.path.join(VAL_DIR, \"details_VAL_HYBRID_EA_ON.csv\"))\n",
        "print(\" -> rows:\", len(dd_val_hyb))\n",
        "\n",
        "print(\"Decoding VAL Baseline (EA=OFF)...\")\n",
        "dd_val_base = decode_two_pools(arts_val, refs_val, EA=False, row_offset=0)\n",
        "save_csv(dd_val_base, os.path.join(VAL_DIR, \"details_VAL_BASELINE_EA_OFF.csv\"))\n",
        "print(\" -> rows:\", len(dd_val_base))\n",
        "\n",
        "# 3) Train reranker ON VAL (use HYBRID details by default; set TRAIN_ON='union' to use both)\n",
        "TRAIN_ON = \"hybrid\"  # or \"union\"\n",
        "if TRAIN_ON == \"hybrid\":\n",
        "    train_df = dd_val_hyb.copy()\n",
        "else:\n",
        "    train_df = pd.concat([dd_val_hyb, dd_val_base], ignore_index=True)\n",
        "\n",
        "print(\"Stage-A on VAL train_df ...\")\n",
        "stageA_val = stageA_select(train_df, M=M_KEEP)\n",
        "stageA_val = add_base_norm(stageA_val)\n",
        "F_val = build_features(stageA_val, with_labels=True)\n",
        "weights = fit_pairwise_lr(F_val)\n",
        "\n",
        "weights_path = os.path.join(VAL_DIR, \"frozen_reranker_weights.json\")\n",
        "save_json({\"weights\": weights, \"features\": RERANK_FEATURES, \"use_ref_len\": USE_REF_LEN_FOR_TARGET}, weights_path)\n",
        "print(\"Saved reranker weights ->\", weights_path)\n",
        "\n",
        "# 4) Apply reranker to BOTH VAL Hybrid & Baseline, evaluate consistently\n",
        "def pick_and_eval(details_df, tag, out_dir):\n",
        "    st = stageA_select(details_df, M=M_KEEP)\n",
        "    st = add_base_norm(st)\n",
        "    best=[]\n",
        "    for rid, grp in st.groupby(\"row_id\"):\n",
        "        best.append(score_group_linear(grp, weights))\n",
        "    best = pd.concat(best, ignore_index=True)\n",
        "    # Eval\n",
        "    R = best[\"reference\"].astype(str).tolist()\n",
        "    H = best[\"summary\"].astype(str).tolist()\n",
        "    A = best[\"article\"].astype(str).tolist()\n",
        "    rouge = rouge_means(R,H)\n",
        "    ent   = entPRF(R,H)\n",
        "    ucer  = UCER(A,H)\n",
        "    # Save\n",
        "    STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    out_best = os.path.join(out_dir, f\"best_{tag}_{STAMP}.csv\")\n",
        "    save_csv(best, out_best)\n",
        "    print(f\"\\n=== VAL {tag} ===\")\n",
        "    print(\"ROUGE :\", {k: round(v,4) for k,v in rouge.items()})\n",
        "    print(\"ENT   :\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in ent.items()})\n",
        "    print(\"UCER  :\", {k: round(v,4) for k,v in ucer.items()})\n",
        "    print(\"Saved:\", out_best)\n",
        "    # Length stats\n",
        "    def word_len(s): return len(str(s).split())\n",
        "    lens = pd.DataFrame({\n",
        "        \"ref_tok\": [toklen(r) for r in R],\n",
        "        \"hyp_tok\": [toklen(hh) for hh in H],\n",
        "        \"ref_word\": [word_len(r) for r in R],\n",
        "        \"hyp_word\": [word_len(hh) for hh in H],\n",
        "    })\n",
        "    print(\"\\n=== LENGTH STATS (words) ===\")\n",
        "    print({\n",
        "        \"reference\": {\n",
        "            \"count\": int(lens.shape[0]),\n",
        "            \"mean\": round(lens[\"ref_word\"].mean(),3),\n",
        "            \"std\": round(lens[\"ref_word\"].std(ddof=0),3),\n",
        "            \"min\": int(lens[\"ref_word\"].min()), \"q25\": float(lens[\"ref_word\"].quantile(0.25)),\n",
        "            \"med\": float(lens[\"ref_word\"].median()), \"q75\": float(lens[\"ref_word\"].quantile(0.75)),\n",
        "            \"max\": int(lens[\"ref_word\"].max())\n",
        "        },\n",
        "        \"summary\": {\n",
        "            \"count\": int(lens.shape[0]),\n",
        "            \"mean\": round(lens[\"hyp_word\"].mean(),3),\n",
        "            \"std\": round(lens[\"hyp_word\"].std(ddof=0),3),\n",
        "            \"min\": int(lens[\"hyp_word\"].min()), \"q25\": float(lens[\"hyp_word\"].quantile(0.25)),\n",
        "            \"med\": float(lens[\"hyp_word\"].median()), \"q75\": float(lens[\"hyp_word\"].quantile(0.75)),\n",
        "            \"max\": int(lens[\"hyp_word\"].max())\n",
        "        }\n",
        "    })\n",
        "    print(\"\\n=== LENGTH STATS (tokens) ===\")\n",
        "    print({\n",
        "        \"reference\": {\n",
        "            \"count\": int(lens.shape[0]),\n",
        "            \"mean\": round(lens[\"ref_tok\"].mean(),3),\n",
        "            \"std\": round(lens[\"ref_tok\"].std(ddof=0),3),\n",
        "            \"min\": int(lens[\"ref_tok\"].min()), \"q25\": float(lens[\"ref_tok\"].quantile(0.25)),\n",
        "            \"med\": float(lens[\"ref_tok\"].median()), \"q75\": float(lens[\"ref_tok\"].quantile(0.75)),\n",
        "            \"max\": int(lens[\"ref_tok\"].max())\n",
        "        },\n",
        "        \"summary\": {\n",
        "            \"count\": int(lens.shape[0]),\n",
        "            \"mean\": round(lens[\"hyp_tok\"].mean(),3),\n",
        "            \"std\": round(lens[\"hyp_tok\"].std(ddof=0),3),\n",
        "            \"min\": int(lens[\"hyp_tok\"].min()), \"q25\": float(lens[\"hyp_tok\"].quantile(0.25)),\n",
        "            \"med\": float(lens[\"hyp_tok\"].median()), \"q75\": float(lens[\"hyp_tok\"].quantile(0.75)),\n",
        "            \"max\": int(lens[\"hyp_tok\"].max())\n",
        "        }\n",
        "    })\n",
        "    return dict(rouge=rouge, ent=ent, ucer=ucer, path=out_best)\n",
        "\n",
        "val_h = pick_and_eval(dd_val_hyb, \"HYBRID_EA_ON\", VAL_DIR)\n",
        "val_b = pick_and_eval(dd_val_base, \"BASELINE_EA_OFF\", VAL_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJV3iMw6u7ne",
        "outputId": "4adc14ec-7848-42cc-9543-cdf91be83223"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes equal?  True\n",
            "All rows equal?  True\n",
            "Summaries equal?  True\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd, os, hashlib\n",
        "\n",
        "VAL_DIR = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/val_test_eval_freeze_reranker/VAL_1k\"\n",
        "p_h = os.path.join(VAL_DIR, \"details_VAL_HYBRID_EA_ON.csv\")\n",
        "p_b = os.path.join(VAL_DIR, \"details_VAL_BASELINE_EA_OFF.csv\")\n",
        "\n",
        "df_h = pd.read_csv(p_h)\n",
        "df_b = pd.read_csv(p_b)\n",
        "\n",
        "print(\"Shapes equal? \", df_h.shape == df_b.shape)\n",
        "print(\"All rows equal? \", df_h.equals(df_b))\n",
        "print(\"Summaries equal? \", df_h[\"summary\"].equals(df_b[\"summary\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LQH4vWzznBc"
      },
      "source": [
        "The “Hybrid” and “Baseline” details files produced identical candidate summaries, the reranker had the exact same inputs and therefore picked the same outputs. The EA/span-aware logits processor never actually affected generation. Therefore, Hybrid summaries will be generated again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKj3PMyzVag4",
        "outputId": "6dadf7e0-9000-415c-d5b2-7b86356c5577"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Decoding VAL_1k (EA=ON, Optimised) ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Group Beam Search is scheduled to be moved to a `custom_generate` repository in v4.55.0. To prevent loss of backward compatibility, add `trust_remote_code=True` to your `generate` call.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " -> details rows: 20000 | saved: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/val_test_eval_freeze_reranker/VAL_1k/details_VAL_HYBRID_EA_ON_OPT.csv\n",
            "Stage-A ...\n",
            "\n",
            "=== VAL_1k EA=ON (Optimised reranker) ===\n",
            "ROUGE : {'r1_f': 0.3232, 'r2_f': 0.1201, 'rlsum_f': 0.2936, 'rl_f': 0.2208}\n",
            "ENT   : {'TP': 1287, 'FP': 3763, 'FN': 2533, 'entP': 0.2549, 'entR': 0.3369, 'entF1': 0.2902}\n",
            "UCER  : {'rate': 0.031, 'avg_core': 0.043}\n",
            "Saved best -> /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/val_test_eval_freeze_reranker/VAL_1k/best_VAL_1k_EA_ON_OPT_20250905_120449.csv\n",
            "\n",
            "=== LENGTH STATS (words) ===\n",
            "{'reference': {'count': 1000, 'mean': 34.373, 'std': 10.235, 'min': 10, 'q25': 27.0, 'med': 33.0, 'q75': 41.0, 'max': 76}, 'summary': {'count': 1000, 'mean': 45.788, 'std': 8.608, 'min': 26, 'q25': 39.0, 'med': 46.0, 'q75': 52.0, 'max': 85}}\n",
            "\n",
            "=== LENGTH STATS (tokens) ===\n",
            "{'reference': {'count': 1000, 'mean': 43.316, 'std': 12.271, 'min': 14, 'q25': 34.0, 'med': 42.0, 'q75': 52.0, 'max': 86}, 'summary': {'count': 1000, 'mean': 59.206, 'std': 9.633, 'min': 36, 'q25': 52.0, 'med': 59.0, 'q75': 68.0, 'max': 100}}\n"
          ]
        }
      ],
      "source": [
        "# ==== HYBRID (EA=ON) — VAL 1k with SAME decoding, Stage-A, EA-trained reranker & metrics ====\n",
        "# GPU-friendly; identical pools/strictness to your prior 1k script, but:\n",
        "\n",
        "\n",
        "import os, sys, re, csv, json, time, random, math, subprocess\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "import torch\n",
        "\n",
        "SEED = 0\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "CKPT_DIR   = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500\"\n",
        "VAL_CSV    = \"/content/drive/MyDrive/cleaned_outputs/naa/val_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "OUT_ROOT   = os.path.join(CKPT_DIR, \"val_test_eval_freeze_reranker\")\n",
        "VAL_DIR    = os.path.join(OUT_ROOT, \"VAL_1k\")\n",
        "os.makedirs(VAL_DIR, exist_ok=True)\n",
        "\n",
        "# ---- Decode settings (same as before) ----\n",
        "MAX_SRC_LEN = 400\n",
        "SAVE_CANDIDATE_SCORES = True\n",
        "CN2_LONG = dict(num_beams=10, num_return_sequences=10, num_beam_groups=5, diversity_penalty=0.3,\n",
        "                max_new_tokens=100, min_new_tokens=55, no_repeat_ngram_size=3,\n",
        "                length_penalty=2.0, early_stopping=True, return_dict_in_generate=True, output_scores=False)\n",
        "CN_MED   = dict(num_beams=10, num_return_sequences=10, num_beam_groups=5, diversity_penalty=0.3,\n",
        "                max_new_tokens=70,  min_new_tokens=35, no_repeat_ngram_size=3,\n",
        "                length_penalty=1.6, early_stopping=True, return_dict_in_generate=True, output_scores=False)\n",
        "\n",
        "# Batch sizes: tuned for T4\n",
        "BATCH_LONG = 32\n",
        "BATCH_MED  = 64\n",
        "\n",
        "# ---- Stage-A / metrics constants (identical strictness) ----\n",
        "M_KEEP       = 10\n",
        "NONCORE_PEN  = 2.0\n",
        "LEN_SIGMA    = 20.0\n",
        "USE_REF_LEN_FOR_TARGET = False\n",
        "\n",
        "KEEP_LABELS = {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"DATE\",\"TIME\",\"CARDINAL\",\"MONEY\",\"PERCENT\",\"QUANTITY\",\"ORDINAL\"}\n",
        "CORE        = {\"PERSON\",\"ORG\",\"GPE\"}\n",
        "\n",
        "# ---- Minimal deps ----\n",
        "def ensure(mod, pip=None):\n",
        "    import importlib.util\n",
        "    if pip is None: pip = mod\n",
        "    if importlib.util.find_spec(mod) is None:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pip])\n",
        "ensure(\"spacy\", \"spacy\")\n",
        "ensure(\"rouge_score\", \"rouge-score==0.1.2\")\n",
        "ensure(\"scikit_learn\", \"scikit-learn\")\n",
        "ensure(\"transformers\", \"transformers\")\n",
        "\n",
        "import spacy\n",
        "from rouge_score import rouge_scorer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, LogitsProcessor, LogitsProcessorList\n",
        "\n",
        "# ---- spaCy once ----\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "except OSError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "\n",
        "# ---- IO helpers ----\n",
        "def robust_read_csv(path: str) -> pd.DataFrame:\n",
        "    import csv as _csv\n",
        "    try:\n",
        "        df = pd.read_csv(path, engine=\"python\", quoting=_csv.QUOTE_ALL, escapechar=\"\\\\\")\n",
        "        cols = {c.lower(): c for c in df.columns}\n",
        "        assert \"article\" in cols and \"highlights\" in cols, f\"columns={list(df.columns)}\"\n",
        "        return df[[cols[\"article\"], cols[\"highlights\"]]].rename(columns={\"article\":\"article\",\"highlights\":\"reference\"})\n",
        "    except Exception:\n",
        "        rows = []\n",
        "        with open(path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "            reader = _csv.reader(f, delimiter=\",\", quotechar='\"', escapechar=\"\\\\\")\n",
        "            header = [h.strip().lower() for h in next(reader)]\n",
        "            i_art, i_sum = header.index(\"article\"), header.index(\"highlights\")\n",
        "            for row in reader: rows.append([row[i_art], row[i_sum]])\n",
        "        return pd.DataFrame(rows, columns=[\"article\",\"reference\"])\n",
        "\n",
        "def save_csv(df: pd.DataFrame, path: str):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    tmp = path + \".tmp\"\n",
        "    df.to_csv(tmp, index=False, quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n",
        "    os.replace(tmp, path)\n",
        "\n",
        "def save_json(obj, path: str):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    with open(path, \"w\") as f: json.dump(obj, f, indent=2)\n",
        "\n",
        "# ---- Tokenizer & Model ----\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "tok   = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR).to(DEVICE).eval()\n",
        "if hasattr(model.config, \"use_cache\"): model.config.use_cache = True\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    try: torch.set_float32_matmul_precision(\"high\")\n",
        "    except: pass\n",
        "    major = torch.cuda.get_device_capability()[0]\n",
        "    use_bf16 = (major >= 8)\n",
        "    model = model.to(dtype=(torch.bfloat16 if use_bf16 else torch.float16))\n",
        "\n",
        "# ---- Sentence split + ROUGE-Lsum ----\n",
        "SC = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\",\"rougeL\"], use_stemmer=True)\n",
        "def sent_split(txt):\n",
        "    ss = re.split(r'(?<=[.!?])\\s+', str(txt or \"\").strip())\n",
        "    return [t for t in ss if t.strip()]\n",
        "def RLsum(a,b):\n",
        "    ra = \"\\n\".join(sent_split(a)); rb = \"\\n\".join(sent_split(b))\n",
        "    return SC.score(ra, rb)[\"rougeLsum\"].fmeasure\n",
        "\n",
        "# ---- NER helpers ----\n",
        "def _ents_norm(text: str):\n",
        "    doc = nlp(str(text))\n",
        "    out = set()\n",
        "    for e in doc.ents:\n",
        "        if e.label_ in KEEP_LABELS:\n",
        "            out.add((\" \".join(e.text.strip().split()), e.label_))\n",
        "    return out\n",
        "\n",
        "def strict_entity_score(src, hyp):\n",
        "    S=_ents_norm(src); H=_ents_norm(hyp)\n",
        "    overlap=H&S; unsupported=H-S\n",
        "    hard_fail = any(lbl in CORE for _,lbl in unsupported)\n",
        "    score = sum(3 if lbl in CORE else 1 for _,lbl in overlap)\n",
        "    return score, hard_fail\n",
        "\n",
        "def unsupported_nonc_count(src, hyp):\n",
        "    S=_ents_norm(src); H=_ents_norm(hyp)\n",
        "    NONCORE = {\"DATE\",\"QUANTITY\",\"MONEY\",\"PERCENT\",\"CARDINAL\",\"ORDINAL\"}\n",
        "    return sum(1 for _,lbl in (H-S) if lbl in NONCORE)\n",
        "\n",
        "# ---- LCS contiguity ----\n",
        "def lcs_span_stats(src_text, hyp_text):\n",
        "    s=str(src_text).split(); h=str(hyp_text).split()\n",
        "    n,m=len(s),len(h); dp=[[0]*(m+1) for _ in range(n+1)]\n",
        "    for i in range(n-1,-1,-1):\n",
        "        si=s[i]; row=dp[i]; row1=dp[i+1]\n",
        "        for j in range(m-1,-1,-1):\n",
        "            row[j]=row1[j+1]+1 if si==h[j] else max(row[j+1], row1[j])\n",
        "    i=j=0; blocks=[]; cur=0\n",
        "    while i<n and j<m:\n",
        "        if s[i]==h[j]: cur+=1; i+=1; j+=1\n",
        "        else:\n",
        "            if cur>=2: blocks.append(cur); cur=0\n",
        "            if dp[i+1][j]>=dp[i][j+1]: i+=1\n",
        "            else: j+=1\n",
        "    if cur>=2: blocks.append(cur)\n",
        "    cont=sum(b-1 for b in blocks)\n",
        "    av=(sum(blocks)/len(blocks)) if blocks else 1.0\n",
        "    return cont, av\n",
        "\n",
        "def minmax(v):\n",
        "    v=np.array(v, float); lo,hi=float(v.min()), float(v.max())\n",
        "    return np.zeros_like(v) if hi-lo<1e-8 else (v-lo)/(hi-lo)\n",
        "\n",
        "# ---- Length targets (same as before) ----\n",
        "def toklen_text(s): return len(tok(str(s))[\"input_ids\"])\n",
        "def lead3(text):    return \" \".join(sent_split(text)[:3])\n",
        "def adaptive_len_target(art):\n",
        "    L = toklen_text(lead3(art))\n",
        "    return int(np.clip(round(0.9 * L), 32, 60))\n",
        "\n",
        "# ---- Metrics ----\n",
        "def rouge_means(R,H):\n",
        "    r1=r2=rls=rl=0.0\n",
        "    for r,h in zip(R,H):\n",
        "        rr=\"\\n\".join(sent_split(r)); hh=\"\\n\".join(sent_split(h))\n",
        "        s=SC.score(rr, hh)\n",
        "        r1+=s[\"rouge1\"].fmeasure; r2+=s[\"rouge2\"].fmeasure\n",
        "        rls+=s[\"rougeLsum\"].fmeasure; rl+=s[\"rougeL\"].fmeasure\n",
        "    n=len(R); return dict(r1_f=r1/n, r2_f=r2/n, rlsum_f=rls/n, rl_f=rl/n)\n",
        "\n",
        "def entPRF(R,H):\n",
        "    TP=FP=FN=0\n",
        "    for r,h in zip(R,H):\n",
        "        R_=_ents_norm(r); H_=_ents_norm(h)\n",
        "        TP+=len(H_&R_); FP+=len(H_-R_); FN+=len(R_-H_)\n",
        "        # Note: counts include non-core entities\n",
        "    P=TP/(TP+FP) if (TP+FP) else 0.0\n",
        "    Rc=TP/(TP+FN) if (TP+FN) else 0.0\n",
        "    F=2*P*Rc/(P+Rc) if (P+Rc) else 0.0\n",
        "    return {\"TP\":TP,\"FP\":FP,\"FN\":FN,\"entP\":P,\"entR\":Rc,\"entF1\":F}\n",
        "\n",
        "def UCER(arts, hyps):\n",
        "    rates=[]; counts=[]\n",
        "    for s,h in zip(arts, hyps):\n",
        "        S=_ents_norm(s); H=_ents_norm(h)\n",
        "        bad=[(t,l) for (t,l) in (H-S) if l in CORE]\n",
        "        rates.append(1 if bad else 0); counts.append(len(bad))\n",
        "    return {\"rate\": float(np.mean(rates)), \"avg_core\": float(np.mean(counts))}\n",
        "\n",
        "# =========================\n",
        "# ENTITY-AWARE LOGITS PROCESSOR (CopyNext-style) — tuned for performance\n",
        "# =========================\n",
        "EA_ARGS = dict(gamma=0.5, gamma_entity=1.6, max_span=6)  # slightly stronger than 0.4/1.5 to reward spans\n",
        "\n",
        "class EntityAwareSpanProcessor(LogitsProcessor):\n",
        "    def __init__(self, maps_list, num_beams, gamma=0.5, gamma_entity=1.6, max_span=6):\n",
        "        self.maps_list = maps_list\n",
        "        self.num_beams = int(num_beams)\n",
        "        self.gamma = float(gamma)\n",
        "        self.gamma_entity = float(gamma_entity)\n",
        "        self.max_span = int(max_span)\n",
        "\n",
        "    def __call__(self, input_ids, scores):\n",
        "        Bx = input_ids.size(0)  # batch_size * num_beams\n",
        "        for b in range(Bx):\n",
        "            sample = b // self.num_beams\n",
        "            maps = self.maps_list[sample]\n",
        "            seq = input_ids[b].tolist()\n",
        "            for k in range(min(self.max_span, len(seq)), 0, -1):\n",
        "                mp = maps[k-1]\n",
        "                key = tuple(seq[-k:])\n",
        "                if key in mp:\n",
        "                    nxt, is_ent = mp[key]\n",
        "                    scores[b, nxt] = scores[b, nxt] + (self.gamma_entity if is_ent else self.gamma)\n",
        "                    break\n",
        "        return scores\n",
        "\n",
        "def build_copy_maps(tok, texts, max_span=EA_ARGS[\"max_span\"], max_src_len=MAX_SRC_LEN):\n",
        "    \"\"\"Build k-gram -> next-token table with an entity flag, using tokenizer offsets to align spans.\"\"\"\n",
        "    maps_all=[]\n",
        "    docs = list(nlp.pipe(texts, batch_size=128))\n",
        "    enc  = tok(texts, max_length=max_src_len, truncation=True, return_offsets_mapping=True)\n",
        "    for i in range(len(texts)):\n",
        "        ids  = enc[\"input_ids\"][i]\n",
        "        offs = enc[\"offset_mapping\"][i]\n",
        "        ent_spans = [(e.start_char, e.end_char) for e in docs[i].ents if e.label_ in KEEP_LABELS]\n",
        "\n",
        "        def in_ent(off):\n",
        "            if off is None: return False\n",
        "            a,b = off\n",
        "            if a is None or b is None: return False\n",
        "            for s,e in ent_spans:\n",
        "                if a>=s and b<=e: return True\n",
        "            return False\n",
        "\n",
        "        maps=[{} for _ in range(max_span)]\n",
        "        n=len(ids)\n",
        "        for k in range(1, max_span+1):\n",
        "            mp=maps[k-1]\n",
        "            for j in range(0, n-k-1):  # ensure j+k is valid\n",
        "                key=tuple(ids[j:j+k]); nxt=ids[j+k]\n",
        "                is_ent = False\n",
        "                if j+k < len(offs):\n",
        "                    off = offs[j+k]\n",
        "                    is_ent = (off is not None and off[0] is not None and off[1] is not None and in_ent(off))\n",
        "                if key not in mp:\n",
        "                    mp[key]=(nxt, is_ent)\n",
        "        maps_all.append(maps)\n",
        "    return maps_all\n",
        "\n",
        "def pretokenize_articles(articles):\n",
        "    enc = tok(articles, max_length=MAX_SRC_LEN, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "    return {k: v.to(DEVICE) for k,v in enc.items()}\n",
        "\n",
        "@torch.inference_mode()\n",
        "def run_pool(enc_slice, decode_args, logits_processor=None):\n",
        "    gen_args = dict(**decode_args)\n",
        "    if logits_processor is not None:\n",
        "        gen_args[\"logits_processor\"] = logits_processor\n",
        "    out = model.generate(**enc_slice, **gen_args)\n",
        "    return out.sequences\n",
        "\n",
        "def decode_two_pools_EA(articles, references):\n",
        "    \"\"\"EA=ON version: builds maps once; injects EA processor per batch in BOTH pools.\"\"\"\n",
        "    rows=[]\n",
        "    enc_all = pretokenize_articles(articles)\n",
        "    maps_all = build_copy_maps(tok, articles, max_span=EA_ARGS[\"max_span\"], max_src_len=MAX_SRC_LEN)\n",
        "\n",
        "    # Pool 1: CN2_LONG\n",
        "    num_beams = CN2_LONG[\"num_beams\"]\n",
        "    for s in range(0, len(articles), BATCH_LONG):\n",
        "        e=min(len(articles), s+BATCH_LONG)\n",
        "        enc = {k: v[s:e] for k,v in enc_all.items()}\n",
        "        proc = LogitsProcessorList([EntityAwareSpanProcessor(maps_all[s:e], num_beams=num_beams,\n",
        "                                                             gamma=EA_ARGS[\"gamma\"],\n",
        "                                                             gamma_entity=EA_ARGS[\"gamma_entity\"],\n",
        "                                                             max_span=EA_ARGS[\"max_span\"])])\n",
        "        seqs = run_pool(enc, CN2_LONG, logits_processor=proc)\n",
        "        K = CN2_LONG[\"num_return_sequences\"]; bs=e-s\n",
        "        seqs = seqs.view(bs, K, -1)\n",
        "        for bi in range(bs):\n",
        "            art=articles[s+bi]; ref=references[s+bi]; rid=s+bi\n",
        "            for k in range(K):\n",
        "                ids = seqs[bi,k]\n",
        "                hyp = tok.decode(ids, skip_special_tokens=True)\n",
        "                rows.append({\"row_id\": rid, \"pool\": \"CN2_LONG\", \"candidate_k\": k,\n",
        "                             \"article\": art, \"reference\": ref, \"summary\": hyp,\n",
        "                             \"len_tok\": max(int(ids.size(0)-1),1), \"base_score\": 0.0})\n",
        "\n",
        "    # Pool 2: CN_MED\n",
        "    num_beams = CN_MED[\"num_beams\"]\n",
        "    for s in range(0, len(articles), BATCH_MED):\n",
        "        e=min(len(articles), s+BATCH_MED)\n",
        "        enc = {k: v[s:e] for k,v in enc_all.items()}\n",
        "        proc = LogitsProcessorList([EntityAwareSpanProcessor(maps_all[s:e], num_beams=num_beams,\n",
        "                                                             gamma=EA_ARGS[\"gamma\"],\n",
        "                                                             gamma_entity=EA_ARGS[\"gamma_entity\"],\n",
        "                                                             max_span=EA_ARGS[\"max_span\"])])\n",
        "        seqs = run_pool(enc, CN_MED, logits_processor=proc)\n",
        "        K = CN_MED[\"num_return_sequences\"]; bs=e-s\n",
        "        seqs = seqs.view(bs, K, -1)\n",
        "        for bi in range(bs):\n",
        "            art=articles[s+bi]; ref=references[s+bi]; rid=s+bi\n",
        "            for k in range(K):\n",
        "                ids = seqs[bi,k]\n",
        "                hyp = tok.decode(ids, skip_special_tokens=True)\n",
        "                rows.append({\"row_id\": rid, \"pool\": \"CN_MED\", \"candidate_k\": k,\n",
        "                             \"article\": art, \"reference\": ref, \"summary\": hyp,\n",
        "                             \"len_tok\": max(int(ids.size(0)-1),1), \"base_score\": 0.0})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# ---- Stage-A (same formula) ----\n",
        "def stageA_select(dd_all, M=M_KEEP):\n",
        "    out=[]\n",
        "    for rid, grp in dd_all.groupby(\"row_id\"):\n",
        "        art = grp.iloc[0][\"article\"]\n",
        "        tgt = (toklen_text(grp.iloc[0][\"reference\"]) if USE_REF_LEN_FOR_TARGET else adaptive_len_target(art))\n",
        "        ent_scores=[]; hard_fail=[]; contigs=[]; nonc=[]\n",
        "        for _,r in grp.iterrows():\n",
        "            s,f = strict_entity_score(r[\"article\"], r[\"summary\"])\n",
        "            c,_ = lcs_span_stats(r[\"article\"], r[\"summary\"])\n",
        "            u   = unsupported_nonc_count(r[\"article\"], r[\"summary\"])\n",
        "            ent_scores.append(s); hard_fail.append(f); contigs.append(c); nonc.append(u)\n",
        "        ent_scores=np.array(ent_scores, float)\n",
        "        contigs=np.array(contigs, float)\n",
        "        nonc=np.array(nonc, float)\n",
        "        L=grp[\"len_tok\"].to_numpy().astype(int)\n",
        "        len_reward = np.exp(-((L - tgt)**2) / (2*(LEN_SIGMA**2)))\n",
        "        penalty = (nonc>0).astype(float) * NONCORE_PEN\n",
        "        THC = (5.0*ent_scores) + (0.2*contigs) + (0.3*len_reward) - penalty\n",
        "        ok = grp.copy()\n",
        "        ok[\"THC\"]=THC; ok[\"unsup_nonc\"]=nonc; ok[\"fail_flag\"]=hard_fail\n",
        "        ok = ok.loc[~ok[\"fail_flag\"]].copy()\n",
        "        if ok.empty:\n",
        "            ok = grp.copy(); ok[\"THC\"]=THC; ok[\"unsup_nonc\"]=nonc\n",
        "        out.append(ok.sort_values(\"THC\", ascending=False).head(M))\n",
        "    return pd.concat(out, ignore_index=True)\n",
        "\n",
        "# ---- base_norm (same def) ----\n",
        "def add_base_norm(stageA_df, batch_size=16):\n",
        "    out = np.zeros(len(stageA_df), dtype=np.float32)\n",
        "    idxs = stageA_df.index.to_list()\n",
        "    for s in range(0, len(idxs), batch_size):\n",
        "        ii = idxs[s:s+batch_size]\n",
        "        sub = stageA_df.loc[ii]\n",
        "        enc = tok(sub[\"article\"].tolist(), max_length=MAX_SRC_LEN, truncation=True, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
        "        dec = tok(sub[\"summary\"].tolist(), max_length=256, truncation=True, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
        "        with torch.inference_mode():\n",
        "            logits = model(**enc, labels=dec[\"input_ids\"]).logits.float()\n",
        "        shift_logits = logits[:, :-1, :]\n",
        "        shift_labels = dec[\"input_ids\"][:, 1:]\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=tok.pad_token_id, reduction=\"none\")\n",
        "        tok_loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.reshape(-1)).view(shift_labels.size(0), -1)\n",
        "        valid = (shift_labels != tok.pad_token_id).float()\n",
        "        loss_per = (tok_loss * valid).sum(dim=1) / (valid.sum(dim=1).clamp_min(1.0))\n",
        "        logp_per = -loss_per.detach().cpu().numpy()\n",
        "        L = sub[\"len_tok\"].to_numpy().clip(min=1)\n",
        "        out[s:s+len(ii)] = logp_per / (L ** 0.3)\n",
        "    stageA_df[\"base_norm\"] = out\n",
        "    return stageA_df\n",
        "\n",
        "# ---- Reranker features (identical) ----\n",
        "RERANK_FEATURES = [\"THC_norm\",\"LeadSim\",\"MBR\",\"len_reward\",\"contig_lcs_mm\",\"base_norm_mm\",\"ent_consensus\",\"unsup_nonc_mm\"]\n",
        "\n",
        "def ent_consensus(cands):\n",
        "    E=[_ents_norm(x) for x in cands]\n",
        "    n=len(E); sc=np.zeros(n)\n",
        "    for i in range(n):\n",
        "        js=[]\n",
        "        for j in range(n):\n",
        "            if i==j: continue\n",
        "            u=E[i]|E[j]; inter=E[i]&E[j]\n",
        "            js.append(len(inter)/max(len(u),1) if u else 0.0)\n",
        "        sc[i]=np.mean(js) if js else 0.0\n",
        "    return sc\n",
        "\n",
        "def build_features(stageA_df, with_labels=True):\n",
        "    feat_rows=[]\n",
        "    for rid, grp in stageA_df.groupby(\"row_id\"):\n",
        "        art = grp.iloc[0][\"article\"]; ref = grp.iloc[0][\"reference\"]\n",
        "        tgt = (toklen_text(ref) if USE_REF_LEN_FOR_TARGET else adaptive_len_target(art))\n",
        "        cands = grp[\"summary\"].astype(str).tolist()\n",
        "        Ls    = grp[\"len_tok\"].astype(int).tolist()\n",
        "        lead  = \" \".join(sent_split(art)[:3])\n",
        "        leadsim = np.array([RLsum(h, lead) for h in cands])\n",
        "        n=len(cands); mbr=np.zeros(n)\n",
        "        if n>1:\n",
        "            M=np.zeros((n,n))\n",
        "            for i in range(n):\n",
        "                for j in range(i+1,n):\n",
        "                    s=RLsum(cands[i], cands[j]); M[i,j]=M[j,i]=s\n",
        "            mbr=M.mean(axis=1)\n",
        "        feats = pd.DataFrame({\n",
        "            \"row_id\": grp[\"row_id\"].values,\n",
        "            \"summary\": grp[\"summary\"].values,\n",
        "            \"THC_norm\": minmax(grp[\"THC\"].values),\n",
        "            \"LeadSim\": leadsim,\n",
        "            \"MBR\": mbr,\n",
        "            \"len_reward\": minmax([math.exp(-((L - tgt)**2) / (2*(LEN_SIGMA**2))) for L in Ls]),\n",
        "            \"contig_lcs_mm\": minmax([lcs_span_stats(art, s)[0] for s in cands]),\n",
        "            \"base_norm_mm\": minmax(grp[\"base_norm\"].values),\n",
        "            \"ent_consensus\": ent_consensus(cands),\n",
        "            \"unsup_nonc_mm\": minmax(grp[\"unsup_nonc\"].astype(float).values),\n",
        "        })\n",
        "        if with_labels:\n",
        "            feats[\"rl_ref\"] = [RLsum(ref, h) for h in cands]\n",
        "        feat_rows.append(feats)\n",
        "    return pd.concat(feat_rows, ignore_index=True)\n",
        "\n",
        "def fit_pairwise_lr(F_with_labels):\n",
        "    pairs_X=[]; pairs_y=[]\n",
        "    for rid, G in F_with_labels.groupby(\"row_id\"):\n",
        "        G=G.reset_index(drop=True); m=len(G)\n",
        "        for i in range(m):\n",
        "            for j in range(i+1,m):\n",
        "                yi, yj = G.loc[i,\"rl_ref\"], G.loc[j,\"rl_ref\"]\n",
        "                if yi==yj: continue\n",
        "                x = (G.loc[i, RERANK_FEATURES].values.astype(float) - G.loc[j, RERANK_FEATURES].values.astype(float))\n",
        "                y = 1 if yi>yj else 0\n",
        "                pairs_X.append(x); pairs_y.append(y)\n",
        "    pairs_X=np.stack(pairs_X); pairs_y=np.array(pairs_y)\n",
        "    clf = LogisticRegression(solver=\"liblinear\", max_iter=1000, random_state=SEED).fit(pairs_X, pairs_y)\n",
        "    return dict(intercept=float(clf.intercept_[0]),\n",
        "                coefs={c: float(w) for c,w in zip(RERANK_FEATURES, clf.coef_[0].tolist())})\n",
        "\n",
        "def score_group_linear(grp, w, art_text=None):\n",
        "    art = grp.iloc[0][\"article\"] if art_text is None else art_text\n",
        "    ref = grp.iloc[0][\"reference\"]\n",
        "    tgt = (toklen_text(ref) if USE_REF_LEN_FOR_TARGET else adaptive_len_target(art))\n",
        "    cands = grp[\"summary\"].astype(str).tolist()\n",
        "    Ls    = grp[\"len_tok\"].astype(int).tolist()\n",
        "    lead  = \" \".join(sent_split(art)[:3])\n",
        "    leadsim = np.array([RLsum(h, lead) for h in cands])\n",
        "    n=len(cands); mbr=np.zeros(n)\n",
        "    if n>1:\n",
        "        M=np.zeros((n,n))\n",
        "        for i in range(n):\n",
        "            for j in range(i+1,n):\n",
        "                s=RLsum(cands[i], cands[j]); M[i,j]=M[j,i]=s\n",
        "        mbr=M.mean(axis=1)\n",
        "    feats = pd.DataFrame({\n",
        "        \"THC_norm\": minmax(grp[\"THC\"].values),\n",
        "        \"LeadSim\": leadsim,\n",
        "        \"MBR\": mbr,\n",
        "        \"len_reward\": minmax([math.exp(-((L - tgt)**2)/(2*(LEN_SIGMA**2))) for L in Ls]),\n",
        "        \"contig_lcs_mm\": minmax([lcs_span_stats(art, s)[0] for s in cands]),\n",
        "        \"base_norm_mm\": minmax(grp[\"base_norm\"].values),\n",
        "        \"ent_consensus\": ent_consensus(cands),\n",
        "        \"unsup_nonc_mm\": minmax(grp[\"unsup_nonc\"].astype(float).values),\n",
        "    })\n",
        "    cols=list(w[\"coefs\"].keys())\n",
        "    score = w[\"intercept\"] + sum(feats[c].values * w[\"coefs\"][c] for c in cols)\n",
        "    k=int(np.argmax(score))\n",
        "    cand = grp.iloc[[k]].copy()\n",
        "    # UCER-safe fallback (no unsupported CORE)\n",
        "    S=_ents_norm(art); H=_ents_norm(cand[\"summary\"].iloc[0])\n",
        "    if any(lbl in CORE for _,lbl in (H-S)):\n",
        "        safe_idx=[]\n",
        "        for i in range(len(grp)):\n",
        "            Hi=_ents_norm(grp.iloc[i][\"summary\"])\n",
        "            if not any(lbl in CORE for _,lbl in (Hi-S)): safe_idx.append(i)\n",
        "        if safe_idx:\n",
        "            s_vals=np.array(score)\n",
        "            k=int(max(safe_idx, key=lambda i: s_vals[i]))\n",
        "            cand=grp.iloc[[k]].copy()\n",
        "    return cand\n",
        "\n",
        "# ---- Runner for one split (VAL only) ----\n",
        "def run_split(name, csv_path, out_dir, details_fname, N_ROWS=1000):\n",
        "    df_all = robust_read_csv(csv_path)\n",
        "    df = df_all.head(N_ROWS).reset_index(drop=False).rename(columns={\"index\":\"row_id\"})\n",
        "    arts = df[\"article\"].astype(str).tolist()\n",
        "    refs = df[\"reference\"].astype(str).tolist()\n",
        "\n",
        "    print(f\"\\nDecoding {name} (EA=ON, Optimised) ...\")\n",
        "    dd = decode_two_pools_EA(arts, refs)\n",
        "    details_path = os.path.join(out_dir, details_fname)\n",
        "    save_csv(dd, details_path)\n",
        "    print(\" -> details rows:\", len(dd), \"| saved:\", details_path)\n",
        "\n",
        "    print(\"Stage-A ...\")\n",
        "    st = stageA_select(dd, M=M_KEEP)\n",
        "    st = add_base_norm(st)\n",
        "\n",
        "    # === RERANKER: TRAIN ON EA SURVIVORS to push performance ===\n",
        "    F = build_features(st, with_labels=True)\n",
        "    weights = fit_pairwise_lr(F)\n",
        "    save_json({\"weights\":weights, \"features\":RERANK_FEATURES, \"use_ref_len\": USE_REF_LEN_FOR_TARGET},\n",
        "              os.path.join(out_dir, f\"reranker_weights_trained_on_{name}_{time.strftime('%Y%m%d_%H%M%S')}.json\"))\n",
        "\n",
        "    best=[]\n",
        "    for rid, grp in st.groupby(\"row_id\"):\n",
        "        best.append(score_group_linear(grp, weights))\n",
        "    best = pd.concat(best, ignore_index=True)\n",
        "\n",
        "    # Eval\n",
        "    R = best[\"reference\"].astype(str).tolist()\n",
        "    H = best[\"summary\"].astype(str).tolist()\n",
        "    A = best[\"article\"].astype(str).tolist()\n",
        "    rouge = {k: round(v,4) for k,v in rouge_means(R,H).items()}\n",
        "    ent   = {k:(round(v,4) if isinstance(v,float) else v) for k,v in entPRF(R,H).items()}\n",
        "    ucer  = {k: round(v,4) for k,v in UCER(A,H).items()}\n",
        "\n",
        "    stamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    best_path = os.path.join(out_dir, f\"best_{name}_EA_ON_OPT_{stamp}.csv\")\n",
        "    save_csv(best, best_path)\n",
        "\n",
        "    # Length stats\n",
        "    def word_len(s): return len(str(s).split())\n",
        "    lens = pd.DataFrame({\n",
        "        \"ref_tok\": [toklen_text(r) for r in R],\n",
        "        \"hyp_tok\": [toklen_text(hh) for hh in H],\n",
        "        \"ref_word\": [word_len(r) for r in R],\n",
        "        \"hyp_word\": [word_len(hh) for hh in H],\n",
        "    })\n",
        "\n",
        "    print(f\"\\n=== {name} EA=ON (Optimised reranker) ===\")\n",
        "    print(\"ROUGE :\", rouge)\n",
        "    print(\"ENT   :\", ent)\n",
        "    print(\"UCER  :\", ucer)\n",
        "    print(\"Saved best ->\", best_path)\n",
        "    print(\"\\n=== LENGTH STATS (words) ===\")\n",
        "    print({\n",
        "        \"reference\": {\n",
        "            \"count\": int(lens.shape[0]),\n",
        "            \"mean\": float(np.round(lens[\"ref_word\"].mean(),3)),\n",
        "            \"std\":  float(np.round(lens[\"ref_word\"].std(ddof=0),3)),\n",
        "            \"min\":  int(lens[\"ref_word\"].min()),\n",
        "            \"q25\":  float(lens[\"ref_word\"].quantile(0.25)),\n",
        "            \"med\":  float(lens[\"ref_word\"].median()),\n",
        "            \"q75\":  float(lens[\"ref_word\"].quantile(0.75)),\n",
        "            \"max\":  int(lens[\"ref_word\"].max()),\n",
        "        },\n",
        "        \"summary\": {\n",
        "            \"count\": int(lens.shape[0]),\n",
        "            \"mean\": float(np.round(lens[\"hyp_word\"].mean(),3)),\n",
        "            \"std\":  float(np.round(lens[\"hyp_word\"].std(ddof=0),3)),\n",
        "            \"min\":  int(lens[\"hyp_word\"].min()),\n",
        "            \"q25\":  float(lens[\"hyp_word\"].quantile(0.25)),\n",
        "            \"med\":  float(lens[\"hyp_word\"].median()),\n",
        "            \"q75\":  float(lens[\"hyp_word\"].quantile(0.75)),\n",
        "            \"max\":  int(lens[\"hyp_word\"].max()),\n",
        "        }\n",
        "    })\n",
        "    print(\"\\n=== LENGTH STATS (tokens) ===\")\n",
        "    print({\n",
        "        \"reference\": {\n",
        "            \"count\": int(lens.shape[0]),\n",
        "            \"mean\": float(np.round(lens[\"ref_tok\"].mean(),3)),\n",
        "            \"std\":  float(np.round(lens[\"ref_tok\"].std(ddof=0),3)),\n",
        "            \"min\":  int(lens[\"ref_tok\"].min()),\n",
        "            \"q25\":  float(lens[\"ref_tok\"].quantile(0.25)),\n",
        "            \"med\":  float(lens[\"ref_tok\"].median()),\n",
        "            \"q75\":  float(lens[\"ref_tok\"].quantile(0.75)),\n",
        "            \"max\":  int(lens[\"ref_tok\"].max()),\n",
        "        },\n",
        "        \"summary\": {\n",
        "            \"count\": int(lens.shape[0]),\n",
        "            \"mean\": float(np.round(lens[\"hyp_tok\"].mean(),3)),\n",
        "            \"std\":  float(np.round(lens[\"hyp_tok\"].std(ddof=0),3)),\n",
        "            \"min\":  int(lens[\"hyp_tok\"].min()),\n",
        "            \"q25\":  float(lens[\"hyp_tok\"].quantile(0.25)),\n",
        "            \"med\":  float(lens[\"hyp_tok\"].median()),\n",
        "            \"q75\":  float(lens[\"hyp_tok\"].quantile(0.75)),\n",
        "            \"max\":  int(lens[\"hyp_tok\"].max()),\n",
        "        }\n",
        "    })\n",
        "\n",
        "    return dict(rouge=rouge, ent=ent, ucer=ucer, details_path=details_path, best_path=best_path)\n",
        "\n",
        "# ==== Run VAL-1k (Optimised) ====\n",
        "VAL = run_split(\"VAL_1k\",  VAL_CSV,  VAL_DIR, details_fname=\"details_VAL_HYBRID_EA_ON_OPT.csv\", N_ROWS=1000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuYjrGIu3vri"
      },
      "source": [
        "# Testing Using the frozen weights and the exact Setup - HYBRID FINAL ON CHECKPOINT 6500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqZrXgd-WEQe",
        "outputId": "7777854f-b230-4605-a3ed-e6e07c8b5ae6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoding TEST Hybrid (EA=ON, per-batch maps)...\n",
            " -> rows: 20000 | saved: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/val_test_eval_freeze_reranker/TEST_1k/details_TEST_HYBRID_EA_ON.csv\n",
            "Stage-A ...\n",
            "Loaded frozen reranker from: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/val_test_eval_freeze_reranker/VAL_1k/frozen_reranker_weights.json\n"
          ]
        }
      ],
      "source": [
        "# ==== HYBRID (EA=ON) — TEST 1k with frozen VAL reranker (per-batch EA maps) ====\n",
        "import os, sys, re, csv, json, time, math, subprocess\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "import torch\n",
        "\n",
        "# ---------------- CONFIG (freeze quality-affecting knobs) ----------------\n",
        "SEED = 0\n",
        "random = np.random\n",
        "random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "CKPT_DIR   = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500\"\n",
        "TEST_CSV   = \"/content/drive/MyDrive/cleaned_outputs/naa/test_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "OUT_ROOT   = os.path.join(CKPT_DIR, \"val_test_eval_freeze_reranker\")\n",
        "TEST_DIR   = os.path.join(OUT_ROOT, \"TEST_1k\")\n",
        "VAL_DIR    = os.path.join(OUT_ROOT, \"VAL_1k\")  # where the reranker was frozen on VAL\n",
        "FROZEN_RERANKER = os.path.join(VAL_DIR, \"frozen_reranker_weights.json\")\n",
        "\n",
        "os.makedirs(TEST_DIR, exist_ok=True)\n",
        "\n",
        "# Decoding (identical to VAL)\n",
        "MAX_SRC_LEN = 400\n",
        "CN2_LONG = dict(num_beams=10, num_return_sequences=10, num_beam_groups=5, diversity_penalty=0.3,\n",
        "                max_new_tokens=100, min_new_tokens=55, no_repeat_ngram_size=3,\n",
        "                length_penalty=2.0, early_stopping=True, return_dict_in_generate=False, output_scores=False)\n",
        "CN_MED   = dict(num_beams=10, num_return_sequences=10, num_beam_groups=5, diversity_penalty=0.3,\n",
        "                max_new_tokens=70,  min_new_tokens=35, no_repeat_ngram_size=3,\n",
        "                length_penalty=1.6, early_stopping=True, return_dict_in_generate=False, output_scores=False)\n",
        "\n",
        "# Throughput (safe defaults; you can raise if RAM allows)\n",
        "BATCH_LONG = 32\n",
        "BATCH_MED  = 64\n",
        "\n",
        "# EA processor strength (as used on VAL)\n",
        "EA_ARGS = dict(gamma=0.4, gamma_entity=1.5, max_span=6)\n",
        "\n",
        "# Stage-A / metrics constants (same strictness as VAL)\n",
        "M_KEEP       = 10\n",
        "NONCORE_PEN  = 2.0\n",
        "LEN_SIGMA    = 20.0\n",
        "USE_REF_LEN_FOR_TARGET = False\n",
        "\n",
        "KEEP_LABELS = {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"DATE\",\"TIME\",\"CARDINAL\",\"MONEY\",\"PERCENT\",\"QUANTITY\",\"ORDINAL\"}\n",
        "CORE        = {\"PERSON\",\"ORG\",\"GPE\"}\n",
        "\n",
        "# ---------------- Minimal deps ----------------\n",
        "def ensure(mod, pip=None):\n",
        "    import importlib.util, sys, subprocess\n",
        "    if pip is None: pip = mod\n",
        "    if importlib.util.find_spec(mod) is None:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pip])\n",
        "\n",
        "ensure(\"spacy\", \"spacy\"); ensure(\"rouge_score\", \"rouge-score==0.1.2\"); ensure(\"scikit_learn\", \"scikit-learn\"); ensure(\"transformers\", \"transformers\")\n",
        "\n",
        "import spacy\n",
        "from rouge_score import rouge_scorer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, LogitsProcessor, LogitsProcessorList\n",
        "\n",
        "# ---------------- IO helpers ----------------\n",
        "def robust_read_csv(path: str) -> pd.DataFrame:\n",
        "    import csv as _csv\n",
        "    try:\n",
        "        df = pd.read_csv(path, engine=\"python\", quoting=_csv.QUOTE_ALL, escapechar=\"\\\\\")\n",
        "        cols = {c.lower(): c for c in df.columns}\n",
        "        assert \"article\" in cols and \"highlights\" in cols, f\"columns={list(df.columns)}\"\n",
        "        return df[[cols[\"article\"], cols[\"highlights\"]]].rename(columns={\"article\":\"article\",\"highlights\":\"reference\"})\n",
        "    except Exception:\n",
        "        rows = []\n",
        "        with open(path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "            reader = _csv.reader(f, delimiter=\",\", quotechar='\"', escapechar=\"\\\\\")\n",
        "            header = [h.strip().lower() for h in next(reader)]\n",
        "            i_art, i_sum = header.index(\"article\"), header.index(\"highlights\")\n",
        "            for row in reader: rows.append([row[i_art], row[i_sum]])\n",
        "        return pd.DataFrame(rows, columns=[\"article\",\"reference\"])\n",
        "\n",
        "def save_csv(df: pd.DataFrame, path: str):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    tmp = path + \".tmp\"\n",
        "    df.to_csv(tmp, index=False, quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n",
        "    os.replace(tmp, path)\n",
        "\n",
        "def save_json(obj, path: str):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    with open(path, \"w\") as f: json.dump(obj, f, indent=2)\n",
        "\n",
        "# ---------------- spaCy + ROUGE ----------------\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "except OSError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "\n",
        "SC = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\",\"rougeL\"], use_stemmer=True)\n",
        "def sent_split(txt):\n",
        "    ss = re.split(r'(?<=[.!?])\\s+', str(txt or \"\").strip())\n",
        "    return [t for t in ss if t.strip()]\n",
        "def RLsum(a,b):\n",
        "    ra = \"\\n\".join(sent_split(a)); rb = \"\\n\".join(sent_split(b))\n",
        "    return SC.score(ra, rb)[\"rougeLsum\"].fmeasure\n",
        "\n",
        "# ---------------- Tokenizer & Model ----------------\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "tok   = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR).to(DEVICE).eval()\n",
        "if hasattr(model.config, \"use_cache\"): model.config.use_cache = True\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    try: torch.set_float32_matmul_precision(\"high\")\n",
        "    except: pass\n",
        "    major = torch.cuda.get_device_capability()[0]\n",
        "    model = model.to(dtype=(torch.bfloat16 if major>=8 else torch.float16))\n",
        "\n",
        "# ---------------- NER / features / metrics ----------------\n",
        "def _ents_norm(text: str):\n",
        "    doc = nlp(str(text)); out=set()\n",
        "    for e in doc.ents:\n",
        "        if e.label_ in KEEP_LABELS:\n",
        "            out.add((\" \".join(e.text.strip().split()), e.label_))\n",
        "    return out\n",
        "\n",
        "def strict_entity_score(src, hyp):\n",
        "    S=_ents_norm(src); H=_ents_norm(hyp)\n",
        "    overlap=H&S; unsupported=H-S\n",
        "    hard_fail = any(lbl in CORE for _,lbl in unsupported)\n",
        "    score = sum(3 if lbl in CORE else 1 for _,lbl in overlap)\n",
        "    return score, hard_fail\n",
        "\n",
        "def unsupported_nonc_count(src, hyp):\n",
        "    S=_ents_norm(src); H=_ents_norm(hyp)\n",
        "    NONCORE = {\"DATE\",\"QUANTITY\",\"MONEY\",\"PERCENT\",\"CARDINAL\",\"ORDINAL\"}\n",
        "    return sum(1 for _,lbl in (H-S) if lbl in NONCORE)\n",
        "\n",
        "def lcs_span_stats(src_text, hyp_text):\n",
        "    s=str(src_text).split(); h=str(hyp_text).split()\n",
        "    n,m=len(s),len(h); dp=[[0]*(m+1) for _ in range(n+1)]\n",
        "    for i in range(n-1,-1,-1):\n",
        "        si=s[i]; row=dp[i]; row1=dp[i+1]\n",
        "        for j in range(m-1,-1,-1):\n",
        "            row[j]=row1[j+1]+1 if si==h[j] else max(row[j+1], row1[j])\n",
        "    i=j=0; blocks=[]; cur=0\n",
        "    while i<n and j<m:\n",
        "        if s[i]==h[j]: cur+=1; i+=1; j+=1\n",
        "        else:\n",
        "            if cur>=2: blocks.append(cur); cur=0\n",
        "            if dp[i+1][j]>=dp[i][j+1]: i+=1\n",
        "            else: j+=1\n",
        "    if cur>=2: blocks.append(cur)\n",
        "    cont=sum(b-1 for b in blocks)\n",
        "    av=(sum(blocks)/len(blocks)) if blocks else 1.0\n",
        "    return cont, av\n",
        "\n",
        "def minmax(v):\n",
        "    v=np.array(v, float); lo,hi=float(v.min()), float(v.max())\n",
        "    return np.zeros_like(v) if hi-lo<1e-8 else (v-lo)/(hi-lo)\n",
        "\n",
        "def toklen_text(s): return len(tok(str(s))[\"input_ids\"])\n",
        "def lead3(text):    return \" \".join(sent_split(text)[:3])\n",
        "def adaptive_len_target(art):\n",
        "    L = toklen_text(lead3(art))\n",
        "    return int(np.clip(round(0.9 * L), 32, 60))\n",
        "\n",
        "def rouge_means(R,H):\n",
        "    r1=r2=rls=rl=0.0\n",
        "    for r,h in zip(R,H):\n",
        "        rr=\"\\n\".join(sent_split(r)); hh=\"\\n\".join(sent_split(h))\n",
        "        s=SC.score(rr, hh)\n",
        "        r1+=s[\"rouge1\"].fmeasure; r2+=s[\"rouge2\"].fmeasure\n",
        "        rls+=s[\"rougeLsum\"].fmeasure; rl+=s[\"rougeL\"].fmeasure\n",
        "    n=len(R); return dict(r1_f=r1/n, r2_f=r2/n, rlsum_f=rls/n, rl_f=rl/n)\n",
        "\n",
        "def entPRF(R,H):\n",
        "    TP=FP=FN=0\n",
        "    for r,h in zip(R,H):\n",
        "        R_=_ents_norm(r); H_=_ents_norm(h)\n",
        "        TP+=len(H_&R_); FP+=len(H_-R_); FN+=len(R_-H_)\n",
        "    P=TP/(TP+FP) if (TP+FP) else 0.0\n",
        "    Rc=TP/(TP+FN) if (TP+FN) else 0.0\n",
        "    F=2*P*Rc/(P+Rc) if (P+Rc) else 0.0\n",
        "    return {\"TP\":TP,\"FP\":FP,\"FN\":FN,\"entP\":P,\"entR\":Rc,\"entF1\":F}\n",
        "\n",
        "def UCER(arts, hyps):\n",
        "    rates=[]; counts=[]\n",
        "    for s,h in zip(arts, hyps):\n",
        "        S=_ents_norm(s); H=_ents_norm(h)\n",
        "        bad=[(t,l) for (t,l) in (H-S) if l in CORE]\n",
        "        rates.append(1 if bad else 0); counts.append(len(bad))\n",
        "    return {\"rate\": float(np.mean(rates)), \"avg_core\": float(np.mean(counts))}\n",
        "\n",
        "# ---------------- EA processor (per-batch maps) ----------------\n",
        "class EntityAwareSpanProcessor(LogitsProcessor):\n",
        "    def __init__(self, maps_list, num_beams, gamma=0.4, gamma_entity=1.5, max_span=6):\n",
        "        self.maps_list = maps_list\n",
        "        self.num_beams = int(num_beams)\n",
        "        self.gamma = float(gamma)\n",
        "        self.gamma_entity = float(gamma_entity)\n",
        "        self.max_span = int(max_span)\n",
        "    def __call__(self, input_ids, scores):\n",
        "        Bx = input_ids.size(0)\n",
        "        for b in range(Bx):\n",
        "            sample = b // self.num_beams\n",
        "            maps = self.maps_list[sample]\n",
        "            seq = input_ids[b].tolist()\n",
        "            for k in range(min(self.max_span, len(seq)), 0, -1):\n",
        "                mp = maps[k-1]; key = tuple(seq[-k:])\n",
        "                if key in mp:\n",
        "                    nxt, is_ent = mp[key]\n",
        "                    scores[b, nxt] = scores[b, nxt] + (self.gamma_entity if is_ent else self.gamma)\n",
        "                    break\n",
        "        return scores\n",
        "\n",
        "def build_copy_maps(tok, texts, max_span=6, max_src_len=400):\n",
        "    maps_all=[]\n",
        "    docs = list(nlp.pipe(texts, batch_size=128))\n",
        "    enc  = tok(texts, max_length=max_src_len, truncation=True, return_offsets_mapping=True)\n",
        "    for i in range(len(texts)):\n",
        "        ids  = enc[\"input_ids\"][i]\n",
        "        offs = enc[\"offset_mapping\"][i]\n",
        "        ent_spans = [(e.start_char, e.end_char) for e in docs[i].ents if e.label_ in KEEP_LABELS]\n",
        "        def in_ent(off):\n",
        "            if off is None: return False\n",
        "            a,b = off\n",
        "            if a is None or b is None: return False\n",
        "            for s,e in ent_spans:\n",
        "                if a>=s and b<=e: return True\n",
        "            return False\n",
        "        maps=[{} for _ in range(max_span)]\n",
        "        n=len(ids)\n",
        "        for k in range(1, max_span+1):\n",
        "            mp=maps[k-1]\n",
        "            for j in range(0, n-k-1):\n",
        "                key=tuple(ids[j:j+k]); nxt=ids[j+k]\n",
        "                is_ent=False\n",
        "                if j+k < len(offs):\n",
        "                    off=offs[j+k]\n",
        "                    is_ent = (off is not None and off[0] is not None and off[1] is not None and in_ent(off))\n",
        "                if key not in mp:\n",
        "                    mp[key]=(nxt, is_ent)\n",
        "        maps_all.append(maps)\n",
        "    return maps_all\n",
        "\n",
        "# ---------------- Decoding (EA=ON, per-batch maps) ----------------\n",
        "from transformers import LogitsProcessorList\n",
        "\n",
        "def pretokenize_articles(articles):\n",
        "    enc = tok(articles, max_length=MAX_SRC_LEN, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "    return {k: v.to(DEVICE) for k,v in enc.items()}\n",
        "\n",
        "@torch.inference_mode()\n",
        "def run_pool(enc_slice, decode_args, logits_processor=None):\n",
        "    gen_args = dict(**decode_args)\n",
        "    if logits_processor is not None:\n",
        "        gen_args[\"logits_processor\"] = logits_processor\n",
        "    out = model.generate(**enc_slice, **gen_args)\n",
        "    return out if isinstance(out, torch.Tensor) else out.sequences\n",
        "\n",
        "def decode_two_pools_EA(articles, references):\n",
        "    rows=[]\n",
        "    enc_all = pretokenize_articles(articles)\n",
        "\n",
        "    # CN2_LONG (per-batch maps)\n",
        "    num_beams = CN2_LONG[\"num_beams\"]\n",
        "    for s in range(0, len(articles), BATCH_LONG):\n",
        "        e=min(len(articles), s+BATCH_LONG)\n",
        "        enc = {k: v[s:e] for k,v in enc_all.items()}\n",
        "        maps_slice = build_copy_maps(tok, articles[s:e], max_span=EA_ARGS[\"max_span\"], max_src_len=MAX_SRC_LEN)\n",
        "        proc = LogitsProcessorList([EntityAwareSpanProcessor(maps_slice, num_beams=num_beams,\n",
        "                                                             gamma=EA_ARGS[\"gamma\"], gamma_entity=EA_ARGS[\"gamma_entity\"],\n",
        "                                                             max_span=EA_ARGS[\"max_span\"])])\n",
        "        seqs = run_pool(enc, CN2_LONG, logits_processor=proc)\n",
        "        K = CN2_LONG[\"num_return_sequences\"]; bs=e-s\n",
        "        seqs = seqs.view(bs, K, -1)\n",
        "        for bi in range(bs):\n",
        "            art=articles[s+bi]; ref=references[s+bi]; rid=s+bi\n",
        "            for k in range(K):\n",
        "                ids = seqs[bi,k]\n",
        "                hyp = tok.decode(ids, skip_special_tokens=True)\n",
        "                rows.append({\"row_id\": rid, \"pool\":\"CN2_LONG\", \"candidate_k\":k,\n",
        "                             \"article\": art, \"reference\": ref, \"summary\": hyp,\n",
        "                             \"len_tok\": max(int(ids.size(0)-1),1)})\n",
        "\n",
        "    # CN_MED (per-batch maps)\n",
        "    num_beams = CN_MED[\"num_beams\"]\n",
        "    for s in range(0, len(articles), BATCH_MED):\n",
        "        e=min(len(articles), s+BATCH_MED)\n",
        "        enc = {k: v[s:e] for k,v in enc_all.items()}\n",
        "        maps_slice = build_copy_maps(tok, articles[s:e], max_span=EA_ARGS[\"max_span\"], max_src_len=MAX_SRC_LEN)\n",
        "        proc = LogitsProcessorList([EntityAwareSpanProcessor(maps_slice, num_beams=num_beams,\n",
        "                                                             gamma=EA_ARGS[\"gamma\"], gamma_entity=EA_ARGS[\"gamma_entity\"],\n",
        "                                                             max_span=EA_ARGS[\"max_span\"])])\n",
        "        seqs = run_pool(enc, CN_MED, logits_processor=proc)\n",
        "        K = CN_MED[\"num_return_sequences\"]; bs=e-s\n",
        "        seqs = seqs.view(bs, K, -1)\n",
        "        for bi in range(bs):\n",
        "            art=articles[s+bi]; ref=references[s+bi]; rid=s+bi\n",
        "            for k in range(K):\n",
        "                ids = seqs[bi,k]\n",
        "                hyp = tok.decode(ids, skip_special_tokens=True)\n",
        "                rows.append({\"row_id\": rid, \"pool\":\"CN_MED\", \"candidate_k\":k,\n",
        "                             \"article\": art, \"reference\": ref, \"summary\": hyp,\n",
        "                             \"len_tok\": max(int(ids.size(0)-1),1)})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# ---------------- Stage-A / base_norm / features / rerank (same as baseline) ----------------\n",
        "def stageA_select(dd_all, M=M_KEEP):\n",
        "    out=[]\n",
        "    for rid, grp in dd_all.groupby(\"row_id\"):\n",
        "        art = grp.iloc[0][\"article\"]\n",
        "        tgt = (toklen_text(grp.iloc[0][\"reference\"]) if USE_REF_LEN_FOR_TARGET else adaptive_len_target(art))\n",
        "        ent_scores=[]; hard_fail=[]; contigs=[]; nonc=[]\n",
        "        for _,r in grp.iterrows():\n",
        "            s,f = strict_entity_score(r[\"article\"], r[\"summary\"])\n",
        "            c,_ = lcs_span_stats(r[\"article\"], r[\"summary\"])\n",
        "            u   = unsupported_nonc_count(r[\"article\"], r[\"summary\"])\n",
        "            ent_scores.append(s); hard_fail.append(f); contigs.append(c); nonc.append(u)\n",
        "        ent_scores=np.array(ent_scores, float); contigs=np.array(contigs, float); nonc=np.array(nonc, float)\n",
        "        L=grp[\"len_tok\"].to_numpy().astype(int)\n",
        "        len_reward = np.exp(-((L - tgt)**2) / (2*(LEN_SIGMA**2)))\n",
        "        penalty = (nonc>0).astype(float) * NONCORE_PEN\n",
        "        THC = (5.0*ent_scores) + (0.2*contigs) + (0.3*len_reward) - penalty\n",
        "        ok = grp.copy()\n",
        "        ok[\"THC\"]=THC; ok[\"unsup_nonc\"]=nonc; ok[\"fail_flag\"]=hard_fail\n",
        "        ok = ok.loc[~ok[\"fail_flag\"]].copy()\n",
        "        if ok.empty: ok = grp.copy(); ok[\"THC\"]=THC; ok[\"unsup_nonc\"]=nonc\n",
        "        out.append(ok.sort_values(\"THC\", ascending=False).head(M))\n",
        "    return pd.concat(out, ignore_index=True)\n",
        "\n",
        "def add_base_norm(stageA_df, batch_size=16):\n",
        "    out = np.zeros(len(stageA_df), dtype=np.float32)\n",
        "    idxs = stageA_df.index.to_list()\n",
        "    for s in range(0, len(idxs), batch_size):\n",
        "        ii = idxs[s:s+batch_size]\n",
        "        sub = stageA_df.loc[ii]\n",
        "        enc = tok(sub[\"article\"].tolist(), max_length=MAX_SRC_LEN, truncation=True, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
        "        dec = tok(sub[\"summary\"].tolist(), max_length=256, truncation=True, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
        "        with torch.inference_mode():\n",
        "            logits = model(**enc, labels=dec[\"input_ids\"]).logits.float()\n",
        "        shift_logits = logits[:, :-1, :]\n",
        "        shift_labels = dec[\"input_ids\"][:, 1:]\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=tok.pad_token_id, reduction=\"none\")\n",
        "        tok_loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.reshape(-1)).view(shift_labels.size(0), -1)\n",
        "        valid = (shift_labels != tok.pad_token_id).float()\n",
        "        loss_per = (tok_loss * valid).sum(dim=1) / (valid.sum(dim=1).clamp_min(1.0))\n",
        "        logp_per = -loss_per.detach().cpu().numpy()\n",
        "        L = sub[\"len_tok\"].to_numpy().clip(min=1)\n",
        "        out[s:s+len(ii)] = logp_per / (L ** 0.3)\n",
        "    stageA_df[\"base_norm\"] = out\n",
        "    return stageA_df\n",
        "\n",
        "RERANK_FEATURES = [\"THC_norm\",\"LeadSim\",\"MBR\",\"len_reward\",\"contig_lcs_mm\",\"base_norm_mm\",\"ent_consensus\",\"unsup_nonc_mm\"]\n",
        "\n",
        "def ent_consensus(cands):\n",
        "    E=[_ents_norm(x) for x in cands]\n",
        "    n=len(E); sc=np.zeros(n)\n",
        "    for i in range(n):\n",
        "        js=[]\n",
        "        for j in range(n):\n",
        "            if i==j: continue\n",
        "            u=E[i]|E[j]; inter=E[i]&E[j]\n",
        "            js.append(len(inter)/max(len(u),1) if u else 0.0)\n",
        "        sc[i]=np.mean(js) if js else 0.0\n",
        "    return sc\n",
        "\n",
        "def build_features(stageA_df, with_labels=True):\n",
        "    feat_rows=[]\n",
        "    for rid, grp in stageA_df.groupby(\"row_id\"):\n",
        "        art = grp.iloc[0][\"article\"]; ref = grp.iloc[0][\"reference\"]\n",
        "        tgt = (toklen_text(ref) if USE_REF_LEN_FOR_TARGET else adaptive_len_target(art))\n",
        "        cands = grp[\"summary\"].astype(str).tolist()\n",
        "        Ls    = grp[\"len_tok\"].astype(int).tolist()\n",
        "        lead  = \" \".join(sent_split(art)[:3])\n",
        "        leadsim = np.array([RLsum(h, lead) for h in cands])\n",
        "        n=len(cands); mbr=np.zeros(n)\n",
        "        if n>1:\n",
        "            M=np.zeros((n,n))\n",
        "            for i in range(n):\n",
        "                for j in range(i+1,n):\n",
        "                    s=RLsum(cands[i], cands[j]); M[i,j]=M[j,i]=s\n",
        "            mbr=M.mean(axis=1)\n",
        "        feats = pd.DataFrame({\n",
        "            \"row_id\": grp[\"row_id\"].values,\n",
        "            \"summary\": grp[\"summary\"].values,\n",
        "            \"THC_norm\": minmax(grp[\"THC\"].values),\n",
        "            \"LeadSim\": leadsim,\n",
        "            \"MBR\": mbr,\n",
        "            \"len_reward\": minmax([math.exp(-((L - tgt)**2) / (2*(LEN_SIGMA**2))) for L in Ls]),\n",
        "            \"contig_lcs_mm\": minmax([lcs_span_stats(art, s)[0] for s in cands]),\n",
        "            \"base_norm_mm\": minmax(grp[\"base_norm\"].values),\n",
        "            \"ent_consensus\": ent_consensus(cands),\n",
        "            \"unsup_nonc_mm\": minmax(grp[\"unsup_nonc\"].astype(float).values),\n",
        "        })\n",
        "        if with_labels:\n",
        "            feats[\"rl_ref\"] = [RLsum(ref, h) for h in cands]\n",
        "        feat_rows.append(feats)\n",
        "    return pd.concat(feat_rows, ignore_index=True)\n",
        "\n",
        "def score_group_linear(grp, w, art_text=None):\n",
        "    art = grp.iloc[0][\"article\"] if art_text is None else art_text\n",
        "    ref = grp.iloc[0][\"reference\"]\n",
        "    tgt = (toklen_text(ref) if USE_REF_LEN_FOR_TARGET else adaptive_len_target(art))\n",
        "    cands = grp[\"summary\"].astype(str).tolist()\n",
        "    Ls    = grp[\"len_tok\"].astype(int).tolist()\n",
        "    lead  = \" \".join(sent_split(art)[:3])\n",
        "    leadsim = np.array([RLsum(h, lead) for h in cands])\n",
        "    n=len(cands); mbr=np.zeros(n)\n",
        "    if n>1:\n",
        "        M=np.zeros((n,n))\n",
        "        for i in range(n):\n",
        "            for j in range(i+1,n):\n",
        "                s=RLsum(cands[i], cands[j]); M[i,j]=M[j,i]=s\n",
        "        mbr=M.mean(axis=1)\n",
        "    feats = pd.DataFrame({\n",
        "        \"THC_norm\": minmax(grp[\"THC\"].values),\n",
        "        \"LeadSim\": leadsim,\n",
        "        \"MBR\": mbr,\n",
        "        \"len_reward\": minmax([math.exp(-((L - tgt)**2)/(2*(LEN_SIGMA**2))) for L in Ls]),\n",
        "        \"contig_lcs_mm\": minmax([lcs_span_stats(art, s)[0] for s in cands]),\n",
        "        \"base_norm_mm\": minmax(grp[\"base_norm\"].values),\n",
        "        \"ent_consensus\": ent_consensus(cands),\n",
        "        \"unsup_nonc_mm\": minmax(grp[\"unsup_nonc\"].astype(float).values),\n",
        "    })\n",
        "    cols=list(w[\"coefs\"].keys())\n",
        "    score = w[\"intercept\"] + sum(feats[c].values * w[\"coefs\"][c] for c in cols)\n",
        "    k=int(np.argmax(score))\n",
        "    cand = grp.iloc[[k]].copy()\n",
        "    # UCER-safe fallback\n",
        "    S=_ents_norm(art); H=_ents_norm(cand[\"summary\"].iloc[0])\n",
        "    if any(lbl in CORE for _,lbl in (H-S)):\n",
        "        safe_idx=[]; s_vals=np.array(score)\n",
        "        for i in range(len(grp)):\n",
        "            Hi=_ents_norm(grp.iloc[i][\"summary\"])\n",
        "            if not any(lbl in CORE for _,lbl in (Hi-S)): safe_idx.append(i)\n",
        "        if safe_idx:\n",
        "            k=int(max(safe_idx, key=lambda i: s_vals[i]))\n",
        "            cand=grp.iloc[[k]].copy()\n",
        "    return cand\n",
        "\n",
        "# ---------------- Load TEST and run ----------------\n",
        "def run_hybrid_test(N_ROWS=1000):\n",
        "    df_all = robust_read_csv(TEST_CSV)\n",
        "    df = df_all.head(N_ROWS).reset_index(drop=False).rename(columns={\"index\":\"row_id\"})\n",
        "    arts = df[\"article\"].astype(str).tolist()\n",
        "    refs = df[\"reference\"].astype(str).tolist()\n",
        "\n",
        "    print(\"Decoding TEST Hybrid (EA=ON, per-batch maps)...\")\n",
        "    dd = decode_two_pools_EA(arts, refs)\n",
        "    details_path = os.path.join(TEST_DIR, \"details_TEST_HYBRID_EA_ON.csv\")\n",
        "    save_csv(dd, details_path)\n",
        "    print(\" -> rows:\", len(dd), \"| saved:\", details_path)\n",
        "\n",
        "    print(\"Stage-A ...\")\n",
        "    st = stageA_select(dd, M=M_KEEP)\n",
        "    st = add_base_norm(st, batch_size=16)\n",
        "\n",
        "    # Load frozen reranker (from VAL)\n",
        "    W = json.load(open(FROZEN_RERANKER, \"r\"))\n",
        "    weights = W.get(\"weights\", W)\n",
        "    print(\"Loaded frozen reranker from:\", FROZEN_RERANKER)\n",
        "\n",
        "    best=[]\n",
        "    for rid, grp in st.groupby(\"row_id\"):\n",
        "        best.append(score_group_linear(grp, weights))\n",
        "    best = pd.concat(best, ignore_index=True)\n",
        "\n",
        "    # Metrics\n",
        "    R = best[\"reference\"].astype(str).tolist()\n",
        "    H = best[\"summary\"].astype(str).tolist()\n",
        "    A = best[\"article\"].astype(str).tolist()\n",
        "    rouge = {k: round(v,4) for k,v in rouge_means(R,H).items()}\n",
        "    ent   = {k:(round(v,4) if isinstance(v,float) else v) for k,v in entPRF(R,H).items()}\n",
        "    ucer  = {k: round(v,4) for k,v in UCER(A,H).items()}\n",
        "\n",
        "    stamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    best_path = os.path.join(TEST_DIR, f\"best_TEST_HYBRID_EA_ON_{stamp}.csv\")\n",
        "    save_csv(best, best_path)\n",
        "\n",
        "    # Length stats\n",
        "    def word_len(s): return len(str(s).split())\n",
        "    lens = pd.DataFrame({\n",
        "        \"ref_tok\": [toklen_text(r) for r in R],\n",
        "        \"hyp_tok\": [toklen_text(hh) for hh in H],\n",
        "        \"ref_word\": [word_len(r) for r in R],\n",
        "        \"hyp_word\": [word_len(hh) for hh in H],\n",
        "    })\n",
        "\n",
        "    print(\"\\n=== TEST HYBRID_EA_ON ===\")\n",
        "    print(\"ROUGE :\", rouge)\n",
        "    print(\"ENT   :\", ent)\n",
        "    print(\"UCER  :\", ucer)\n",
        "    print(\"Saved best ->\", best_path)\n",
        "    print(\"\\n=== LENGTH STATS (words) ===\")\n",
        "    print({\n",
        "        \"reference\": {\"count\": int(lens.shape[0]), \"mean\": float(np.round(lens[\"ref_word\"].mean(),3)),\n",
        "                      \"std\": float(np.round(lens[\"ref_word\"].std(ddof=0),3)), \"min\": int(lens[\"ref_word\"].min()),\n",
        "                      \"q25\": float(lens[\"ref_word\"].quantile(0.25)), \"med\": float(lens[\"ref_word\"].median()),\n",
        "                      \"q75\": float(lens[\"ref_word\"].quantile(0.75)), \"max\": int(lens[\"ref_word\"].max())},\n",
        "        \"summary\":  {\"count\": int(lens.shape[0]), \"mean\": float(np.round(lens[\"hyp_word\"].mean(),3)),\n",
        "                      \"std\": float(np.round(lens[\"hyp_word\"].std(ddof=0),3)), \"min\": int(lens[\"hyp_word\"].min()),\n",
        "                      \"q25\": float(lens[\"hyp_word\"].quantile(0.25)), \"med\": float(lens[\"hyp_word\"].median()),\n",
        "                      \"q75\": float(lens[\"hyp_word\"].quantile(0.75)), \"max\": int(lens[\"hyp_word\"].max())}\n",
        "    })\n",
        "    print(\"\\n=== LENGTH STATS (tokens) ===\")\n",
        "    print({\n",
        "        \"reference\": {\"count\": int(lens.shape[0]), \"mean\": float(np.round(lens[\"ref_tok\"].mean(),3)),\n",
        "                      \"std\": float(np.round(lens[\"ref_tok\"].std(ddof=0),3)), \"min\": int(lens[\"ref_tok\"].min()),\n",
        "                      \"q25\": float(lens[\"ref_tok\"].quantile(0.25)), \"med\": float(lens[\"ref_tok\"].median()),\n",
        "                      \"q75\": float(lens[\"ref_tok\"].quantile(0.75)), \"max\": int(lens[\"ref_tok\"].max())},\n",
        "        \"summary\":  {\"count\": int(lens.shape[0]), \"mean\": float(np.round(lens[\"hyp_tok\"].mean(),3)),\n",
        "                      \"std\": float(np.round(lens[\"hyp_tok\"].std(ddof=0),3)), \"min\": int(lens[\"hyp_tok\"].min()),\n",
        "                      \"q25\": float(lens[\"hyp_tok\"].quantile(0.25)), \"med\": float(lens[\"hyp_tok\"].median()),\n",
        "                      \"q75\": float(lens[\"hyp_tok\"].quantile(0.75)), \"max\": int(lens[\"hyp_tok\"].max())}\n",
        "    })\n",
        "\n",
        "    return dict(rouge=rouge, ent=ent, ucer=ucer, best_path=best_path, details_path=details_path)\n",
        "\n",
        "# ---- RUN ----\n",
        "HYB_TEST = run_hybrid_test(N_ROWS=1000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V42_31LkOzug",
        "outputId": "948811d8-a562-4343-d24e-4d238c40c057"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] Loaded 20000 candidates (~20 per article) from:\n",
            "  /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/val_test_eval_freeze_reranker/TEST_1k/details_TEST_HYBRID_EA_ON.csv\n",
            "Stage-A selection …\n",
            "Adding base_norm (teacher-forced logp; batched) …\n",
            "Loaded frozen reranker from: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/val_test_eval_freeze_reranker/VAL_1k/frozen_reranker_weights.json\n",
            "\n",
            "=== TEST HYBRID_EA_ON (RESUMED) ===\n",
            "ROUGE : {'r1_f': 0.3224, 'r2_f': 0.1201, 'rlsum_f': 0.2954, 'rl_f': 0.2234}\n",
            "ENT   : {'TP': 1246, 'FP': 3797, 'FN': 2295, 'entP': 0.2471, 'entR': 0.3519, 'entF1': 0.2903}\n",
            "UCER  : {'rate': 0.031, 'avg_core': 0.035}\n",
            "Saved best -> /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/val_test_eval_freeze_reranker/TEST_1k/best_TEST_HYBRID_EA_ON_20250905_182753.csv\n",
            "\n",
            "=== LENGTH STATS (words) ===\n",
            "{'reference': {'count': 1000, 'mean': 34.456, 'std': 9.65, 'min': 14, 'q25': 27.0, 'med': 34.0, 'q75': 41.0, 'max': 80}, 'summary': {'count': 1000, 'mean': 45.548, 'std': 8.658, 'min': 22, 'q25': 39.0, 'med': 45.0, 'q75': 51.0, 'max': 84}}\n",
            "\n",
            "=== LENGTH STATS (tokens) ===\n",
            "{'reference': {'count': 1000, 'mean': 43.346, 'std': 11.475, 'min': 16, 'q25': 34.0, 'med': 43.0, 'q75': 51.0, 'max': 99}, 'summary': {'count': 1000, 'mean': 59.376, 'std': 9.95, 'min': 36, 'q25': 52.0, 'med': 59.0, 'q75': 67.0, 'max': 100}}\n"
          ]
        }
      ],
      "source": [
        "# ==== RESUME FROM SAVED DETAILS (no decoding) ====\n",
        "import os, re, csv, json, time, math\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "import torch\n",
        "\n",
        "# ---------------- PATHS (adapt only if yours differ) ----------------\n",
        "CKPT_DIR   = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500\"\n",
        "OUT_ROOT   = os.path.join(CKPT_DIR, \"val_test_eval_freeze_reranker\")\n",
        "TEST_DIR   = os.path.join(OUT_ROOT, \"TEST_1k\")\n",
        "VAL_DIR    = os.path.join(OUT_ROOT, \"VAL_1k\")\n",
        "\n",
        "DETAILS_CSV      = os.path.join(TEST_DIR, \"details_TEST_HYBRID_EA_ON.csv\")\n",
        "FROZEN_RERANKER  = os.path.join(VAL_DIR, \"frozen_reranker_weights.json\")\n",
        "\n",
        "assert os.path.exists(DETAILS_CSV), f\"Missing: {DETAILS_CSV}\"\n",
        "assert os.path.exists(FROZEN_RERANKER), f\"Missing: {FROZEN_RERANKER}\"\n",
        "os.makedirs(TEST_DIR, exist_ok=True)\n",
        "\n",
        "# ---------------- Decoding caps used previously ----------------\n",
        "MAX_SRC_LEN = 400\n",
        "LEN_SIGMA   = 20.0\n",
        "USE_REF_LEN_FOR_TARGET = False\n",
        "M_KEEP      = 10\n",
        "NONCORE_PEN = 2.0\n",
        "\n",
        "KEEP_LABELS = {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"DATE\",\"TIME\",\"CARDINAL\",\"MONEY\",\"PERCENT\",\"QUANTITY\",\"ORDINAL\"}\n",
        "CORE        = {\"PERSON\",\"ORG\",\"GPE\"}\n",
        "\n",
        "# ---------------- Minimal deps ----------------\n",
        "def ensure(mod, pip=None):\n",
        "    import importlib.util, sys, subprocess\n",
        "    if pip is None: pip = mod\n",
        "    if importlib.util.find_spec(mod) is None:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pip])\n",
        "\n",
        "ensure(\"spacy\", \"spacy\"); ensure(\"rouge_score\", \"rouge-score==0.1.2\"); ensure(\"transformers\", \"transformers\")\n",
        "\n",
        "import spacy\n",
        "from rouge_score import rouge_scorer\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# ---------------- IO helpers ----------------\n",
        "def robust_read_csv(path: str) -> pd.DataFrame:\n",
        "    import csv as _csv\n",
        "    try:\n",
        "        return pd.read_csv(path, engine=\"python\", quoting=_csv.QUOTE_ALL, escapechar=\"\\\\\")\n",
        "    except Exception:\n",
        "        rows=[]\n",
        "        with open(path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "            reader = _csv.reader(f, delimiter=\",\", quotechar='\"', escapechar=\"\\\\\")\n",
        "            header = [h.strip() for h in next(reader)]\n",
        "            for row in reader: rows.append(row)\n",
        "        return pd.DataFrame(rows, columns=header)\n",
        "\n",
        "def save_csv(df: pd.DataFrame, path: str):\n",
        "    tmp = path + \".tmp\"\n",
        "    df.to_csv(tmp, index=False, quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n",
        "    os.replace(tmp, path)\n",
        "\n",
        "# ---------------- spaCy + ROUGE ----------------\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "except OSError:\n",
        "    import subprocess, sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "\n",
        "SC = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\",\"rougeL\"], use_stemmer=True)\n",
        "\n",
        "def sent_split(txt):\n",
        "    ss = re.split(r'(?<=[.!?])\\s+', str(txt or \"\").strip())\n",
        "    return [t for t in ss if t.strip()]\n",
        "\n",
        "def RLsum(a,b):\n",
        "    ra = \"\\n\".join(sent_split(a)); rb = \"\\n\".join(sent_split(b))\n",
        "    return SC.score(ra, rb)[\"rougeLsum\"].fmeasure\n",
        "\n",
        "# ---------------- Tokenizer & Model (for base_norm) ----------------\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "tok   = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR).to(DEVICE).eval()\n",
        "if hasattr(model.config, \"use_cache\"): model.config.use_cache = True\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    try: torch.set_float32_matmul_precision(\"high\")\n",
        "    except: pass\n",
        "    major = torch.cuda.get_device_capability()[0]\n",
        "    model = model.to(dtype=(torch.bfloat16 if major>=8 else torch.float16))\n",
        "\n",
        "# ---------------- NER / metrics helpers ----------------\n",
        "def _ents_norm(text: str):\n",
        "    doc = nlp(str(text)); out=set()\n",
        "    for e in doc.ents:\n",
        "        if e.label_ in KEEP_LABELS:\n",
        "            out.add((\" \".join(e.text.strip().split()), e.label_))\n",
        "    return out\n",
        "\n",
        "def strict_entity_score(src, hyp):\n",
        "    S=_ents_norm(src); H=_ents_norm(hyp)\n",
        "    overlap=H&S; unsupported=H-S\n",
        "    hard_fail = any(lbl in CORE for _,lbl in unsupported)\n",
        "    score = sum(3 if lbl in CORE else 1 for _,lbl in overlap)\n",
        "    return score, hard_fail\n",
        "\n",
        "def unsupported_nonc_count(src, hyp):\n",
        "    S=_ents_norm(src); H=_ents_norm(hyp)\n",
        "    NONCORE = {\"DATE\",\"QUANTITY\",\"MONEY\",\"PERCENT\",\"CARDINAL\",\"ORDINAL\"}\n",
        "    return sum(1 for _,lbl in (H-S) if lbl in NONCORE)\n",
        "\n",
        "def lcs_span_stats(src_text, hyp_text):\n",
        "    s=str(src_text).split(); h=str(hyp_text).split()\n",
        "    n,m=len(s),len(h); dp=[[0]*(m+1) for _ in range(n+1)]\n",
        "    for i in range(n-1,-1,-1):\n",
        "        si=s[i]; row=dp[i]; row1=dp[i+1]\n",
        "        for j in range(m-1,-1,-1):\n",
        "            row[j]=row1[j+1]+1 if si==h[j] else max(row[j+1], row1[j])\n",
        "    i=j=0; blocks=[]; cur=0\n",
        "    while i<n and j<m:\n",
        "        if s[i]==h[j]: cur+=1; i+=1; j+=1\n",
        "        else:\n",
        "            if cur>=2: blocks.append(cur); cur=0\n",
        "            if dp[i+1][j]>=dp[i][j+1]: i+=1\n",
        "            else: j+=1\n",
        "    if cur>=2: blocks.append(cur)\n",
        "    cont=sum(b-1 for b in blocks)\n",
        "    av=(sum(blocks)/len(blocks)) if blocks else 1.0\n",
        "    return cont, av\n",
        "\n",
        "def minmax(v):\n",
        "    v=np.array(v, float); lo,hi=float(v.min()), float(v.max())\n",
        "    return np.zeros_like(v) if hi-lo<1e-8 else (v-lo)/(hi-lo)\n",
        "\n",
        "def toklen_text(s): return len(tok(str(s))[\"input_ids\"])\n",
        "def lead3(text):    return \" \".join(sent_split(text)[:3])\n",
        "\n",
        "def adaptive_len_target(art):\n",
        "    L = toklen_text(lead3(art))\n",
        "    return int(np.clip(round(0.9 * L), 32, 60))\n",
        "\n",
        "def rouge_means(R,H):\n",
        "    r1=r2=rls=rl=0.0\n",
        "    for r,h in zip(R,H):\n",
        "        rr=\"\\n\".join(sent_split(r)); hh=\"\\n\".join(sent_split(h))\n",
        "        s=SC.score(rr, hh)\n",
        "        r1+=s[\"rouge1\"].fmeasure; r2+=s[\"rouge2\"].fmeasure\n",
        "        rls+=s[\"rougeLsum\"].fmeasure; rl+=s[\"rougeL\"].fmeasure\n",
        "    n=len(R); return dict(r1_f=r1/n, r2_f=r2/n, rlsum_f=rls/n, rl_f=rl/n)\n",
        "\n",
        "def entPRF(R,H):\n",
        "    TP=FP=FN=0\n",
        "    for r,h in zip(R,H):\n",
        "        R_=_ents_norm(r); H_=_ents_norm(h)\n",
        "        TP+=len(H_&R_); FP+=len(H_-R_); FN+=len(R_-H_)\n",
        "    P=TP/(TP+FP) if (TP+FP) else 0.0\n",
        "    Rc=TP/(TP+FN) if (TP+FN) else 0.0\n",
        "    F=2*P*Rc/(P+Rc) if (P+Rc) else 0.0\n",
        "    return {\"TP\":TP,\"FP\":FP,\"FN\":FN,\"entP\":P,\"entR\":Rc,\"entF1\":F}\n",
        "\n",
        "def UCER(arts, hyps):\n",
        "    rates=[]; counts=[]\n",
        "    for s,h in zip(arts, hyps):\n",
        "        S=_ents_norm(s); H=_ents_norm(h)\n",
        "        bad=[(t,l) for (t,l) in (H-S) if l in CORE]\n",
        "        rates.append(1 if bad else 0); counts.append(len(bad))\n",
        "    return {\"rate\": float(np.mean(rates)), \"avg_core\": float(np.mean(counts))}\n",
        "\n",
        "# ---------------- Stage-A / base_norm / rerank ----------------\n",
        "def stageA_select(dd_all, M=M_KEEP):\n",
        "    out=[]\n",
        "    for rid, grp in dd_all.groupby(\"row_id\"):\n",
        "        art = grp.iloc[0][\"article\"]\n",
        "        tgt = (toklen_text(grp.iloc[0][\"reference\"]) if USE_REF_LEN_FOR_TARGET else adaptive_len_target(art))\n",
        "        ent_scores=[]; hard_fail=[]; contigs=[]; nonc=[]\n",
        "        for _,r in grp.iterrows():\n",
        "            s,f = strict_entity_score(r[\"article\"], r[\"summary\"])\n",
        "            c,_ = lcs_span_stats(r[\"article\"], r[\"summary\"])\n",
        "            u   = unsupported_nonc_count(r[\"article\"], r[\"summary\"])\n",
        "            ent_scores.append(s); hard_fail.append(f); contigs.append(c); nonc.append(u)\n",
        "        ent_scores=np.array(ent_scores, float); contigs=np.array(contigs, float); nonc=np.array(nonc, float)\n",
        "        L=grp[\"len_tok\"].to_numpy().astype(int)\n",
        "        len_reward = np.exp(-((L - tgt)**2) / (2*(LEN_SIGMA**2)))\n",
        "        penalty = (nonc>0).astype(float) * NONCORE_PEN\n",
        "        THC = (5.0*ent_scores) + (0.2*contigs) + (0.3*len_reward) - penalty\n",
        "        ok = grp.copy()\n",
        "        ok[\"THC\"]=THC; ok[\"unsup_nonc\"]=nonc; ok[\"fail_flag\"]=hard_fail\n",
        "        ok = ok.loc[~ok[\"fail_flag\"]].copy()\n",
        "        if ok.empty: ok = grp.copy(); ok[\"THC\"]=THC; ok[\"unsup_nonc\"]=nonc\n",
        "        out.append(ok.sort_values(\"THC\", ascending=False).head(M))\n",
        "    return pd.concat(out, ignore_index=True)\n",
        "\n",
        "def add_base_norm(stageA_df, batch_size=16):\n",
        "    out = np.zeros(len(stageA_df), dtype=np.float32)\n",
        "    idxs = stageA_df.index.to_list()\n",
        "    for s in range(0, len(idxs), batch_size):\n",
        "        ii = idxs[s:s+batch_size]\n",
        "        sub = stageA_df.loc[ii]\n",
        "        enc = tok(sub[\"article\"].tolist(), max_length=MAX_SRC_LEN, truncation=True, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
        "        dec = tok(sub[\"summary\"].tolist(), max_length=256, truncation=True, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
        "        with torch.inference_mode():\n",
        "            logits = model(**enc, labels=dec[\"input_ids\"]).logits.float()\n",
        "        shift_logits = logits[:, :-1, :]\n",
        "        shift_labels = dec[\"input_ids\"][:, 1:]\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=tok.pad_token_id, reduction=\"none\")\n",
        "        tok_loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.reshape(-1)).view(shift_labels.size(0), -1)\n",
        "        valid = (shift_labels != tok.pad_token_id).float()\n",
        "        loss_per = (tok_loss * valid).sum(dim=1) / (valid.sum(dim=1).clamp_min(1.0))\n",
        "        logp_per = -loss_per.detach().cpu().numpy()\n",
        "        L = sub[\"len_tok\"].to_numpy().clip(min=1)\n",
        "        out[s:s+len(ii)] = logp_per / (L ** 0.3)\n",
        "    stageA_df[\"base_norm\"] = out\n",
        "    return stageA_df\n",
        "\n",
        "def ent_consensus(cands):\n",
        "    E=[_ents_norm(x) for x in cands]\n",
        "    n=len(E); sc=np.zeros(n)\n",
        "    for i in range(n):\n",
        "        js=[]\n",
        "        for j in range(n):\n",
        "            if i==j: continue\n",
        "            u=E[i]|E[j]; inter=E[i]&E[j]\n",
        "            js.append(len(inter)/max(len(u),1) if u else 0.0)\n",
        "        sc[i]=np.mean(js) if js else 0.0\n",
        "    return sc\n",
        "\n",
        "def score_group_linear(grp, w):\n",
        "    art = grp.iloc[0][\"article\"]\n",
        "    ref = grp.iloc[0][\"reference\"]\n",
        "    tgt = (toklen_text(ref) if USE_REF_LEN_FOR_TARGET else adaptive_len_target(art))\n",
        "    cands = grp[\"summary\"].astype(str).tolist()\n",
        "    Ls    = grp[\"len_tok\"].astype(int).tolist()\n",
        "    lead  = \" \".join(sent_split(art)[:3])\n",
        "    leadsim = np.array([RLsum(h, lead) for h in cands])\n",
        "    n=len(cands); mbr=np.zeros(n)\n",
        "    if n>1:\n",
        "        M=np.zeros((n,n))\n",
        "        for i in range(n):\n",
        "            for j in range(i+1,n):\n",
        "                s=RLsum(cands[i], cands[j]); M[i,j]=M[j,i]=s\n",
        "        mbr=M.mean(axis=1)\n",
        "    feats = pd.DataFrame({\n",
        "        \"THC_norm\": minmax(grp[\"THC\"].values),\n",
        "        \"LeadSim\": leadsim,\n",
        "        \"MBR\": mbr,\n",
        "        \"len_reward\": minmax([math.exp(-((L - tgt)**2)/(2*(LEN_SIGMA**2))) for L in Ls]),\n",
        "        \"contig_lcs_mm\": minmax([lcs_span_stats(art, s)[0] for s in cands]),\n",
        "        \"base_norm_mm\": minmax(grp[\"base_norm\"].values),\n",
        "        \"ent_consensus\": ent_consensus(cands),\n",
        "        \"unsup_nonc_mm\": minmax(grp[\"unsup_nonc\"].astype(float).values),\n",
        "    })\n",
        "    cols=list(w[\"coefs\"].keys())\n",
        "    score = w[\"intercept\"] + sum(feats[c].values * w[\"coefs\"][c] for c in cols)\n",
        "    k=int(np.argmax(score))\n",
        "    cand = grp.iloc[[k]].copy()\n",
        "\n",
        "    # UCER-safe fallback\n",
        "    S=_ents_norm(art); H=_ents_norm(cand[\"summary\"].iloc[0])\n",
        "    if any(lbl in CORE for _,lbl in (H-S)):\n",
        "        safe_idx=[]; s_vals=np.array(score)\n",
        "        for i in range(len(grp)):\n",
        "            Hi=_ents_norm(grp.iloc[i][\"summary\"])\n",
        "            if not any(lbl in CORE for _,lbl in (Hi-S)): safe_idx.append(i)\n",
        "        if safe_idx:\n",
        "            k=int(max(safe_idx, key=lambda i: s_vals[i]))\n",
        "            cand=grp.iloc[[k]].copy()\n",
        "    return cand\n",
        "\n",
        "# ---------------- RUN: load details -> Stage-A -> rerank -> metrics ----------------\n",
        "dd = robust_read_csv(DETAILS_CSV)\n",
        "\n",
        "# quick sanity checks\n",
        "assert dd[\"row_id\"].nunique() == 1000, f\"Expected 1000 rows, got {dd['row_id'].nunique()}\"\n",
        "per = dd.shape[0] // dd[\"row_id\"].nunique()\n",
        "print(f\"[OK] Loaded {dd.shape[0]} candidates (~{per} per article) from:\\n  {DETAILS_CSV}\")\n",
        "\n",
        "print(\"Stage-A selection …\")\n",
        "st = stageA_select(dd, M=M_KEEP)\n",
        "\n",
        "print(\"Adding base_norm (teacher-forced logp; batched) …\")\n",
        "st = add_base_norm(st, batch_size=16)\n",
        "\n",
        "# load frozen linear reranker\n",
        "W = json.load(open(FROZEN_RERANKER, \"r\"))\n",
        "weights = W.get(\"weights\", W)\n",
        "print(\"Loaded frozen reranker from:\", FROZEN_RERANKER)\n",
        "\n",
        "best=[]\n",
        "for rid, grp in st.groupby(\"row_id\"):\n",
        "    best.append(score_group_linear(grp, weights))\n",
        "best = pd.concat(best, ignore_index=True)\n",
        "\n",
        "# Metrics\n",
        "R = best[\"reference\"].astype(str).tolist()\n",
        "H = best[\"summary\"].astype(str).tolist()\n",
        "A = best[\"article\"].astype(str).tolist()\n",
        "\n",
        "rouge = {k: round(v,4) for k,v in rouge_means(R,H).items()}\n",
        "ent   = {k:(round(v,4) if isinstance(v,float) else v) for k,v in entPRF(R,H).items()}\n",
        "ucer  = {k: round(v,4) for k,v in UCER(A,H).items()}\n",
        "\n",
        "stamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "best_path = os.path.join(TEST_DIR, f\"best_TEST_HYBRID_EA_ON_{stamp}.csv\")\n",
        "save_csv(best, best_path)\n",
        "\n",
        "# Length stats\n",
        "def word_len(s): return len(str(s).split())\n",
        "lens = pd.DataFrame({\n",
        "    \"ref_tok\": [toklen_text(r) for r in R],\n",
        "    \"hyp_tok\": [toklen_text(hh) for hh in H],\n",
        "    \"ref_word\": [word_len(r) for r in R],\n",
        "    \"hyp_word\": [word_len(hh) for hh in H],\n",
        "})\n",
        "\n",
        "print(\"\\n=== TEST HYBRID_EA_ON (RESUMED) ===\")\n",
        "print(\"ROUGE :\", rouge)\n",
        "print(\"ENT   :\", ent)\n",
        "print(\"UCER  :\", ucer)\n",
        "print(\"Saved best ->\", best_path)\n",
        "\n",
        "print(\"\\n=== LENGTH STATS (words) ===\")\n",
        "print({\n",
        "    \"reference\": {\"count\": int(lens.shape[0]), \"mean\": float(np.round(lens[\"ref_word\"].mean(),3)),\n",
        "                  \"std\": float(np.round(lens[\"ref_word\"].std(ddof=0),3)), \"min\": int(lens[\"ref_word\"].min()),\n",
        "                  \"q25\": float(lens[\"ref_word\"].quantile(0.25)), \"med\": float(lens[\"ref_word\"].median()),\n",
        "                  \"q75\": float(lens[\"ref_word\"].quantile(0.75)), \"max\": int(lens[\"ref_word\"].max())},\n",
        "    \"summary\":  {\"count\": int(lens.shape[0]), \"mean\": float(np.round(lens[\"hyp_word\"].mean(),3)),\n",
        "                  \"std\": float(np.round(lens[\"hyp_word\"].std(ddof=0),3)), \"min\": int(lens[\"hyp_word\"].min()),\n",
        "                  \"q25\": float(lens[\"hyp_word\"].quantile(0.25)), \"med\": float(lens[\"hyp_word\"].median()),\n",
        "                  \"q75\": float(lens[\"hyp_word\"].quantile(0.75)), \"max\": int(lens[\"hyp_word\"].max())}\n",
        "})\n",
        "\n",
        "print(\"\\n=== LENGTH STATS (tokens) ===\")\n",
        "print({\n",
        "    \"reference\": {\"count\": int(lens.shape[0]), \"mean\": float(np.round(lens[\"ref_tok\"].mean(),3)),\n",
        "                  \"std\": float(np.round(lens[\"ref_tok\"].std(ddof=0),3)), \"min\": int(lens[\"ref_tok\"].min()),\n",
        "                  \"q25\": float(lens[\"ref_tok\"].quantile(0.25)), \"med\": float(lens[\"ref_tok\"].median()),\n",
        "                  \"q75\": float(lens[\"ref_tok\"].quantile(0.75)), \"max\": int(lens[\"ref_tok\"].max())},\n",
        "    \"summary\":  {\"count\": int(lens.shape[0]), \"mean\": float(np.round(lens[\"hyp_tok\"].mean(),3)),\n",
        "                  \"std\": float(np.round(lens[\"hyp_tok\"].std(ddof=0),3)), \"min\": int(lens[\"hyp_tok\"].min()),\n",
        "                  \"q25\": float(lens[\"hyp_tok\"].quantile(0.25)), \"med\": float(lens[\"hyp_tok\"].median()),\n",
        "                  \"q75\": float(lens[\"hyp_tok\"].quantile(0.75)), \"max\": int(lens[\"hyp_tok\"].max())}\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEeL51D_LpK2",
        "outputId": "3ff7aa98-b035-489e-a148-10e0b35590dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UCER (baseline): {'rate': 0.031, 'avg_core': 0.035}\n"
          ]
        }
      ],
      "source": [
        "import csv, re, pandas as pd\n",
        "import spacy\n",
        "\n",
        "KEEP_LABELS = {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"DATE\",\"TIME\",\"CARDINAL\",\"MONEY\",\"PERCENT\",\"QUANTITY\",\"ORDINAL\"}\n",
        "CORE = {\"PERSON\",\"ORG\",\"GPE\"}\n",
        "\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "except OSError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "\n",
        "def _ents_norm(text: str):\n",
        "    doc = nlp(str(text)); out=set()\n",
        "    for e in doc.ents:\n",
        "        if e.label_ in KEEP_LABELS:\n",
        "            out.add((\" \".join(e.text.strip().split()), e.label_))\n",
        "    return out\n",
        "\n",
        "def UCER_from_pairs(arts, hyps):\n",
        "    rates=[]; counts=[]\n",
        "    for s,h in zip(arts, hyps):\n",
        "        S=_ents_norm(s); H=_ents_norm(h)\n",
        "        bad=[(t,l) for (t,l) in (H-S) if l in CORE]\n",
        "        rates.append(1 if bad else 0); counts.append(len(bad))\n",
        "    return {\"rate\": float(sum(rates)/len(rates)), \"avg_core\": float(sum(counts)/len(counts))}\n",
        "\n",
        "BEST_BASELINE = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/val_test_eval_freeze_reranker/TEST_1k/best_TEST_HYBRID_EA_ON_20250905_182753.csv\"\n",
        "import glob\n",
        "best_path = sorted(glob.glob(BEST_BASELINE))[-1]\n",
        "df = pd.read_csv(best_path, engine=\"python\", quoting=csv.QUOTE_ALL, escapechar=\"\\\\\")\n",
        "assert {\"article\",\"summary\"}.issubset(df.columns), df.columns\n",
        "\n",
        "ucer = UCER_from_pairs(df[\"article\"].tolist(), df[\"summary\"].tolist())\n",
        "print(\"UCER (baseline):\", {k: round(v,4) for k,v in ucer.items()})\n",
        "#this is hybrid not baseline, baseline is the next cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxk7HwQp9WNW",
        "outputId": "b7eef69a-1f1c-4c8c-bd8e-660d6b21bbc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoding TEST Baseline (EA=OFF)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Group Beam Search is scheduled to be moved to a `custom_generate` repository in v4.55.0. To prevent loss of backward compatibility, add `trust_remote_code=True` to your `generate` call.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " -> rows: 20000 | saved: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/val_test_eval_freeze_reranker/TEST_1k/details_TEST_BASELINE_EA_OFF.csv\n",
            "Stage-A ...\n",
            "Loaded frozen reranker from: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/val_test_eval_freeze_reranker/VAL_1k/frozen_reranker_weights.json\n",
            "\n",
            "=== TEST BASELINE_EA_OFF ===\n",
            "ROUGE : {'r1_f': 0.3207, 'r2_f': 0.1199, 'rlsum_f': 0.2935, 'rl_f': 0.2219}\n",
            "ENT   : {'TP': 1233, 'FP': 3723, 'FN': 2308, 'entP': 0.2488, 'entR': 0.3482, 'entF1': 0.2902}\n",
            "UCER  : {'rate': 0.032, 'avg_core': 0.038}\n",
            "Saved best -> /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/val_test_eval_freeze_reranker/TEST_1k/best_TEST_BASELINE_EA_OFF_20250906_153101.csv\n",
            "\n",
            "=== LENGTH STATS (words) ===\n",
            "{'reference': {'count': 1000, 'mean': 34.456, 'std': 9.65, 'min': 14, 'q25': 27.0, 'med': 34.0, 'q75': 41.0, 'max': 80}, 'summary': {'count': 1000, 'mean': 45.29, 'std': 8.683, 'min': 22, 'q25': 39.0, 'med': 45.0, 'q75': 51.0, 'max': 87}}\n",
            "\n",
            "=== LENGTH STATS (tokens) ===\n",
            "{'reference': {'count': 1000, 'mean': 43.346, 'std': 11.475, 'min': 16, 'q25': 34.0, 'med': 43.0, 'q75': 51.0, 'max': 99}, 'summary': {'count': 1000, 'mean': 58.98, 'std': 9.895, 'min': 36, 'q25': 52.0, 'med': 59.0, 'q75': 67.0, 'max': 100}}\n"
          ]
        }
      ],
      "source": [
        "# ==== BASELINE (EA=OFF) — TEST 1k with frozen VAL reranker ====\n",
        "import os, sys, re, csv, json, time, math, subprocess\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "import torch\n",
        "\n",
        "# ---------------- CONFIG (freeze quality-affecting knobs) ----------------\n",
        "SEED = 0\n",
        "random = np.random\n",
        "random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "CKPT_DIR   = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500\"\n",
        "TEST_CSV   = \"/content/drive/MyDrive/cleaned_outputs/naa/test_cleaned_safe_boilerfree_sourcetag_near_deduped_fast.csv\"\n",
        "OUT_ROOT   = os.path.join(CKPT_DIR, \"val_test_eval_freeze_reranker\")\n",
        "TEST_DIR   = os.path.join(OUT_ROOT, \"TEST_1k\")\n",
        "VAL_DIR    = os.path.join(OUT_ROOT, \"VAL_1k\")  # where the reranker was frozen on VAL\n",
        "FROZEN_RERANKER = os.path.join(VAL_DIR, \"frozen_reranker_weights.json\")\n",
        "\n",
        "os.makedirs(TEST_DIR, exist_ok=True)\n",
        "\n",
        "# Decoding (identical to VAL)\n",
        "MAX_SRC_LEN = 400\n",
        "CN2_LONG = dict(num_beams=10, num_return_sequences=10, num_beam_groups=5, diversity_penalty=0.3,\n",
        "                max_new_tokens=100, min_new_tokens=55, no_repeat_ngram_size=3,\n",
        "                length_penalty=2.0, early_stopping=True, return_dict_in_generate=False, output_scores=False)\n",
        "CN_MED   = dict(num_beams=10, num_return_sequences=10, num_beam_groups=5, diversity_penalty=0.3,\n",
        "                max_new_tokens=70,  min_new_tokens=35, no_repeat_ngram_size=3,\n",
        "                length_penalty=1.6, early_stopping=True, return_dict_in_generate=False, output_scores=False)\n",
        "\n",
        "# Throughput (safe defaults; you can raise if RAM allows)\n",
        "BATCH_LONG = 32\n",
        "BATCH_MED  = 64\n",
        "\n",
        "# Stage-A / metrics constants (same strictness as VAL)\n",
        "M_KEEP       = 10\n",
        "NONCORE_PEN  = 2.0\n",
        "LEN_SIGMA    = 20.0\n",
        "USE_REF_LEN_FOR_TARGET = False\n",
        "\n",
        "KEEP_LABELS = {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"DATE\",\"TIME\",\"CARDINAL\",\"MONEY\",\"PERCENT\",\"QUANTITY\",\"ORDINAL\"}\n",
        "CORE        = {\"PERSON\",\"ORG\",\"GPE\"}\n",
        "\n",
        "# ---------------- Minimal deps ----------------\n",
        "def ensure(mod, pip=None):\n",
        "    import importlib.util, sys, subprocess\n",
        "    if pip is None: pip = mod\n",
        "    if importlib.util.find_spec(mod) is None:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pip])\n",
        "\n",
        "ensure(\"spacy\", \"spacy\"); ensure(\"rouge_score\", \"rouge-score==0.1.2\"); ensure(\"scikit_learn\", \"scikit-learn\"); ensure(\"transformers\", \"transformers\")\n",
        "\n",
        "import spacy\n",
        "from rouge_score import rouge_scorer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# ---------------- IO helpers ----------------\n",
        "def robust_read_csv(path: str) -> pd.DataFrame:\n",
        "    import csv as _csv\n",
        "    try:\n",
        "        df = pd.read_csv(path, engine=\"python\", quoting=_csv.QUOTE_ALL, escapechar=\"\\\\\")\n",
        "        cols = {c.lower(): c for c in df.columns}\n",
        "        assert \"article\" in cols and \"highlights\" in cols, f\"columns={list(df.columns)}\"\n",
        "        return df[[cols[\"article\"], cols[\"highlights\"]]].rename(columns={\"article\":\"article\",\"highlights\":\"reference\"})\n",
        "    except Exception:\n",
        "        rows = []\n",
        "        with open(path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "            reader = _csv.reader(f, delimiter=\",\", quotechar='\"', escapechar=\"\\\\\")\n",
        "            header = [h.strip().lower() for h in next(reader)]\n",
        "            i_art, i_sum = header.index(\"article\"), header.index(\"highlights\")\n",
        "            for row in reader: rows.append([row[i_art], row[i_sum]])\n",
        "        return pd.DataFrame(rows, columns=[\"article\",\"reference\"])\n",
        "\n",
        "def save_csv(df: pd.DataFrame, path: str):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    tmp = path + \".tmp\"\n",
        "    df.to_csv(tmp, index=False, quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n",
        "    os.replace(tmp, path)\n",
        "\n",
        "def save_json(obj, path: str):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    with open(path, \"w\") as f: json.dump(obj, f, indent=2)\n",
        "\n",
        "# ---------------- spaCy + ROUGE ----------------\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "except OSError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"textcat\"])\n",
        "\n",
        "SC = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\",\"rougeL\"], use_stemmer=True)\n",
        "def sent_split(txt):\n",
        "    ss = re.split(r'(?<=[.!?])\\s+', str(txt or \"\").strip())\n",
        "    return [t for t in ss if t.strip()]\n",
        "def RLsum(a,b):\n",
        "    ra = \"\\n\".join(sent_split(a)); rb = \"\\n\".join(sent_split(b))\n",
        "    return SC.score(ra, rb)[\"rougeLsum\"].fmeasure\n",
        "\n",
        "# ---------------- Tokenizer & Model ----------------\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "tok   = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(CKPT_DIR).to(DEVICE).eval()\n",
        "if hasattr(model.config, \"use_cache\"): model.config.use_cache = True\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    try: torch.set_float32_matmul_precision(\"high\")\n",
        "    except: pass\n",
        "    major = torch.cuda.get_device_capability()[0]\n",
        "    model = model.to(dtype=(torch.bfloat16 if major>=8 else torch.float16))\n",
        "\n",
        "# ---------------- NER / features / metrics ----------------\n",
        "def _ents_norm(text: str):\n",
        "    doc = nlp(str(text)); out=set()\n",
        "    for e in doc.ents:\n",
        "        if e.label_ in KEEP_LABELS:\n",
        "            out.add((\" \".join(e.text.strip().split()), e.label_))\n",
        "    return out\n",
        "\n",
        "def strict_entity_score(src, hyp):\n",
        "    S=_ents_norm(src); H=_ents_norm(hyp)\n",
        "    overlap=H&S; unsupported=H-S\n",
        "    hard_fail = any(lbl in CORE for _,lbl in unsupported)\n",
        "    score = sum(3 if lbl in CORE else 1 for _,lbl in overlap)\n",
        "    return score, hard_fail\n",
        "\n",
        "def unsupported_nonc_count(src, hyp):\n",
        "    S=_ents_norm(src); H=_ents_norm(hyp)\n",
        "    NONCORE = {\"DATE\",\"QUANTITY\",\"MONEY\",\"PERCENT\",\"CARDINAL\",\"ORDINAL\"}\n",
        "    return sum(1 for _,lbl in (H-S) if lbl in NONCORE)\n",
        "\n",
        "def lcs_span_stats(src_text, hyp_text):\n",
        "    s=str(src_text).split(); h=str(hyp_text).split()\n",
        "    n,m=len(s),len(h); dp=[[0]*(m+1) for _ in range(n+1)]\n",
        "    for i in range(n-1,-1,-1):\n",
        "        si=s[i]; row=dp[i]; row1=dp[i+1]\n",
        "        for j in range(m-1,-1,-1):\n",
        "            row[j]=row1[j+1]+1 if si==h[j] else max(row[j+1], row1[j])\n",
        "    i=j=0; blocks=[]; cur=0\n",
        "    while i<n and j<m:\n",
        "        if s[i]==h[j]: cur+=1; i+=1; j+=1\n",
        "        else:\n",
        "            if cur>=2: blocks.append(cur); cur=0\n",
        "            if dp[i+1][j]>=dp[i][j+1]: i+=1\n",
        "            else: j+=1\n",
        "    if cur>=2: blocks.append(cur)\n",
        "    cont=sum(b-1 for b in blocks)\n",
        "    av=(sum(blocks)/len(blocks)) if blocks else 1.0\n",
        "    return cont, av\n",
        "\n",
        "def minmax(v):\n",
        "    v=np.array(v, float); lo,hi=float(v.min()), float(v.max())\n",
        "    return np.zeros_like(v) if hi-lo<1e-8 else (v-lo)/(hi-lo)\n",
        "\n",
        "def toklen_text(s): return len(tok(str(s))[\"input_ids\"])\n",
        "def lead3(text):    return \" \".join(sent_split(text)[:3])\n",
        "def adaptive_len_target(art):\n",
        "    L = toklen_text(lead3(art))\n",
        "    return int(np.clip(round(0.9 * L), 32, 60))\n",
        "\n",
        "def rouge_means(R,H):\n",
        "    r1=r2=rls=rl=0.0\n",
        "    for r,h in zip(R,H):\n",
        "        rr=\"\\n\".join(sent_split(r)); hh=\"\\n\".join(sent_split(h))\n",
        "        s=SC.score(rr, hh)\n",
        "        r1+=s[\"rouge1\"].fmeasure; r2+=s[\"rouge2\"].fmeasure\n",
        "        rls+=s[\"rougeLsum\"].fmeasure; rl+=s[\"rougeL\"].fmeasure\n",
        "    n=len(R); return dict(r1_f=r1/n, r2_f=r2/n, rlsum_f=rls/n, rl_f=rl/n)\n",
        "\n",
        "def entPRF(R,H):\n",
        "    TP=FP=FN=0\n",
        "    for r,h in zip(R,H):\n",
        "        R_=_ents_norm(r); H_=_ents_norm(h)\n",
        "        TP+=len(H_&R_); FP+=len(H_-R_); FN+=len(R_-H_)\n",
        "    P=TP/(TP+FP) if (TP+FP) else 0.0\n",
        "    Rc=TP/(TP+FN) if (TP+FN) else 0.0\n",
        "    F=2*P*Rc/(P+Rc) if (P+Rc) else 0.0\n",
        "    return {\"TP\":TP,\"FP\":FP,\"FN\":FN,\"entP\":P,\"entR\":Rc,\"entF1\":F}\n",
        "\n",
        "def UCER(arts, hyps):\n",
        "    rates=[]; counts=[]\n",
        "    for s,h in zip(arts, hyps):\n",
        "        S=_ents_norm(s); H=_ents_norm(h)\n",
        "        bad=[(t,l) for (t,l) in (H-S) if l in CORE]\n",
        "        rates.append(1 if bad else 0); counts.append(len(bad))\n",
        "    return {\"rate\": float(np.mean(rates)), \"avg_core\": float(np.mean(counts))}\n",
        "\n",
        "# ---------------- Decoding (NO EA) ----------------\n",
        "def pretokenize_articles(articles):\n",
        "    enc = tok(articles, max_length=MAX_SRC_LEN, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "    return {k: v.to(DEVICE) for k,v in enc.items()}\n",
        "\n",
        "@torch.inference_mode()\n",
        "def run_pool(enc_slice, decode_args):\n",
        "    out = model.generate(**enc_slice, **decode_args)\n",
        "    return out if isinstance(out, torch.Tensor) else out.sequences\n",
        "\n",
        "def decode_two_pools_noEA(articles, references):\n",
        "    enc_all = pretokenize_articles(articles)\n",
        "    rows=[]\n",
        "    # CN2_LONG\n",
        "    for s in range(0, len(articles), BATCH_LONG):\n",
        "        e=min(len(articles), s+BATCH_LONG)\n",
        "        enc = {k: v[s:e] for k,v in enc_all.items()}\n",
        "        seqs = run_pool(enc, CN2_LONG)\n",
        "        K = CN2_LONG[\"num_return_sequences\"]; bs=e-s\n",
        "        seqs = seqs.view(bs, K, -1)\n",
        "        for bi in range(bs):\n",
        "            art=articles[s+bi]; ref=references[s+bi]; rid=s+bi\n",
        "            for k in range(K):\n",
        "                ids = seqs[bi,k]\n",
        "                hyp = tok.decode(ids, skip_special_tokens=True)\n",
        "                rows.append({\"row_id\": rid, \"pool\":\"CN2_LONG\", \"candidate_k\":k,\n",
        "                             \"article\": art, \"reference\": ref, \"summary\": hyp,\n",
        "                             \"len_tok\": max(int(ids.size(0)-1),1)})\n",
        "    # CN_MED\n",
        "    for s in range(0, len(articles), BATCH_MED):\n",
        "        e=min(len(articles), s+BATCH_MED)\n",
        "        enc = {k: v[s:e] for k,v in enc_all.items()}\n",
        "        seqs = run_pool(enc, CN_MED)\n",
        "        K = CN_MED[\"num_return_sequences\"]; bs=e-s\n",
        "        seqs = seqs.view(bs, K, -1)\n",
        "        for bi in range(bs):\n",
        "            art=articles[s+bi]; ref=references[s+bi]; rid=s+bi\n",
        "            for k in range(K):\n",
        "                ids = seqs[bi,k]\n",
        "                hyp = tok.decode(ids, skip_special_tokens=True)\n",
        "                rows.append({\"row_id\": rid, \"pool\":\"CN_MED\", \"candidate_k\":k,\n",
        "                             \"article\": art, \"reference\": ref, \"summary\": hyp,\n",
        "                             \"len_tok\": max(int(ids.size(0)-1),1)})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# ---------------- Stage-A / base_norm / features / rerank ----------------\n",
        "def stageA_select(dd_all, M=M_KEEP):\n",
        "    out=[]\n",
        "    for rid, grp in dd_all.groupby(\"row_id\"):\n",
        "        art = grp.iloc[0][\"article\"]\n",
        "        tgt = (toklen_text(grp.iloc[0][\"reference\"]) if USE_REF_LEN_FOR_TARGET else adaptive_len_target(art))\n",
        "        ent_scores=[]; hard_fail=[]; contigs=[]; nonc=[]\n",
        "        for _,r in grp.iterrows():\n",
        "            s,f = strict_entity_score(r[\"article\"], r[\"summary\"])\n",
        "            c,_ = lcs_span_stats(r[\"article\"], r[\"summary\"])\n",
        "            u   = unsupported_nonc_count(r[\"article\"], r[\"summary\"])\n",
        "            ent_scores.append(s); hard_fail.append(f); contigs.append(c); nonc.append(u)\n",
        "        ent_scores=np.array(ent_scores, float); contigs=np.array(contigs, float); nonc=np.array(nonc, float)\n",
        "        L=grp[\"len_tok\"].to_numpy().astype(int)\n",
        "        len_reward = np.exp(-((L - tgt)**2) / (2*(LEN_SIGMA**2)))\n",
        "        penalty = (nonc>0).astype(float) * NONCORE_PEN\n",
        "        THC = (5.0*ent_scores) + (0.2*contigs) + (0.3*len_reward) - penalty\n",
        "        ok = grp.copy()\n",
        "        ok[\"THC\"]=THC; ok[\"unsup_nonc\"]=nonc; ok[\"fail_flag\"]=hard_fail\n",
        "        ok = ok.loc[~ok[\"fail_flag\"]].copy()\n",
        "        if ok.empty: ok = grp.copy(); ok[\"THC\"]=THC; ok[\"unsup_nonc\"]=nonc\n",
        "        out.append(ok.sort_values(\"THC\", ascending=False).head(M))\n",
        "    return pd.concat(out, ignore_index=True)\n",
        "\n",
        "def add_base_norm(stageA_df, batch_size=16):\n",
        "    out = np.zeros(len(stageA_df), dtype=np.float32)\n",
        "    idxs = stageA_df.index.to_list()\n",
        "    for s in range(0, len(idxs), batch_size):\n",
        "        ii = idxs[s:s+batch_size]\n",
        "        sub = stageA_df.loc[ii]\n",
        "        enc = tok(sub[\"article\"].tolist(), max_length=MAX_SRC_LEN, truncation=True, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
        "        dec = tok(sub[\"summary\"].tolist(), max_length=256, truncation=True, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
        "        with torch.inference_mode():\n",
        "            logits = model(**enc, labels=dec[\"input_ids\"]).logits.float()\n",
        "        shift_logits = logits[:, :-1, :]\n",
        "        shift_labels = dec[\"input_ids\"][:, 1:]\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=tok.pad_token_id, reduction=\"none\")\n",
        "        tok_loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.reshape(-1)).view(shift_labels.size(0), -1)\n",
        "        valid = (shift_labels != tok.pad_token_id).float()\n",
        "        loss_per = (tok_loss * valid).sum(dim=1) / (valid.sum(dim=1).clamp_min(1.0))\n",
        "        logp_per = -loss_per.detach().cpu().numpy()\n",
        "        L = sub[\"len_tok\"].to_numpy().clip(min=1)\n",
        "        out[s:s+len(ii)] = logp_per / (L ** 0.3)\n",
        "    stageA_df[\"base_norm\"] = out\n",
        "    return stageA_df\n",
        "\n",
        "RERANK_FEATURES = [\"THC_norm\",\"LeadSim\",\"MBR\",\"len_reward\",\"contig_lcs_mm\",\"base_norm_mm\",\"ent_consensus\",\"unsup_nonc_mm\"]\n",
        "\n",
        "def ent_consensus(cands):\n",
        "    E=[_ents_norm(x) for x in cands]\n",
        "    n=len(E); sc=np.zeros(n)\n",
        "    for i in range(n):\n",
        "        js=[]\n",
        "        for j in range(n):\n",
        "            if i==j: continue\n",
        "            u=E[i]|E[j]; inter=E[i]&E[j]\n",
        "            js.append(len(inter)/max(len(u),1) if u else 0.0)\n",
        "        sc[i]=np.mean(js) if js else 0.0\n",
        "    return sc\n",
        "\n",
        "def build_features(stageA_df, with_labels=True):\n",
        "    feat_rows=[]\n",
        "    for rid, grp in stageA_df.groupby(\"row_id\"):\n",
        "        art = grp.iloc[0][\"article\"]; ref = grp.iloc[0][\"reference\"]\n",
        "        tgt = (toklen_text(ref) if USE_REF_LEN_FOR_TARGET else adaptive_len_target(art))\n",
        "        cands = grp[\"summary\"].astype(str).tolist()\n",
        "        Ls    = grp[\"len_tok\"].astype(int).tolist()\n",
        "        lead  = \" \".join(sent_split(art)[:3])\n",
        "        leadsim = np.array([RLsum(h, lead) for h in cands])\n",
        "        n=len(cands); mbr=np.zeros(n)\n",
        "        if n>1:\n",
        "            M=np.zeros((n,n))\n",
        "            for i in range(n):\n",
        "                for j in range(i+1,n):\n",
        "                    s=RLsum(cands[i], cands[j]); M[i,j]=M[j,i]=s\n",
        "            mbr=M.mean(axis=1)\n",
        "        feats = pd.DataFrame({\n",
        "            \"row_id\": grp[\"row_id\"].values,\n",
        "            \"summary\": grp[\"summary\"].values,\n",
        "            \"THC_norm\": minmax(grp[\"THC\"].values),\n",
        "            \"LeadSim\": leadsim,\n",
        "            \"MBR\": mbr,\n",
        "            \"len_reward\": minmax([math.exp(-((L - tgt)**2) / (2*(LEN_SIGMA**2))) for L in Ls]),\n",
        "            \"contig_lcs_mm\": minmax([lcs_span_stats(art, s)[0] for s in cands]),\n",
        "            \"base_norm_mm\": minmax(grp[\"base_norm\"].values),\n",
        "            \"ent_consensus\": ent_consensus(cands),\n",
        "            \"unsup_nonc_mm\": minmax(grp[\"unsup_nonc\"].astype(float).values),\n",
        "        })\n",
        "        if with_labels:\n",
        "            feats[\"rl_ref\"] = [RLsum(ref, h) for h in cands]\n",
        "        feat_rows.append(feats)\n",
        "    return pd.concat(feat_rows, ignore_index=True)\n",
        "\n",
        "def score_group_linear(grp, w, art_text=None):\n",
        "    art = grp.iloc[0][\"article\"] if art_text is None else art_text\n",
        "    ref = grp.iloc[0][\"reference\"]\n",
        "    tgt = (toklen_text(ref) if USE_REF_LEN_FOR_TARGET else adaptive_len_target(art))\n",
        "    cands = grp[\"summary\"].astype(str).tolist()\n",
        "    Ls    = grp[\"len_tok\"].astype(int).tolist()\n",
        "    lead  = \" \".join(sent_split(art)[:3])\n",
        "    leadsim = np.array([RLsum(h, lead) for h in cands])\n",
        "    n=len(cands); mbr=np.zeros(n)\n",
        "    if n>1:\n",
        "        M=np.zeros((n,n))\n",
        "        for i in range(n):\n",
        "            for j in range(i+1,n):\n",
        "                s=RLsum(cands[i], cands[j]); M[i,j]=M[j,i]=s\n",
        "        mbr=M.mean(axis=1)\n",
        "    feats = pd.DataFrame({\n",
        "        \"THC_norm\": minmax(grp[\"THC\"].values),\n",
        "        \"LeadSim\": leadsim,\n",
        "        \"MBR\": mbr,\n",
        "        \"len_reward\": minmax([math.exp(-((L - tgt)**2)/(2*(LEN_SIGMA**2))) for L in Ls]),\n",
        "        \"contig_lcs_mm\": minmax([lcs_span_stats(art, s)[0] for s in cands]),\n",
        "        \"base_norm_mm\": minmax(grp[\"base_norm\"].values),\n",
        "        \"ent_consensus\": ent_consensus(cands),\n",
        "        \"unsup_nonc_mm\": minmax(grp[\"unsup_nonc\"].astype(float).values),\n",
        "    })\n",
        "    cols=list(w[\"coefs\"].keys())\n",
        "    score = w[\"intercept\"] + sum(feats[c].values * w[\"coefs\"][c] for c in cols)\n",
        "    k=int(np.argmax(score))\n",
        "    cand = grp.iloc[[k]].copy()\n",
        "    # UCER-safe fallback\n",
        "    S=_ents_norm(art); H=_ents_norm(cand[\"summary\"].iloc[0])\n",
        "    if any(lbl in CORE for _,lbl in (H-S)):\n",
        "        safe_idx=[]; s_vals=np.array(score)\n",
        "        for i in range(len(grp)):\n",
        "            Hi=_ents_norm(grp.iloc[i][\"summary\"])\n",
        "            if not any(lbl in CORE for _,lbl in (Hi-S)): safe_idx.append(i)\n",
        "        if safe_idx:\n",
        "            k=int(max(safe_idx, key=lambda i: s_vals[i]))\n",
        "            cand=grp.iloc[[k]].copy()\n",
        "    return cand\n",
        "\n",
        "# ---------------- Load TEST and run ----------------\n",
        "def run_baseline_test(N_ROWS=1000):\n",
        "    df_all = robust_read_csv(TEST_CSV)\n",
        "    df = df_all.head(N_ROWS).reset_index(drop=False).rename(columns={\"index\":\"row_id\"})\n",
        "    arts = df[\"article\"].astype(str).tolist()\n",
        "    refs = df[\"reference\"].astype(str).tolist()\n",
        "\n",
        "    print(\"Decoding TEST Baseline (EA=OFF)...\")\n",
        "    dd = decode_two_pools_noEA(arts, refs)\n",
        "    details_path = os.path.join(TEST_DIR, \"details_TEST_BASELINE_EA_OFF.csv\")\n",
        "    save_csv(dd, details_path)\n",
        "    print(\" -> rows:\", len(dd), \"| saved:\", details_path)\n",
        "\n",
        "    print(\"Stage-A ...\")\n",
        "    st = stageA_select(dd, M=M_KEEP)\n",
        "    st = add_base_norm(st, batch_size=16)\n",
        "\n",
        "    # Load frozen reranker (from VAL)\n",
        "    W = json.load(open(FROZEN_RERANKER, \"r\"))\n",
        "    weights = W.get(\"weights\", W)\n",
        "    print(\"Loaded frozen reranker from:\", FROZEN_RERANKER)\n",
        "\n",
        "    best=[]\n",
        "    for rid, grp in st.groupby(\"row_id\"):\n",
        "        best.append(score_group_linear(grp, weights))\n",
        "    best = pd.concat(best, ignore_index=True)\n",
        "\n",
        "    # Metrics\n",
        "    R = best[\"reference\"].astype(str).tolist()\n",
        "    H = best[\"summary\"].astype(str).tolist()\n",
        "    A = best[\"article\"].astype(str).tolist()\n",
        "    rouge = {k: round(v,4) for k,v in rouge_means(R,H).items()}\n",
        "    ent   = {k:(round(v,4) if isinstance(v,float) else v) for k,v in entPRF(R,H).items()}\n",
        "    ucer  = {k: round(v,4) for k,v in UCER(A,H).items()}\n",
        "\n",
        "    stamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    best_path = os.path.join(TEST_DIR, f\"best_TEST_BASELINE_EA_OFF_{stamp}.csv\")\n",
        "    save_csv(best, best_path)\n",
        "\n",
        "    # Length stats\n",
        "    def word_len(s): return len(str(s).split())\n",
        "    lens = pd.DataFrame({\n",
        "        \"ref_tok\": [toklen_text(r) for r in R],\n",
        "        \"hyp_tok\": [toklen_text(hh) for hh in H],\n",
        "        \"ref_word\": [word_len(r) for r in R],\n",
        "        \"hyp_word\": [word_len(hh) for hh in H],\n",
        "    })\n",
        "\n",
        "    print(\"\\n=== TEST BASELINE_EA_OFF ===\")\n",
        "    print(\"ROUGE :\", rouge)\n",
        "    print(\"ENT   :\", ent)\n",
        "    print(\"UCER  :\", ucer)\n",
        "    print(\"Saved best ->\", best_path)\n",
        "    print(\"\\n=== LENGTH STATS (words) ===\")\n",
        "    print({\n",
        "        \"reference\": {\"count\": int(lens.shape[0]), \"mean\": float(np.round(lens[\"ref_word\"].mean(),3)),\n",
        "                      \"std\": float(np.round(lens[\"ref_word\"].std(ddof=0),3)), \"min\": int(lens[\"ref_word\"].min()),\n",
        "                      \"q25\": float(lens[\"ref_word\"].quantile(0.25)), \"med\": float(lens[\"ref_word\"].median()),\n",
        "                      \"q75\": float(lens[\"ref_word\"].quantile(0.75)), \"max\": int(lens[\"ref_word\"].max())},\n",
        "        \"summary\":  {\"count\": int(lens.shape[0]), \"mean\": float(np.round(lens[\"hyp_word\"].mean(),3)),\n",
        "                      \"std\": float(np.round(lens[\"hyp_word\"].std(ddof=0),3)), \"min\": int(lens[\"hyp_word\"].min()),\n",
        "                      \"q25\": float(lens[\"hyp_word\"].quantile(0.25)), \"med\": float(lens[\"hyp_word\"].median()),\n",
        "                      \"q75\": float(lens[\"hyp_word\"].quantile(0.75)), \"max\": int(lens[\"hyp_word\"].max())}\n",
        "    })\n",
        "    print(\"\\n=== LENGTH STATS (tokens) ===\")\n",
        "    print({\n",
        "        \"reference\": {\"count\": int(lens.shape[0]), \"mean\": float(np.round(lens[\"ref_tok\"].mean(),3)),\n",
        "                      \"std\": float(np.round(lens[\"ref_tok\"].std(ddof=0),3)), \"min\": int(lens[\"ref_tok\"].min()),\n",
        "                      \"q25\": float(lens[\"ref_tok\"].quantile(0.25)), \"med\": float(lens[\"ref_tok\"].median()),\n",
        "                      \"q75\": float(lens[\"ref_tok\"].quantile(0.75)), \"max\": int(lens[\"ref_tok\"].max())},\n",
        "        \"summary\":  {\"count\": int(lens.shape[0]), \"mean\": float(np.round(lens[\"hyp_tok\"].mean(),3)),\n",
        "                      \"std\": float(np.round(lens[\"hyp_tok\"].std(ddof=0),3)), \"min\": int(lens[\"hyp_tok\"].min()),\n",
        "                      \"q25\": float(lens[\"hyp_tok\"].quantile(0.25)), \"med\": float(lens[\"hyp_tok\"].median()),\n",
        "                      \"q75\": float(lens[\"hyp_tok\"].quantile(0.75)), \"max\": int(lens[\"hyp_tok\"].max())}\n",
        "    })\n",
        "\n",
        "    return dict(rouge=rouge, ent=ent, ucer=ucer, best_path=best_path, details_path=details_path)\n",
        "\n",
        "# ---- RUN ----\n",
        "BASE_TEST = run_baseline_test(N_ROWS=1000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hard failed (Rejected Candidate summaries count in hybrid results for both checkpoints)"
      ],
      "metadata": {
        "id": "1GjTOSzb7dTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Setup: mount Drive and imports ---\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Paths & target columns ---\n",
        "PATH_A = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/val_test_eval_freeze_reranker/TEST_1k/best_TEST_HYBRID_EA_ON_20250905_182753.csv\"\n",
        "COL_A  = \"fail_flag\"   # count FALSE in this column\n",
        "\n",
        "PATH_B = \"/content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/Hybrid1/test_20250827_090855/best_h1_test_20250827_090855.csv\"\n",
        "COL_B  = \"hard_fail\"   # count FALSE in this column\n",
        "\n",
        "# --- Robust reader (tries multiple engines/encodings) ---\n",
        "def _strict_read_csv(path, required_cols=None):\n",
        "    errors = []\n",
        "    for engine in (\"c\", \"python\"):\n",
        "        for enc in (\"utf-8\", \"utf-8-sig\", \"latin-1\"):\n",
        "            try:\n",
        "                df = pd.read_csv(path, engine=engine, encoding=enc)\n",
        "                if required_cols:\n",
        "                    missing = [c for c in required_cols if c not in df.columns]\n",
        "                    if missing:\n",
        "                        raise ValueError(f\"Missing required columns: {missing}\")\n",
        "                return df\n",
        "            except Exception as e:\n",
        "                errors.append(f\"[{engine}/{enc}] {e}\")\n",
        "    raise RuntimeError(\"Failed to read CSV with multiple strategies:\\n\" + \"\\n\".join(errors))\n",
        "\n",
        "# --- Coerce values to boolean flags and test for FALSE specifically ---\n",
        "FALSE_LITERALS = {\"false\", \"f\", \"0\", \"no\", \"n\"}\n",
        "\n",
        "def is_false(val):\n",
        "    # Treat NaN as not FALSE\n",
        "    if pd.isna(val):\n",
        "        return False\n",
        "    # Native booleans\n",
        "    if isinstance(val, (bool, np.bool_)):\n",
        "        return val is False\n",
        "    # Numbers (0 considered FALSE)\n",
        "    if isinstance(val, (int, float, np.integer, np.floating)):\n",
        "        return (not np.isnan(val)) and (val == 0)\n",
        "    # Strings\n",
        "    s = str(val).strip().lower()\n",
        "    return s in FALSE_LITERALS\n",
        "\n",
        "def count_false(df, col):\n",
        "    if col not in df.columns:\n",
        "        raise KeyError(f\"Column '{col}' not found. Available: {list(df.columns)[:10]} ...\")\n",
        "    mask_false = df[col].apply(is_false)\n",
        "    return int(mask_false.sum()), int(mask_false.shape[0])\n",
        "\n",
        "# --- Read, count, and report ---\n",
        "df_a = _strict_read_csv(PATH_A, required_cols=[COL_A])\n",
        "df_b = _strict_read_csv(PATH_B, required_cols=[COL_B])\n",
        "\n",
        "count_a, total_a = count_false(df_a, COL_A)\n",
        "count_b, total_b = count_false(df_b, COL_B)\n",
        "\n",
        "pct_a = 100.0 * count_a / total_a if total_a else 0.0\n",
        "pct_b = 100.0 * count_b / total_b if total_b else 0.0\n",
        "\n",
        "print(\"File A:\")\n",
        "print(f\"  Path: {PATH_A}\")\n",
        "print(f\"  Column: {COL_A}\")\n",
        "print(f\"  FALSE count: {count_a} / {total_a} ({pct_a:.2f}%)\\n\")\n",
        "\n",
        "print(\"File B:\")\n",
        "print(f\"  Path: {PATH_B}\")\n",
        "print(f\"  Column: {COL_B}\")\n",
        "print(f\"  FALSE count: {count_b} / {total_b} ({pct_b:.2f}%)\")\n",
        "\n",
        "# (Optional) Save a small summary CSV\n",
        "summary = pd.DataFrame([\n",
        "    {\"file\": \"A\", \"path\": PATH_A, \"column\": COL_A, \"false_count\": count_a, \"total\": total_a, \"percent_false\": pct_a},\n",
        "    {\"file\": \"B\", \"path\": PATH_B, \"column\": COL_B, \"false_count\": count_b, \"total\": total_b, \"percent_false\": pct_b},\n",
        "])\n",
        "summary_path = \"/content/drive/MyDrive/ckpt_false_flag_summary.csv\"\n",
        "summary.to_csv(summary_path, index=False)\n",
        "print(f\"\\nSaved summary to: {summary_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HROxeQOU7izf",
        "outputId": "4787493f-6f70-4ff9-8d2e-df40b2575f32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "File A:\n",
            "  Path: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6500/val_test_eval_freeze_reranker/TEST_1k/best_TEST_HYBRID_EA_ON_20250905_182753.csv\n",
            "  Column: fail_flag\n",
            "  FALSE count: 969 / 1000 (96.90%)\n",
            "\n",
            "File B:\n",
            "  Path: /content/drive/MyDrive/student_pgc_bartbase/ckpt_step6000/Hybrid1/test_20250827_090855/best_h1_test_20250827_090855.csv\n",
            "  Column: hard_fail\n",
            "  FALSE count: 973 / 1000 (97.30%)\n",
            "\n",
            "Saved summary to: /content/drive/MyDrive/ckpt_false_flag_summary.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "soTROQsf4lft",
        "vLQRpRsc44KM",
        "nU-LsfS_SLtS",
        "BMleRKn3wMS3",
        "JR_K5N8phs6U",
        "6d8knqlbp9ZZ",
        "WqViuSbh-_Lw",
        "Qm7OWbn6VyBT",
        "WcIYhZQcspjF",
        "Bfm6kju_5irH",
        "GD2DNDKLuEHw",
        "ekSeXuw-uNqO",
        "UYPAObtLtvbe",
        "U-aE8gXSV_6x",
        "Xpn8nqlMXjjg",
        "b67tYAfEhQR9",
        "X9FNsFrI9eNo",
        "viqrML50mL-p",
        "0V5Q48mP0Gx6",
        "t_pSXdfi8gaM",
        "5a_7jGJsZn8q",
        "1miaElcVzI54",
        "-syRkhOKHsGj",
        "ZwOpQWShjFv7",
        "QP3yAGvClZfX",
        "gL78exPW0TdR",
        "pv61z45y5t9r",
        "Mh7Qa2DXJCf6",
        "CazFR00ZSYcF",
        "QAMsiXolV6Lr",
        "hcXt3aGGXaPg",
        "r3DDKW56akZN",
        "FI3lrEVfdf5G",
        "_MEvfqPQKzfB",
        "Kvv6MmOepaNI",
        "S8XGTYNgAD7J",
        "jmheqDIuJYyr",
        "SbyU721Yeb-E",
        "6YUyLyd1zRN-",
        "KLeZp-egTtA2",
        "A5xOUm7kjo5m",
        "tj_jJ18zewwL",
        "YhLZQZZ_cr99",
        "BuYjrGIu3vri",
        "1GjTOSzb7dTz"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}